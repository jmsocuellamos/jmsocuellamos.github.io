[
["index.html", "Modelos Estadísticos Introducción", " Modelos Estadísticos Javier Morales (Universidad Miguel Hernández de Elche) Mª Asunción Martínez (Universidad Miguel Hernández de Elche) 2021-03-02 Introducción Este compendio de unidades sobre modelización estadística trata de mostrar una versión aplicada para el tratamiento de los modelos estadísticos más básicos. El objetivo no es mostrar una versión teórica de estos modelos sino una versión aplicada. Se recomienda a los lectores interesado que complementen la teoría de estos modelos con manuales más específicos. Para poder seguir los contenidos aquí expuestos se recomiendan conocimientos básicos de estadística descriptiva, probabilidad e inferencia estadística, así como las funciones de R necesarias para llevar a cabo dichos análisis. Los modelos contenidos en este manual son: Modelos de regresión lineal simple Modelos de regresión lineal múltiple y polinómicos Modelos ANOVA Modelos ANCOVA Modelos aditivos lineales Modelos lineales generalizados Respuesta Binomial Respuesta Poisson Tablas de contingencia Supervivencia "],
["antes-de-empezar.html", "Antes de empezar Conceptos básicos del diseño experimental Primeros pasos con R y RStudio Informes prediseñados Librerías de R necesarías", " Antes de empezar La importancia de la estadística dentro del campo experimental siempre ha sido muy relevante, ya que para poder extraer conclusiones de un conjunto de datos experimentales se hace necesaria la utilización de procedimientos estadísticos más o menos sofisticados. Con la irrupción de los ordenadores personales y de los programas estadísticos para legos en la materia, así como la explosión tecnológica que estamos viviendo en los últimos años, la importancia de un correcto estudio estadístico de los datos experimentales se hace más necesaria que nunca. Se siguen publicando trabajos de investigación basados en datos experimentales donde el tratamiento estadístico de la información allí recogida puede considerarse como decepcionante. Con esta materia pretendemos guiar al estudiante en un correcto uso y análisis de las técnicas estadísticas más habituales en los diseños experimentales. El tratamiento estadístico de datos experimentales se puede caracterizar en dos grandes áreas: estudios descriptivos y análisis y modelización. Los estudios descriptivos se centran en el procesado de los datos experimentales obtenidos con el objetivo de establecer o reflejar posibles patrones o tendencias en su comportamiento. Se engloban dentro de este ámbito todas la técnicas estadísticas que permiten los resúmenes numéricos y gráficos de la información observada, así como la detección de observaciones anómalas, la transformación y el filtrado de los datos experimentales. Sin embargo, los estudios descriptivos tienen la gran limitación de que sus resultados están circunscritos a los datos observados, y por tanto no se pueden generalizar a la población más general de la que se han obtenido. En el análisis y modelización se pretende generalizar los posibles patrones de comportamiento observados, en la fase descriptiva, mediante la construcción de modelos que nos permiten aproximar el comportamiento de datos experimentales no observados. Evidentemente la construcción de dichos modelos estadísticos no es una tarea rutinaria que debe tomarse a la ligera. La propia naturaleza de los datos observados puede dar una idea de los posibles modelos que se pueden utilizar, pero el modelo final obtenido es el resultados de un proceso iterativo de construcción, verificación y validación que puede resultar costoso en algunas situaciones. La modelización estadística resulta relevante para representar el comportamiento de los datos experimentales de la forma más sencilla posible mediante modelos matemáticos donde se introduce de forma natural la incertidumbre de cualquier diseño experimental. Esta asignatura se centrará en la fase de modelización pero para poder llegar a comprender su naturaleza es necesario introducir primero los conceptos básicos de cualquier estudio estadístico, así como los procedimientos de estadística descriptiva y el estudio de la aleatoriedad en los diseños experimentales. Este tema establece las definiciones básicas de cualquier estudio estadístico sobre diferentes ejemplos e introduce la nomenclatura básica de los modelos estadísticos que estudiaremos más adelante. Usar la estadística no necesariamente es sinónimo de utilizar palabras raras o de hacer cálculos complicados. Significa que deseamos ver la realidad de forma objetiva, a través de datos que reflejen de la mejor manera posible qué es lo que está ocurriendo. Una vez se tienen los datos hay que saber sacarles la información y saberla plasmar de forma clara y convincente. Conceptos básicos del diseño experimental En esta sección presentamos los conceptos básicos que utilizaremos a lo largo de la materia. Se trata únicamente de un resumen muy esquemática, pero nos sirve para sentar las bases de los temas siguientes. Objetivo del diseño experimental El objetivo de cualquier diseño experimental es aquellos que pretendemos estudiar en función del tipo de información que se ha recogido y del tipo de premisas establecidas antes de la recolección de los datos. Además es importante establecer el número de repeticiones del experimento que vamos a realizar, ya que eso condicionará el análisis de dichos datos. Si nuestro diseño experimental es muy complejo puede ocurrir que plantemos más de un objetivo. Ejemplo 1 (Degradación compuesto orgánico). Se va a realizar un experimento para conocer el tiempo que tarda en degradarse un compuesto orgánico. En este caso nuestro objetivo es el tiempo hasta la degradación. Si el experimneto considera diferentes tipos de compuestos nuestro objetivo podría ser comparar el tiempo de degradación en función del tipo de compuesto. Población y muestra Se define la población como el conjunto de sujetos u objetos que son de interés para el objetivo u objetivos planteados en nuestro diseño experimental. EL problema principal es que la población de sujetos u objetos suele ser demasiado grande para poder analizarla de forma completa, y por tanto debemos acudir a un subconjunto de dicha población para llevar a cabo nuestro diseño experimental. Se define la muestra como el subconjunto de la población a la que accedemos para obtener la información necesaria de cara a responder de la forma más precisa posible al objetivo u objetivos planteados. Medidas y escalas de medida Una medida es un número o atributo que se puede calcular para cada uno de los miembros de la población que está relacionado directamente con el objetivo de interés de la investigación. El conjunto de medidas obtenidas para cada uno de los elementos muestrales se denominan datos muestrales. EL conjunto de medidas que se pueden observar y registrar para un conjunto de sujetos u objetos bajo investigación se denominan variables. Por tanto, una variable es el conjunto de valores que puede tomar cierta característica de la población sobre la que se realiza el estudio estadístico. Se distinguen dos tipos que pasamos a describir a continuación. Variables cualitativas Son el tipo de variables que como su nombre lo indica expresan distintas cualidades, características o modalidad. Cada modalidad que se presenta se denomina atributo o categoría, y la medición consiste en una clasificación de dichos atributos. Las variables cualitativas pueden ser dicotómicas cuando sólo pueden tomar dos valores posibles, como sí y no, hombre y mujer o ser politómicas cuando pueden adquirir tres o más valores. Dentro de ellas podemos distinguir: Variable cualitativa ordinal: La variable puede tomar distintos valores ordenados siguiendo una escala establecida, aunque no es necesario que el intervalo entre mediciones sea uniforme, por ejemplo: leve, moderado, fuerte. Variable cualitativa nominal: En esta variable los valores no pueden ser sometidos a un criterio de orden, como por ejemplo los colores. Variables cuantitativas Son las variables que toman como argumento cantidades numéricas. Las variables cuantitativas además pueden ser: Variable discreta: Es la variable que presenta separaciones o interrupciones en la escala de valores que puede tomar. Estas separaciones o interrupciones indican la ausencia de valores entre los distintos valores específicos que la variable pueda asumir. Ejemplo: El número de hijos (1, 2, 3, 4, 5). En muchas ocasiones una variable cualitativa ordinal puede ser interpretada como una variable discreta asociando a las categorías de la variable valores numéricos respetando el orden o escala establecida. Por ejemplo a la escala leve, moderado y fuerte le podríamos asociar la escala 1, 2 y 3 para mantener el orden. Variable continua: Es la variable que puede adquirir cualquier valor dentro de un intervalo especificado de valores. Por ejemplo el peso (2,3 kg, 2,4 kg, 2,5 kg,…), la altura (1,64 m, 1,65 m, 1,66 m,…), o el salario. Solamente se está limitado por la precisión del aparato medidor, en teoría permiten que existan valores infinitos entre dos valores observados. De forma habitual, la estructura de cualquier banco de datos (asociado a un diseño experimental) tiene una estructura matricial donde en las filas se colocan los sujetos bajo estudio y en las columnas se sitúan las variables medidas para cada uno de ellos. Asociada a cada variable de nuestro banco de datos se puede establecer lo que conocemos como parámetro o parámetros de interés de la variable. Ejemplo 2 (Variable de interés). Para el diseño experimental del estudio de la degradación del compuesto orgánico presentado en el ejemplo ??, la variable de interés es de tipo continuo y viene dada por el tiempo de degradación asociado a cada repetición del experimento. Sin embargo, a la hora de extraer conclusiones no podemos presentar todo el conjunto de datos sino que recurrimos a un resumen de dichos datos. Parámetros poblacionales y estadísticos Asociado a cada variable se puede establecer lo que conocemos como parámetro o parámetros de interés de la variable. En el ejemplo anterior el parámetro de interés es el tiempo medio de degradación. Dado que generalmente no es posible examinar toda la población y debemos recurrir a una muestra de dicha población, es imposible conocer el verdadero valor del parámetro asociado con dicha variable. Para sortear este problema definimos el estadístico como una realización del parámetro para los datos muestrales observados. Por tanto el valor del estadístico (denominado estimación) varia entre dos muestras de las misma población. Cuanto mayor es la muestra más se parecerá el valor del estadístico al del parámetro. En ocasiones ocurrirá que el número de parámetros asociado con una variable no es único, ya que se pueden establecer varios parámetros para estudiar el comportamiento de una variable. En el caso de variables de tipo cuantitativo siempre existen dos parámetros de interés: la media y la desviación típica. El primero nos indica como se sitúan los datos mientras que el segundo nos indica como se reparten los datos muestrales alrededor de la media. Ejemplo 3 (Parámetro de interés). Para el diseño experimental del estudio de la degradación del compuesto orgánico presentado en el ejemplo ??, el parámetro poblacional de interés es el tiempo medio de degradación, mientras que el estadístico es la media del tiempo de degradación observado para los sujetos de la muestra. Distinguimos entonces entre media poblacional (parámetro) y media muestral (estadístico). Primeros pasos con R y RStudio Para poder utilizar el código expuesto en estos materiales es necesario la instalación del programa R, del programa RStudio y de diferentes librerías de R. En los puntos siguientes se describe brevemente cada uno de estos puntos. Se recomienda además la consulta de los materiales electrónicos siguientes para complmentar la formación. Childs, D. Z. (2017). APS 135: Introduction to Exploratory Data Analysis with R. Versión electrónica. Grosser, M. 2017. Tidyverse Cookbook. Versión electrónica incompleta. Wickham, H. 2015. Advanced R. CRC Press. Versión electrónica resumida. Wickham, H. 2010. ggplot2. Third Edition. Springer. Recursos electrónicos. Wickham, H. &amp; Grolemund, G. 2016. R for Data Science. O’Reilly. Versión electrónica resumida. Instalación y puesta en marcha Instalación de R y RStudio. Leer el capítulo de introducción del libro de Childs (2017) e instalar ambos programas descargándolos de sus correspondientes webs: Para instalar el programa R: https://cran.r-project.org/ Para instalar RStudio: https://rstudio.com/ Primeros pasos en R y RStudio. Leer los capítulos 1, 2, y 3 de Childs (2017), el capítulo 4 de Wickham (2015), y los capítulos 4 y 6 de Wickham (2016) para un desarrollo más amplio. Realizar los ejercicios que van a apareciendo a lo largo de los capítulos. Estructuras de datos. Leer los capítulos 4, 5, 6 y 9 de Childs (2017) para una breve introducción y los capítulos 2 y 3 de Wickham (2015) para completar la información. Realiza los ejercicios que van apareciendo. Instalación y uso de librerías en RStudio. Leer el capítulo 8 de Childs (2017) e instala las librerías tidyverse, stringr, forcats, lubridate, magrittr, broom y datasets mediante la consola de RStudio. Carga las librerías con el comando library para comprobar que se han instalando de forma correcta. Creación de proyectos y entornos de trabajo. Leer el capítulo 8 de Wickham (2016). Crea un proyecto para esta unidad, selecciona el directorio de trabajo donde se encuentra situado el proyecto, y guarda el entorno de trabajo. Informes prediseñados En los últimos tiempos se ha puesto de modo la creación de informes directos apartir del código utilizado en Rstudio mediante la creación de documentos específicos. Su puede consulyar una guía sencilla de uso en enlace. Un desaroollo más completo se puede ver en este enlace. También se puede consultar este vídeo. Librerías de R necesarías Para poder utilizar el código expuesto en estos materiales es necesario la instalación de diferentes librerías de R. A continuación se encuentra el código para cargar dichas librerías. Para algún análisis especifíco se utilizará alguna librería accesoria que será cargada con el comando require(). library(tidyverse) library(tidymodels) library(stringr) library(forcats) library(lubridate) library(magrittr) library(broom) library(datasets) library(lmtest) library(MASS) library(kableExtra) library(mosaic) library(latex2exp) library(pubh) library(moonBook) library(sjlabelled) library(sjPlot) library(reshape2) library(olsrr) library(ggfortify) library(mgcv) library(modelr) library(alr4) library(equatiomatic) Configuramos además el tema de los gráficos para que tengan un aspecto más limpio y más fácil de exportar en formato pdf o word. Para ellos utilizamos la función theme_set(). theme_set(theme_sjplot2()) "],
["modelstats.html", "Unidad 1 Modelos estadísticos 1.1 Componentes del modelo 1.2 Tipos de modelos 1.3 Fases en la construcción de un modelo", " Unidad 1 Modelos estadísticos De forma habitual, cuando el investigador (o investigadores) se plantea un diseño experimental y comienza con la recogida de datos es porque persigue el estudio de o verificación de un objetivo planteado sobre la población bajo estudio. Estos objetivos se suelen establecer en base a teorías o hipótesis que se desean verificar sobre le funcionamiento de la población bajo ciertas condiciones experimentales. Por ejemplo: Teorías que establezcan la posible relación entre dos características de la población. Teorías que plateen la idea de comportamientos distintas para una característica de la población en función de una variable que clasifica a los sujetos bajo estudio en diferentes grupos. Es entonces cuando la modelización estadística interviene y el analista busca el mejor modelo que ajusta los datos disponibles y proporciona predicciones fiables. El objetivo de la modelización estadística es el planteamiento de una expresión matemática que representa el comportamiento general de la población bajo estudio, teniendo en cuenta el diseño experimental establecido y el objetivo u objetivos que se desean verificar 1.1 Componentes del modelo Un primer paso en la modelización estadística es el planteamiento de una expresión matemática que represente el comportamiento general de la población bajo estudio teniendo en cuenta el diseño experimental establecido y el objetivo u objetivos que se desean verificar. Esto es lo que se conoce como componente sistemática del modelo y se basa únicamente en la parte controlada del diseño experimental. Por ejemplo, si nos planteamos como objetivo conocer la suma de dos números \\(a\\) y \\(b\\), la función matemática (sistemática) que permite expresar la suma de forma única es \\(a+b\\). Esta componente sistemática es una función determinista, pues siempre proporciona el mismo resultado si los valores de entrada son iguales. Al proponer la parte sistemática (o determinista) de un modelo será siempre necesario concretar la variable que se asocia al objetivo o hipótesis planteada sobre la población (representada por \\(Y\\)) y la variable o variables \\((X_1, X_2,…)\\) relacionadas o supuestamente relacionadas con ella a través de la función matemática especificada. Supongamos un diseño experimental en el que tenemos una variable \\(Y\\) que está ligada directamente con el objetivo de la investigación, y un conjunto de variables \\(X_1, X_2,…,X_k\\), que se supone que pueden influir en el comportamiento de \\(Y\\). Habitualmente a \\(Y\\) se la denomina variable respuesta o variable dependiente y a las \\(X’s\\) variables predictoras, variables explicativas e incluso covariables cuando se trata de variables de tipo numérico continuo. Cuando las variables \\(X\\) son de tipo categórico se suelen denominar factores explicativos o de clasificación. A las variables \\(X\\) se las suele denominar también variables independientes, asumiendo independencia entre ellas, aunque esta acepción puede estar algo alejada de la realidad como discutiremos más adelante; en adelante no utilizaremos esta denominación y optaremos por cualquiera de las anteriormente presentadas. En la situación más sencilla donde la respuesta puede venir influenciada de forma directa por las posibles predictoras, la respuesta media (\\(\\hat{Y}\\)) se puede modelizar a través de una función \\(f\\) que describe la componente sistemática del modelo: \\[\\begin{equation} \\hat{Y} = f(X_1,X_2,...,X_k) \\tag{1.1} \\end{equation}\\] Si nuestro modelo es adecuado, esta función debe reflejar el comportamiento medio esperado de la variable respuesta. Dado que sujetos distintos con los mismos valores de las \\(X’s\\) producirán generalmente valores distintos en la respuesta, se hace necesaria la introducción de una componente variable en el modelo. Esta componente se denomina componente aleatoria y está relacionada directamente con la variabilidad de los sujetos en la respuesta para una misma combinación de valores de las variables predictoras. La denotaremos habitualmente por: \\[\\begin{equation} \\epsilon \\tag{1.2} \\end{equation}\\] que es una variable aleatoria con distribución de probabilidad \\(F\\). Asumiendo que (1.1) y (1.2) tienen un efecto aditivo sobre la respuesta, nuestro modelo base de partida vendrá dado por la expresión: \\[\\begin{equation} Y = \\hat{Y} + \\epsilon = f(X_1,X_2,...,X_k) + \\epsilon \\tag{1.3} \\end{equation}\\] En función del tipo de variable respuesta, las predictoras, de la relación que se pueden establecer entre ellas a través de \\(f\\), y del establecimiento de las estructuras aleatorias \\(F\\) para los errores tendremos diferentes tipos de modelos. A lo largo de esta materia veremos las diferentes posibilidades de modelización. 1.2 Tipos de modelos En función del tipo de variable respuesta, las predictoras, de la relación que se pueden establecer entre ellas a través de \\(f\\), y del establecimiento de las estructuras aleatorias \\(F\\) para los errores tendremos diferentes tipos de modelos. A lo largo de esta materia veremos las diferentes posibilidades de modelización. A lo largo de las unidades siguientes iremos estudiando las características de los diferentes modelos, pero estos se pueden agrupar en dos grandes apartados: Modelos Lineales (LM), que engloban los modelos de regresión, los modelos ANOVA y los modelos ANCOVA. Modelos Lineales Generalizados (GLM), que engloba los modelos de respuesta binomial (modelos de regresión logística), modelos de respuesta poisson, modelos para tablas de contingencia (modelos log-lineales), y modelos de supervivencia. Introduciremos además los modelos de suavizado y una breve introducción a los modelos de efectos aleatorios, que pueden ser utilizados en conjunción con los LM y los GLM. 1.3 Fases en la construcción de un modelo El proceso de modelización y análisis estadístico de un banco de datos se puede estructurar según las siguientes pautas de actuación: Contextualización del problema. Definición de objetivos y variables. Diseño del experimento y recogida de información. Registro y procesado previo de la información disponible. Inspección gráfica e identificación de tendencias. Consideración de hipótesis distribucionales y relacionales. Propuesta de modelización. Ajuste del modelo. Comparación y selección del mejor modelo. Diagnóstico y validación del modelo ajustado. Valoración de la capacidad predictiva del modelo y predicción. Interpretación y conclusiones. Si la revisión/validación del modelo nos lleva a descartarlo (punto 7), será preciso una nueva propuesta, de modo que entraremos en un bucle entre los puntos (5)-(7) que culminará cuando quedemos satisfechos con el diagnóstico y la validación del modelo. A la hora de representar gráficamente la información de cada banco de datos tendremos en cuenta esta serie de principios básicos: La información asociada con la variable respuesta que identifica el objetivo del estudio debe situarse siempre en el eje Y o eje de ordenadas. El tipo de las variables que pueden influir en nuestra variable objetivo condiciona el tipo de gráfico. Así si estas son de tipo numérico debemos realizar un gráfico de dispersión, situando cada una de las variables predictoras \\(X\\) en el eje de abcisas. Si las predictoras son de tipo categórico deberemos realizar un gráfico de cajas, visualizando las distintas categorías en el eje X (si bien siempre podremos invertir los ejes para mostrar las cajas en sentido horizontal y no vertical). Si combinamos variables de tipo numérico y categórico debemos realizar gráficos múltiples de dispersión donde mostremos la relación \\(Y\\) versus \\(X\\) para las variables numéricas en cada uno de los niveles de las variables \\(X\\) categóricas. "],
["rls.html", "Unidad 2 Regresión Lineal Simple (RLS) 2.1 Bancos de datos 2.2 El modelo RLS 2.3 Estimación del modelo 2.4 Bondad del Ajuste 2.5 Diagnóstico del Modelo 2.6 Predicción del modelo", " Unidad 2 Regresión Lineal Simple (RLS) Nos preocupamos en este tema del Modelo de Regresión Lineal Simple (RLS), que podemos catalogar como el modelo lineal más sencillo, a través del cual pretendemos explicar (predecir) una variable respuesta continua \\(Y\\) a partir de una variable predictora también continua \\(X\\). Tal modelo vendrá justificado por unos buenos resultados previos en el análisis de correlación (lineal) entre las dos variables en cuestión. En el experimento o estudio del que obtenemos los datos, los valores de \\(Y\\) se han observado y los de \\(X\\), bien se han observado, bien se han prefijado por parte del investigador. En cualquier caso, asumimos que la aleatoriedad (incertidumbre) está contenida sólo en la variable \\(Y\\), mientras que la \\(X\\) carece de aleatoriedad y simplemente informa de lo que ocurre en los valores observados. La variable explicativa puede ser, tanto una causa de la respuesta, como un mero testigo que informa sobre cómo varía la respuesta. De ahora en adelante denotamos por \\((x_1, x_2,...,x_n)\\) e \\((y_1, y_2,...,y_n)\\) a los valores observados de las variables \\(X\\) e \\(Y\\) en un experimento dado. 2.1 Bancos de datos Presentamos a continuación los bancos de datos con los que trabajamos a lo largo de esta unidad. Ejemplo 1. Datos de Corrosión. Treinta aleaciones del tipo 90/10 Cu-Ni, cada una con un contenido específico de hierro son estudiadas bajo un proceso de corrosión. Tras un período de 60 días se obtiene la pérdida de peso (en miligramos al cuadrado por decímetro y día) de cada una de las aleaciones debido al proceso de corrosión. El objetivo es estudiar el nivel de corrosión en función del contenido de hierro. A continuación se presenta el banco de datos y se realiza la primera inspección gráfica. hierro &lt;- c(0.01, 0.48, 0.71, 0.95, 1.19, 0.01, 0.48, 1.44, 0.71, 1.96, 0.01, 1.44, 1.96) peso &lt;- c(127.6, 124, 110.8, 103.9, 101.5, 130.1, 122, 92.3, 113.1, 83.7, 128, 91.4, 86.2) corrosion &lt;- data.frame(hierro,peso) ggplot(corrosion, aes(x = hierro, y = peso)) + geom_point() + labs(x = &quot;Contenido en hierro&quot;, y = &quot;Pérdida de peso&quot;) Figura 2.1: Gráfico de dispersión de pérdida de peso vs contenido en hierro. En la figura 2.1 se observa cómo al ir aumentando el contenido en hierro de la aleación disminuye linealmente la pérdida de peso. El modelo estadístico que propongamos deberá ser capaz de explicar dicho comportamiento. Ejemplo 2. Datos de Papel Queremos estudiar la relación existente entre la concentración de madera contenida en la pulpa a partir de la que se elabora papel (madera), y la resistencia (tension, en términos de tensión que soporta) del papel resultante. El objetivo del análisis es describir la tendencia observada. A continuación se presenta el banco de datos y se realiza la primera inspección gráfica. madera &lt;- c(1, 1.5, 2, 3, 4, 4.5, 5, 5.5, 6, 6.5, 7, 8, 9, 10, 11, 12, 13, 14, 15) tension &lt;- c(6.3, 11.1, 20.0, 24, 26.1, 30, 33.8, 34, 38.1, 39.9, 42, 46.1, 53.1, 52, 52.5, 48, 42.8, 27.8, 21.9) papel &lt;- data.frame(madera, tension) ggplot(papel, aes(x = madera, y = tension)) + geom_point() + labs(x = &quot;Concentración de madera&quot;, y = &quot;Resistencia del papel&quot;) Figura 2.2: Gráfico de dispersión de resistencia del papel vs concentración de madera. En la figura 2.2 podemos ver cómo la resistencia del papel crece al aumentar la concentración de madera hasta llegar a valores de 9 y disminuye a partir de ese valor. En este caso la relación apreciada es de tipo parabólico (descrita por una parábola). Este hecho se debe tener en cuenta en la propuesta de un modelo preliminar. Ejemplo 3. Datos de Viscosidad. Se ha realizado un experimento para tratar de conocer la viscosidad de cierto compuesto en función de la cantidad de un tipo der aceite que se usa en su fabricación. Se asume una relación de tipo lineal entre la viscosidad y la cantidad de aceite utilizada. aceite &lt;- c(0, 12, 24, 36, 48, 60, 0, 12, 24, 36, 48, 60, 0, 12, 24, 36, 48, 60, 12, 24, 36, 48, 60) viscosidad &lt;- c(26, 38, 50, 76, 108, 157, 17, 26, 37, 53, 83, 124, 13, 20, 27, 37, 57, 87, 15, 22, 27, 41, 63) aceites&lt;-data.frame(aceite, viscosidad) ggplot(aceites, aes(x = aceite, y = viscosidad)) + geom_point() + labs(x = &quot;Cantidad de aceite&quot;, y = &quot;Viscosidad&quot;) Figura 2.3: Gráfico de dispersión de viscosidad vs cantidad de aceite. 2.2 El modelo RLS El modelo de Regresión lineal Simple (RLS) de la variable respuesta (\\(Y\\)) sobre la variable predictora (\\(X\\)) se formula prediciendo la respuesta media para un valor observado de \\(X = x\\), con una recta de regresión: \\[\\begin{equation} E(y\\mid x=x) = \\beta_{0} + \\beta_{1}x. \\tag{2.1} \\end{equation}\\] Es de esperar cierta desviación ‘aleatoria’ entre la respuesta observada y la respuesta media. Dicha desviación es denominada error aleatorio y denotada habitualmente por \\(\\epsilon\\). Así, el modelo completo de regresión simple se formula según: \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\epsilon. \\tag{2.2} \\end{equation}\\] Los coeficientes de la regresión, esto es, los parámetros que hemos de estimar para ajustar el modelo RLS son: \\(\\beta_{0}\\).- la interceptación de la recta, esto es, la altura de la recta cuando \\(x = 0\\). \\(\\beta_{1}\\).- la pendiente de la recta, que refleja cuánto varía la respuesta media \\(E\\)(y) cuando pasamos de observar x = \\(x\\) a x = \\(x\\) + 1. Dada una muestra de valores observados \\(\\{{(x_{i},y_{i})}_{i=1}^{n}\\}\\), el modelo propuesto implica que todas las observaciones responden a la ecuación (2.2), de forma que: \\[\\begin{equation} y_{i} = \\beta_{0} + \\beta_{1}x_{i}+\\epsilon_{i}, \\ \\ \\ \\ i=1,\\ldots,n, \\tag{2.3} \\end{equation}\\] donde \\(\\epsilon_{i}\\) son errores aleatorios, que además se consideran incorrelados, con media cero y varianza constante \\(\\sigma^{2}\\). Estas características constituyen las hipótesis básicas del modelo RLS, que formulamos con más detalle a continuación sobre los errores aleatorios \\(\\epsilon_{i}\\): Incorrelación: \\(Corr(\\epsilon_{i},\\epsilon_{j}) = 0\\). Significa que las observaciones de la respuesta y, \\(y_{1},y_{2},\\ldots,y_{n}\\) están incorreladas entre sí, esto es, los valores de unas no afectan a los de otras. Media cero: \\(E(\\epsilon_{i}) = 0\\). Lo que implica que la respuesta esperada según el modelo RLS depende linealmente de los coeficientes de regresión \\(\\beta_{0}\\) y \\(\\beta_{1}\\). Varianza constante: \\(Var(\\epsilon_{i} = \\sigma^{2})\\). Lo que significa que las observaciones \\(\\{y_{i},i=1,\\ldots,n\\}\\) provienen de una misma población cuya variabilidad respecto de su media, \\(\\{\\beta_{0} + \\beta_{1}x_{i}, i=1,\\ldots,n\\}\\), viene dada por \\(\\sigma^{2}\\). 2.3 Estimación del modelo Estimar la recta de regresión consiste en estimar los coeficientes de la regresión \\(\\beta_{0}\\) y \\(\\beta_{1}\\) para obtener la recta: \\[\\begin{equation} \\hat{Y} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X, \\tag{2.4} \\end{equation}\\] donde \\(\\hat{Y}\\) denota el valor de Y predicho por la recta para el valor observado de \\(X = x\\). Disponemos de dos criterios básicos de estimación, que proporcionan la misma solución. Utilizar uno u otro depende de nuestros intereses estadísticos. Si tan sólo queremos determinar la recta, basta con considerar el criterio de Mínimos Cuadrados. Si además pretendemos utilizarla con fines inferenciales o predictivos, hablaremos de que nuestra solución es la máximo-verosímil, pero a su vez habremos de ser más exigentes con las hipótesis del modelo, como veremos a continuación. 2.3.1 Estimación Mínimos Cuadrados El criterio de mínimos cuadrados o minimización del error cuadrático medio, consiste en minimizar las distancias entre los puntos observados y los predichos por la recta de ajuste. El error cuadrático medio de la recta se define como: \\[\\begin{equation} S(\\beta) = \\sum_{i=1}^{n} (y_{i}-\\hat{y_{i}}(\\beta))^2 = \\sum_{i=1}^{n} [y_{i}-(\\beta_{0}+\\beta_{1}x_{i})]^2 \\tag{2.5} \\end{equation}\\] La solución de mínimos cuadrados \\(\\hat{\\beta} = (\\hat{\\beta_{0}},\\hat{\\beta_{1}})\\) se obtiene minimizando \\(S(\\beta)\\). El mínimo se consigue derivando \\(S(\\beta)\\) respecto de \\(\\beta_{0}\\) y \\(\\beta_{1}\\) e igualando a cero: \\[ \\frac{\\partial S(\\beta)}{\\partial \\beta_{0}} \\mid_{\\hat{\\beta_{0}},\\hat{\\beta_{1}}} = -2 \\sum_{i=1}^{n} (y_{i} - \\hat{\\beta_{0}}-\\hat{\\beta_{1}}x_{i}) = 0 \\] \\[ \\frac{\\partial S(\\beta)}{\\partial \\beta_{1}} \\mid_{\\hat{\\beta_{0}},\\hat{\\beta_{1}}} = -2 \\sum_{i=1}^{n} (y_{i} - \\hat{\\beta_{0}}-\\hat{\\beta_{1}}x_{i})x_{i} = 0. \\] De ahí se obtienen las ecuaciones normales: \\[ n\\hat{\\beta_{0}}+\\hat{\\beta_{1}}\\sum_{i=1}^{n} x_{i}= \\sum_{i=1}^{n} y_{i} \\] \\[ \\hat{\\beta_{0}}\\sum_{i=1}^{n} x_{i} + \\hat{\\beta_{1}} \\sum_{i=1}^{n} x_{i}^{2} = \\sum_{i=1}^{n} y_{i}x_{i} \\] de donde las estimaciones para \\(\\beta_{0}\\) y \\(\\beta_{1}\\) resultan: \\[ \\hat{\\beta_{0}}=\\bar{y}-\\hat{\\beta_{1}}\\bar{x} \\] \\[ \\hat{\\beta_{1}}=\\frac{S_{xy}}{S_{xx}}, \\] con: \\[ \\bar{y} = \\frac{\\sum_{i=1}^{n} y_{i}}{n} \\] \\[ \\bar{x} = \\frac{\\sum_{i=1}^{n} x_{i}}{n} \\] \\[ S_{xx} = \\sum_{i=1}^{n} (x_{i}-\\bar{x})^2 \\] \\[ S_{xy} = \\sum_{i=1}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y}). \\] 2.3.2 Estimación Máximo Verosímil Habitualmente el objetivo de un análisis de regresión no consiste únicamente en estimar la recta, sino en inferir con ella, esto es, asociar un error a las estimaciones obtenidas, contrastar un determinado valor de los parámetros, y/o incluso predecir la respuesta, junto con una banda de confianza, para un \\(X = x\\) dado. En ese caso, precisamos de distribuciones de probabilidad para controlar la incertidumbre y el error. Añadimos pues, una hipótesis más sobre la distribución de la variable respuesta, o lo que es lo mismo, sobre el error aleatorio \\(\\epsilon\\). Dicha hipótesis es la de normalidad de los errores. Así, el total de hipótesis básicas del modelo de regresión con fines inferenciales, viene resumido en la siguiente expresión: \\[\\begin{equation} \\epsilon_{i} \\overset{iid}{\\sim} N(0,\\sigma^{2}), \\qquad i=1,\\ldots,n. \\tag{2.6} \\end{equation}\\] esto es, hablamos de errores aleatorios independientes e idénticamente distribuidos (iid) según una distribución Normal con media cero y varianza \\(\\sigma^{2}\\), lo que implica directamente que la distribución para la variable respuesta será: \\[\\begin{equation} y_{i} \\overset{iid}{\\sim} N(\\beta_{0} + \\beta_{1}x_{i}, \\sigma^{2}), \\qquad i=1,\\ldots,n. \\tag{2.7} \\end{equation}\\] Desde este momento, los datos proporcionan información sobre los parámetros del modelo, \\(\\beta = (\\beta_{0},\\beta_{1})\\), a través de la verosimilitud conjunta: \\[\\begin{equation} L(\\beta;y)=exp\\left\\{-\\frac{\\sum_{i=1}^n (y_i-\\beta_0-\\beta_1 x_i)^2}{2 \\sigma^2}\\right\\}. \\tag{2.8} \\end{equation}\\] Por tanto, obtener la solución más factible a la vista de los datos observados \\(\\{(x_i,y_i), i=1,\\ldots, n\\}\\) equivale a obtener la solución máximo-verosímil, esto es, la que maximiza la verosimilitud (2.8). Maximizar la verosimilitud es equivalente a maximizar la log-verosimilitud \\(l(\\beta,y)\\), que tiene una expresión más sencilla sin exponenciales. La solución máximo-verosímil se obtiene derivando e igualando a cero \\(l(\\beta,y)\\), lo que da lugar, de nuevo, a las ecuaciones normales. Así pues, la solución máximo-verosímil coincide con la de mínimos cuadrados. 2.3.3 Estimación con R Para obtener el ajuste máximo verosímil con R utilizamos la función lm() que permite el ajuste de cualquier modelo lineal. Su expresión más básica viene dada por: \\[model &lt;- lm(y \\sim x, data = ´´data´´)\\] donde \\(y\\) es la respuesta y \\(x\\) es la predictora. Para obtener las estimaciones del modelo podemos hacer uso de diferentes funciones: tidy(model) de la librería tidymodels que nos proporciona el modelo estimado, los errores en la estimación y la soluciones del contraste sobre cada parámetro del modelo que veremos en el apartado de inferencia sobre los coeficientes del modelo. glm_coef(model) de la librería pubh que nos proporciona las estimaciones del modelo, los intervalos de confianza al 95% de cada parámetro, y el p-valor asociado a los contrastes sobre cada uno de los parámetros del modelo. summary(model) que proporciona un resumen completo del modelo (inferencia sobre los parámetros del modelo y bondad de ajuste). Por el momento utilizamos las dos primeras para mostrar los resultados del ajuste. Además utilizaremos la función plot_model para representar gráficamente el modelo obtenido como alternativa a la función ggplot() que hemos utilizado en la figura 2.4. 2.3.4 Ejemplos Para los datos de Corrosión se propone un modelo de regresión lineal simple para estudiar la relación entre la pérdida de peso debida a la corrosión y el contenido de hierro de la forma siguiente: \\[ \\text{peso} = \\beta_{0} + \\beta_{1}*\\text{hierro} + \\epsilon \\] # Ajuste del modelo fit &lt;- lm(peso ~ hierro, data = corrosion) # Solución con tidy tidy(fit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 130. 1.40 92.5 2.93e-17 ## 2 hierro -24.0 1.28 -18.8 1.06e- 9 # Solución con glm_coef glm_coef(fit) ## Coefficient Pr(&gt;|t|) ## (Intercept) 129.79 (127.47, 132.11) &lt; 0.001 ## hierro -24.02 (-26.03, -22.01) &lt; 0.001 de forma que el modelo estimado viene dado por: \\[ \\widehat{\\text{peso}} = 129.79 - 24.02*\\text{hierro} \\] esto es, un aumento de una unidad del contenido de hierro reporta una pérdida de peso de 24.02 unidades. Representamos gráficamente la recta del ajuste obtenida: # Gráfico del ajuste plot_model(fit,&quot;pred&quot;, terms = ~hierro, ci.lvl = NA, show.data = TRUE, axis.title = c(&quot;Contenido en hierro&quot;, &quot;Peso&quot;), title = &quot; &quot;) Figura 2.4: Ajuste de mínimos cuadrados para los datos de corrosión. 2.3.5 Propiedades de la recta de regresión. Las propiedades más relevantes y básicas del ajuste de la recta de regresión son las siguientes: La estimación de la respuesta para un valor de x=\\(x\\) concreto según el modelo de regresión lineal simple se obtiene de la recta de regresión ajustada: \\[ \\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1} x. \\] La suma de los residuos de una recta de regresión con término de interceptación \\(\\beta_0\\) es cero, \\[ e_i=y_i-\\hat{y} \\rightsquigarrow \\sum_i e_i=0. \\] La media de los valores observados \\(y_i\\) coincide con la media de los valores predichos \\(\\hat{y_i}\\), \\[ \\frac{1}{n}\\,\\sum_i y_i=\\frac{1}{n} \\,\\sum_i \\hat{y}_i. \\] La recta de regresión pasa por el centroide de medias \\((\\bar{x},\\bar{y})\\). La suma de los residuos ponderados por el valor correspondiente de la variable predictora \\(x\\) es cero, \\[ \\sum_i x_i e_i=0. \\] La suma de los residuos ponderados por el valor ajustado por la recta \\(\\hat{y}\\) es cero, \\[ \\sum_i \\hat{y}_i e_i=0. \\] 2.3.6 Estimación varianza del modelo. La varianza \\(\\sigma^2\\) de los errores es una medida de la variabilidad (heterogeneidad) entre los individuos respecto a la media cuando el modelo RLS describe adecuadamente la tendencia entre las variables \\(y\\) y \\(x\\), o lo que es lo mismo, de la dispersión de las observaciones respecto de la recta de regresión. Así pues, da una medida de bondad de ajuste del modelo de regresión a los datos observados. Cuando el modelo de regresión ajustado es bueno para nuestros datos, es posible conseguir una estimación de la varianza \\(\\sigma^2\\) a partir de la suma de cuadrados residual \\(SSE\\), también llamada suma de cuadrados debida al error: \\[ SSE=\\sum_i (y_i-\\hat{y}_i)^2=S_{yy}-\\hat{\\beta}_1 S_{xy}. \\] \\(SSE\\) da una medida de la desviación entre las observaciones \\(y_i\\) y las estimaciones que proporciona la recta de regresión, \\(\\hat{y}_i\\). Puesto que en el modelo de regresión lineal simple se estiman \\(2\\) parámetros, los grados de libertad asociados a \\(SSE\\) son \\(n-2\\). Se define pues el cuadrado medio residual, \\(MSE\\), como un estimador de \\(\\sigma^2\\), que además resulta ser insesgado (esto es, su valor esperado es \\(\\sigma^2\\)): \\[ s^2=MSE=\\frac{SSE}{n-2}. \\] El error estándar residual viene dado por \\(s=\\sqrt{MSE}\\). 2.3.7 Inferencia sobre los coeficientes del modelo Los estimadores de mínimos cuadrados \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son insesgados y de mínima varianza de entre todos los estimadores insesgados. El hecho de especificar una distribución normal sobre los errores para la estimación máximo-verosímil, permite derivar de forma directa la distribución de dichos estimadores, que resulta también normal: \\[ \\hat{\\beta}_0 \\sim N\\left( \\beta_0, \\frac{\\sum_{i=1}^n x_{i}^2}{nS_{xx}} \\sigma^2 \\right) \\] \\[ \\hat{\\beta}_1 \\sim N\\left( \\beta_1, \\frac{\\sigma^2}{S_{xx}}\\right), \\] Cuando el modelo de regresión es adecuado, podemos estimar las varianzas de dichas distribuciones sustituyendo \\(\\sigma^2\\) por \\(s^2\\) . De ahí podemos construir los estadísticos \\(t\\) para inferir sobre los parámetros: \\[ t_0 = \\frac{\\hat{\\beta}_0-\\beta_0}{s \\ \\sqrt{\\sum_i x_i^2/n S_{xx}}} \\] \\[ t_1 = \\frac{\\hat{\\beta}_1-\\beta_1}{s/\\sqrt{S_{xx}}} \\] Ambos estadísticos se distribuyen según una distribución \\(t\\)-Student con \\(n-2\\) grados de libertad, que nos permite inferir (estimar y resolver contrastes de hipótesis) sobre los coeficientes del modelo, y en particular contestar a preguntas sobre la relación entre las variables respuesta y explicativa. 2.3.7.1 Procedimientos de estimación Las estimaciones puntuales de \\(\\beta_0\\) y \\(\\beta_1\\) las obtenemos directamente de las ecuaciones normales. Los intervalos de confianza al nivel de confianza \\((1-\\alpha)100\\%\\) para \\(\\beta_0\\) y \\(\\beta_1\\) se construyen a partir de los estadísticos \\(t\\) y resultan: \\[ IC( \\beta_0;1-\\alpha) = \\hat{\\beta}_0 \\pm t_{\\left(n-2,1-\\frac{\\alpha}{2}\\right)} \\sqrt{\\frac{\\sum_{i=1}^n x_i^2}{n S_{xx}} \\ s^2} \\] \\[ IC(\\beta_1;1-\\alpha) = \\hat{\\beta}_1 \\pm t_{\\left(n-2,1-\\frac{\\alpha}{2}\\right)} \\ \\sqrt{\\frac{s^2}{S_{xx}}}, \\] donde \\(t_{\\left(n-2,1-\\frac{\\alpha}{2}\\right)}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución \\(t\\)-Student con \\(n-2\\) grados de libertad (los correspondientes a \\(s^2\\)). 2.3.7.2 Procedimientos de Contrastes de Hipótesis Si queremos contrastar Hipótesis sobre los coeficientes de la regresión: \\[\\left\\{\\begin{array}{ll} H_{0}:&amp; \\beta_{i} = \\beta^{*} \\\\ H_{1}:&amp; \\beta_{i} \\neq \\beta^{*}, i=0,1\\\\ \\end{array} \\right.\\] basta con considerar los estadísticos \\(t\\) anteriores, y sustituir el valor \\(\\beta_i\\) por el que se pretende contrastar, \\(\\beta^*\\). Estos estadísticos, bajo \\(H_0\\), tienen una distribución \\(t\\)-Student con \\(n-2\\) grados de libertad. La resolución del contraste consiste en calcular el p-valor asociado al valor absoluto de la estimación, \\(|t_0|\\) o \\(|t_1|\\), según el caso, esto es, \\(p-valor=Pr[t_{n-2}&gt;|t_i|]\\), donde \\(t_{n-2}\\) representa una variable t-Student con \\(n-2\\) grados de libertad, y \\(t_i\\) es el valor observado para el estadístico correspondiente. Dicho contraste se resuelve de la forma habitual: se rechaza \\(H_{0}\\) a nivel de confianza \\(1-\\alpha\\) cuando \\(p-valor \\leq \\alpha\\), si \\(p-valor &gt; \\alpha\\), se dice que los datos no proporcionan suficientes evidencias en contra de la hipótesis nula y ésta no se puede rechazar. Cuando el contraste propuesto sobre \\(\\beta_0\\) o \\(\\beta_1\\) tiene \\(\\beta^*=0\\), en realidad se está contrastando, respectivamente, si la recta de regresión tiene interceptación o pendiente nula. Contrastar \\(\\beta_1=0\\) es equivalente a contrastar correlación nula entre las variables \\(X\\) e \\(Y\\), esto es, ausencia de relación lineal. Si conseguimos rechazar esta hipótesis con significatividad, concluiremos que la variable \\(X\\) está relacionada linealmente con \\(Y\\) y por lo tanto se puede utilizar para predecir \\(Y\\) a través de la recta de regresión ajustada. 2.3.8 Ejemplo Realizamos el proceso de inferencia para el modelo para los datos de corrosión e interpretamos los resultados obtenidos. Concretamente: Construir intervalos de confianza al 95% para \\(\\beta_0\\) y \\(\\beta_1\\). ¿Qué podemos decir de la relación entre dichas variables? Concluir sobre los contrastes \\(\\beta_0=0\\) y \\(\\beta_1=0\\). Comprobar también que el último contraste \\(\\beta_1=0\\) es equivalente al contraste de correlación nula entre las variables del modelo. Recapturamos el resumen del modelo obtenido con la función glm_coef()- # Ajuste del modelo fit &lt;- lm(peso ~ hierro, data = corrosion) glm_coef(fit) ## Coefficient Pr(&gt;|t|) ## (Intercept) 129.79 (127.47, 132.11) &lt; 0.001 ## hierro -24.02 (-26.03, -22.01) &lt; 0.001 Como se puede observar en los resultados ninguno de los intervalos de confianza incluye al cero, lo que habla positivamente de su significatividad estadística, esto es, tenemos evidencias para predecir la pérdida de peso con el contenido de hierro inicial a través de una recta con interceptación y pendientes (significativamente) distintas de cero. De esta forma podemos ver que el efecto asociado con un incremento en una unidad de hierro produce una pérdida de peso de entre 22.01 y 26.03 unidades. Puestos a resolver el contraste \\(H_0^i:\\beta_i=0\\), para \\(i=0,1\\), observamos los p-valores obtenidos en el proceso de estimación que resultan ambos significativos (&lt;0.001 para \\(\\beta_0\\) y &lt;0.001 para \\(\\beta_1\\)), lo que concluye contundentemente sobre la significatividad de ambos a favor de que son distintos de cero (se rechazan \\(H_0^0\\) y \\(H_0^1\\)), como ya habíamos comentado a partir de los intervalos de confianza. En particular, el contenido en hierro explica significativamente la pérdida de peso a través del modelo lineal ajustado. 2.4 Bondad del Ajuste Cuando hemos realizado el ajuste de un modelo de regresión lineal, hemos de verificar que efectivamente dicho modelo proporciona un buen ajuste a la hora de explicar (predecir) la variable respuesta. Básicamente la bondad del ajuste la cuantificamos con el tanto por ciento de variabilidad de la respuesta, que consigue ser explicada por el modelo ajustado. Para ello contamos con varios tipos de medidas que cuantifican esta variabilidad de diversos modos. Como medidas fundamentales de bondad de ajuste contamos con: el error residual estimado \\(s = \\hat{\\sigma}\\); el test \\(F\\) de bondad de ajuste que se obtiene de la Tabla de Anova; el coeficiente de determinación \\(R^2\\). Todas estas medidas las desglosamos a continuación. Para obtenerlas con R utilizaremos las funciones glance(), anova() y summary(). 2.4.1 Error residual Es una medida de bondad del ajuste relativa a la escala de medida utilizada. En general, se prefieren modelos con menor error residual estimado \\(s\\), donde \\(s^2\\) denota la estimación de la varianza \\(\\sigma^2\\) del modelo, dada en el apartado @ref(rls_varmodel). 2.4.2 Tabla Anova Una medida de lo bueno que resulta un modelo para ajustar unos datos pasa por cuantificar cuánta de la variabilidad contenida en éstos ha conseguido ser explicada por dicho modelo. Un modelo es bueno si la variabilidad explicada es mucha, o lo que es lo mismo, si las diferencias entre los datos y las predicciones según el modelo son pequeñas. Construir la tabla de ANOVA o Análisis de la Varianza consiste en: descomponer la variabilidad de los datos en la parte que es explicada por el modelo y la parte que se deja sin explicar, es decir, la variabilidad de los residuos, compararlas y valorar estadísticamente si la variabilidad explicada por el modelo ajustado es suficientemente grande. Si partimos de la identidad: \\[\\begin{equation} y_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y}) \\tag{2.9} \\end{equation}\\] y el hecho de que \\(\\sum_{i} (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) = 0\\), podemos escribir: \\[\\begin{equation} \\underbrace{\\sum_{i=1}^n (y_i-\\bar{y})^2}_{SST} = \\underbrace{\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}_{SSE} + \\underbrace{\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}_{SSR} \\tag{2.10} \\end{equation}\\] Las abreviaturas \\(SST\\), \\(SSE\\) y \\(SSR\\) provienen del inglés para suma de cuadrados (Sum of Squares): Total, debida al Error (o residual) y debida a la Regresión, respectivamente. A partir de ellas es posible calcular la variabilidad total, la variabilidad explicada por el modelo obtenido, y la variabilidad que queda por explicar o variabilidad residual, sin más que dividir las sumas de cuadrados por sus respectivos grados de libertad. Obtenemos así los cuadrados medios asociados, \\(MST=SST/(n-1)\\), \\(MSE=SSE/(n-2)\\) y \\(MSR=SSR/1\\). Contrastar la bondad del ajuste de la recta de regresión significa resolver el contraste: \\[\\begin{array}{cc} H_0:&amp; \\mbox{ el modelo lineal NO explica bien la respuesta} \\\\ H_1:&amp; \\mbox{ el modelo lineal explica bien la respuesta}, \\tag{2.11} \\end{array}\\] que, en el modelo RLS, resulta equivalente a contrastar \\(H_0:\\beta_1=0, \\ vs. \\ H_1:\\beta_1 \\neq 0\\), esto es, si la variable predictora \\(X\\) explica suficientemente bien la variable respuesta \\(Y\\) a través del modelo lineal propuesto. El estadístico de bondad de ajuste de la regresión está basado en comparar la variabilidad explicada por el modelo con la que queda sin explicar, esto es, en el cociente de las sumas de cuadrados medias \\(MSR\\) y \\(MSE\\), que resulta tener una distribución \\(F\\) con \\(1\\) y \\(n-2\\) grados de libertad cuando el modelo es correcto: \\[\\begin{equation} F=\\frac{SSR/\\sigma^2}{\\frac{SSE/\\sigma^2}{n-2}}=\\frac{MSR}{MSE} \\sim F_{1,n-2}. \\tag{2.12} \\end{equation}\\] En el modelo RLS, el estadístico \\(F\\) es igual al estadístico \\(t\\) asociado a \\(\\beta_1\\), elevado al cuadrado. Ya hemos dicho antes que el contraste de bondad de ajuste es equivalente al de \\(\\beta_1=0\\). Concluiremos que la recta de regresión es significativa para predecir la respuesta \\(Y\\) al nivel de confianza \\((1-\\alpha)100\\%\\), cuando el valor que obtenemos para el estadístico \\(F\\) supera el valor crítico que se corresponde con el cuantil \\(1-\\alpha\\) de una distribución \\(F\\) con \\(1\\) y \\(n-2\\) grados de libertad. Esto es equivalente a que el p-valor asociado al contraste resulte inferior a \\(\\alpha\\). En otro caso, diremos que no hemos obtenido evidencias suficientes para rechazar que el modelo lineal no es útil para predecir la variable \\(Y\\) a través de \\(X\\). Todas estas sumas de cuadrados y estadísticos se suelen presentar en una tabla de análisis de la variabilidad o tabla ANOVA, cuya apariencia es: Fuente gl SS MS estadístico \\(F\\) p-valor Regresión 1 \\(SSR\\) \\(MSR=\\frac{SSR}{1}\\) \\(F=\\frac{MSR}{MSE}\\) \\(Pr(F_{1,n-2}&gt;F)\\) Error \\(n-2\\) \\(SSE\\) \\(MSE=\\frac{SSE}{n-2}\\) Total \\(n-2\\) \\(S_{yy}\\) En R la \\(SSR\\) se descompone a su vez para cada uno de los efectos o variables predictoras en el modelo. 2.4.3 Coeficiente de determinación Otro estadístico útil para chequear la bondad del ajuste de la recta de regresión es el coeficiente de determinación \\(R^2\\). Éste se define como la proporción de la varianza que es explicada por la recta de regresión y se obtiene a partir de la descomposición (2.10) como: \\[\\begin{equation} R^2=\\frac{SSR}{SST}. \\tag{2.13} \\end{equation}\\] De hecho, en el modelo RLS, \\(R^2\\) es el cuadrado del coeficiente de regresión lineal entre la respuesta \\(Y\\) y el predictor \\(X\\). Puesto que \\(0\\leq R^2 \\leq 1\\) (al tratarse del coeficiente de correlación al cuadrado), un valor cercano a \\(1\\) (entre 0.6 y 1) implicará que buena parte de la varianza es explicada por la recta de regresión, y \\(R^2\\approx 0\\) significará que prácticamente toda la variabilidad de los datos queda sin explicar por la recta. Sin embargo, \\(R^2\\) no sirve para medir la idoneidad del modelo de regresión para describir los datos. De hecho, \\(R^2\\) puede resultar grande a pesar de que la relación entre \\(X\\) e \\(Y\\) no sea lineal (de hecho tiene la misma interpretación que un coeficiente de correlación, válido para cuantificar la relación lineal sólo cuando ésta existe). Siempre ha de ser utilizado con cautela. Así por ejemplo, la magnitud de \\(R^2\\) depende del rango de variabilidad de la variable explicativa. Cuando el modelo de regresión es adecuado, la magnitud de \\(R^2\\) aumenta (o disminuye) cuando lo hace la dispersión de \\(X\\). Por otro lado, podemos obtener un valor muy pequeño de \\(R^2\\) debido a que el rango de variación de \\(X\\) es demasiado pequeño, y entonces impedirá que se detecte su relación con \\(Y\\). 2.4.4 Ejemplo Analizamos la bondad del ajuste obtenido para los datos de corrosión. # Medidas de bondad del ajuste glance(fit) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.970 0.967 3.06 352. 1.06e-9 2 -31.9 69.8 71.5 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Esta función proporciona diferentes medidas de bondad de ajuste, algunas de ellas las utilizaremos en las unidades siguientes, pero en este caso nos centramos en las que hace referencia al modelo de RLS: r.squared: \\(R^2\\) del modelo ajustado, sigma: error residual, statistic: valor del estadístico de contraste (2.12) asociado a la tabla ANOVA, p.value: p-valor del contraste (2.12), df: grados de libertad asociados con \\(MSR\\), df.residual: grados de libertad asociados con \\(MSE\\). Para este modelo el error residual tiene una magnitud de 3.05778, pero dado que no podemos comparar con otro modelo resulta difícil interpretar este valor como una medida de bondad de ajuste al no tener una escala de medida que nos indique si este valor es lo suficientemente pequeño. El valor del estadístico F (352.27) y su p-valor (1.055e-09) nos permiten concluir que podemos rechazar la hipótesis \\(H_0:\\beta_1=0\\), o lo que es lo mismo, \\(H_0\\): el modelo no explica los datos, a favor de que el contenido en hierro resulta útil para predecir el la pérdida de peso debido a la corrosión a través de un modelo de regresión lineal. Veamos la descomposición de la tabla ANOVA. anova(fit) ## Analysis of Variance Table ## ## Response: peso ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## hierro 1 3293.8 3293.8 352.27 1.055e-09 *** ## Residuals 11 102.9 9.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Simplemente observando la Tabla de Anova, vemos que la variabilidad explicada por la recta (en términos de sumas de cuadrados), \\(SSR = 3293.8\\), es superior a la que queda por explicar, \\(SSE = 102.9\\) (casi tres veces superior). En este caso, al tener una única variable en el modelo la \\(SSR\\) coincide con la correspondiente a la variable hierro tal y como aparece en la tabla anterior. A la vista de estos resultados podemos concluir que efectivamente el modelo obtenido resulta útil para explicar la mayor parte de la variabilidad existente en la respuesta a partir de la variabilidad explicada por dicho modelo. Por último, el valor del coeficiente de determinación es \\(R^2= 0.9697198\\), lo que implica que alrededor del \\(97\\%\\) de la variabilidad de la pérdida de peso es explicada por la recta ajustada. Es un valor especialmente alto que refleja el gran poder predictivo del contenido de hierro para tratar de conocer la pérdida de peso final del compuesto. Una versión resumida de las características del modelo ajustado se puede obtener con la función summary(). Con ella podemos obtener la ecuación del modelo, contrastes individuales sobre cada coeficiente, la varianza residual y el test F asociado. Los criterios utilizados nos permiten concluir que el modelo obtenido es adecuado desde el punto de vista de su capacidad explicativa, es decir, a la hora de medir la asociación entre la respuesta y la predictora. Sin embargo, es importante tener presente los pasos que hemos de dar a la hora de aceptar finalmente un modelo como bueno. No sólo es preciso superar la bondad del ajuste. Una vez superada esta prueba, hay que llevar a cabo el diagnóstico y validación del modelo, o verificación de las hipótesis del modelo RLS y de la capacidad predictiva del mismo. De entre todos los modelos propuestos para predecir una respuesta \\(Y\\) que hayan superado la bondad del ajuste, el diagnóstico y la validación, podremos optar por el mejor según algún criterio preferido de comparación y selección de modelos. En los modelos de RLS esta tarea es sencilla ya que el modelo ajustado es único, pero se complicará cuando se añadan más variables predictoras como veremos en las unidades siguientes. 2.5 Diagnóstico del Modelo Una vez ajustado un modelo y habiendo superado las pruebas de bondad de ajuste pertinentes (fundamentalmente el test \\(F\\) de Anova), hemos de proceder con el diagnóstico del modelo, que consiste en verificar si éste satisface las hipótesis básicas del modelo de regresión, que son: linealidad entre las variables \\(X\\) e \\(Y\\) \\[\\begin{array}{ll} H_{0}: &amp; Linealidad\\\\ H_{1}: &amp; No\\ linealidad \\tag{2.14} \\end{array}\\] para los errores del modelo, \\(\\epsilon_{i}\\): media cero varianza constante u homocedasticidad \\[\\begin{array}{ll} H_{0}: &amp; Varianza\\ constante\\\\ H_{1}: &amp; Varianza\\ no\\ constante \\tag{2.15} \\end{array}\\] Si rechazamos la hipótesis nula estaremos concluyendo que nuestro modelo incumple la hipótesis de varianza constante. incorrelación \\[\\begin{array}{ll} H_{0}: &amp; Residuos\\ no\\ correlados \\\\ H_{1}: &amp; Residuos\\ correlados \\tag{2.16} \\end{array}\\] normalidad \\[\\begin{array}{ll} H_{0}: &amp; Residuos\\ normales\\\\ H_{1}: &amp; Residuos\\ no\\ normales \\tag{2.17} \\end{array}\\] El análisis de los residuos del modelo nos permitirá detectar deficiencias en la verificación de estas hipótesis, así como descubrir observaciones anómalas o especialmente influyentes en el ajuste. Una vez encontradas las deficiencias, si existen, cabrá considerar el replanteamiento del modelo, bien empleando transformaciones de las variables, bien proponiendo modelos alternativos al de RLS, que trataremos con detalle en las unidades siguientes. El diagnóstico del modelo se lleva a cabo fundamentalmente a partir de la inspección de los residuos del modelo. Éstos sólo son buenos estimadores de los errores cuando el modelo ajustado es bueno. Aun así, es lo más aproximado con lo que contamos para indagar qué ocurre con los errores y si éstos satisfacen las hipótesis del modelo. El análisis de los residuos habitual es básicamente gráfico, si bien existen varios tests estadísticos útiles para detectar inadecuaciones del modelo, que presentaremos brevemente. Definimos los residuos de un modelo lineal como las desviaciones entre las observaciones y los valores ajustados: \\[ r_i = y_i - \\hat{y}_i, \\qquad i=1,\\ldots,n. \\] En ocasiones, es preferible trabajar con los residuos estandarizados, que tienen media cero y varianza aproximadamente unidad, y facilitan la visualización de las hipótesis: \\[ d_i = \\frac{r_i}{\\sqrt{MSE}}, \\qquad i=1,\\ldots,n. \\] Los residuos asociados a un modelo ajustado se pueden obtener con la función fortify(). Esta función proporciona además las medidas de influencia para detectar observaciones anómalas, es decir, observaciones con residuos excesivamente grandes. 2.5.1 Linealidad y homocedasticidad. Los gráficos de residuos estandarizados frente a valores ajustados nos permiten detectar varios tipos de deficiencias del modelo ajustado. Si los residuos están distribuidos alrededor del cero y el gráfico no presenta ninguna tendencia entonces el modelo se considera adecuado. Cuando aparece alguna tendencia como una forma de embudo o un abombamiento, etc., podemos tener algún problema con la hipótesis de varianza constante para los errores (heterocedasticidad). Cuando se aprecia alguna tendencia hablamos de violación de la hipótesis de linealidad: el modelo lineal ha sido incapaz de capturar una tendencia no lineal apreciada en los residuos, posiblemente debido a que existen otras variables explicativas adicionales no consideradas en el modelo, o a que la variable predictora explica la respuesta de un modo más complejo (quizás polinómico, etc.) al considerado en el modelo lineal. Para verificar la hipótesis de homocedasticidad podemos usar el test de Breusch-Pagan para variables predictoras de tipo numérico, y el de Bartlett para variables predictoras de tipo categórico (ver unidades siguientes). Para realizar el test de Breusch-Pagan utilizamos la función bptest() de la librería lmtest, mientras que para realizar el test de Bartlett utilizamos la función bartlett.test(). 2.5.2 Ejemplo Analizamos la hipótesis de homocedasticidad para el modelo obtenido para los datos de corrosión. En primer lugar obtenemos los residuos del modelo. # Residuos y medidas de diagnóstico diagnostico &lt;- fortify(fit) # Gráfico de residuos estandarizados vs ajustados ggplot(diagnostico, aes(x = .fitted, y = .stdresid)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() Figura 2.5: Gráfico de residuos estandarizados vs valores ajustados. Se puede concluir que se verifica la hipótesis de linealidad (recta horizontal), ya que no existen tendencias en los residuos. Además con los residuos se comportan de forma aleatoria sin agrupaciones ni tendencias podemos concluir que se cumple la hipótesis de homogeneidad de varianzas. Hay que tener mucho cuidad con la interpretación de estos gráficos, ya que cuando el tamaño de muestra es pequeño, resulta difícil apreciar tenencias o agrupaciones en los residuos. Por este motivo realizamos el test de diagnóstico. # Test de diagnóstico bptest(fit) ## ## studentized Breusch-Pagan test ## ## data: fit ## BP = 0.024539, df = 1, p-value = 0.8755 Dado que el p-valor del contraste es superior a 0.05 no podemos rechazar la hipótesis nula dada en (2.15), y por tanto podemos concluir que se verifica la hipótesis de homogeneidad o varianza constante. 2.5.3 Normalidad Para verificar la normalidad de los errores, disponemos de gráficos qq-plot de normalidad, en los que se representan los residuos ordenados \\(r_{[i]}\\) (cuantiles empíricos) versus los cuantiles correspondientes a una normal estándar, \\(\\Phi^{-1}[(i-1)/n]\\). Si es cierta la normalidad de los residuos, los puntos han de estar alineados con la diagonal. Desviaciones de la diagonal más o menos severas en las colas, e incluso en el centro de la distribución, dan indicios de desviaciones de normalidad. La hipótesis de normalidad se puede chequear también con histogramas de los residuos cuando el tamaño muestral es grande. Los residuos estandarizados también son útiles para detectar desviaciones de la normalidad. Si los errores se distribuyen según una normal, entonces aproximadamente el \\(68\\%\\) de los residuos estandarizados quedarán entre \\(-1\\) y \\(+1\\), y el \\(95\\%\\) entre \\(-2\\) y \\(+2\\). Para diagnosticar la hipótesis de normalidad se utiliza el test de Shapiro-Wilks, donde rechazar la hipótesis nula implica el rechazo de la hipótesis de normalidad. Para realizar dicho contraste utilizamos la función shapiro.test(). # grafico qq ggplot(diagnostico, aes(sample = .stdresid)) + stat_qq() + geom_abline() + theme_bw() (#fig:rls011, )Gráfico de normalidad de los residuos estandarizados. El gráfico de normalidad muestra un comportamiento correcto ya que los punto se distribuyen a lo largo de la recta de normalidad. No se realiza el histograma ya que el tamaño muestral es demasiado pequeño. Pasamos al test de diagnóstico. # Test de diagnóstico shapiro.test(diagnostico$.stdresid) ## ## Shapiro-Wilk normality test ## ## data: diagnostico$.stdresid ## W = 0.92905, p-value = 0.3312 Dado que el p-valor del contraste es superior a 0.05 no podemos rechazar la hipótesis nula dada en (2.17), y por tanto podemos considerar que los residuos se distribuyen normalmente. 2.5.4 Independencia. La correlación entre los datos es un proceso intrínseco al muestreo; saber cómo se ha llevado a cabo éste da información, generalmente suficiente, para poder hablar de correlación o incorrelación. En todo caso, los gráficos secuenciales de residuos sirven para detectar problemas de correlación de éstos (autocorrelación), o de inestabilidad de la varianza a lo largo del tiempo. También son útiles para esto los gráficos en que se representa un residuo versus el anterior en la secuencia en que han sido observados; si hay correlación se apreciará tendencia. Detectar autocorrelación llevará a considerar otro tipo de modelos distintos (autocorrelados: modelos de series temporales). Aparte de los métodos gráficos, para resolver dicho contraste se utiliza el test de Durbin-Watson, cuya función es dwtest(). Rechazar la hipótesis nula implica el rechazo de la hipótesis de incorrelación. # grafico función autocorrelación acf(diagnostico$.stdresid) (#fig:rls013, )Gráfico de autocorrelación de los residuos estandarizados. El gráfico de la función de autocorrelación muestra la independencia de las observaciones. Todos los lags quedan dentro del rango de independencia. # Test de diagnóstico dwtest(fit, alternative = &quot;two.sided&quot;) ## ## Durbin-Watson test ## ## data: fit ## DW = 2.5348, p-value = 0.2952 ## alternative hypothesis: true autocorrelation is not 0 Dado que el p-valor del contraste es superior a 0.05 no podemos rechazar la hipótesis nula dada en (2.16), y por tanto podemos considerar que los residuos se distribuyen de froma independiente. 2.5.5 Otros gráficos de diagnóstico Los gráficos de residuos versus valores de la variable predictora son útiles para apreciar tendencias en los residuos que han quedado sin explicar por el modelo ajustado. Básicamente se interpretan como los gráficos de residuos versus valores ajustados \\(\\hat{y}_i\\). Es deseable que los residuos aparezcan representados en una banda horizontal sin tendencias alrededor del cero. Por ejemplo, si hay tendencias de tipo cuadrático, posiblemente hayamos de incorporar la variable \\(x^2\\) en el modelo, o bien abordar algún tipo de transformación que permita una relación de tipo lineal entre predictor y respuesta. 2.5.6 Incumplimiento de hipótesis Una vez identificado el incumplimiento de alguna de las hipótesis del modelo, hay que tratar de identificar porque se produce dicho incumplimiento. Se estudia si el incumplimiento es debido a: Subconjunto de los datos que influye desproporcionadamente en el ajuste del modelo propuesto, con lo cual las estimaciones y predicciones dependen mucho de él. En primer lugar, el objetivo es identificar dichas observaciones. Una vez detectadas la forma de proceder es la siguiente: Comprobar si la influencia se debe a un error en la toma de observaciones, si es así se corrigen los defectos encontrados y se comienza de nuevo. Si los datos son correctos y el subconjunto de influyentes es pequeño se opta casi siempre por su eliminación del banco de datos. En otras ocasiones se puede optar por estudiar de forma separada a dichas observaciones. Comportamiento sistemático del modelo. Este caso es más complicado y requiere de procedimientos más sofisticados para corregir los defectos que aparezcan en el modelo. De la primera parte se encarga de analizarla los diagnósticos de influencia, mientras que en el segundo caso se trata principalmente de realizar transformaciones de las variables involucradas en el modelo. 2.5.7 Análisis de influencia El análisis de influencia pretende detectar aquellas observaciones cuya inclusión/exclusión en el ajuste altera sustancialmente los resultados. Es interesante siempre, localizar este tipo de datos, si existen, y evaluar su impacto en el modelo. Si estos datos influyentes son “malos” (provienen de errores en la medición, o de condiciones de experimentación diferentes, etc.) habrían de ser excluidos del ajuste; si son “buenos”, contendrán información sobre ciertas características relevantes a considerar en el ajuste. A primera vista, observaciones que dan lugar a un residuo grande, pueden influir notablemente en el ajuste. Las denominaremos OBSERVACIONES ALEJADAS. Su existencia puede indicar también la inadecuación del modelo asumido a la realidad experimental. Si dicha observación tiene un residuo exageradamente grande la denominamos ANÓMALA (outlier en inglés). Por otra parte, observaciones que adoptan valores extremos de alguna o varias variables explicativas pueden tener más influencia que las usuales. Las denominaremos OBSERVACIONES ATÍPICAS. Sin embargo, las dos características no siempre suponen que las observaciones que las manifiestan sean también influyentes. Generalmente se dice que una observación es alejada si el valor absoluto del residuo es mayor que 2. Se considera anómala si el valor absoluto del residuo es mayor que 3. Un criterio para valora la influencia de una observación sobre los coeficientes del modelo es el cálculo de la distancia de CooK. Se consideran como observaciones influyentes todas aquellas cuyo valor de la distancia sea mayor que 1. Dicha distancia se obtiene directamente con la función fortify(modelo) en al columna denominada .cooksd. Existen otro tipo de medidas de influencia (se pueden obtener con la función influence.measures(ajuste)) pero las estudiaremos en las unidades siguientes. Si el incumplimiento de las hipótesis no es debido a la presencia de observaciones influyentes, sino más bien a un comportamiento sistemático del modelo, los remedios para corregir estas deficiencias pasan principalmente por: Propuesta de otros modelos adecuados a la distribución de la respuesta y su relación con los predictores. Este punto o trataremos ampliamente más adelante Transformar la variable respuesta (si es de tipo continuo), o las variables predictoras (si son de tipo continuo). 2.5.8 Transformaciones El tipo de transformaciones que podemos realizar se pueden dividir en tres apartados: Transformaciones debidas al modelo teórico. Existen situaciones experimentales donde ya partimos de un tipo de modelo de carácter no lineal pero que se podría convertir en lineal con una sencilla transformación de la respuesta o la predictora, o de ambas. Ejemplos de estos modelos teóricos que se pueden convertir en modelos de RLS son: Modelo Teórico Transformación y modelo a plantear \\(Y = \\beta_0 X^{\\beta_1}\\) \\(log(Y) \\sim log(X)\\) \\(Y = \\beta_0 exp^{\\beta_1 X}\\) \\(log(Y) \\sim X\\) \\(Y = \\beta_0 + \\beta_1 log(X)\\) \\(Y \\sim log(X)\\) \\(Y = \\frac{X}{\\beta_0 + \\beta_1 X}\\) \\(1/Y \\sim 1/X\\) \\(Y = \\frac{1}{\\beta_0 + \\beta_1 X}\\) \\(1/Y \\sim X\\) \\(Y = \\beta_0 + \\beta_1 \\frac{1}{X}\\) \\(Y \\sim 1/X\\) Transformaciones sobre la predictora. Se utilizan principalmente ante la falta de linealidad, y se basan principalmente en la construcción de modelos de predicción polinómicos. Estos modelos los estudiaremos con más detalle en la unidad siguiente. Transformaciones sobre la respuesta. Esta suele ser la opción más habitual. Obtener una transformación adecuada de la respuesta sin alterar las variables predictoras. Se suelen utilizar ante el incumplimiento de las hipótesis de normalidad o varianza constante. Como buscar una transformación adecuada es un tema que puede resulta costoso, se utilizar un procedimiento automático que nos da una transformación rápida. Dicho procedimiento se conoce con el nombre de transformaciones de Box-Cox, y sde puede obtener en R con la función boxcox(). Dicho procedimiento consiste en obtener un intervalo de confianza para un parámetro (\\(\\lambda\\)) que refleja la transformación de la respuesta a utilizar. Las transformaciones más habituales son: \\(\\lambda\\) Transformación -2 \\(1/Y^2\\) -1 \\(1/Y\\) -1/2 \\(1/\\sqrt{Y}\\) 0 \\(log(Y)\\) 1/2 \\(\\sqrt{Y}\\) 1 \\(Y\\) 2 \\(Y^2\\) Una vez realizado el estudio de influencia la forma de proceder consiste en eliminar las observaciones influyentes y obtener un nuevo modelo sin ellas, o bien realizar alguna de las transformaciones planteadas y ajustar el nuevo modelo. Una vez construido deberemos ajustar el nuevo modelo y realizar un nuevo diagnóstico para verificar que se cumple las hipótesis. Se trata pues de un proceso circular donde a cada modificación debemos obtener un nuevo modelo y analizarlo completamente hasta llegar a un modelo que cumpla con todas las especificaciones. Sin embargo, en ocasiones puede ocurrir que no seamos capaces de encontrar un modelo que cumpla las hipótesis y deberemos buscar entre modelos más complejos de los planteados aquí. 2.5.9 Ejemplos Procedemos con el análisis de los bancos de datos de Papel y Viscosidad presentados al inicio de esta unidad para estudiar los posibles problemas de diagnóstico que hemos venido trabajando. 2.5.9.1 Papel Planteamos y ajustamos el modelo correspondiente a los datos de Papel. # Ajuste del modelo fit.papel &lt;- lm(tension ~ madera, data = papel) # Solución con glm_coef glm_coef(fit.papel) ## Coefficient Pr(&gt;|t|) ## (Intercept) 21.32 (3.15, 39.49) 0.024 ## madera 1.77 (-0.41, 3.95) 0.105 # Gráfico del ajuste plot_model(fit.papel, &quot;pred&quot;, terms = ~madera, ci.lvl = NA, show.data = TRUE, axis.title = c(&quot;Concentración de madera&quot;, &quot;Tensión del papel&quot;), title = &quot; &quot;) Figura 2.6: Ajuste para los datos de resitencia del papel Parece obvio que el modelo planteado no es adecuado, ya que no captura de forma adecuada la tendencia de los datos observados. De hecho, el coeficiente asociado con madera resulta no significativo, lo que daría a entender que la concentración de madera no es relevante para explicar la tensión del papel. Esta afirmación es claramente falsa ya que se aprecia claramente una tendencia de tipo cuadrático. Realizamos los gráficos de diagnóstico para corroborar este hecho. # Obtenemos los residuos del modelo diganostico.papel &lt;- fortify(fit.papel) # Gráfico de residuos vs ajustados ggplot(diganostico.papel, aes(x = .fitted, y = .stdresid)) + geom_point() + theme_bw() Figura 2.7: Gráfico de residuos vs ajustados para el modelo de papel Se observa claramente una tendencia de tipo cuadrática en los residuos lo que indica que un modelo más adecuado para estos datos sería: \\[tension \\sim madera + madera^2\\] Realizamos el análisis de influencia para completar el diagnóstico, a pesar de que la introducción de la nueva pedictora proporcionará un modelo más adecuado. # Valoramos si hay alguna observación con distancia mayor que 1 abs(diganostico.papel$.cooksd) &gt; 1 ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE No se observa ninguna observación influyente por lo que el problema de ajuste se debe a la falta de tendencia del modelo propuesto. 2.5.9.2 Viscosidad Planteamos y ajustamos el modelo correspondiente a los datos de Viscosidad. # Ajuste del modelo fit.aceite &lt;- lm(viscosidad ~ aceite, data = aceites) # Solución con glm_coef glm_coef(fit.aceite) ## Coefficient Pr(&gt;|t|) ## (Intercept) 6.32 (-6.1, 18.73) 0.302 ## aceite 1.47 (0.86, 2.08) &lt; 0.001 # Gráfico del ajuste plot_model(fit.aceite,&quot;pred&quot;, terms = ~aceite, ci.lvl = NA, show.data = TRUE, axis.title = c(&quot;Cantidad de aceite&quot;, &quot;Viscosidad&quot;), title = &quot; &quot;) Figura 2.8: Ajuste para los datos de viscosidad El modelo ajustado indica que la cantidad de aceite puede explicar la viscosidad final (p-valor significativo), de forma que por cada unidad que aumentamos la cantidad de aceite la viscosidad aumenta en 1.47 unidades. El modelo obtenido viene dado por: \\[ \\widehat{\\text{viscosidad}} = 6.32 + 1.47*\\text{aceite} \\] Estudiamos la capacidad explicativa del modelo: glance(fit.aceite) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.624 0.606 23.8 34.9 7.31e-6 2 -104. 215. 218. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; El \\(R^2\\) nos indica que el 62.4% de la variabilidad viene explicada por el modelo. Además el p-valor asociado a la tabla ANOVA resulta significativo indicando que el modelo tiene capacidad explicativa, es decir, podemos utilizar la cantidad de aceite para conocer el grado de viscosidad. Pasamos al diagnóstico del modelo. diagnostico.aceite &lt;- fortify(fit.aceite) En este cao utilizamos los tests estadísticos en lugar de los gráficos para concluir sobre el diagnóstico: # Varianza constante bptest(fit.aceite) ## ## studentized Breusch-Pagan test ## ## data: fit.aceite ## BP = 6.0786, df = 1, p-value = 0.01368 # Normalidad shapiro.test(diagnostico.aceite$.stdresid) ## ## Shapiro-Wilk normality test ## ## data: diagnostico.aceite$.stdresid ## W = 0.95818, p-value = 0.4276 # Independencia dwtest(fit.aceite) ## ## Durbin-Watson test ## ## data: fit.aceite ## DW = 0.51202, p-value = 5.084e-06 ## alternative hypothesis: true autocorrelation is greater than 0 Como se puede ver debemos rechazar las hipótesis de varianza constante y de independencia (p-valores inferiores a 0.05). Planteamos la familia de transformaciones de Box-Cox para tratar de corregir los problemas con las hipótesis del modelo: boxcox(fit.aceite) El intervalo de confianza al 95% para \\(\\lambda\\) incluye el valor de \\(\\lambda = 0\\), de forma que podríamos utilizar la transformación logaritmo para tratar de corregir los defectos encontrados en el modelo propuesto inicialmente. # Calculamos la nueva variable aceites &lt;- aceites %&gt;% mutate(lviscosidad = log(viscosidad)) # Ajuste el nuevo modelo fit.aceite2 &lt;- lm(lviscosidad ~ aceite, data = aceites) # Solución con glm_coef glm_coef(fit.aceite2) ## Coefficient Pr(&gt;|t|) ## (Intercept) 2.81 (2.45, 3.17) &lt; 0.001 ## aceite 0.03 (0.02, 0.03) &lt; 0.001 # Gráfico del ajuste plot_model(fit.aceite2,&quot;pred&quot;, terms = ~aceite, ci.lvl = NA, show.data = TRUE, axis.title = c(&quot;Cantidad de aceite&quot;, &quot;log(Viscosidad)&quot;), title = &quot; &quot;) El modelo resulta significativo con ecuación dada por: \\[ \\widehat{\\text{lviscosidad}} = 2.81 + 0.03*\\text{aceite} \\] La bondad del ajuste glance(fit.aceite2) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.738 0.726 0.363 59.3 1.52e-7 2 -8.31 22.6 26.0 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; nos da una capacidad explicativa del 73.8%. Hemos mejorado nuestra capacidad explicativa al transformar la respuesta. Por último realizamos el diagnóstico del nuevo modelo: diagnostico.aceite2 &lt;- fortify(fit.aceite2) # Varianza constante bptest(fit.aceite2) ## ## studentized Breusch-Pagan test ## ## data: fit.aceite2 ## BP = 0.39756, df = 1, p-value = 0.5284 # Normalidad shapiro.test(diagnostico.aceite2$.stdresid) ## ## Shapiro-Wilk normality test ## ## data: diagnostico.aceite2$.stdresid ## W = 0.92282, p-value = 0.07659 # Independencia dwtest(fit.aceite2) ## ## Durbin-Watson test ## ## data: fit.aceite2 ## DW = 0.24384, p-value = 6.044e-10 ## alternative hypothesis: true autocorrelation is greater than 0 El modelo verifica las hipótesis de varianza constante y normalidad. la hipótesis de independencia resulta significativa debido a la propia estructura de los datos, y más concretamente de la variable predictora, ya que como se puede ver solo se dan ciertos valores específicos (como si se tratara de una variable categórica más que una numérica). Podemos verificar este hecho con el gráfico de autocorrelación: # gráfico función autocorrelación acf(diagnostico.aceite2$.stdresid) Figura 2.9: Gráfico de autocorrelación de los residuos estandarizados. En esta situación este incumplimiento no resulta concluyente y podemos utilizar el modelo construido para establecer una relación entre la cantidad de aceite y el logaritmo de la viscosidad. 2.6 Predicción del modelo Una vez obtenido un modelo definitivo, la última fase de la modelización consiste en la predicción de la respuesta a partir de un nuevo conjunto de valores de la predictora o predictoras. Básicamente se trata de utilizar valores dentro del rango de la variable predictora para conocer el valor de la respuesta sin necesidad de realizar el diseño experimental. Una vez ajustado el modelo si consideramos una observación \\(X = x_0\\) dentro del rango de valores de \\(X\\) la predicción de la respuesta se puede obtener a través del modelo ajustado mediante: \\[y_0 = \\widehat{\\beta_0} + \\widehat{\\beta_1} x_0.\\] Esto nos proporciona una estimación puntual del valor predicho, pero sin embargo es necesario proporcionar un intervalo de confianza para dicho valor para tener en cuenta la variabilidad observada en el modelo propuesto. Existen dos posibilidades de predicción en este sentido: Predicción del valor medio de la respuesta. Se trata de predecir el valor medio de la respuesta para un valor especifico de la variable predictora (\\(X = x_0\\)). Esta es la herramienta de predicción habitual ya que tiene una menor variabilidad. La idea es que para un mismo valor de \\(X = x_0\\) obtendremos diferentes valores predichos de la respuesta, y por tanto, más que interesarnos la predicción de la respuesta, nos centramos en predecir la media de todos esos posibles valores de la respuesta. Predicción del valor de la respuesta. Se trata de predecir el valor de la respuesta para un valor especifico de la variable predictora (\\(X = x_0\\)). Dado que estamos intentando predecir un único valor y no la media de un conjunto de valores el intervalo de confianza de predicción es mayor que en el caso anterior. Tenemos más variabilidad cuando queremos predecir un valor que cuando queremos predecir la media de un conjunto de valores. Utilizaremos la función predict() para construir la predicción para un modelo dado. Veremos su aplicación en los diferentes ejemplos. 2.6.1 Respuesta media Como ya hemos indicado resulta posible obtener una estimación puntual del valor de la respuesta media a través de: \\[\\begin{equation} \\hat{y}_{x_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0. \\tag{2.18} \\end{equation}\\] para un \\(X = x_0\\) dado. Un intervalo de confianza para la estimación del valor esperado de la respuesta para un \\(X = x_0\\) dado es: \\[\\begin{equation} IC(E(\\bar{y}_n \\mid x_0);1-\\alpha) = \\hat{y}_{x_0} \\pm t_{(n-2,1-\\frac{\\alpha}{2})} \\ s \\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}}}. \\tag{2.19} \\end{equation}\\] 2.6.2 Nueva observación Predeciremos una futura observación de la variable \\(Y\\) para cierto valor de \\(X = x_0\\), con \\[\\begin{equation} \\hat{y}_{x_0}=\\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 = \\bar{y} + \\hat{\\beta}_1 (x_0 - \\bar{x}), \\tag{2.20} \\end{equation}\\] y el intervalo de confianza vendrá dado por: \\[\\begin{equation} IC(y_{x_0};1-\\alpha) = \\hat{y}_{x_0} \\pm t_{\\left(n-2,1-\\frac{\\alpha}{2}\\right)} \\ s \\ \\sqrt{1+\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_{xx}}} \\tag{2.21} \\end{equation}\\] Notar que tanto la estimación de la respuesta media como la predicción coinciden, aunque difieren en cuanto al grado de incertidumbre de la misma. Como es de esperar, predecir un hecho puntual en el futuro conlleva más incertidumbre que estimar en términos medios qué va a a ocurrir. Por último, comentar que cuando hemos utilizado alguna transformación (monótona) sobre la respuesta y queremos recuperar la estimación o predicción de ésta en su escala original, basta con utilizar la transformación recíproca sobre el valor predicho para obtener la predicción en la escala original. 2.6.3 Ejemplos Dado que la estimación puntual de la predicción coincide con el modelo ajustado, ya hemos mostrado anteriormente como representar gráficamente la ecuación del modelo o predicción de la respuesta en diferentes ejemplos. En este punto nos limitamos a mostrar como obtener y representar los intervalos de confianza asociados, y como obtener la predicción para un conjunto determinado de valores de la variable predictora. 2.6.3.1 Corrosión En primer lugar recuperamos los datos y modelo obtenido para los datos de corrosión. Estamos interesados en conocer la pérdida de peso medio (estimación de la media) y la pérdida de peso específica (predicción de una futura observación) para una barra en particular cuando el contenido en hierro es de 0.5, 1, y 1.5. # cargamos datos de predicción newpred &lt;- data.frame(hierro = c(0.5, 1, 0.5)) # Predicción para la media de la respuesta # Opción interval = &quot;confidence&quot; newdata &lt;- data.frame(newpred, predict(fit, newpred, interval = &quot;confidence&quot;)) round(newdata, 2) ## hierro fit lwr upr ## 1 0.5 117.78 115.63 119.92 ## 2 1.0 105.77 103.87 107.67 ## 3 0.5 117.78 115.63 119.92 Hemos obtenido la estimación (fit) e intervalo de confianza ((lwr,upr)) al 95% de la predicción de la pérdida de peso medio para valores específicos de contenido de hierro. # cargamos datos de predicción newpred &lt;- data.frame(hierro = c(0.5, 1, 0.5)) # Predicción de un único valor # Opción interval = &quot;prediction&quot; newdata &lt;- data.frame(newpred, predict(fit, newpred, interval = &quot;prediction&quot;)) round(newdata, 2) ## hierro fit lwr upr ## 1 0.5 117.78 110.71 124.84 ## 2 1.0 105.77 98.77 112.76 ## 3 0.5 117.78 110.71 124.84 Como ya habíamos comentado la estimación que obtenemos es la misma pero los intervalos de confianza son más amplios. A continuación se muestra como representar gráficamente la predicción de la respuesta media y los intervalos de predicción al 95% de confianza para todo el rango de valores de la predictora. # Gráfico del ajuste plot_model(fit, &quot;pred&quot;, terms = ~hierro, ci.lvl = 0.95, show.data = TRUE, axis.title = c(&quot;Contenido en hierro&quot;, &quot;Peso&quot;), title = &quot; &quot;) Figura 2.10: Predicción para los datos de corrosión (media e IC95%). 2.6.3.2 Viscosidad Vamos a realizar la predicción para el modelo ajustado a los datos de viscosidad. Recordemos que en este caso hemos transformado la respuesta con la función logaritmo para asegurar que se cumple las hipótesis del modelo, y por tanto nuestra predicción inicial es sobre dicha variable y no sobre la viscosidad. Resulta necesario deshacer la transformación logaritmo para poder obtener la predicción en la escala original de la viscosidad. Estamos interesados en conocer la viscosidad media (estimación de la media) del producto final cuando el contenido de aceite es de 10, 20, 30, 40, y 50. # cargamos datos de predicción newpred &lt;- data.frame(aceite = c(10, 20, 30, 40, 50)) # Predicción para la media de la respuesta # Opción interval = &quot;confidence&quot; newdata &lt;- data.frame(newpred, predict(fit.aceite2, newpred, interval = &quot;confidence&quot;)) round(newdata, 2) ## aceite fit lwr upr ## 1 10 3.10 2.87 3.33 ## 2 20 3.40 3.21 3.58 ## 3 30 3.69 3.53 3.85 ## 4 40 3.98 3.81 4.15 ## 5 50 4.27 4.06 4.49 # Deshacemos la transformación para volver a la escala de viscosidad newdata[,2:4] &lt;- exp(newdata[,2:4]) round(newdata,2) ## aceite fit lwr upr ## 1 10 22.27 17.68 28.05 ## 2 20 29.84 24.90 35.77 ## 3 30 39.99 34.15 46.83 ## 4 40 53.59 45.12 63.64 ## 5 50 71.80 57.85 89.12 De esta forma obtenemos las predicciones en la escala original de la variable viscosidad. ¿Cómo interpretamos los valores de predicción obtenidos? Realizamos ahora los gráficos de predicción para log(viscosidad) y viscosidad. Para este último introducimos el código necesario para deshacer la transformación. # Gráfico del ajuste plot_model(fit.aceite2, &quot;pred&quot;, terms = ~aceite, ci.lvl = 0.95, show.data = TRUE, axis.title = c(&quot;Contenido de aceite&quot;, &quot;log(Viscosidad)&quot;), title = &quot; &quot;) Figura 2.11: Predicción para los datos de log(viscosidad) (media e IC95%). # Construímos una secuencia de predicción newdata &lt;- data.frame(aceite = seq(min(aceites$aceite), max(aceites$aceite), length = 50)) # Predicción para la media de la respuesta newdata &lt;- data.frame(newdata, predict(fit.aceite2, newdata, interval = &quot;confidence&quot;)) # Deshacemos la transformación para volver a la escala de viscosidad newdata[,2:4] &lt;- exp(newdata[,2:4]) # Gráfico del ajuste ggplot(newdata, aes(x = aceite, y = fit)) + geom_line() + geom_ribbon(aes(ymax = upr, ymin = lwr), alpha = 1/5) + geom_point(data = aceites, aes(x = aceite, y = viscosidad)) + labs(x = &quot;Cantidad de aceite&quot;, y = &quot;Viscosidad&quot;) + theme_bw() Figura 2.12: Predicción para los datos de viscosidad (media e IC95%). En este segundo gráfico se puede ver el efecto de la transformación propuesta. De hecho, la predicción obtenida captura la tendencia observada en los datos originales. "],
["rlm.html", "Unidad 3 Regresión Lineal Múltiple y Polinómica 3.1 Tipos de modelos 3.2 Estimación e inferencia 3.3 Bondad del ajuste 3.4 Comparación y selección de modelos 3.5 Multicolinealidad 3.6 Diagnóstico 3.7 Predicción 3.8 Ejercicios", " Unidad 3 Regresión Lineal Múltiple y Polinómica Como extensión a los modelos de regresión lineal simple presentados en la Unidad 2 estudiamos los modelos de regresión lineal múltiple (RLM) y los modelos polinómicos (MP). La diferencia principal entre estos modelos y el de RLS es que estos involucran al menos dos variables predictoras de tipo numérico para tratar de explicar el comportamiento de la respuesta. Aunque la base de construcción del modelo es similar a lo tratado en la unidad anterior veremos y estudiaremos con detalle las particularidades de estos modelos. De hecho, veremos que todos los modelos se pueden expresar matemáticamente de una forma única lo que facilita su estudio, y nos permite considerar tanto modelos simples (con pocas predictoras) como los más complejos (con muchas predictoras). Antes de pasar a la presentación de estos modelos vamos a ver los ejemplos que iremos trabajando a lo largo de esta unidad. Al igual que en el modelo RLS el primer paso es la representación de los datos recogidos y realizar un pequeño estudio descriptivo sobre la posible asociación entre la respuesta y cada una de las predictoras consideradas, dado que resulta imposible realizar gráficos multivariantes de la respuesta vs todas las predictoras. Veamos los diferentes ejemplos con los que vamos a trabajar a lo largo de esta unidad. Ejemplo 1. Datos de Bosque. Para estimar la producción en madera de un bosque se suele realizar un muestreo previo en el que se realizan una serie de medidas no destructivas. Disponemos de mediciones para 20 árboles, así como el volumen (VOL) de madera que producen una vez cortados. Las variables consideradas son: HT o altura en pies, DBH el diámetro del tronco a 4 píes de altura (en pulgadas), D16 el diámetro del tronco a 16 pies de altura (en pulgadas), y VOL el volumen de madera conseguida (en pies cúbicos). El objetivo del análisis es determinar cuál es la relación entre dichas medidas y el volumen de madera, con el fin de poder predecir este último en función de las primeras. dbh &lt;- c(10.2, 13.72, 15.43, 14.37, 15, 15.02, 15.12, 15.24, 15.24, 15.28, 13.78, 15.67, 15.67, 15.98, 16.5, 16.87, 17.26, 17.28, 17.87, 19.13) d16 &lt;- c(9.3, 12.1, 13.3, 13.4, 14.2, 12.8, 14, 13.5, 14, 13.8, 13.6, 14, 13.7, 13.9, 14.9, 14.9, 14.3, 14.3, 16.9, 17.3) ht &lt;- c(89, 90.07, 95.08, 98.03, 99, 91.05, 105.6, 100.8, 94, 93.09, 89, 102, 99, 89.02, 95.09, 95.02, 91.02, 98.06, 96.01, 101) vol &lt;- c(25.93, 45.87, 56.2, 58.6, 63.36, 46.35, 68.99, 62.91, 58.13, 59.79, 56.2, 66.16, 62.18, 57.01, 65.62, 65.03, 66.74, 73.38, 82.87, 95.71) bosque &lt;- data.frame(vol, dbh, d16, ht) # Gráficos parciales datacomp = melt(bosque, id.vars = &#39;vol&#39;) ggplot(datacomp) + geom_jitter(aes(value, vol, colour = variable)) + facet_wrap(~variable, scales = &quot;free_x&quot;) + labs(x = &quot;&quot;, y = &quot;Volumen&quot;) Figura 3.1: Gráfico de dispersión de Volumen respecto de cada predictora. A simple vista todas las predictoras tienen un efecto positivo en el volumen de madera obtenido, lo cual es bastante obvio, ya que cuanto más grande sea el árbol se espera que su volumen sea más grande. Sin embargo, parece que el efecto de los diámetros es superior al de la altura del árbol (pendientes más pronunciadas) aunque resulta difícil distinguir que diámetro puede ser más relevante ya que ambos se comportan de forma similar. Podemos confirmar este hecho realizando un análisis de correlación para este banco de datos. Ejemplo 2. Datos de Concentración. Se ha llevado a cabo un experimento para estudiar la concentración presente de un fármaco en el hígado después de sufrir un tratamiento. Se piensa que las variables que pueden influir en la concentración son el peso del cuerpo, el peso del hígado y la dosis de fármaco administrada. p.cuerpo &lt;- c(176, 176, 190, 176, 200, 167, 188, 195, 176, 165, 158, 148, 149, 163, 170, 186, 146, 181, 149) p.higado &lt;- c(6.5, 9.5, 9.0, 8.9, 7.2, 8.9, 8.0, 10.0, 8.0, 7.9, 6.9, 7.3, 5.2, 8.4, 7.2, 6.8, 7.3, 9.0, 6.4) dosis &lt;- c(.88, .88, 1.0, .88, 1.0, .83, .94, .98, .88, .84, .80, .74, .75, .81, .85, .94, .73, .90, .75) concen &lt;- c(.42, .25, .56, .23, .23, .32, .37, .41, .33, .38, .27, .36, .21, .28, .34, .28, .30, .37, .46) concentracion &lt;- data.frame(p.cuerpo, p.higado, dosis, concen) # Gráficos parciales datacomp = melt(concentracion, id.vars = &#39;concen&#39;) ggplot(datacomp) + geom_jitter(aes(value, concen, colour = variable)) + facet_wrap(~variable, scales = &quot;free_x&quot;) + labs(x = &quot;&quot;, y = &quot;Concentración del fármaco&quot;) Figura 3.2: Gráfico de dispersión de la concentración del fármaco respecto de cada predictora. En este caso ninguno de los gráficos parciales muestra una gran asociación entre la concentración del fármaco y cada una de las predictoras. En todos ellos se aprecia una observación un poco más alejada del resto (concentración &gt; 0.6) que podría ser influyente en la obtención del modelo correspondiente. Ejemplo 3. Datos de Papel. Banco de datos de Papel de la unidad anterior, donde ya pudimos ver que la tendencia observada se comportaba más como una parábola (polinomio de grado 2) que como una recta. madera &lt;- c(1, 1.5, 2, 3, 4, 4.5, 5, 5.5, 6, 6.5, 7, 8, 9, 10, 11, 12, 13, 14, 15) tension &lt;- c(6.3, 11.1, 20.0, 24, 26.1, 30, 33.8, 34, 38.1, 39.9, 42, 46.1, 53.1, 52, 52.5, 48, 42.8, 27.8, 21.9) papel &lt;- data.frame(madera, tension) ggplot(papel, aes(x = madera, y = tension)) + geom_point() + labs(x = &quot;Concentración de madera&quot;, y = &quot;Resistencia del papel&quot;) Figura 3.3: Gráfico de dispersión de resistencia del papel vs concentración de madera. 3.1 Tipos de modelos Vemos las diferencias de expresión de cada uno de los modelos que trabajaremos en esta unidad. 3.1.1 Modelos de RLM Los modelos de regresión lineal múltiple surgen cuando tratamos de explicar el comportamiento de una variable predictora de tipo continuo a través de un conjunto de variables predictoras de tipo continuo mediante una función lineal. De hecho, se trata de describir dicha relación a través de una superficie, lineal en las variables explicativas, lo más próxima posible a los valores observados de la respuesta. Si \\(X_1, X_2, ..., X_p\\) son las variables predictoras el modelo viene dado por: \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon \\tag{3.1} \\end{equation}\\] Las hipótesis de este modelo es que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante \\(\\sigma^2\\). Los parámetros desconocidos de este modelo son \\((\\beta_0, \\beta_1, ... , \\beta_p, \\sigma^2)\\) donde: \\(\\beta_0\\) se conoce como interceptación y representa el valor de la respuesta cuando la variable predictora toma el valor cero, interpretándose como un efecto común en la relación entre la predictora y la respuesta. Los \\(\\beta_i\\) son las pendientes de la recta asociadas con cada predictora y representa el aumento o disminución del valor de la respuesta cuando aumentamos en una unidad el valor de la predictora. En este tipo de modelos dicho parámetro se conoce también como el efecto de la predictora sobre la respuesta. \\(\\sigma^2\\) es la varianza residual del modelo. Dada un muestra de \\(n\\) sujetos de la variable respuesta \\((y_1, ..., y_n)\\) y de las variables predictoras \\((x_{11}, ..., x_{n1}), (x_{12}, ..., x_{n2}), ..., (x_{1p}, ..., x_{np})\\), el modelo de regresión lineal múltiple se puede escribir como: \\[Y = \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ ...\\\\ y_n\\\\ \\end{array} \\right) = \\left(\\begin{array}{cccc} 1 &amp; x_{11} &amp; ... &amp; x_{1p}\\\\ 1 &amp; x_{21} &amp; ...&amp; x_{2p}\\\\ ...&amp; ... &amp; ...&amp; ...\\\\ 1 &amp; x_{n1}&amp; ...&amp; x_{np}\\\\ \\end{array} \\right) \\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ ....\\\\ \\beta_p\\\\ \\end{array} \\right) + \\left(\\begin{array}{c} e_1 \\\\ e_2 \\\\ ...\\\\ e_n \\end{array} \\right) = X \\beta + \\epsilon\\] donde \\(X\\) se denomina matriz del diseño, representando el efecto común (columna de 1’s) y el efecto de las predictoras (cada columna con los valores de la variable), y los \\(e_i\\) representan los errores aleatorias para cada uno de los sujetos de la muestra. Los bancos de datos de bosque y concentración quedarían englobados dentro de este conjunto de modelos con la siguiente propuesta: Datos de bosque \\[ \\text{vol} = \\beta_{0} + \\beta_{1}\\text{dbh} + \\beta_{2}\\text{d16} + \\beta_{3}\\text{ht} + \\epsilon \\] Datos de concentración \\[ \\text{concen} = \\beta_{0} + \\beta_{1}\\text{p.cuerpo} + \\beta_{2}\\text{p.higado} + \\beta_{3}\\text{dosis} + \\epsilon \\] 3.1.2 Modelos de RP Los modelos de regresión lineal múltiple surgen cuando tratamos de explicar el comportamiento de una variable predictora de tipo continuo a través de una variable predictora de tipo continuo mediante una función polinómica lineal. En general, los modelos polinómicos son útiles cuando se aprecia una tendencia curvilínea entre los predictores y la respuesta. Asimismo, a veces constituyen una aproximación sencilla (por serie de Taylor) a modelos complejos e incluso no-lineales. Si \\(X\\) es la variable predictora y queremos un polinomio de grado \\(k\\) el modelo viene dado por: \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_k X^k + \\epsilon \\tag{3.2} \\end{equation}\\] Las hipótesis de este modelo es que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante \\(\\sigma^2\\). Los parámetros desconocidos de este modelo son \\((\\beta_0, \\beta_1, ... , \\beta_k, \\sigma^2)\\) donde: \\(\\beta_0\\) se conoce como interceptación y representa el valor de la respuesta cuando la variable predictora toma el valor cero, interpretándose como un efecto común en la relación entre la predictora y la respuesta. Los \\(\\beta_i\\) son las pendientes de la recta asociadas con cada predictora y representa el aumento o disminución del valor de la respuesta cuando aumentamos en una unidad el valor de la predictora. En este tipo de modelos dicho parámetro se conoce también como el efecto de la potencia de la predictora sobre la respuesta. \\(\\sigma^2\\) es la varianza residual del modelo. Dada un muestra de \\(n\\) sujetos de la variable respuesta \\((y_1, ..., y_n)\\) y de la variable predictora \\((x_{11}, ..., x_{n1})\\), el modelo de regresión polinómico se puede escribir como: \\[Y = \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ ...\\\\ y_n\\\\ \\end{array} \\right) = \\left(\\begin{array}{cccc} 1 &amp; x_{11} &amp; ... &amp; x^k_{11}\\\\ 1 &amp; x_{21} &amp; ...&amp; x^k_{21}\\\\ ...&amp; ... &amp; ...&amp; ...\\\\ 1 &amp; x_{n1}&amp; ...&amp; x^k_{n1}\\\\ \\end{array} \\right) \\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ ....\\\\ \\beta_k\\\\ \\end{array} \\right) + \\left(\\begin{array}{c} e_1 \\\\ e_2 \\\\ ...\\\\ e_n \\end{array} \\right) = X \\beta + \\epsilon\\] donde \\(X\\) se denomina matriz del diseño, representando el efecto común (columna de 1’s) y el efecto del grado del polinomio (cada columna con los valores de la variable), y los \\(e_i\\) representan los errores aleatorias para cada uno de los sujetos de la muestra. El banco de datos de papel quedaría englobado dentro de este conjunto de modelos con la siguiente propuesta: \\[ \\text{tension} = \\beta_{0} + \\beta_{1}\\text{madera} + \\beta_{2}\\text{madera}^2 + \\epsilon \\] Ambos tipos de modelos se pueden describir mediante una única formulación: \\[\\begin{equation} Y = X \\beta + \\epsilon \\tag{3.3} \\end{equation}\\] 3.1.3 Expresión en R de los modelos Antes de ver como afecta a la estimación del modelo la presencia de más de una predictora o posible efecto sobre la respuesta, vamos a ver como podemos expresar los modelos RLM y MP en R. El modelo RLM dado en (3.1) se expresa como: \\[Y \\sim X_1 + X_2 + ... + X_p\\] El modelo RLM para una predictora \\(X\\) dado en (3.2) se expresa como: \\[Y \\sim X + I(X^2) + ... + I(X^k)\\] Estas expresiones son una generalización directa del modelo RLS presentado en la unidad anterior. 3.1.4 Modelo saturado y anidado En modelos donde hay más de un efecto sobre la predictora, es decir, tenemos diferentes predictoras o un modelo polinómico, debemos introducir dos conceptos que resultan muy relevantes, y que utilizaremos de forma muy habitual en la selección del mejor modelo. El modelo saturado es aquel que contiene todos los efectos asociados con las diferentes predictoras consideradas. Para los tres ejemplos considerados tendríamos: \\[\\left\\{ \\begin{array}{lc} \\text{Ejemplo 1} &amp; vol \\sim dbh + d16 + ht\\\\ \\text{Ejemplo 2} &amp; concen \\sim p.cuerpo + p.higado + dosis\\\\ \\text{Ejemplo 3} &amp; resistencia \\sim madera + madera^2\\\\ \\end{array} \\right. \\] Los modelos anidados son todos los modelos que podemos considerar y que no contienen todos los efectos asociados con las predictoras. Si tenemos un modelo con dos predictoras \\(X_1\\), y \\(X_2\\) lo modelos anidados del modelo saturado \\[Y \\sim X_1 +X_2\\] son: \\[\\left\\{ \\begin{array}{lc} \\text{con } X_1 &amp; Y \\sim X_1\\\\ \\text{con } X_2 &amp; Y \\sim X_2\\\\ \\text{Sin ninguna} &amp; Y \\sim 1\\\\ \\end{array} \\right. \\] Todos ellos están “anidados” dentro del modelo saturado y reflejan diferente información. El primero refleja que la respuesta sólo está relacionada con \\(X_1\\), el segundo que la respuesta está relacionada con \\(X_2\\), y el último refleja que no hay ninguna predictora relacionada con la respuesta. Debemos tener en cuenta que al incluir más de una predictora debemos decidir si todas ellas son relevantes para explicar el comportamiento de la respuesta, o bien si podemos prescindir de algunas de ellas. La consideración de los diferentes modelos anidados varía en función del modelo con el que trabajemos. En el caso de los de RLM el orden de los modelos anidados no es relevante, pero sin embargo si lo es los modelos polinómicos. No tiene sentido considerar un modelo en el que sólo se incluya el efecto del polinomio de grado 2 pero que no se incluya el de grado 1. Por su propia construcción cuando consideramos un modelo polinómico de grado \\(k\\) se deben considerar obligatoriamente todos los grados desde \\(1\\) hasta \\(k-1\\). Si consideramos un modelo polinómico de grado 4, el orden de los modelos anidados viene dado por: \\[\\left\\{ \\begin{array}{ll} \\text{saturado } &amp; Y \\sim X + X^2 + X^3 + X^4\\\\ \\text{grado 3 } &amp; Y \\sim X + X^2 + X^3 \\\\ \\text{grado 2 } &amp; Y \\sim X + X^2 \\\\ \\text{grado 1 } &amp; Y \\sim X \\\\ \\text{sin efectos } &amp; Y \\sim 1 \\\\ \\end{array} \\right. \\] A la hora de ajustar un modelo polinómico, siempre serán preferibles modelos con órdenes pequeños antes que grandes (principio de parsimonia o simplicidad). Siempre trataremos de seleccionar le modelo con un orden más pequeño, es decir, con menos efectos pero con igual predictivo que el modelo saturado. 3.2 Estimación e inferencia Los procesos de estimación e inferencia del RLM y MP se basan en los mismos principios que los del modelo RLS estudiados en la unidad anterior. De hecho, las hipótesis sobre los errores de incorrelación, varianza constante y media cero son suficientes para obtener el ajuste por mínimos cuadrados del modelo propuesto. La normalidad es necesaria para obtener las inferencias y concluir sobre su fiabilidad. Sin embargo, este tipo de modelos de regresión que consideran más de una predictora adolecen de un problema que puede ser muy relevante en su análisis. Dado que todas las predictoras no vendrán medidas en la misma escala de medida, el modelo obtenido (más concretamente los coeficientes del modelo) exhibe una dependencia de dicha escala que puede provocar que una variable con una variabilidad pequeña aparezca con un coeficiente grande en el modelo estimado. Para evitar esa dependencia se suele trabajar con las variables estandarizadas, es decir, corregidas por su media y desviación típica para eliminar los efectos de escala. Aunque en el apartado teórico mostraremos la solución para las variables en escala original, en la parte práctica mostraremos los coeficientes para las variables estandarizadas y veremos los cambios entre ambos modelos. Para denotar las variables transformadas añadiremos el prefijo Z al nombre de la predictora a la hora de escribir los modelos obtenidos. 3.2.1 Mínimos cuadrados Para estimar \\(\\beta\\) seguimos el criterio de minimizar la suma de cuadrados debida al error, esto es, \\[ min_{\\beta} \\quad \\epsilon&#39;\\epsilon = min_{\\beta} \\quad (Y-X \\beta)&#39;(Y-X \\beta) = min_{\\beta} \\quad Y&#39;Y -2 \\beta&#39;X&#39;Y + \\beta&#39;X&#39;X\\beta. \\] Tras derivar la expresión anterior respecto de \\(\\beta\\) e igualarlo a cero, se obtiene el estimador de mínimos cuadrados de \\(\\beta\\) para el modelo (3.3), \\(\\hat{\\beta}\\), resolviendo las \\(p\\) ecuaciones normales: \\[\\begin{equation} X&#39;X \\beta=X&#39;Y. \\tag{3.4} \\end{equation}\\] A la hora de resolver (3.4), se pueden presentar dos situaciones: Las \\(p\\) ecuaciones normales que resultan de (3.4) no son independientes y por lo tanto no existe la inversa de \\(X&#39;X\\). Esto ocurre cuando las variables explicativas no son independientes entre sí. Entonces el modelo ha de expresarse en términos de menos parámetros (modificarse) o han de incorporarse restricciones adicionales sobre los parámetros para dar una matriz no singular. Cuando \\((X&#39;X)\\) es singular, el estimador de \\(\\beta\\) se obtiene a partir de una matriz inversa generalizada \\(X&#39;X\\), \\((X&#39;X)^{-}\\), como: \\[\\begin{equation} \\hat{\\beta}=(X&#39;X)^{-} X&#39;Y. \\tag{3.5} \\end{equation}\\] Así, diferentes elecciones de la inversa generalizada \\((X&#39;X)^{-}\\) producen diferentes estimaciones de \\(\\beta\\). Sin embargo, el modelo ajustado es el mismo, esto es, \\(\\hat{y}=X \\hat{\\beta}\\) es invariante a la inversa generalizada elegida. Las \\(p\\) ecuaciones normales son independientes, con lo que \\(X&#39;X\\) es no singular y existe su inversa. El estimador de mínimos cuadrados resulta: \\[\\begin{equation} \\hat{\\beta}=(X&#39;X)^{-1} (X&#39;Y). \\tag{3.6} \\end{equation}\\] 3.2.2 Propiedades Cuando prescindimos de la hipótesis de normalidad de los errores, obtenemos la estimación por mínimos cuadrados, que tiene las siguientes propiedades: El estimador de mínimos cuadrados \\(\\hat{\\beta}\\) minimiza \\(\\epsilon&#39;\\epsilon\\), independientemente de la distribución de los errores. La hipótesis de normalidad se añade para justificar las inferencias basadas en estadísticos \\(t\\) o \\(F\\). Los elementos de \\(\\hat{\\beta}\\) son funciones lineales de las observaciones \\(y_1, \\ldots, y_n\\) y son estimadores insesgados de mínima varianza, sea cual sea la distribución de los errores. Así tenemos: \\[ E(\\hat{\\beta})=\\beta \\ \\quad \\mbox{ y }\\ \\quad Var(\\hat{\\beta})=\\sigma^2 (X&#39;X)^{-1} . \\] Las estimaciones/predicciones de la variable respuesta \\(y\\) se obtienen con el modelo lineal ajustado: \\[ \\hat{y}=X\\hat{\\beta}. \\] Los residuos \\(e=y-X\\hat{\\beta}\\) verifican: \\(\\sum_{i=1}^n e_i \\hat{y}_i = 0 \\ \\Leftrightarrow \\ e&#39;\\hat{y}=\\hat{y}&#39; e = 0\\) La ortogonalidad entre los vectores de estimaciones y de residuos, \\(\\hat{y}\\) y \\(e\\) respectivamente, implica el teorema de Pitágoras: \\[ |y|^2=|\\hat{y}|^2+|e|^2 \\ \\Leftrightarrow \\ \\sum_{i=1}^n y_i^2= \\sum_{i=1}^n \\hat{y}_i^2 + \\sum_{i=1}^n e_i^2. \\] \\(\\sum_{i=1}^n e_i = 0 \\ \\Leftrightarrow \\ e&#39;\\mathbf{1}=\\mathbf{1}&#39; e = 0\\) 3.2.3 Máxima verosimilitud Como ocurría en el modelo RLS el estimador de mínimos cuadrados coincide con el máximo verosímil, ya que bajo la hipótesis de normalidad de los errores aleatorios, la verosimilitud conjunta tiene la forma: \\[ L(\\beta;y) \\propto f(y;\\beta) \\propto \\left(\\frac{1}{\\sigma^2}\\right)^{n/2} \\quad exp\\left\\{-\\frac{(y-X\\beta)&#39;(y-X\\beta)}{2 \\sigma^2}\\right\\}, \\] y maximizar la verosimilitud es equivalente a minimizar la log-verosimilitud cambiada de signo, que coincide con la suma de cuadrados del error para un valor fijo de \\(\\sigma^2\\). De nuevo utilizaremos la hipótesis de normalidad para proceder con el proceso de inferencia sobre el modelo (3.3). 3.2.4 Inferencia Para establecer los procedimientos de inferencia asociados con el modelo (3.3) es preciso incorporar la hipótesis de normalidad de los errores. A partir de ella podemos obtener la distribución de los estadísticos y estimadores involucrados en el proceso de inferencia con el modelo lineal ajustado. 3.2.4.1 Varianza del modelo Podemos obtener un estimador de \\(\\sigma^2\\) basado en la variabilidad que ha quedado sin explicar por el modelo, cuantificada por lo que llamamos suma de cuadrados residual SSE: \\[ \\begin{array}{ll} SSE=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2 &amp;= e&#39;e \\\\ &amp;= y&#39;y - 2 \\hat{\\beta}&#39; X&#39;y + \\hat{\\beta}&#39;X&#39;X \\hat{\\beta} \\\\ &amp;= y&#39;y - \\hat{\\beta}&#39; X&#39;y. \\end{array} \\] Puesto que en el modelo lineal propuesto se estiman \\(p\\) parámetros, la suma de cuadrados residual \\(SSE\\) tiene asociados \\(n-p\\) grados de libertad (el número de datos menos el de coeficientes del modelo). El cociente entre \\(SSE\\) y sus grados de libertad, \\(n-p\\), es el estimador de mínimos cuadrados de \\(\\sigma^2\\), y es además, un estimador insesgado: \\[ \\hat{\\sigma}^2=s^2 = MSE=\\frac{SSE}{n-p}. \\] Asumiendo que el modelo es cierto, la distribución de probabilidad de la varianza del modelo es proporcional a una \\(\\chi^2\\) con \\(n-p\\) grados de libertad, \\[ \\frac{(n-p)s^2}{\\sigma^2} \\sim \\chi^2_{n-p}. \\] 3.2.4.2 Coeficientes del modelo Bajo la hipótesis de normalidad de los errores, tenemos que el estimador máximo-verosímil \\(\\hat{\\beta}\\) tiene una distribución normal: \\[ \\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X&#39;X)^{-1}). \\] Esto implica que la distribución marginal de cada uno de los coeficientes de la regresión, \\(\\hat{\\beta}_i\\), también es normal, \\[ \\hat{\\beta}_i \\sim N(\\beta_i, \\sigma^2 C^{X}_{ii}), \\ \\ i=0, \\ldots, p-1, \\] con \\(C^{X}_{ii}\\) el \\(i\\)-ésimo elemento de la diagonal de la matriz \\((X&#39;X)^{-1}\\). En consecuencia, para construir intervalos de confianza o resolver contrastes sobre cada uno de los coeficientes del modelo, individualmente, podemos utilizar estadísticos \\(t\\) que se distribuyen con una distribución t de Student con \\(n-p\\) grados de libertad: \\[ \\frac{\\hat{\\beta}_i-\\beta_i}{\\sqrt{s^2 C^X_{ii}}} \\ \\sim \\ t_{n-p}, \\ \\ i=1, \\ldots, n, \\] construidos a partir del estimador de \\(\\sigma^2\\), \\(s^2\\). Así, un intervalo de confianza para un coeficiente de interés \\(\\beta_i\\) al nivel \\((1-\\alpha)100\\%\\) viene dado por: \\[ \\hat{\\beta}_i \\pm t_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 \\ C^X_{ii}}, \\] donde \\(t_{(n-p, 1-\\alpha/2)}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución \\(t\\) con \\(n-p\\) grados de libertad. El contraste \\(H_0:\\beta_i=0\\) se resolverá con el rechazo de \\(H_0\\) a nivel \\(1-\\alpha\\) si \\[ |\\hat{\\beta}_i| &gt; t_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 \\ C^X_{ii}}. \\] Cuando se pretende obtener intervalos de confianza para varios coeficientes del modelo a la vez, es recomendable ser más conservador. Hay diversas soluciones propuestas para realizar “comparaciones múltiples”, esto es, testar todos los coeficientes a la vez, y obtener regiones de confianza conjuntas. Quizá el más conocido es el ajuste de Bonferroni, basado en sustituir el cuantil \\(t_{(n-p, 1-\\alpha/2)}\\) en la expresión anterior, por \\(t_{(n-p, 1-\\alpha/2q)}\\), si \\(q\\) es el número de coeficientes para los que se desea una estimación en intervalo. Se obtendrán entonces unos intervalos de confianza ‘ensanchados’ respecto a los intervalos de confianza individuales. Si no tenemos ninguna prioridad particular sobre determinados coeficientes, lo lógico será obtener conjuntamente los intervalos de confianza para todos los coeficientes del modelo, esto es, \\(q=p\\). Otra opción para la estimación en intervalo es construir una región de confianza conjunta para todos los parámetros \\(\\beta\\) del modelo, determinando los puntos \\(\\beta\\) de la elipse definida por: \\[ (\\beta-\\hat{\\beta})&#39;X&#39;X (\\beta-\\hat{\\beta})= (p+1) \\ s^2 \\ F_{(p, n-p, 1-\\alpha)}, \\] donde \\(F_{(p, n-p, 1-\\alpha)}\\) es el cuantil \\(1-\\alpha\\) de una distribución \\(F\\) con \\(p\\) y \\(n-p\\) grados de libertad. Es posible construir regiones de confianza conjuntas de este tipo para cualquier subconjunto de coeficientes del modelo. Bastará variar adecuadamente los grados de libertad \\(p\\) y \\(n-p\\). Estas regiones acaban siendo complicadas de interpretar, especialmente cuando la dimensión de \\(\\beta\\) es grande. Sin embargo, en la práctica no se suele hacer cuando el número de predictoras es elevado. 3.2.5 Ejemplos Realizamos el proceso de estimación e inferencia para los modelos saturados correspondientes a los ejemplos presentados al inicio de esta unidad. Obtendremos el modelo para las predictoras en escala original y estandarizadas, representaremos los intervalos de confianza de los coeficientes del modelo, y obtendremos el ajuste final del modelo. Utilizaremos la función tab_model de la libreria sjplot para el análisis de los coeficientes del modelo, ya que nos proporciona más información que la función glm_coef. 3.2.5.1 Datos de Bosque Ajustamos un modelo RLM para el conjunto de datos bosque. # Ajuste del modelo fit.bosque &lt;- lm(vol ~ dbh + d16 + ht, data = bosque) # Inferencia sobre los parámetros del modelo glm_coef(fit.bosque) ## Coefficient Pr(&gt;|t|) ## (Intercept) -108.58 (-138.53, -78.62) &lt; 0.001 ## dbh 1.63 (-1.29, 4.54) 0.254 ## d16 5.67 (2.47, 8.87) 0.002 ## ht 0.69 (0.45, 0.94) &lt; 0.001 de forma que el ajuste obtenido viene dado por: \\[ \\widehat{\\text{vol}} = -108.58 + 1.63*\\text{dbh} + 5.67*\\text{d16} + 0.69*\\text{ht} \\] La interpretación de los coeficientes nos indica que el valor predicho de volumen aumenta en 1.63 unidades por el aumenta de una unidad de DBH, en 5.67 unidades por cada unidad de D16, y 0.69 unidades por cada unidad de HT. A la vista de los contrastes individuales (p-valores) podemos concluir que los coeficientes asociados con D16 y HT son significativos, es decir, sus coeficientes si sólo esa variable estuviera presente en el modelo serían distintos de cero. Esta información se ve reforzada por los intervalos de confianza individuales, que además muestran que dichos coeficientes son positivos indicando que el VOL aumenta directamente al aumentar los valores de D16 y HT. Por tanto, el modelo anidado dado por: \\[vol \\sim d16 + ht\\] podría ser igualmente válido que el que contiene todas las predictoras. A continuación, se presenta el ajuste obtenido para cada variable de forma marginal. Representamos gráficamente la estimación e intervalo de confianza de los coeficientes del modelo para apreciar los efectos descritos: # Gráfico del ajuste plot_model(fit.bosque, show.values = TRUE, vline.color = &quot;yellow&quot;) Comparamos los resultados con los del modelo estandarizado. La tabla proporciona las estimaciones e intervalo de confianza al 95% de los parámetros en la escala original (Estimates y CI), las estimaciones y CI de los coeficientes del modelo estandarizado (std.Beta y standarized CI), y el p-valor asociado a cada coeficiente # Inferencia sobre los parámetros del modelo tab_model(fit.bosque, show.std = TRUE, show.r2 = FALSE) vol Predictors Estimates std. Beta CI standardized CI p (Intercept) -108.58 0.00 -138.56 – -78.60 -0.10 – 0.10 &lt;0.001 dbh 1.63 0.21 -0.55 – 3.80 -0.07 – 0.50 0.133 d16 5.67 0.65 3.12 – 8.22 0.36 – 0.95 &lt;0.001 ht 0.69 0.24 0.35 – 1.04 0.12 – 0.36 0.001 Observations 20 Se aprecia como la variable más relevante para explicar el comportamiento del volumen de madera es el diámetro del tronco a 16 pies de altura con un coeficiente estandarizado de 0.65, que es tres veces superior a los coeficientes de las otras predictoras. Podemos ver el gráfico de las estimaciones para el modelo estandarizado: # Gráfico del ajuste plot_model(fit.bosque, show.values = TRUE, vline.color = &quot;yellow&quot;, type = &quot;std&quot;) Por último, obtenemos los gráficos del modelo ajustado. Para obtener estos gráficos se asume como valor para la predictoras que no están en el gráfico igual a su media muestral Por ejemplo, para el gráfico de vol con respecto dbh utilizamos el modelo: \\[ \\widehat{\\text{vol}} = -108.58 + 1.63*\\text{dbh} + 5.67*\\overline{\\text{d16}} + 0.69*\\overline{\\text{ht}} \\] donde \\(\\overline{\\text{d16}}\\) y \\(\\overline{\\text{ht}}\\) son respectivamente las medias muestrales de d16 y ht. Los gráficos para cada predictora son: # Gráfico del ajuste plot_model(fit.bosque, &quot;pred&quot;, ci.lvl = NA, show.data = TRUE, title = &quot;Modelo ajustado&quot;) ## $dbh ## ## $d16 ## ## $ht 3.2.5.2 Datos de concentración Ajustamos un modelo RLM para el conjunto de datos de concentración. # Ajuste del modelo fit.concen &lt;- lm(concen ~ p.cuerpo + p.higado + dosis, data = concentracion) # Inferencia tab_model(fit.concen, show.std = TRUE, show.r2 = FALSE) concen Predictors Estimates std. Beta CI standardized CI p (Intercept) 0.27 0.00 -0.15 – 0.68 -0.43 – 0.43 0.192 p.cuerpo -0.02 -3.96 -0.04 – -0.00 -7.13 – -0.79 0.018 p.higado 0.01 0.20 -0.02 – 0.05 -0.31 – 0.70 0.419 dosis 4.18 4.05 0.93 – 7.42 0.90 – 7.20 0.015 Observations 19 de forma que el ajuste obtenido viene dado por: \\[ \\widehat{\\text{concen}} = 0.27 - 0.02*\\text{p.cuerpo} + 0.01*\\text{p.higado} + 4.18*\\text{dosis} \\] La interpretación de los coeficientes nos indica que el valor predicho de la concentración aumenta con el peso del hígado y dosis suministrada, pero disminuye con el peso del cuerpo. Los valores tan pequeños de los coeficientes asociados a los pesos podrían indicar que dichas variables no tienen gran capacidad predictiva, pero hay que tener en cuenta que dichas variables están medidas en un escala distinta de la dosis, y que por tanto la estimación de los coeficientes se ve influenciada por dicha escala. Si nos fijamos en los coeficientes estandarizados apreciamos la misma tendencia en todas la preditoras (signo del coeficiente) pero vemos como tanto el p.cuerpo como la dosis tienen un peso similar para explicar el comportamiento de la concentración. Del análisis de los contrastes individuales podríamos descartar la variable peso del hígado (p-valor &gt;0.05) para explicar el comportamiento de la concentración del compuesto (algo que deberemos comprobar posteriomente), y considerar el resto de predictoras en el modelo anidado: \\[concen \\sim p.cuerpo + dosis\\] Los gráficos de los coeficientes del modelo (no estandarizados y estandarizados) nos permite ver gráficamente estas conclusiones: # Gráfico del ajuste sin estandarizar plot_model(fit.concen, show.values = TRUE, vline.color = &quot;yellow&quot;) # Gráfico del ajuste estandarizados plot_model(fit.concen, show.values = TRUE, vline.color = &quot;yellow&quot;, type = &quot;std&quot;) En el gráfico de coeficientes sin estandarizar se aprecia el efecto de trabajar en la escala original, dado que el intervalo de confianza del peso del cuerpo es inapreciable y puede llegar a parecer que no tiene efecto sobre la concentración, lo que si queda más claro en el gráfico de los coeficientes estandarizados. ¿qué ocurre cuando realizamos el gráfico del ajuste para este conjunto de datos? 3.2.5.3 Datos de papel Ajustamos un modelo MP de grado 2 para el conjunto de datos de papel presentados en la unidad anterior. # Ajuste del modelo fit.papel &lt;- lm(tension ~ madera + I(madera^2), data = papel) # Inferencia tab_model(fit.papel, show.std = TRUE, show.r2 = FALSE) tension Predictors Estimates std. Beta CI standardized CI p (Intercept) -6.67 0.81 -13.88 – 0.53 0.58 – 1.03 0.067 madera 11.76 0.79 9.64 – 13.89 0.63 – 0.96 &lt;0.001 madera^2 -0.63 -0.85 -0.77 – -0.50 -1.03 – -0.68 &lt;0.001 Observations 19 de forma que el ajuste obtenido viene dado por: \\[ \\widehat{\\text{tension}} = -6.67 + 11.76*\\text{madera} - 0.63*\\text{madera}^2 \\] El ajuste obtenido es una parábola invertida (coeficiente negativo en la potencia 2) tal y como se observaba en el gráfico de los datos (Figura 2.2). En este tipo de modelos el análisis inferencial se debe centrar en el estudio del orden más alto, para determinar si es adecuado o si podríamos construir un modelo de un orden más simple. La significatividad del coeficiente (p-valor &lt; 0.05) indica que dicho grado es necesario en el modelo. De hecho, los coeficientes estandarizados muestran un efecto similar tanto en el grado 1 como en grado 2. Lo vemos gráficamente: # Gráfico del ajuste sin estandarizar plot_model(fit.papel, show.values = TRUE, vline.color = &quot;yellow&quot;) # Gráfico del ajuste estandarizados plot_model(fit.papel, show.values = TRUE, vline.color = &quot;yellow&quot;, type = &quot;std&quot;) Vemos el gráfico del ajuste obtenido: # Gráfico del ajuste plot_model(fit.papel, &quot;pred&quot;, ci.lvl = NA, show.data = TRUE, title = &quot;Modelo ajustado&quot;) ## $madera La tendencia ajustada se corresponde con la observada en los datos del experimento. 3.3 Bondad del ajuste En este punto presentamos los procedimientos de bondad de ajuste habituales en los modelos de regresión: Análisis de la tabla Anova, el coeficiente de determinación, y el coeficiente de determinación ajustado. Por el momento nos centraremos en el estudio de bondad de ajuste del modelo saturado. En las secciones siguientes veremos como determinar el conjunto de predictoras más relevantes para explicar el comportamiento de la respuesta, y utilizaremos de nuevo estos criterios para valorar el ajuste obtenido. 3.3.1 Tabla ANOVA Habitualmente, la primera forma de juzgar la calidad del ajuste obtenido consiste en valorar la variabilidad de la respuesta que se ha podido explicar con el modelo propuesto. En lo que sigue, asumiremos que el modelo ajustado es \\(\\hat{y}=X\\hat{\\beta}\\), donde la matriz de diseño \\(X\\) tiene por columnas todas las variables explicativas consideradas, sean continuas o dummies definidas para representar algún factor. En todo caso, suponemos que hemos estimado \\(p\\) coeficientes, esto es, \\(\\hat{\\beta} \\in \\mathbb{R}^p\\). Descomponemos pues la variabilidad de las observaciones \\(y\\), en la parte explicada por el modelo ajustado y corregida por la media de los datos (suma de cuadrados de la regresión), \\(SSR\\), y la parte residual (suma de cuadrados debida al error) que ha quedado sin explicar, \\(SSE\\): \\[ \\underbrace{(y-\\bar{y}1)&#39;(y-\\bar{y}1)}_{S_{yy}}= \\underbrace{(\\hat{y}-\\bar{y}1)&#39;(\\hat{y}-\\bar{y}1)}_{SSR} +\\underbrace{e&#39;e}_{SSE}, \\] donde \\(\\bar{y}=\\sum_i y_i/n\\). Los grados de libertad asociados a \\(SSR\\) son \\(p-1\\), pues se pierde un parámetro al corregir la estimación \\(\\hat{y}\\) (obtenida a partir de \\(p\\) parámetros) por la media \\(\\bar{y}\\). La suma de cuadrados del error \\(SSE\\) tiene asociados \\(n-p\\) grados de libertad, esto es, el número de datos menos el número de parámetros estimados en el modelo. Al dividir las sumas de cuadrados por sus grados de libertad respectivos, obtenemos los cuadrados medios correspondientes, \\(MSR=SSR/(p-1)\\) y \\(MSE=SSE/(n-p)\\), que nos resultan útiles para valorar la bondad del ajuste. El test de bondad de ajuste propone el contraste: \\[\\begin{equation} H_0: \\beta_1=\\beta_2=\\ldots=\\beta_{p-1}=0, \\qquad H_1: \\mbox{ algún } \\beta_i \\neq 0. \\tag{3.7} \\end{equation}\\] Cuando el modelo es bueno, \\(MSR\\) y \\(MSE\\) siguen sendas distribuciones proporcionales a chi-cuadrados independientes (con la misma constante de proporcionalidad \\(\\sigma^2\\)), con \\(p-1\\) y \\(n-p\\) grados de libertad respectivamente; de ahí que su cociente (libre ya de la constante desconocida \\(\\sigma^2\\)) resulta tener una distribución \\(F\\) con \\(p-1\\) y \\(n-p\\) grados de libertad: \\[\\begin{equation} F=\\frac{SSR/(p-1)}{SSE/(n-p)}=\\frac{MSR}{MSE} \\ \\sim \\ F_{(p-1, n-p)}. \\tag{3.8} \\end{equation}\\] Así, con dicho estadístico \\(F\\) contrastamos si la variabilidad explicada por el modelo ajustado es suficientemente grande comparada con la que queda sin explicar (la de los residuos); en otras palabras, si el modelo ajustado es significativo para explicar la variabilidad de los datos. Si el p-valor asociado al estadístico F es inferior a la significatividad considerada (generalmente 0.05), rechazamos que el modelo propuesto no explique conjuntamente la respuesta, y concluimos a favor de que algunas de las covariables contienen información significativa para predecir la respuesta, esto es, a favor de la bondad del ajuste. En otro caso, no podemos garantizar significativamente la bondad del modelo propuesto. La Tabla de Anova es la forma habitual de presentar toda la información de las sumas, medias de cuadrados, estadísticos \\(F\\) y p-valores asociados al contraste de bondad de ajuste del modelo. La salida de la Tabla Anova que proporciona R no es exactamente la habitual presentada en todos los libros. En dicha tabla, en lugar de contrastar globalmente el ajuste a través de la suma de cuadrados asociada a la regresión, se contrasta secuencialmente la significatividad de cada una de las covariables a la hora de explicar la variable respuesta en presencia de las variables que ya han sido incorporadas al modelo (las que quedan por encima en la salida). Sin embargo, con dicha salida es posible calcular el test F de bondad de ajuste. Utilizaremos diferentes funciones para obtener el estadístico F y el p-valor asociado, y las funciones anova() para obtener la descomposición de la tabla ANOVA. 3.3.2 Coeficiente determinación El coeficiente de determinación, \\(R^2\\), se define como la parte proporcional de la variabilidad de los datos que es explicada por el modelo ajustado: \\[\\begin{equation} R^2 = \\frac{SSR}{S_{yy}} = 1 - \\frac{SSE}{S_{yy}} \\tag{3.9} \\end{equation}\\] Por definición tenemos que \\(0 \\leq R^2 \\leq 1\\). Un ajuste perfecto de los datos produciría \\(R^2=1\\). Si ninguna de las variables predictoras \\(X_1, \\ldots, X_{p-1}\\) es útil para explicar la respuesta \\(Y\\), entonces \\(R^2=0\\). Siempre es posible conseguir \\(R^2\\) suficientemente grande, simplemente añadiendo más términos en el modelo. Por ejemplo, si hay más de un valor de \\(y\\) para un mismo \\(x\\) observado, un polinomio de grado \\(n-1\\) proporcionará un ajuste “perfecto” (\\(R^2=1\\)) para \\(n\\) datos. Cuando esto no ocurre y hay únicamente un valor de \\(y\\) por cada \\(x\\), \\(R^2\\) nunca puede ser igual a 1 porque el modelo no puede explicar la variabilidad debida al error puro. Aunque \\(R^2\\) siempre aumenta cuando añadimos una variable explicativa al modelo, esto no significa necesariamente que el nuevo modelo sea superior al antiguo, es decir, que dicha variable sea útil para explicar mejor los datos. A pesar de que la suma de cuadrados residual \\(SSE\\) del nuevo modelo se reduce por una cantidad igual al anterior \\(MSE\\), el nuevo modelo tendrá un \\(MSE\\) mayor debido a que pierde un grado de libertad. Por lo tanto, el nuevo modelo será de hecho, peor que el antiguo. En consecuencia, algunos analistas prefieren utilizar una versión ajustada del estadístico \\(R^2\\). El \\(R^2\\) ajustado penaliza los modelos que incorporan variables innecesarias dividiendo las sumas de cuadrados por sus grados de libertad, esto es, \\[\\begin{equation} R^2_a=1-\\frac{SSE/(n-p)}{S_{yy}/(n-1)}=1-(1-R^2)\\left(\\frac{n-1}{n-p}\\right). \\tag{3.10} \\end{equation}\\] \\(R^2_a\\) es preferible a \\(R^2\\) cuando sus valores difieren mucho. Su interpretación tiene algún problema debido a que puede tomar valores negativos; esto ocurre cuando el estadístico \\(F\\) toma valores inferiores a 1 (o produce p-valores mayores que 0.05). 3.3.3 Ejemplos Analizamos los diferentes ejemplos con los que venimos trabajando a lo largo de la unidad. 3.3.3.1 Datos de Bosque Bondad de ajuste para los datos bosque. # Bondad del ajuste glance(fit.bosque) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.959 0.951 3.10 125. 2.59e-11 4 -48.7 107. 112. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; # Tabla ANOVA anova(fit.bosque) ## Analysis of Variance Table ## ## Response: vol ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## dbh 1 3085.79 3085.79 322.064 5.051e-12 *** ## d16 1 331.85 331.85 34.635 2.303e-05 *** ## ht 1 173.42 173.42 18.100 0.0006056 *** ## Residuals 16 153.30 9.58 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Tanto el \\(R^2\\) como el \\(R^2\\) ajustado muestran porcentajes del 95% indicando que el modelo ajustado tiene buena capacidad explicativa. Además, el test \\(F\\) de la regresión resulta significativo (p-valor &lt; 0.05) indicando que las predictoras consideradas pueden ser utilizadas para describir el comportamiento del volumen. Los tests individuales de la tabla ANOVA 3.3.3.2 Datos de Concentración Bondad de ajuste para los datos de concentración # Bondad del ajuste glance(fit.concen) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.364 0.237 0.0773 2.86 0.0720 4 23.9 -37.9 -33.1 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; # Tabla ANOVA anova(fit.concen) ## Analysis of Variance Table ## ## Response: concen ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## p.cuerpo 1 0.003216 0.003216 0.5383 0.47446 ## p.higado 1 0.003067 0.003067 0.5134 0.48467 ## dosis 1 0.044982 0.044982 7.5296 0.01507 * ## Residuals 15 0.089609 0.005974 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El \\(R^2\\) y el \\(R^2\\) ajustado muestran valores bastante bajos indicando poco poder explicativo. Además, el p-valor del test \\(F\\) resulta no significativo indicando que todos los coeficientes del modelo podrían ser considerados iguales a cero. Esto contradice lo visto durante el proceso de estimación de los parámetros del modelo y la tabla ANOVA obtenida donde se aprecia que el efecto asociado con dosis resulta significativo. Este comportamiento puede ser debido al considerar más predictoras de las necesarias o simplemente a que las predictoras no son adecuadas para explicar el comportamiento de la concentración. En el punto siguiente, donde se tratará la selección del mejor modelo, analizaremos este modelo con más detalle y podremos concluir sobre la validez de las predcitoras. 3.3.3.3 Datos de Papel Bondad de ajuste para los datos de papel # Bondad del ajuste glance(fit.papel) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.909 0.897 4.42 79.4 4.91e-9 3 -53.6 115. 119. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; # Tabla ANOVA anova(fit.papel) ## Analysis of Variance Table ## ## Response: tension ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## madera 1 1043.43 1043.43 53.40 1.758e-06 *** ## I(madera^2) 1 2060.82 2060.82 105.47 1.894e-08 *** ## Residuals 16 312.64 19.54 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 En términos de \\(R^2\\) y \\(R^2\\) ajustado el modelo tiene buena capacidad explicativa (porcentajes del 90%) y el test \\(F\\) resulta significativo, indicando que alguno de los coeficientes del modelo debe ser considerado distinto de cero. Dado que se trata de un modelo polinómico nos debemos fijar de la significatividad del término de mayor orden en la tabla ANOVA. En este caso dicho efecto resulta significativo indicando que el modelo propuesto de grado 2 es necesario para explicar el comportamiento de la tensión del papel. 3.4 Comparación y selección de modelos La modelización de datos es siempre una faena tediosa debido a la innumerable cantidad de alternativas posibles. Está por determinar el tipo de modelo, las transformaciones más adecuadas, identificar las variables más relevantes, descartar las innecesarias, y posteriormente abordar la diagnosis y validación del modelo, que trataremos en las secciones siguientes. Si el modelo está mal especificado, las estimaciones de los coeficientes pueden resultar considerablemente sesgadas. Una buena especificación del modelo es un trabajo, en general, complicado de obtener. Si se ha optado por la modelización lineal de una respuesta en función de una serie de posibles variables predictoras, y el objetivo es seleccionar el mejor subconjunto de predictores para explicar la respuesta, el planteamiento es siempre el de obtener “buenas” predicciones. Sin embargo, sabemos que cuantos más predictores incluyamos en el modelo, mejores predicciones tendremos (menos sesgo), pero a la vez menos precisión sobre ellas (ya que la varianza es proporcional al número de variables predictoras en el modelo). Para la selección del “mejor” modelo habremos de llegar a un compromiso entre estos dos propósitos. Tratamos pues la selección de variables como un problema de comparación y selección de modelos. Vamos a presentar diversos criterios para comparar modelos y seleccionar el mejor modelo de entre dos alternativas. En ocasiones todos darían los mismos resultados, pero generalmente no, por lo que habrá de ser el analista el que decida qué criterio utilizar en función de sus intereses prioritarios. La selección del modelo se puede realizar con múltiples criterios pero aquí presentamos los más habituales basados en: la significatividad de los predictores que están presentes en el modelo y los que no; Los estadísticos \\(AIC\\) (Akaike Information Criteria) y \\(BIC\\) (Bayesian Information Criteria); Una vez seleccionado el “mejor” modelo según el criterio elegido, habremos de proseguir la confirmación del mismo realizando la diagnosis y la validación del modelo, que puede fallar en algún paso, lo que nos conduciría de nuevo a la reformulación del modelo (y todos los pasos que le siguen), optando por aquellas correcciones y/o transformaciones de variables sugeridas en el diagnóstico. La consecución del mejor modelo será pues, un procedimiento iterativo, basado en selección y valoración de la calidad del ajuste, diagnóstico y validación. En muchas situaciones prácticas nos conformaremos con encontrar el modelo que tenga un funcionamiento más adecuado aunque no sea prefecto. 3.4.1 Significatividad de los predictores Este procedimiento se basa en la comparación de modelos basada en las sumas de cuadrados y el test \\(F\\) resultante de compararlas. Supongamos que tenemos ajustado un modelo definido por \\(p\\) coeficientes. Si queremos valorar la contribución que hacen al ajuste de los datos un subconjunto de \\(q\\) variables predictoras adicionales, debemos plantear el contraste \\[\\begin{equation} H_0: y=X_p \\beta_p + \\epsilon, \\ \\ vs. \\ \\ H_1:y=X_{p+q} \\beta_{p+q} + \\epsilon. \\tag{3.11} \\end{equation}\\] Para resolver el contraste (3.11) se utiliza una versión del test \\(F\\) de regresión de la tabla ANOVA. Para resolverlo basta con ajustar los modelos con \\(p\\) y \\(p+q\\) predictoras, para obtener las sumas de cuadrados del error respectivas, \\(SSE(p)\\) y \\(SSE(p+q)\\). Su diferencia representa la reducción del error debida a la inclusión de los \\(q\\) regresores adicionales, y bajo \\(H_0\\) tienen una distribución chi-cuadrado, independiente de \\(SSE(p)\\). Se puede definir entonces un estadístico \\(F\\) para realizar la comparación de modelos y resolver el contraste (3.11), dado por: \\[\\begin{equation} F_q=\\frac{(SSE(p)-SSE(p+q))/q}{SSE(p)/(n-p)} \\sim F_{q, n-p}. \\tag{3.12} \\end{equation}\\] Las \\(q\\) variables adicionales se consideran relevantes (significativas) en la explicación de la respuesta, si \\(F_q\\) tiene asociado un p-valor significativo. Un criterio para seleccionar el mejor modelo es quedarse con aquel menos complejo (en términos de predictoras presentes en el modelo) y que pueda considerarse con la misma capacidad predictiva (test \\(F\\) parcial no significativo) que cualquier otro más complejo. 3.4.2 Estadísticos AIC y BIC El criterio de información de Akaike (Akaike, 1973) está basado en la función de verosimilitud e incluye una penalización que aumenta con el número de parámetros estimados en el modelo. Premia pues, los modelos que dan un buen ajuste en términos de verosimilitud y a la vez son parsimoniosos (tienen pocos parámetros). Si \\(\\hat{\\beta}\\) es el estimador máximo-verosímil del modelo de dimensión \\(p\\), y \\(l(\\theta)\\) denota el logaritmo (neperiano) de la verosimilitud asociada con dicho modelo, el estadístico \\(AIC\\) se define por: \\[\\begin{equation} AIC=-2\\, l(\\hat{\\beta})+2p. \\tag{3.13} \\end{equation}\\] Una versión del \\(AIC\\) que tiene en cuenta también el número de datos utilizados en el ajuste, es el Schwarz’s Bayesian criterion (Schwarz, 1978), conocido como \\(BIC\\), y definido por: \\[\\begin{equation} BIC=-2\\, l(\\hat{\\beta})+log(n)\\, p. \\tag{3.14} \\end{equation}\\] Si queremos comparar dos modelos con estos criterios, se debe seleccionar el modelo con un menor valor en estos estadísticos. 3.4.3 Selección automática Los criterios anteriores resultan de utilidad cunado queremos comparar dos modelos diferentes (“modelos en competencia”), pero pueden resultar poco prácticos si el número de modelos en competencia es muy elevado, es decir, tenemos muchas posibles variables predictoras. Por ese motivo se introducen los conocidos como procedimientos secuenciales que permiten la evaluación de muchos modelos en competencia en muy poco tiempo, utilizando cualquiera de los criterios anteriores. La idea básica es partir de un modelo con cierto número de regresores, y secuencialmente moverse hacia modelos mejores (según el criterio elegido) con más o menos regresores de entre todos los observados. Una vez elegido el criterio para la selección, distinguimos básicamente entre los siguientes procedimientos secuenciales, en función de cuál es el punto (modelo) de partida y la forma de ir considerando modelos alternativos: hacia adelante, se parte del modelo más simple y se van incluyendo una a una las variables que satisfacen el criterio de inclusión; hacia atrás, se parte del modelo más complejo y se van excluyendo una a una las variables que satisfacen el criterio de exclusión; paso a paso, se suele partir de un modelo y en cada paso se incluye o excluye la variable que satisface el criterio de inclusión/exclusión. Hay que tener en cuenta que dependiendo del tipo de modelo deberemos utilizar un tipo de procedimiento u otro. En el caso de los MRP no podemos utilizar el procedimiento hacia adelante, ya que se parte siempre del modelo con un mayor grado y se trata de identificar si dicho grado puede ser eliminado, dado que siempre tratamos de obtener el modelo más parsimonioso. En el caso de los modelos RLM no hay una preferencia con respecto al procedimiento secuencial. Los procedimientos hacia adelante y hacia atrás los hemos de llevar a cabo en R de forma manual y generalmente se utiliza el test F asociado a cada paso para resolver si una variable o efecto debe entrar o salir del modelo. Para ello utilizaremos las funciones drop1() y add1(). El procedimiento paso a paso es automático y se realiza con la función step(). 3.4.4 Funciones en R En la librería olsrr dedicada exclusivamente al análisis de modelos de regresión (simple, múltiple y polinómica) se presentan diferentes funciones para los procesos de selección automática de variables utilizando el test \\(F\\) parcial y el criterio AIC. Presentamos sólo aquellas funciones que utilizan como punto de partida el modelo saturado. Dichas funciones son: ols_step_backward_p(model): selección desde el modelo saturado mediante el test \\(F\\). Fijamos el parámetro prem igual a 0.05 para marcar el nivel de significatividad del contraste. ols_step_backward_aic(model): selección desde el modelo saturado mediante AIC. Aunque estas funciones pueden mostrar todo el desarrollo de selección (al igual que la función step()), la ventaja principal es que puede mostrar un resumen del proceso final para estudiar el modelo final obtenido. En los ejemplos mostraremos el uso de estas funciones. 3.4.5 Ejemplos A continuación, se muestra como utilizar los criterios de selección de variables y los procedimientos secuenciales de selección en los bancos de datos que venimos trabajando en esta unidad. 3.4.5.1 Datos de Bosque Veamos como seleccionar el mejor modelo para los datos de bosque. En puntos anteriores ya hemos obtenido el modelo saturado y pudimos ver como la variable dbh parecía no resultar relevante para explicar el comportamiento del volumen obtenido. Proponemos un nuevo modelo sin dicha variable y comparamos ambos modelos utilizando los criterios de comparación. Para la comparación de modelos anidados siempre deberemos empezar desde el modelo más sencillo al más complejo. Los modelos que deseamos comparar son: \\[\\begin{array}{ll} M_2: &amp; vol \\sim d16 + ht\\\\ M_1: &amp; vol \\sim dbh + d16 + ht \\end{array}\\] # Modelo saturado M1 &lt;- lm(vol ~ dbh + d16 + ht, data = bosque) # Construimos modelo sin dbh M2 &lt;- lm(vol ~ d16 + ht, data = bosque) # Comparación mediante test F anova(M2, M1) ## Analysis of Variance Table ## ## Model 1: vol ~ d16 + ht ## Model 2: vol ~ dbh + d16 + ht ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 17 177.36 ## 2 16 153.30 1 24.06 2.5111 0.1326 El test \\(F\\) parcial resulta no significativo (p-valor = 0.1326) al comparar los modelos \\(M_1\\) y \\(M_2\\), lo que implica que ambos modelos pueden ser considerados iguales. Comparamos el proceso inferencial en cada modelo, dado que al considerar el modelo más simple estamos admitiendo que dbh no es relevante para explicar el comportamiento del volumen. # Comparativa de modelos tab_model(M1, M2, show.ci = FALSE) vol vol Predictors Estimates p Estimates p (Intercept) -108.58 &lt;0.001 -105.90 &lt;0.001 dbh 1.63 0.133 d16 5.67 &lt;0.001 7.41 &lt;0.001 ht 0.69 0.001 0.68 0.001 Observations 20 20 R2 / R2 adjusted 0.959 / 0.951 0.953 / 0.947 Se puede ver que el \\(R^2\\) ajustado para ambos modelos es prácticamente idéntico reflejando que poseen la misma capacidad explicativa. Los modelos obtenidos muestran estimaciones de los coeficientes muy parecidos para ambos modelos. Eliminar la variable dbh no afecta a la capacidad explicativa del modelo, y no altera la contribución de cada predictora a la explicación de la respuesta. El modelo resultante viene dado por: \\[ \\widehat{\\text{vol}} = -105.90 + 7.41*\\text{d16} + 0.68*\\text{ht} \\] Utilzamos ahora los criterios \\(AIC\\) y \\(BIC\\) para comparar ambos modelos: g2 &lt;- glance(M2) g1 &lt;- glance(M1) kable(rbind(g1, g2), digits = 2) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.96 0.95 3.10 124.93 0 4 -48.75 107.49 112.47 153.30 16 0.95 0.95 3.23 170.95 0 3 -50.20 108.41 112.39 177.36 17 Si utilizamos el \\(AIC\\) podemos concluir que el modelo preferido es \\(M_2\\) dado que obtenemos un valor más pequeño, mientras que si usamos el \\(BIC\\) el preferido es \\(M1\\). Sin embargo, dado que en ambos casos las diferencias entre ambos modelos son excesivamente pequeñas concluir que uno es mejor que otro resulta complicado y utilizamos el criterio de simplicidad. Ante modelos parecidos elegimos el menos complejo que en este caso sería el que tiene menos predictoras (modelo \\(M2\\)). Por último, veremos como utilizar el procedimiento secuencial automático por pasos para obtener el mejor modelo para este conjunto de datos. En este caso partimos del modelo saturado. stats::step(fit.bosque) ## Start: AIC=48.73 ## vol ~ dbh + d16 + ht ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 153.30 48.733 ## - dbh 1 24.06 177.36 49.649 ## - ht 1 173.42 326.72 61.867 ## - d16 1 213.21 366.51 64.166 ## ## Call: ## lm(formula = vol ~ dbh + d16 + ht, data = bosque) ## ## Coefficients: ## (Intercept) dbh d16 ht ## -108.5758 1.6258 5.6714 0.6938 El proceso de selección comienza a partir del moldeo saturado y determina para cada predictora cual sería el cambio en el \\(AIC\\) (columna AIC) si dicha variable fuera eliminada del modelo. El modelo saturado tiene un \\(AIC\\) de 48.733 (fila &lt;none&gt;), mientras que el modelo donde se elimina la variable dbh (fila - dbh) tiene un \\(AIC\\) de 49.649. Atendiendo al criterio establecido de quedarnos con el modelo con un menor \\(AIC\\) el modelo preferido sería el saturado. Esto contradice los resultados obtenidos con el test \\(F\\) parcial pero es posible cuando tenemos pocos datos o los valores de \\(AIC\\) están muy próximos. Repetimos el análisis de selección automática con las funciones de la libreria olsrr. ols_step_backward_p(fit.bosque, prem = 0.05) ## ## ## Elimination Summary ## ------------------------------------------------------------------------ ## Variable Adj. ## Step Removed R-Square R-Square C(p) AIC RMSE ## ------------------------------------------------------------------------ ## 1 dbh 0.9526 0.9471 4.5111 108.4066 3.2300 ## ------------------------------------------------------------------------ ols_step_backward_aic(fit.bosque) ## [1] &quot;No variables have been removed from the model.&quot; Como se puede ver la solución es la misma que la obtenida con la función step()pero la forma de mostrar los resultados es mucho más simple. En casos con muchas posibles predictoras puede resultar más útil compara solo los mejores modelos que podríamos obtener con todas las posibles combinaciones de predictoras. Para realizar esta tarea podemos utilizar la función ols_step_best_subset(). Veamos su funcionamiento con este ejemplo a pesar de que el número de predictoras es pequeño, y el número de posibles modelos es reducido. ols_step_best_subset(fit.bosque) ## Best Subsets Regression ## ------------------------- ## Model Index Predictors ## ------------------------- ## 1 d16 ## 2 d16 ht ## 3 dbh d16 ht ## ------------------------- ## ## Subsets Regression Summary ## ---------------------------------------------------------------------------------------------------------------------------------- ## Adj. Pred ## Model R-Square R-Square R-Square C(p) AIC SBIC SBC MSEP FPE HSP APC ## ---------------------------------------------------------------------------------------------------------------------------------- ## 1 0.9084 0.9033 0.8839 19.8001 119.5982 60.6857 122.5854 381.3477 20.9618 1.1210 0.1120 ## 2 0.9526 0.9471 0.933 4.5111 108.4066 52.1187 112.3895 209.5068 11.9979 0.6521 0.0641 ## 3 0.9591 0.9514 0.9242 4.0000 107.4909 52.6084 112.4696 193.1589 11.4976 0.6388 0.0614 ## ---------------------------------------------------------------------------------------------------------------------------------- ## AIC: Akaike Information Criteria ## SBIC: Sawa&#39;s Bayesian Information Criteria ## SBC: Schwarz Bayesian Criteria ## MSEP: Estimated error of prediction, assuming multivariate normality ## FPE: Final Prediction Error ## HSP: Hocking&#39;s Sp ## APC: Amemiya Prediction Criteria Se presentan diferentes criterios para valorar el mejor modelo de entre todas las combinaciones posibles. En este caso el analista debe decidir cual de los propuestos es más adecuado. El único criterio que no se encuentra disponible es el test \\(F\\) parcial. Dado que la capacidad explicativa los dos modelos propuestos es muy similar será preferible el menos complejo. En la fase de diagnóstico ya comprobaremos si ese modelo más simple debe ser modificado o si por el contrario es adecuado para proceder con la fase de predicción. Almacenamos el nuevo modelo: # Modelo seleccionado fit.bosque &lt;- lm(vol ~ d16 + ht, data = bosque) 3.4.5.2 Datos de Concentración Veamos como seleccionar el mejor modelo para los datos de concentración. En puntos anteriores ya hemos podido ver que el peso del hígado resultaba poco relevante, con lo que podríamos plantear un contraste para saber si podemos prescindir de dicha variable. Sin embargo, la forma habitual de proceder sería utilizar en primer lugar un procedimiento automático para seleccionar las predictoras y chequear posteriormente mediante un test \\(F\\) parcial si el modelo obtenido posee la misma capacidad explicativa que el modelo saturado. Planteamos el proceso secuencial: stats::step(fit.concen) ## Start: AIC=-93.78 ## concen ~ p.cuerpo + p.higado + dosis ## ## Df Sum of Sq RSS AIC ## - p.higado 1 0.004120 0.093729 -94.924 ## &lt;none&gt; 0.089609 -93.778 ## - p.cuerpo 1 0.042408 0.132017 -88.416 ## - dosis 1 0.044982 0.134591 -88.049 ## ## Step: AIC=-94.92 ## concen ~ p.cuerpo + dosis ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.093729 -94.924 ## - p.cuerpo 1 0.039851 0.133580 -90.192 ## - dosis 1 0.043929 0.137658 -89.621 ## ## Call: ## lm(formula = concen ~ p.cuerpo + dosis, data = concentracion) ## ## Coefficients: ## (Intercept) p.cuerpo dosis ## 0.28552 -0.02044 4.12533 En la primera iteración el \\(AIC\\) del modelo saturado es igual a -93.78 mientras que el del modelo que prescinde de p.higado es de -94.92. Por tanto, dicha variable se elimina del modelo que pasa a tener un \\(AIC\\) de -94.92. En la segunda iteración la variable candidata a salir es p.cuerpo, pero su \\(AIC\\) asociado es superior al del modelo actual y no se descarta. Verificamos mediante el test \\(F\\) parcial: # Modelo saturado M1 &lt;- lm(concen ~ p.higado + p.cuerpo + dosis, data = concentracion) # Construimos modelo sin dbh M2 &lt;- lm(concen ~ p.cuerpo + dosis, data = concentracion) # Comparación mediante test F anova(M2, M1) ## Analysis of Variance Table ## ## Model 1: concen ~ p.cuerpo + dosis ## Model 2: concen ~ p.higado + p.cuerpo + dosis ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 16 0.093729 ## 2 15 0.089609 1 0.00412 0.6897 0.4193 EL test \\(F\\) resulta no significativo indicando que el modelo más simple tiene la misma capacidad explicativa que el más complejo. Estudiamos dicho modelo comparándolo con el saturado. # Comparativa de modelos tab_model(M1, M2, show.ci = FALSE) concen concen Predictors Estimates p Estimates p (Intercept) 0.27 0.192 0.29 0.155 p.higado 0.01 0.419 p.cuerpo -0.02 0.018 -0.02 0.019 dosis 4.18 0.015 4.13 0.015 Observations 19 19 R2 / R2 adjusted 0.364 / 0.237 0.335 / 0.251 Podemos ver coo el \\(R^2\\) ajustado mejora al eliminar p.higado y los coeficientes son prácticamente idénticos: \\[ \\widehat{\\text{concen}} = 0.29 - 0.02*\\text{p.cuerpo} + 4.13*\\text{dosis} \\] Utilizamos ahora las funciones específicas: ols_step_backward_p(fit.concen, prem = 0.05) ## ## ## Elimination Summary ## ------------------------------------------------------------------------ ## Variable Adj. ## Step Removed R-Square R-Square C(p) AIC RMSE ## ------------------------------------------------------------------------ ## 1 p.higado 0.3347 0.2515 2.6897 -39.0043 0.0765 ## ------------------------------------------------------------------------ ols_step_backward_aic(fit.concen) ## ## ## Backward Elimination Summary ## ---------------------------------------------------------------- ## Variable AIC RSS Sum Sq R-Sq Adj. R-Sq ## ---------------------------------------------------------------- ## Full Model -37.858 0.090 0.051 0.36390 0.23668 ## p.higado -39.004 0.094 0.047 0.33466 0.25149 ## ---------------------------------------------------------------- Almacenamos el modelo resultante para la fase de diagnóstico: fit.concen &lt;- lm(concen ~ p.cuerpo + dosis, data = concentracion) 3.4.5.3 Datos de Papel Para el bando de datos de Papel se ha propuesto como modelo uno del tipo polinómico de grado 2. El proceso de selección en este caso se basa en comparar el modelo cuadrático frente al lineal para saber si es posible prescindir del grado 2, o si por el contrario es necesario para explicar la tensión del papel. Los modelos que deseamos comparar son: \\[\\begin{array}{ll} M_2: &amp; tension \\sim madera\\\\ M_1: &amp; tension \\sim madera + I(madera^2) \\end{array}\\] # Modelo saturado M1 &lt;- lm(tension ~ madera + I(madera^2), data = papel) # Construimos modelo sin dbh M2 &lt;- lm(tension ~ madera, data = papel) # Comparación mediante test F anova(M2, M1) ## Analysis of Variance Table ## ## Model 1: tension ~ madera ## Model 2: tension ~ madera + I(madera^2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 17 2373.46 ## 2 16 312.64 1 2060.8 105.47 1.894e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El test \\(F\\) parcial resulta significativo indicando que los modelos considerados tienen capacidades explicativas estadísticamente distintas. No podemos rechazar el modelo cuadrático frente al modelo lineal. Veamos la tabla de estimación de ambos modelos: # Comparativa de modelos tab_model(M1, M2, show.ci = FALSE) tension tension Predictors Estimates p Estimates p (Intercept) -6.67 0.067 21.32 0.001 madera 11.76 &lt;0.001 1.77 0.014 madera^2 -0.63 &lt;0.001 Observations 19 19 R2 / R2 adjusted 0.909 / 0.897 0.305 / 0.265 El \\(R^2\\) ajustado pasa del 26.5% en el modelo lineal al 89.7% en el modelo cuadrático indicando una gran mejora en la capacidad explicativa, y por tanto eligiendo este último como el modelo que debe pasar a la fase de diagnóstico. Utilizamos las funciones resumen Utilizamos ahora las funciones específicas: ols_step_backward_p(fit.papel, prem = 0.05) ## [1] &quot;No variables have been removed from the model.&quot; ols_step_backward_aic(fit.papel) ## [1] &quot;No variables have been removed from the model.&quot; 3.5 Multicolinealidad La multicolinealidad es un problema relativamente frecuente en regresión lineal múltiple, y en general en análisis con varias variables explicativas, entre cuyas soluciones se halla la selección de variables. Cuando los regresores no están relacionados linealmente entre sí, se dice que son ortogonales. Que exista multicolinealidad significa que las columnas de \\(X\\) no son linealmente independientes. Si existiera una dependencia lineal total entre algunas de las columnas, tendríamos que el rango de la matriz \\(X&#39;X\\) sería menor a \\(p\\) y \\((X&#39;X)^{-1}\\) no existiría. El hecho de que haya multicolinealidad, esto es, una relación casi lineal entre algunos regresores, afecta a la estimación e interpretación de los coeficientes del modelo. La multicolinealidad no es un problema de violación de hipótesis; simplemente es una situación que puede ocasionar problemas en las inferencias con el modelo de regresión. Nos ocupamos a continuación de examinar las causas de la multicolinealidad, algunos de los efectos que tiene en las inferencias, los métodos básicos para detectar el problema y algunas formas de tratarlo. 3.5.1 Causas Montgomery y Peck (1992) comentan que la colinealidad puede surgir por el método de recogida de datos, restricciones en el modelo o en la población, especificación y sobreformulación del modelo (consideración de más variables de las necesarias); en modelos polinómicos, por ejemplo, se pueden presentar problemas serios de multicolinealidad en la matriz de diseño \\(X\\) cuando el rango de variación de los predictores es muy pequeño. Obviamente, modelos con más covariables son más propicios a padecer problemas de multicolinealidad. 3.5.2 Efectos Los principales efectos de la multicolinealidad son los siguientes: Una multicolinealidad fuerte produce varianzas y covarianzas grandes para los estimadores de mínimos cuadrados. Así, muestras con pequeñas diferencias podrían dar lugar a estimaciones muy diferentes de los coeficientes del modelo. Es decir, las estimaciones de los coeficientes resultan poco fiables cuando hay un problema de multicolinealidad. De hecho, dichos coeficientes vienen a explicar cómo varía la respuesta cuando varía la variable independiente en cuestión y todas las demás quedan fijas; si las variables predictoras están relacionadas entre sí, es inviable que al variar una no lo vayan a hacer las demás y en consecuencia puedan quedar fijas. La multicolinealidad reduce la efectividad del ajuste lineal si su propósito es determinar los efectos de las variables independientes. A consecuencia de la gran magnitud de los errores estándar de las estimaciones, muchas de éstas no resultarían significativamente distintas de cero: los intervalos de confianza serán ‘grandes’ y por tanto, con frecuencia contendrán al cero. La multicolinealidad tiende a producir estimaciones de mínimos cuadrados \\(\\hat{\\beta}_j\\) muy grandes en valor absoluto. Los coeficientes del ajuste con todos los predictores difieren bastante de los que se obtendrían con una regresión simple entre la respuesta y cada variable explicativa. La multicolinealidad no afecta al ajuste global del modelo (medidas como la \\(R^2\\), etc.) y por lo tanto no afecta a la habilidad del modelo para estimar puntualmente la respuesta o la varianza residual. Sin embargo, al aumentar los errores estándar de las estimaciones de los coeficientes del modelo, también lo hacen los errores estándar de las estimaciones de la respuesta media y de la predicción de nuevas observaciones, lo que afecta a la estimación en intervalo. 3.5.3 Diagnósticos Existen diversos diagnósticos propuestos para detectar problemas de multicolinealidad. Consideramos los más relevantes, que son: Los gráficos entre variables explicativas son útiles para estudiar la relación entre las variables explicativas y su disposición en el espacio, y con ello detectar correlaciones o identificar observaciones muy alejadas del resto de datos y que pueden influenciar notablemente la estimación. Consisten en gráficos de dispersión entre un par de covariables continuas o un par de factores (a través de sus códigos), y gráficos de cajas cuando se trata de investigar la relación entre un factor y una covariable. Una medida simple de multicolinealidad consiste en la inspección de los elementos fuera de la diagonal de la matriz \\(X&#39;X\\), es decir, las correlaciones simples \\(r_{ij}\\) entre todos los regresores. Si dos regresores \\(x_i\\) y \\(x_j\\) son casi linealmente dependientes, entonces \\(|r_{ij}| \\approx 1\\). Sin embargo, cuando la multicolinealidad involucra a varias variables, no hay garantías de detectarla a través de las correlaciones bivariadas. Puesto que uno de los efectos principales de la multicolinealidad es la inflación de la varianza y covarianza de las estimaciones, es posible calcular unos factores de inflación de la varianza, FIV, que permiten apreciar tal efecto. En concreto, la varianza de \\(\\hat{\\beta}_j\\) viene estimada por \\(Var(\\hat{\\beta}_j)=s^2 \\, C_{jj}\\), donde \\(C_{jj}^X\\) son los elementos de la diagonal de la matriz \\((X&#39;X)^{-1}\\), es decir, \\[ C_{jj}^X=\\frac{1}{(1-R_j^2) \\, S_{x_j x_j}}, \\ \\ j=1, 2, \\ldots, p, \\] con \\(R_j^2\\) el coeficiente de determinación múltiple para la regresión de \\(x_j\\) sobre las restantes \\(p-1\\) covariables. Si hay una correlación muy alta entre \\(x_j\\) y los restantes regresores, entonces \\(R_j^2 \\approx 1\\). En particular, puesto que \\(s^2\\) no varía ante un problema de multicolinealidad, si ésta existe, la varianza de \\(\\hat{\\beta}_j\\) aumenta por un factor igual a \\(1/(1-R_j^2)\\), que se define como el FIV para \\(x_j\\): \\[ FIV_j=1/(1-R_j^2). \\] Generalmente, valores de un FIV superiores a 10 dan indicios de un problema de multicolinealidad, si bien su magnitud depende del modelo ajustado. Lo ideal es compararlo con su equivalente en el modelo ajustado, esto es, \\(1/(1-R^2)\\), donde \\(R^2\\) es el coeficiente de determinación del modelo. Los valores FIV mayores que esta cantidad implican que la relación entre las variables independientes es mayor que la que existe entre la respuesta y los predictores, y por tanto dan indicios de multicolinealidad. Dado que la multicolinealidad afecta a la singularidad (rango menor que \\(p\\)) de la matriz \\(X&#39;X\\), sus valores propios \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p\\) pueden revelar multicolinealidad en los datos. De hecho, si hay una o más dependencias casi lineales en los datos, entonces uno o más de los valores propios será pequeño. En lugar de buscar valores propios pequeños, se puede optar por calcular el número de condición de \\(X&#39;X\\), definido por: \\[ \\kappa = \\lambda_{max}/\\lambda_{min}, \\] que es una medida de dispersión en el espectro de valores propios de \\(X&#39;X\\). Generalmente, si el número de condición es menor que 100, no hay problemas de multicolinealidad. Números de condición entre 100 y 1000 implican multicolinealidad moderada, y mayores que 1000 implican multicolinealidad severa. Los índices de condición de la matriz \\(X&#39;X\\) también son útiles para el diagnóstico de multicolinealidad y se definen por: \\[ \\kappa_j = \\lambda_{max}/\\lambda_j, \\ \\ \\ j=1, \\ldots, p. \\] El número de índices de condición que son grandes (por ejemplo, \\(\\geq 1000\\)) es una medida útil del número de dependencias casi lineales en \\(X&#39;X\\). Otra posibilidad de diagnóstico es a través de un análisis de componentes principales. Este tipo de análisis multivariante se plantea sobre conjuntos de variables relacionadas linealmente entre sí y tiene como finalidad la de definir un conjunto menor de nuevas variables obtenidas como combinación lineal de las originales, y que a la vez resultan ortogonales entre sí. Si el análisis de componentes principales resulta significativo, estamos reconociendo multicolinealidad. 3.5.4 Soluciones Una vez detectado un problema de multicolinealidad, es recomendable intentar aliviarlo (por sus efectos). Para ello disponemos de diversos recursos, y en función del objetivo del análisis, será más aconsejable uno u otro. Básicamente podemos distinguir como objetivos esenciales: Estimar bien la respuesta media en función de un conjunto de variables explicativas, sin importar demasiado la contribución individual de cada una de esas variables. Hacer un análisis de estructura, esto es, describir el efecto de las variables explicativas en la predicción de la respuesta. Las magnitudes y significatividades de los coeficientes son entonces de interés. Así, en un análisis de estructura es importante conseguir un buen modelo de ajuste para cuantificar bien la información que aportan las variables explicativas sobre la respuesta. Hay tres aproximaciones básicas como remedio a la multicolinealidad: Selección de variables (ver Sección XX). Respecto a la selección de variables, lo ideal ante un problema de multicolinealidad es seleccionar aquellas variables predictoras que son más significativas y contienen la mayor parte de la información sobre la respuesta. Sin embargo, hay que actuar con precaución, pues los métodos automáticos de selección de variables son bastante sensibles cuando existe relación entre los regresores y no está garantizado que el modelo resultante tenga menor multicolinealidad. Por otro lado, la capacidad predictiva del modelo puede verse seriamente menguada al reducir el número de covariables consideradas, de modo que este remedio iría más indicado cuando el objetivo del análisis es el 2. Redefinición de variables. Otra alternativa es transformar las covariables. Para ello es importante identificar entre qué covariables hay relación, con el fin de utilizar transformaciones apropiadas. Si varias variables están relacionadas linealmente, a veces funciona considerar la más completa de ellas tal y como es, y transformaciones de las otras con cocientes o diferencias respecto de la más completa. Es decir, si \\(x_i\\) y \\(x_j\\) están relacionadas y \\(x_i\\) da una información más completa que \\(x_j\\), se puede considerar un nuevo ajuste que involucre a las variables \\(x_i\\) y \\(x_j/x_i\\), o bien a \\(x_i\\) y \\(x_j-x_i\\). Cuando la intuición o el conocimiento de las variables no sugiere ninguna transformación concreta, una opción es llevar a cabo un análisis de componentes principales con el fin de obtener nuevas variables, expresables como combinación lineal de las originales, ortogonales entre sí y que contengan toda la información disponible en las primeras. En ocasiones, las componentes que resultan tienen un significado intuitivo por la forma de asimilar la información de las variables originales, y en ocasiones no, en cuyo caso se puede proceder a la realización de un análisis factorial y a la búsqueda de alguna rotación geométrica que permita llegar a variables “interpretables”. Una vez obtenidas las componentes \\(Z\\), se pueden seguir dos alternativas: i) plantear una regresión de la respuesta explicada por todas las componentes principales obtenidas, o ii) ajustar un modelo de regresión sólo con las componentes más relevantes como variables predictoras (componentes principales incompletas). En el primer caso, a partir del modelo ajustado \\(y=Z\\gamma+\\epsilon\\), es posible recuperar el efecto de las variables originales sobre la respuesta sin más que deshacer el cambio. Esto no es posible para la segunda alternativa, pues las estimaciones que se consiguen están sesgadas; sin embargo, esta opción reduce la varianza de las estimaciones respecto del modelo original. Estimación sesgada. Si uno de los efectos de la multicolinealidad es que aumenta el error estándar de las estimaciones por mínimos cuadrados de los coeficientes del modelo, cabe la posibilidad de utilizar estimadores que, aun sesgados, produzcan estimaciones con menor error estándar y un error cuadrático medio inferior al de los estimadores de mínimos cuadrados (que son, de los insesgados, los de mínima varianza). Hay varios procedimientos de estimación sesgada. Las componentes principales incompletas es uno de ellos. La regresión Ridge es otro método interesante. La regresión Ridge consiste en utilizar como estimador de \\(\\beta\\), el siguiente: \\[ \\hat{\\beta}_k=(X&#39;X+kI)^{-1} X&#39;y, \\] donde \\(k\\) es una constante pequeña arbitraria. Cuando todos los predictores están estandarizados, tenemos que \\(X&#39;X\\) es la matriz de correlaciones, con unos en la diagonal. Así, la correlación “efectiva” que se consigue ahora entre \\(x_i\\) y \\(x_j\\) es \\(r_{ij}/(1+k)\\). Es decir, todas las correlaciones se reducen artificialmente en un factor \\(1/(1+k)\\), reduciendo entonces la multicolinealidad. Valores grandes de \\(k\\) reducen la multicolinealidad pero, como contraprestación, aumentan el sesgo de las estimaciones. Para determinar el valor de \\(k\\) a utilizar, se suelen considerar gráficos en los que se representa \\(k\\) versus las estimaciones del modelo (ridge plots). Para valores pequeños de \\(k\\), las estimaciones de los coeficientes cambian mucho, mientras que a medida que \\(k\\) aumenta, las estimaciones parecen estabilizarse. Se dice que se consigue un valor óptimo para \\(k\\) cuando se da dicha estabilización en las estimaciones. Este procedimiento resulta pues, algo subjetivo, pero sin embargo ha resultado efectivo en la práctica. Hay otros procedimientos sesgados de estimación propuestos en la literatura que alivian el problema de la multicolinealidad. 3.5.5 Ejemplos A continuación, realizamos el estudio de multicolinealidad para los diferentes bancos de datos que hemos venido trabajando. Para el calculo de los factores de inflacción de la varianza y los números de condición utilizamos la función ols_coll_diag() de la librería olsrr. Con ella obtenemos el VIF asociado con cada variable, el índice de condición asociado con cada valor propio, y la matriz de correlaciones asociada al modelo ajustado. En caso de detectar multicolinealidad trataremos de corregirla con los procedimientos presentados. 3.5.5.1 Datos de bosque Para el análisis de multicolinealidad tomamos el modelo obtenido después del proceso de selección de variables de la sección anterior. # Modelos fit.bosque&lt;- lm(vol ~ d16 + ht, data = bosque) # Análisis de multicolinealidad ols_coll_diag(fit.bosque) ## Tolerance and Variance Inflation Factor ## --------------------------------------- ## Variables Tolerance VIF ## 1 d16 0.813726 1.228915 ## 2 ht 0.813726 1.228915 ## ## ## Eigenvalue and Condition Index ## ------------------------------ ## Eigenvalue Condition Index intercept d16 ht ## 1 2.991530311 1.00000 0.000270918 0.001143249 0.0002206333 ## 2 0.007330472 20.20137 0.077611612 0.924370594 0.0288009800 ## 3 0.001139217 51.24405 0.922117470 0.074486157 0.9709783867 Del análisis realizado no parece detectarse multicolinealidad a través de \\(VIF\\), ni a través de los índices de condición. El valor de \\(1/(1-R^2)\\) para dicho modelo es 21.28 que es superior a los valore de \\(VIF\\) observados. Por tanto, no parece haber un problema de multicolinealidad con el modelo obtenido. 3.5.5.2 Datos de concentración En este caso analizamos el modelo saturado en primer lugar. # Modelos fit.concen&lt;- lm(concen ~ p.cuerpo + p.higado + dosis, data = concentracion) # Análisis de multicolinealidad ols_coll_diag(fit.concen) ## Tolerance and Variance Inflation Factor ## --------------------------------------- ## Variables Tolerance VIF ## 1 p.cuerpo 0.01919315 52.101917 ## 2 p.higado 0.74868308 1.335679 ## 3 dosis 0.01944498 51.427154 ## ## ## Eigenvalue and Condition Index ## ------------------------------ ## Eigenvalue Condition Index intercept p.cuerpo p.higado ## 1 3.980955e+00 1.00000 0.0005211255 1.049346e-05 0.001061756 ## 2 1.307262e-02 17.45068 0.0912614776 7.180392e-04 0.963765107 ## 3 5.885352e-03 26.00803 0.8549633200 4.879743e-03 0.028213823 ## 4 8.720917e-05 213.65475 0.0532540769 9.943917e-01 0.006959314 ## dosis ## 1 1.138353e-05 ## 2 8.095775e-04 ## 3 6.508870e-03 ## 4 9.926702e-01 Hay dos \\(VIF\\) que indican multicolinealidad y un número de condición por encima de 100. Probamos con el modelo obtenido en el proceso de selección de variables: # Modelos fit.concen&lt;- lm(concen ~ p.cuerpo + dosis, data = concentracion) # Análisis de multicolinealidad ols_coll_diag(fit.concen) ## Tolerance and Variance Inflation Factor ## --------------------------------------- ## Variables Tolerance VIF ## 1 p.cuerpo 0.01947892 51.33755 ## 2 dosis 0.01947892 51.33755 ## ## ## Eigenvalue and Condition Index ## ------------------------------ ## Eigenvalue Condition Index intercept p.cuerpo dosis ## 1 2.993933e+00 1.00000 0.0009364888 1.884523e-05 2.017998e-05 ## 2 5.978794e-03 22.37764 0.9398136118 4.217372e-03 5.578232e-03 ## 3 8.781609e-05 184.64349 0.0592498994 9.957638e-01 9.944016e-01 Se siguen presentando problemas de multicolinealidad. Sin embargo, aunque esto puede parecer un problema muy grave no lo es dada la situación experimental dada. Es de esperar que la dosis suministrada este claramente asociada con el peso del sujeto, y por tanto dichas variables tienen que estar relacionadas. Aunque la multicolinealidad afecta a la precisión del modelo (en este caso es poco relevante porque nuestro ajuste es bastante malo) no es un problema con el diagnóstico del modelo. En la sección siguiente determinaremos si el modelo debe ser modificado o si por el contrario nos quedamos con el modelo obtenido tras la selección de variables. 3.5.5.3 Datos de papel Para este conjunto de datos es de esperar que los indicadores de multicolinealidad proporcionen resultados altos, ya que al tratarse de un MP la variable predictora es la misma. # Análisis de multicolinealidad ols_coll_diag(fit.papel) ## Tolerance and Variance Inflation Factor ## --------------------------------------- ## Variables Tolerance VIF ## 1 madera 0.05840859 17.12077 ## 2 I(madera^2) 0.05840859 17.12077 ## ## ## Eigenvalue and Condition Index ## ------------------------------ ## Eigenvalue Condition Index intercept madera I(madera^2) ## 1 2.7005883 1.000000 0.01001057 0.001973208 0.003447858 ## 2 0.2904492 3.049257 0.18853949 0.001001403 0.035409847 ## 3 0.0089625 17.358596 0.80144993 0.997025390 0.961142295 El resultado del \\(VIF\\) muestra multicolinealidad pero como es el comportamiento natural para este tipo de modelos se decide no actuar. 3.6 Diagnóstico Estudiamos en este punto el proceso de diagnóstico de un modelo RLM o MP de los que hemos venido estudiando hasta ahora. El diagnóstico del modelo es realmente valioso por cuanto nos permite corroborar que se cumplen (o no) cada una de las hipótesis asumidas para el ajuste del modelo y que dan credibilidad a las conclusiones que obtenemos. Este diagnóstico suele sugerir con frecuencia alguna modificación correctora del modelo propuesto y nos obliga a repetir la dinámica de análisis (modelo alternativo y selección de variables) hasta dar con una solución satisfactoria. La herramienta básica para el diagnóstico del modelo es el análisis de los residuos, tanto a través de gráficos, como de tests que verifican la validez de las hipótesis asumidas en el ajuste del modelo lineal: \\(E(\\epsilon_i)=0 , \\ \\forall i=1, \\ldots, n \\ \\rightsquigarrow\\) bondad del ajuste o linealidad. \\(Var(\\epsilon_i)=\\sigma^2, \\ \\forall i \\ \\rightsquigarrow\\) Varianza constante (homocedasticidad). \\(\\epsilon \\sim N(0, \\sigma^2I) \\ \\rightsquigarrow\\) Normalidad de los errores. \\(Cov(\\epsilon_i, \\epsilon_j)=0, \\forall \\ i\\neq j \\ \\rightsquigarrow\\) Independencia de los errores. Si encontramos indicios de violación de alguna de ellas, en ocasiones podremos resolverlas a través de las soluciones que proponemos a continuación. Tanto las herramientas de diagnóstico como las soluciones propuestas para cuando encontramos problemas, son una ampliación del análisis de residuos que ya estudiamos para el modelo de regresión lineal simple. Aunque se pueden definir diferentes tipos de residuos, aquí nos concentramos en los que son de uso habitual en el diagnóstico de modelos lineales. 3.6.1 Tipos de Residuos Presentamos diversos tipos de residuos, útiles tanto para la diagnosis del modelo como para el análisis de influencia (detección de observaciones influyentes y/o raras o anómalas). Generalmente, los procedimientos de diagnóstico del modelo basados en residuos son gráficos, si bien en ocasiones disponemos de algunos tests basados en ellos. 3.6.1.1 Residuos comunes Los residuos comunes del modelo lineal \\(y=X\\beta+\\epsilon\\) consisten simplemente en las desviaciones entre los datos observados \\(y_i\\) y los predichos \\(\\hat{y}_i\\), esto es, los obtenidos de: \\[ \\textbf{e}=\\textbf{y}-\\hat{\\textbf{y}}=y-X\\hat{\\beta}=\\textbf{y}-X\\hat{\\beta}=(I-X(X&#39;X)^{-1}X&#39;)\\textbf{y} \\] cuando \\(X&#39;X\\) es no singular. Surge así una matriz básica en la definición de los residuos, denominada matriz gorro y definida por: \\[ H=X(X&#39;X)^{-1}X&#39;, \\] que tiene su importancia en la interpretación y redefinición de nuevos tipos de residuos, como veremos. A sus elementos nos referiremos como \\(h_{ij}\\). Esta matriz \\(H\\) es simétrica (\\(H&#39;=H\\)) e idempotente (\\(HH=H\\)), de dimensión \\(n \\times n\\) y de rango \\(p=rang(X)\\). En términos de \\(H\\), los residuos \\(\\textbf{e}\\) se pueden escribir como: \\[ \\textbf{e} = \\textbf{y}-\\hat{\\textbf{y}} = (I-H) \\textbf{y}, \\] esto es, \\[ e_i=(1-\\sum_{j=1}^n h_{ij}) \\, y_i=y_i-\\hat{y}_i, \\ \\ \\ i=1, \\ldots, n. \\] De esta forma se puede demostrar que la varianza de cada residuo viene dada por: \\[ Var(e_i)=(1-h_{ii})\\sigma^2, \\ \\ i=1, \\ldots, n, \\] y la correlación entre los residuos \\(e_i\\) y \\(e_j\\): \\[ Cor(e_i, e_j)=\\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}. \\] 3.6.1.2 Residuos estandarizados. Son residuos de media cero y varianza aproximadamente unidad, definidos por: \\[ r_i=\\frac{e_i}{\\sqrt{s^2}}, \\qquad i=1, \\ldots, n, \\] donde \\(s^2\\) es la estimación habitual de \\(\\sigma^2\\) que da el cuadrado medio residual. Una modificación de estos ´residuos son los denominados residuos estudentizados que se interpretan de forma similar a estos. En los ejemplos introduciremos los procedimientos gráficos que podemos utilizar con este tipo de residuos para el estudio de la linealidad y homocedasticidad. Dado que la verificación de hipótesis se basa en los residuos del modelo, los procedimientos que utilizamos para verificarlas son los mismos a los descritos para el modelo RLS en la unidad anterior. La única diferencia es la existencia de más de una predictora. 3.6.2 Linealidad Si hay alguna variable explicativa que no ha sido incluida en el ajuste del modelo, representarla versus los residuos ayuda a identificar algún tipo de tendencia que dicha variable pueda explicar. Si no se detecta ninguna tendencia en el gráfico de dispersión en principio no tenemos ninguna evidencia que nos sugiera incorporar dicha variable al modelo para predecir mejor la respuesta. Estos gráficos son útiles también para detectar outliers y heterocedasticidad. 3.6.3 Homocedasticidad La heterocedasticidad, que es como se denomina el problema de varianza no constante, aparece generalmente cuando el modelo está mal especificado, bien en la relación de la respuesta con los predictores, bien en la distribución de la respuesta, bien en ambas cuestiones. La violación de la hipótesis de varianza constante, \\(Var(\\epsilon)=\\sigma^2 I\\), se detecta usualmente a través del análisis gráfico de los residuos: Gráficos de residuos versus valores ajustados \\(\\hat{y}_i\\).- Cuando aparece alguna tendencia como una forma de embudo o un abombamiento, etc., entonces decimos que podemos tener algún problema con la violación de la hipótesis de varianza constante para los errores. Gráficos de residuos versus predictores \\(\\textbf{x}_j\\).- Básicamente se interpretan como los gráficos de residuos versus valores ajustados \\(\\hat{y}_i\\). Es deseable que los residuos aparezcan representados en una banda horizontal sin tendencias alrededor del cero. Hay numerosos tests en la literatura para reconocer heterocedasticidad. Unos están basados en considerar la variabilidad de los residuos que consiguen explicar las variables explicativas sospechosas de inducir heterocedasticidad. Otros tests están basados en diferenciar las observaciones en grupos de varianza constante y comparar los ajustes obtenidos respecto a la hipótesis de tener una misma varianza común: El test de Breusch-Pagan. El test de Bartlett o el test de Levene. 3.6.4 Normalidad La hipótesis de normalidad de los errores \\(\\epsilon_i\\) en el modelo lineal justifica la utilización de los tests \\(F\\) y \\(t\\) para realizar los contrastes habituales y obtener conclusiones confiables a cierto nivel de confianza \\(1-\\alpha\\) dado. En muestras pequeñas, la no normalidad de los errores es muy difícil de diagnosticar a través del análisis de los residuos, pues éstos pueden diferir notablemente de los errores aleatorios \\(\\epsilon_i\\). En muestras grandes no se esperan demasiadas diferencias entre residuos y errores, y por lo tanto hacer un diagnóstico de normalidad sobre los residuos equivale prácticamente a hacerlo sobre los errores mismos. La forma habitual de diagnosticar no normalidad es a través de los gráficos qq de normalidad y de tests como el de Shapiro-Wilks, específico para normalidad, o el de bondad de ajuste de Kolmogorov-Smirnov. 3.6.5 Incorrelación Para los modelos RLM y MP asumimos que los errores observacionales están incorrelados dos a dos. Si esta hipótesis no es cierta, cabe esperar que un gráfico secuencial de los residuos manifieste alguna tendencia. Sin embargo, hay muchas formas en que los errores pueden estar correlados. De hecho, la independencia entre observaciones es una cuestión justificada básicamente por el muestreo realizado. Un gráfico de los residuos en función de la secuencia temporal en que se observaron los datos puede ayudar a apreciar un problema de correlación de los residuos. Los gráficos de autocorrelación ayudan a detectar correlación serial, es decir, que un residuo de pende de los residuos anteriores. Dichos gráficos consisten en representar cada residuo (excepto el primero) versus el residuo anterior en la secuencia temporal sospechosa de inducir la correlación. Un test habitual para detectar cierto tipo de correlación serial es el test de Durbin-Watson. 3.6.6 Soluciones a problemas detectados en el diagnóstico del modelo Las soluciones a los posibles problemas detectados en el diagnóstico son similares a las utilizadas para los modelos RLS: Propuesta de otros modelos adecuados a la distribución de la respuesta y su relación con los predictores (Modelos Lineales Generalizados que trataremos más adelante). Transformar la variable respuesta (Transformaciones de Box-Cox). Transformar las predictoras (Modelos de suavizado). Algunas de las soluciones, como las de transformar las predictoras mediante modelos de suavizado, tendrán una unidad especial de tratamiento ya que se tratan de modelos más generalistas que permiten ajustar muchos tipos de tendencias entre respuesta y predictoras. 3.6.7 Análisis de influencia En ocasiones hay algún subconjunto de los datos que influencia desproporcionadamente el ajuste del modelo propuesto, con lo cual las estimaciones y predicciones dependen mucho de él. Es interesante siempre, localizar este tipo de datos, si existen, y evaluar su impacto en el modelo. Si estos datos influyentes son “malos” (provienen de errores en la medición, o de condiciones de experimentación diferentes, etc.) habrían de ser excluidos del ajuste; si son “buenos”, esto es, efectivamente proceden de buenas mediciones aunque raras, contendrán información sobre ciertas características relevantes a considerar en el ajuste. En todo caso, es importante localizarlos, y para ello existen una serie de procedimientos basados en diversos estadísticos que presentamos a continuación. Hay diversos criterios para valorar la influencia de las observaciones en el ajuste, y en base a los cuales se proponen diversos estadísticos. Vamos a considerar tres de ellos: i) contribución a la estimación de los coeficientes; ii) influencia en la predicción y iii) influencia sobre la precisión de las estimaciones. 3.6.7.1 Sobre los coeficientes del modelo Se han construido diversas medidas para valorar la influencia de las observaciones en la estimación de los coeficientes del modelo. Entre ellas, las más habituales son: Distancia de Cook. Medida de influencia para una observación \\(y_i\\), basada en la distancia entre la estimación de mínimos cuadrados obtenida con las \\(n\\) observaciones, \\(\\hat{\\textbf{y}}=X \\hat{\\beta}\\), y la obtenida eliminando dicha observación, \\(\\hat{\\textbf{y}}^{(i)}\\). Una formulación habitual del estadístico de Cook es: \\[ D_i=\\frac{(\\hat{\\textbf{y}}-\\hat{\\textbf{y}}^{(i)})&#39;(\\hat{\\textbf{y}}-\\hat{\\textbf{y}}^{(i)})}{p s^2}=\\frac{(\\hat{\\beta}^{(i)}-\\hat{\\beta})&#39; X&#39;X (\\hat{\\beta}^{(i)}-\\hat{\\beta})}{p s^2}, \\ \\ i=1, \\ldots, n, \\] donde \\(\\hat{\\beta}^{(i)}\\) es el vector de parámetros estimados en la regresión \\(\\hat{\\textbf{y}}^{(i)}\\). Los puntos con un valor grande del estadístico \\(D_i\\) identifican observaciones tales que el hecho de incluirlas o no en el ajuste dan lugar a diferencias considerables en las estimaciones de los coeficientes. Generalmente se consideran como influyentes aquellas observaciones con un valor del estadístico \\(D_i&gt;1\\), pero se identifican como potencialemnte influyentes todas aquellas con \\(D_i&gt;4/n\\), con \\(n\\) el tamaño de la muestra. DFBETAS. Estadístico que indica cuánto cambia el coeficiente estimado \\(\\hat{\\beta}_j\\) en desviaciones estándar para un modelo dado cuando se excluye la \\(i\\)-ésima observación: \\[ DFBETAS_{j, i}=\\frac{\\hat{\\beta}_j-\\hat{\\beta}^{(i)}_j}{s^2_{(i)} C_{jj}^X}, \\quad j=0, 1, \\ldots, p; \\ i=1, \\ldots, n \\] donde \\(\\hat{\\beta}^{(i)}_j\\) es la j-ésima componente del vector \\(\\hat{\\beta}_{(i)}\\), y \\(C_{jj}^X\\) es el elemento \\(j\\) de la diagonal de \\((X&#39;X)^{-1}\\). De forma habitual se considera como potencialmente influyente una observación si \\(|DFBETAS_{j, i}|&gt;2/\\sqrt{n}\\), con \\(n\\) el tamaño muestral. 3.6.7.2 Influencia sobre las predicciones Para investigar la influencia de la \\(i\\)-ésima observación sobre los valores predichos por el modelo utilizamos el estadístico DFFITS. Se define el estadístico DFFITS para la observación i-ésima como: \\[ DFFITS_i = \\frac{\\hat{y}_i-\\hat{y}^{(i)}_i}{\\sqrt{s^2_{(i)} h_{ii}}}, \\ \\ i=1, \\ldots, n, \\] donde \\(\\hat{y}^{(i)}_i\\) es el valor predicho para \\(y_i\\) por el modelo sin utilizar en la estimación la observación \\(i\\). Así, \\(DFFITS_i\\) se puede interpretar como el número de desviaciones estándar que cambia la predicción de la \\(i\\)-ésima respuesta cuando dicha observación es excluida del ajuste. Generalmente, una observación para la que \\(|DFFITS_i|&gt;2 \\sqrt{p/n}\\) merece ser tratada con atención. 3.6.7.3 Influencia sobre la precisión de las estimaciones Los diagnósticos vistos hasta ahora cuantifican de algún modo el efecto de las observaciones en las estimaciones. Sin embargo, no proporcionan información alguna sobre la precisión conjunta del ajuste. La precisión de la estimación de \\(\\hat{\\beta}\\) se puede medir en función del estadístico \\(\\textsf{COVRATIO}\\). Si \\(COVRATIO_i&lt;1\\), excluir la \\(i\\)-ésima observación proporciona un ajuste más preciso; si \\(COVRATIO_i&gt;1\\), la \\(i\\)-ésima observación mejora la precisión de la estimación. En este manual no utilizaremos este criterio y nos centraremos en los puntos anteriores 3.6.8 Funciones para diagnóstico e influencia EN la unidad anterior mostramos como realizar los gráficos y test de diagnóstico para un modelo RLS. Esos gráficos se pueden utilizar también en los modelos tratados en esta unidad, pero además se muestran las funciones de la librería olsrr que pueden ser utilizadas para esta tarea. Concretamente: ols_plot_resid_stand(): gráfico secuencial de residuos estandarizados. ols_plot_resid_stud(): gráfico secuencial de residuos estudentizados. ols_plot_resid_stud_fit(): residuos estudentizados vs valores ajustados. ols_plot_resid_qq(): gráfico qq de normalidad. ols_test_normality(): tests de normalidad. ols_test_breusch_pagan(): Test de Bresuch-Pagan. Aunque la función por excelencia para obtener las medidas de influencia es influence.measures(), la librería olsrr presenta diversas funciones para este análisis con la ventaja de proporcionar herramientas gráficas y valores de detección, que permiten visualizar de forma rápida las posibles observaciones influyentes. Dichas funciones son: ols_plot_cooksd_chart(): proporciona la distancia de Cook. ols_plot_dfbetas(): proporciona los \\(DFBETAS\\). ols_plot_dffits(): proporciona \\(DFFITS\\). 3.6.9 Ejemplos Realizamos el diagnóstico y análisis de influencia de los modelos ajustados, tras el proceso de selección de variables. 3.6.9.1 Datos de Bosque En primer lugar, ajustamos el modelo correspondiente y obtenemos los valores de diagnóstico. # Modelos fit.bosque &lt;- lm(vol ~ d16 + ht, data = bosque) # Valores de diagnóstico diag.bosque &lt;- fortify(fit.bosque) Estudiamos los gráficos de diagnóstico para detectar residuos grandes (gráfico secuencial de residuos), linealidad y homocedasticidad (gráfico de residuos estandarizados versus valores ajustados), y normalidad (gráfico qq). ols_plot_resid_stand(fit.bosque) ggplot(diag.bosque, aes(x = .fitted, y = .stdresid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ols_plot_resid_qq(fit.bosque) acf(diag.bosque$.stdresid) El gráfico de secuencia de los residuos indica que la observación 18 es potencialemente una observación anómala. Cuando realizamos el análisis de influencia deberemos verificar esta situación para considerar la posible eliminación de esta observación. El gráfico de residuos versus valores ajustados no muestra ningún tipo de tendencia (linealidad) ni comportamientos extraños que permitan pensar que se incumple la hipótesis de varianza constante. El gráfico de normalidad también muestra un comportamiento adecuado teniendo en cuenta que el tamaño muestral es muy pequeño. El gráfico de autocorelación no muestra dependencia entre los residuos, indicando que se cumple la hipótesis de independencia. Dado que no se ha detectado falta de linealidad entre residuos y valores ajustados no es necesario realizar el gráfico de residuos versus predictoras. Sin embargo, los mostramos aquí para ver el código necesario. ggplot(diag.bosque, aes(x = ht, y = .stdresid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(diag.bosque, aes(x = d16, y = .stdresid)) + geom_point()+ geom_smooth(method = &quot;lm&quot;, se = FALSE) Como era de esperar los gráficos no muestran ningún tipo de tendencia. Realizamos ahora los tests necesarios para verificar las hipótesis de normalidad, homocedasticidad, e independencia. ols_test_normality(fit.bosque) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9281 0.1420 ## Kolmogorov-Smirnov 0.1544 0.6712 ## Cramer-von Mises 1.6812 0.0000 ## Anderson-Darling 0.5362 0.1485 ## ----------------------------------------------- ols_test_breusch_pagan(fit.bosque) ## ## Breusch Pagan Test for Heteroskedasticity ## ----------------------------------------- ## Ho: the variance is constant ## Ha: the variance is not constant ## ## Data ## ------------------------------- ## Response : vol ## Variables: fitted values of vol ## ## Test Summary ## ---------------------------- ## DF = 1 ## Chi2 = 0.8615473 ## Prob &gt; Chi2 = 0.353306 car::durbinWatsonTest(fit.bosque) ## lag Autocorrelation D-W Statistic p-value ## 1 0.1645488 1.487166 0.17 ## Alternative hypothesis: rho != 0 Todos los tests resultan no significativos indicando que se cumplen las hipótesis del modelo. Para la hipótesis de normalidad nos debemos fijar en los resultados de Kolmogorov-Smirnov que tiene un mejor comportamiento, desde el punto de vista estadístico, que Shapiro-Wilk. En caso de discrepancias entre ellos nos debemos quedar con Kolmmogorov-Smirnov. A pesar de que se cumplen las hipótesis del modelo, obtenemos las medidas de influencia asociadas con el modelo ajustado: ols_plot_cooksd_chart(fit.bosque) ols_plot_dfbetas(fit.bosque) ols_plot_dffits(fit.bosque) La distancia de Cook muestra dos observaciones (1 y 20) como potencialmente influyentes utilizando el punto de corte más restrictivo. Si utilizamos el punto de corte estándar que determina como influyente a los que tienen una distancia de Cook mayor que 1, ninguna de ellas sería clasificada como influyente. EL resto de medidas de influencia siguen mostrando a las observaciones 1 y 20 como potencialmente influyentes, pero dado que se cumplen las hipótesis del modelo no nos planteamos la eliminación de dichas observaciones. Además, con tamaños de muestras tan pequeños sólo nos planteamos su eliminación si es la única solución para que se cumplan las hipótesis del modelo. 3.6.9.2 Datos de Concentración Ajustamos el modelo y obtenemos los valores de diagnóstico. # Modelos fit.concen &lt;- lm(concen ~ p.cuerpo + dosis, data = concentracion) # Valores de diagnóstico diag.concen &lt;- fortify(fit.concen) Estudiamos los gráficos de diagnóstico para detectar residuos grandes (gráfico secuencial de residuos), linealidad y homocedasticidad (gráfico de residuos estandarizados versus valores ajustados), y normalidad (gráfico qq). ols_plot_resid_stand(fit.concen) ggplot(diag.concen, aes(x = .fitted, y = .stdresid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ols_plot_resid_qq(fit.concen) acf(diag.concen$.stdresid) Aunque todos los gráficos parecen mostrar comportamientos adecuados, el gráfico de residuos versus ajustados muestra un punto alejado (valor ajustado alto) del resto lo que podría indicar que debemos tratar ese valor como anómalo y considerar su eliminación del banco de datos. Antes de tomar una decisión revisamos toda la batería de gráficos y tests de diagnóstico e influencia. ggplot(diag.concen, aes(x = p.cuerpo, y = .stdresid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(diag.concen, aes(x = dosis, y = .stdresid)) + geom_point()+ geom_smooth(method = &quot;lm&quot;, se = FALSE) En los gráficos de residuos versus predictoras no se observan comportamientos anómalos. Realizamos ahora los tests necesarios para verificar las hipótesis de normalidad, homocedasticidad, e independencia. ols_test_normality(fit.concen) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9544 0.4672 ## Kolmogorov-Smirnov 0.1414 0.7919 ## Cramer-von Mises 5.4464 0.0000 ## Anderson-Darling 0.3769 0.3742 ## ----------------------------------------------- ols_test_breusch_pagan(fit.concen) ## ## Breusch Pagan Test for Heteroskedasticity ## ----------------------------------------- ## Ho: the variance is constant ## Ha: the variance is not constant ## ## Data ## ---------------------------------- ## Response : concen ## Variables: fitted values of concen ## ## Test Summary ## ----------------------------- ## DF = 1 ## Chi2 = 0.05255342 ## Prob &gt; Chi2 = 0.8186782 car::durbinWatsonTest(fit.concen) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.0227244 1.762427 0.536 ## Alternative hypothesis: rho != 0 Todos los tests resultan no significativos indicando que se cumplen las hipótesis del modelo. En último lugar realizamos el análisis de influencia: ols_plot_cooksd_chart(fit.concen) ols_plot_dfbetas(fit.concen) ols_plot_dffits(fit.concen) La distancia de Cook muestra que la observación en la posición 3 es claramente influyente. De hecho, también es influyente en los coeficientes del modelo, y en el valor ajustado. Pasamos a eliminar dicha observación y a ajustar un nuevo modelo. Comenzaremos con el modelo saturado e iremos completando todo el análisis. Creamos el nuevo banco de datos y ajustamos el nuevo modelo: # Datos sin observación 3 concentracion &lt;- concentracion[-3, ] # Ajuste del modelo fit.concen &lt;- lm(concen ~ p.cuerpo + dosis, data = concentracion) # Modelo ajustado tab_model(fit.concen) concen Predictors Estimates CI p (Intercept) 0.33 -0.08 – 0.75 0.110 p.cuerpo -0.00 -0.04 – 0.03 0.797 dosis 0.88 -6.37 – 8.12 0.800 Observations 18 R2 / R2 adjusted 0.005 / -0.128 # Bondad del ajuste glance(fit.concen) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00483 -0.128 0.0762 0.0364 0.964 3 22.4 -36.9 -33.3 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; EL test \\(F\\) de la regresión nos indica que la variables predictoras no están relacionadas con la respuesta (p-valor &gt; 0.05), con lo que no tendría sentido seguir trabajando con este modelo y la conclusión obtenida es que no hemos podido obtener una relación entre concentración y peso cuerpo, peso del hígado, y dosis. El efecto de eliminar una observación (a pesar de que se cumplen las hipótesis del modelo), es que la débil relación que habíamos establecido entre concentración frente a peso del cuerpo y dosis resulta inexistente. En este caso debe ser el investigador el que decida entre las dos opciones: Quedarse con un modelo malo (sin quitar la observación influyente) que verifica las hipótesis. Concluir que no existe relación entre respuesta y predictoras, desechando el experimento realizado. 3.6.9.3 Datos de Papel Ajustamos el modelo y obtenemos los valores de diagnóstico. # Modelos fit.papel &lt;- lm(tension ~ madera + I(madera^2), data = papel) # Valores de diagnóstico diag.papel &lt;- fortify(fit.papel) Estudiamos los gráficos de diagnóstico para detectar residuos grandes (gráfico secuencial de residuos), linealidad y homocedasticidad (gráfico de residuos estandarizados versus valores ajustados), y normalidad (gráfico qq). ols_plot_resid_stand(fit.papel) ggplot(diag.papel, aes(x = .fitted, y = .stdresid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ols_plot_resid_qq(fit.papel) acf(diag.papel$.stdresid) No se observan residuos excesivamente grandes, ni comportamientos anómalos pero si cierta autocorrelación en los residuos debido a la propia estructura del modelo polinómico considerado. Realizamos los tests de diagnóstico: ols_test_normality(fit.papel) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9113 0.0783 ## Kolmogorov-Smirnov 0.198 0.3942 ## Cramer-von Mises 1.5965 1e-04 ## Anderson-Darling 0.6399 0.0806 ## ----------------------------------------------- ols_test_breusch_pagan(fit.papel) ## ## Breusch Pagan Test for Heteroskedasticity ## ----------------------------------------- ## Ho: the variance is constant ## Ha: the variance is not constant ## ## Data ## ----------------------------------- ## Response : tension ## Variables: fitted values of tension ## ## Test Summary ## ---------------------------- ## DF = 1 ## Chi2 = 0.2755593 ## Prob &gt; Chi2 = 0.5996267 car::durbinWatsonTest(fit.papel) ## lag Autocorrelation D-W Statistic p-value ## 1 0.6040252 0.6974667 0 ## Alternative hypothesis: rho != 0 Se verifican las hipótesis de homocedasticidad y normalidad, y como era de espera no se cumple la hipótesis de incorrrelación. Dado que este incumplimiento se debe a la propia estructura del modelo no tendremos en cuenta este resultado par concluir sobre este modelo. En último lugar realizamos el análisis de influencia: ols_plot_cooksd_chart(fit.papel) ols_plot_dfbetas(fit.papel) ols_plot_dffits(fit.papel) Tenemos dos observaciones (18 y 19) que se detectan como potencialmente influyentes (distancia de Cook) y que podrían ser consideradas para su eliminación. Como el modelo tiene un buen ajuste y cumple con las hipótesis consideramos el modelo como válido. 3.7 Predicción El proceso de predicción en este tipo de modelos es similar al de los modelos de regresión lineal simple. Si \\(X_0 \\in \\mathbb{R}^p\\) representa un vector fijo de valores de las variables explicativas contenidas en la matriz de diseño \\(X\\), podemos predecir la respuesta \\(y\\) en \\(X_0\\) a través del modelo ajustado con \\[ \\hat{y}=X_0 \\hat{\\beta}, \\] pero el error asociado a la estimación depende de la situación que estemos prediciendo: 3.7.1 Estimación de la respuesta media. La varianza asociada a dicha estimación viene dada por: \\[ Var[\\hat{E}(y|X_0)]=\\sigma^2 X_0(X&#39;X)^{-1}X_0&#39;. \\] Un intervalo de confianza a nivel \\(1-\\alpha\\) está basado en la distribución \\(t-Student\\): \\[ \\hat{y}_{X_0} \\pm t_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 X_0(X&#39;X)^{-1}X_0&#39;}, \\] siendo \\(t_{(n-p, 1-\\alpha/2)}\\) el cuantil \\(1-\\alpha/2\\) de una distribución \\(t-Student\\) con \\(n-p\\) grados de libertad, con \\(p\\) el número de coeficientes en el modelo y \\(n\\) el número de datos. 3.7.2 Predicción de nuevas observaciones. La predicción de la respuesta \\(y\\) para un determinado valor \\(X_0\\) de las variables explicativas involucra más incertidumbre que la estimación de un promedio. En este caso, la varianza asociada a la predicción es: \\[ Var(\\hat{y}|X_0)=\\sigma^2 (1+ X_0(X&#39;X)^{-1}X_0&#39;). \\] Un intervalo de confianza a nivel \\(1-\\alpha\\) para dicha predicción viene dado por: \\[ \\hat{y}_{X_0} \\pm t_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 \\ (1+X_0&#39;(X&#39;X)^{-1}X_0)}. \\] 3.7.3 Ejemplos Obtenemos únicamente la predicción de la respuesta media para ciertos valores de las predictoras y los gráficos de predicción marginales. 3.7.3.1 Datos de bosque Estamos interesados en conocer el volumen de madera que podemos obtener para las combinaciones de d16 y ht dadas por (10, 90), (12, 95), (14, 100) y (16, 105). # cargamos datos de predicción newpred &lt;- data.frame(d16 = c(10, 12, 14, 16), ht = c(90, 95, 100, 105)) # Predicción para la media de la respuesta # Opción interval = &quot;confidence&quot; newdata &lt;- data.frame(newpred, predict(fit.bosque, newpred, interval = &quot;confidence&quot;)) round(newdata, 2) ## d16 ht fit lwr upr ## 1 10 90 29.11 25.03 33.20 ## 2 12 95 47.32 44.83 49.82 ## 3 14 100 65.53 63.35 67.71 ## 4 16 105 83.74 80.24 87.24 Veamos ahora las predicciones marginales de cada variable: # Gráfico del ajuste plot_model(fit.bosque, &quot;pred&quot;, ci.lvl = 0.95, show.data = TRUE, title = &quot; &quot;) ## $d16 ## ## $ht 3.7.3.2 Datos de papel Estamos interesados en conocer la tensión del papel para los contenidos de madera dados por 4, 8, y 12. # cargamos datos de predicción newpred &lt;- data.frame(madera = c(4, 8, 12)) # Predicción para la media de la respuesta # Opción interval = &quot;confidence&quot; newdata &lt;- data.frame(newpred, predict(fit.papel, newpred, interval = &quot;confidence&quot;)) round(newdata, 2) ## madera fit lwr upr ## 1 4 30.23 27.48 32.98 ## 2 8 46.83 43.63 50.03 ## 3 12 43.12 39.87 46.37 Veamos ahora las predicciones marginales de cada variable: # Gráfico del ajuste plot_model(fit.papel, &quot;pred&quot;, ci.lvl = 0.95, show.data = TRUE, axis.title = c(&quot;Contenido madera&quot;, &quot;Tensión&quot;), title = &quot; &quot;) ## $madera 3.8 Ejercicios A continuación, se presenta una colección de ejercicios referidos a los modelos de regresión. Los pasos a seguir para la obtención del modelo son los que hemos ido desarrollando: representación gráfica y propuesta de modelo, ajuste, bondad de ajuste, diagnóstico y predicción. En caso de encontrar problemas con el diagnóstico se deberá proponer un nuevo modelo alternativo. Ejercicio 1. Se propone a una empresa que fabrica vasos de cristal un nuevo proceso de control de calidad. Hasta ahora la empresa seleccionaba una caja de vasos al final de la fabricación y observaba si había alguno roto. Esto provocaba un gran gasto ya que en caso de encontrar algún defecto la caja se desembala y los vasos vuelven a la cadena de embalaje. Ahora se propone seleccionar vasos antes de embalar y determinar así el porcentaje de defectos. Se desea saber si ambos porcentajes están relacionados. Los datos aparecen a continuación: # carga de datos cajas &lt;- c(3.0, 3.1, 3.0, 3.6, 3.8, 2.7, 3.1, 2.7, 2.7, 3.3, 3.2, 2.1, 3.0, 2.6) vasos &lt;- c(3.1, 3.9, 3.4, 4.0, 3.6, 3.6, 3.1, 3.6, 2.9, 3.6, 4.1, 2.6, 3.1, 2.8) ejer01 &lt;- data.frame(cajas, vasos) Ejercicio 2. En 1929 Edwin Hubble investiga la relación entre la distancia (en megaparsecs. 1 parsec=3.26 años luz) de una galaxia a la tierra y la velocidad (en Km/sg) con la cual parece retroceder. Observo que las galaxias aparecen alejarse de nosotros no importa cual sea la dirección en que miremos. Se piensa de hecho que esto es el resultado del ”Big Bang”. Hubble esperaba aportar conocimiento sobre la formación del universo y sobre cual podría ser su evolución en el futuro. Los datos que recogió incluyen las distancias de la tierra a 24 galaxias, así como sus correspondientes velocidades de retroceso. El objetivo principal del estudio que Hubble llevo a cabo era determinar la edad del universo, y para ello establece lo que se conoce hasta ahora como la ley de Hubble: \\[V = H_0*D\\] donde \\(V\\) es la velocidad, \\(D\\) es la distancia, y \\(H_0\\) es la constante de Hubble. distancia &lt;- c(.032, .034, .214, .263, .275, .275, .45, .5, .5, .63, .8, .9, .9, .9, .9, 1.0, 1.1, 1.1, 1.4, 1.7, 2.0, 2.0, 2.0, 2.0) velocidad &lt;- c(170, 290, -130, -70, -185, -220, 200, 290, 270, 200, 300, -30, 650, 150, 500, 920, 450, 500, 500, 960, 500, 850, 800, 1090) ejer02 &lt;- data.frame(distancia, velocidad) (HINT: En primer lugar debes transformar el modelo teórico propuesto para expresarlo en términos de un modelo RLS. Recuerda almacenar las transformaciones en tu banco de datos para proceder con el ajuste del modelo.) Ejercicio 3. Un grupo de ingenieros agrónomos esta estudiando la evolución de un proceso infeccioso sobre un conjunto de plantas. En concreto durante un período de días se calcula de forma aproximada la fracción de infección de dichas plantas. Por este motivo, dos días consecutivos pueden tener fracciones de infección inferiores. Para evitar esto se proponen dos modelos teóricos: \\[(I) \\text{ } y = \\beta_0 x^{\\beta_1}\\] \\[(II) \\text{ } y = \\beta_0 + \\beta_1 log(x)\\] donde y es el grado de infección y x el número de días transcurridos dia &lt;- 1:200 infecc &lt;- c(0.02063762, 0.05637206, 0.11889346, 0.11376972, 0.13772089, 0.19055130, 0.18509772, 0.16887353, 0.19753055, 0.21835949, 0.26677238, 0.26360653, 0.27772497, 0.28447172, 0.28300082, 0.34108177, 0.32594214, 0.28675461, 0.34971616, 0.33537176, 0.33217888, 0.35748261, 0.34925483, 0.36278067, 0.37211460, 0.35783240, 0.41498583, 0.40769174, 0.38800213, 0.44174297, 0.43087261, 0.42190606, 0.45097338, 0.45570700, 0.45946960, 0.46153400, 0.46340109, 0.45549254, 0.45487365, 0.45750686, 0.45521341, 0.46881463, 0.45141048, 0.52372846, 0.50803021, 0.46482596, 0.48254773, 0.48449405, 0.51255671, 0.49833262, 0.50802495, 0.50526564, 0.50777983, 0.53873568, 0.50950327, 0.54693458, 0.48815063, 0.53327501, 0.52645577, 0.53063462, 0.53618898, 0.52077545, 0.52633078, 0.51474555, 0.51575426, 0.54528626, 0.55015967, 0.54419107, 0.56346905, 0.58787669, 0.53886562, 0.50427534, 0.57230842, 0.53970820, 0.54179538, 0.57769618, 0.55308538, 0.53593047, 0.56550374, 0.56060245, 0.56496884, 0.57400395, 0.56030226, 0.58199322, 0.56606007, 0.57844415, 0.59505931, 0.58311616, 0.56916054, 0.59989923, 0.59801493, 0.59031303, 0.58529898, 0.56912505, 0.61003513, 0.57193641, 0.62878888, 0.61677661, 0.58247460, 0.56770688, 0.57505675, 0.59541545, 0.58634056, 0.58530427, 0.57418797, 0.59326985, 0.57940758, 0.56266765, 0.58932866, 0.61620602, 0.58719856, 0.61173102, 0.56806743, 0.60015458, 0.61248238, 0.60893367, 0.60582869, 0.59169408, 0.58829584, 0.58557803, 0.60917339, 0.58862023, 0.59849746, 0.60391548, 0.64663334, 0.59742612, 0.61587231, 0.61341390, 0.59329848, 0.61178139, 0.64276168, 0.62355522, 0.61599580, 0.60735889, 0.57537341, 0.63968664, 0.58846078, 0.63307852, 0.65706008, 0.59059116, 0.63408846, 0.61538542, 0.58975607, 0.59146830, 0.59028687, 0.61224876, 0.59417456, 0.63770437, 0.66647829, 0.59925939, 0.64127259, 0.64141050, 0.63317968, 0.60686830, 0.62514131, 0.62241142, 0.63976259, 0.62153212, 0.64899315, 0.62242964, 0.65143794, 0.60985758, 0.60609047, 0.69656194, 0.62384676, 0.63858650, 0.64578674, 0.62380855, 0.64424572, 0.64170765, 0.63043627, 0.63646096, 0.63488074, 0.67853395, 0.62153691, 0.61483840, 0.63790480, 0.64374543, 0.64664922, 0.62913057, 0.61740673, 0.66430865, 0.63241999, 0.62246721, 0.63541282, 0.63655235, 0.66304830, 0.64289529, 0.65662894, 0.63190605, 0.64652159, 0.63607656, 0.64479640, 0.62532881, 0.61734833, 0.68383389, 0.65622608, 0.61950582, 0.63262438, 0.62145169) ejer03 &lt;- data.frame(dia, infecc) Los modelos teóricos anteriores se pueden transformar a modelos de regresión lineal simple sin muchos problemas. Para el modelo (I) consideramos la transformación logarítmica: \\[log(y) = log(\\beta_0) + \\beta_1 log(x) \\longrightarrow y&#39; = \\beta_0 + \\beta_1 x&#39;\\] donde el \\(&#39;\\) indica la nueva variable o parámetro. Podemos estudiar el comportamiento lineal obteniendo dichas variables, y volver al modelo original utilizando las transformaciones. Para el modelo (II) ya viene expresado como un modelo lineal donde únicamente tenemos que obtener la variable predictora transformada mediante la función logaritmo. Ejercicio 4. Se trata de determinar la pérdida de color sufrida por cierto compuesto cuando es sometido a altas temperaturas. Los datos recogidos son los siguientes: temperatura &lt;- c(460, 450, 440, 430, 420, 410, 450, 440, 430, 420, 410, 400, 420, 410, 400) perdida &lt;- c(0.3, 0.3, 0.4, 0.4, 0.6, 0.5, 0.5, 0.6, 0.6, 0.6, 0.7, 0.6, 0.6, 0.6, 0.6) ejer04 &lt;- data.frame(temperatura, perdida) Ejercicio 5. Karl Pearson recogió información sobre 1100 familias en Inglaterra en el periodo de 1893 a 1898. En particular este banco de datos contiene las alturas de las madres y las hijas de dichas familias. En concreto se registran las alturas de las hijas mayores de 18 años y las madres de menos de 65 años. Originalmente se estaba interesado en estudiar una posible asociación entre la altura de las madres y las hijas ¿Que podríamos concluir a la vista de los datos? las variables recogidas son: “Mheight” (altura de la madre) y “Dheight” (altura de la hija) data(&quot;heights&quot;) ejer05 &lt;-heights Ejercicio 6. Se realiza un estudio de campo para conocer el desarrollo de cierta especie de pez del lago lakemary en EEUU. Para medir el desarrollo se establece la edad de cada pez capturado mediante un procedimiento proporcionado por los biólogos. Además se mide la longitud del pez para tratar de establecer el estado de maduración de cada ejemplar. La investigación trata de relacionar la longitud del pez con su edad para determinar el número de capturas permitidas. Las variables recogidas son: “Age” (edad del pez), y “Length” (longitud del pez en mm). data(&quot;lakemary&quot;) ejer06 &lt;-lakemary Ejercicio 7. Se realiza un estudio para determinar el tiempo de erupción del geyser (Old Faithful Geyser, dentro del parque nacional de Yellowstone) a partir del tiempo de espera entre dos erupciones consecutivas. Se trata de analizar la información para determinar el periodo de visitas. Para ellos se recogieron datos durante un periodo determinado que se considera estándar para establecer la distribución de erupciones. Las variables consideradas son: “Duration” (duración de la erupción en segundos) y “Interval” (tiempo de espera entres dos erupciones consecutivas). data(&quot;geyser&quot;) ejer07 &lt;-geyser Ejercicio 8. Los datos muestran el porcentaje de calorías totales obtenidas de carbohidratos complejos, para veinte diabéticos dependientes de insulina que habían seguido una dieta alta en carbohidratos durante seis meses. Se consideró que el cumplimiento del régimen estaba relacionado con la edad (en años), age, el peso corporal (relativo al peso “ideal” para la altura), weight, y otros componentes de la dieta como el porcentaje de proteínas ingeridas. Los datos corresponden con la tabla 6.3 de Dobson (2002). ejer08 &lt;- read_csv(&quot;https://goo.gl/Grm8xM&quot;, col_types = &quot;dddd&quot;) Ejercicio 9. Es bien sabido que la concentración de colesterol en el suero sanguíneo aumenta con la edad, pero es menos claro si el nivel de colesterol también está asociado con el peso corporal. Los datos muestran para una treinta de mujeres el colesterol sérico (milimoles por litro), la edad (años) y el índice de masa corporal (peso dividido por la altura al cuadrado, donde el peso se midió en kilogramos y la altura en metros). Los datos corresponden con la tabla 6.17 de Dobson (2002). ejer09 &lt;- read_csv(&quot;https://goo.gl/EKXWRc&quot;, col_types = &quot;ddd&quot;) Ejercicio 10. En un estudio sobre las diferentes clases de queso cheddar que se fabrican en LaTrobe Valley de Victoria, Australia, se analizaron muestras de queso por su composición química: concentración de ácido acético (escala logarítmica); concentración de sulfuro de hidrógeno (escala logarítmica); y la concentración de ácido láctico. Por otro lado, se paso una muestra de cada uno de ellos a un conjunto de catadores y se registro la puntuación obtenida por cada uno de ellos. Estamos interesados en relacionar la puntuación final de los catadores con los resultados del análisis químico. ejer10 &lt;- read_csv(&quot;https://goo.gl/V4lDNs&quot;, col_types = &quot;dddd&quot;) Ejercicio 11. Los datos correspondientes a esta base de datos muestran la producción, Q, en toneladas, la mano de obra, L, en horas, y el capital, K, en horas máquina, de 14 empresas de un sector industrial. Se desea ajustar los datos a la función de producción Cobb-Douglas dada por: \\[Q = A L^l K^k e^{\\epsilon}\\] Identifica el objetivo u objetivos del problema propuesto. Describe el tipo de variables involucradas para el modelo propuesto. Escribe los modelos, identifica el tipo, e indica el número de parámetros involucrados. Ajusta los modelos mediante el proceso de selección de variables que consideres más adecuado. Describe los modelos obtenidos (ecuación del modelo) interpretando en términos económicos los coeficientes estimados de dicho modelo, y las medidas de bondad de ajuste de cada uno de ellos. Realiza el diagnóstico del modelo ajustado indicando los gráficos utilizados y las conclusiones obtenidas. Si el modelo debe ser mejorados indica como lo haces, y describe los resultados del nuevo modelo obtenido. ¿Cuál es la predicción de la producción media para una empresa con 1900 horas de trabajo y 475 de capital? ¿Cuál consideras que es la combinación óptima para determinar las empresas más eficientes en términos de productividad?. ejer11 &lt;- read_csv(&quot;https://bit.ly/2UIq9F9&quot;, col_types = &quot;dddd&quot;) ejer11 &lt;- ejer11 %&gt;% mutate_if(sapply(ejer11, is.character), as.factor) Ejercicio 12. La producción de cereales viene determinada principalmente por las condiciones climáticas previas a la recolección. En concreto se recogen las condiciones climáticas en diferentes años. Las características medidas son: anyo = año de la medición. preinv = precipitación de invierno. tempmay = temperatura de mayo. prejun = precipitación de junio. tempjun = temperatura de junio. prejul = precipitación de julio. tempjul = temperatura de julio. preago = precipitación de agosto. tempago = temperatura de agosto. produccion = producción de cereales. Para el banco de datos correspondiente Propón el tipo de modelo a utilizar. Selecciona el mejor modelo basado en el AIC. Expresa el modelo obtenido y extrae todas las conclusiones que se deriven de él. ¿Crees que un procedimiento de selección basado en el test F proporcionaría el mismo modelo? Las variables no incluidas en dicho modelo puede considerarse que no contribuyen a explicar la producción de cereales. ¿Qué haces para comprobar esto? ¿Las variables incluidas contribuyen de forma significativa a explicar la producción de cereales? Realiza un análisis de influencia y comenta los resultados obtenidos. ¿Crees necesario plantear un nuevo modelo? Comprueba las hipótesis sobre el modelo ajustado y comenta los resultados. Si resulta necesario propón un modelo alternativo y analízalo de forma completa. lectura &lt;- read.table(&quot;https://goo.gl/Yi5g6S&quot;, header = TRUE) ejer12 &lt;- as_tibble(lectura) Ejercicio 13. En un estudio medio ambiental sobre la diversidad de especies de tortuga en las islas Galápagos se recogió información sobre el número de especies endémicas (Endemics), así como el área de la isla (área), la altura del pico más alto de a isla (Elevation), la distancia a la isla más cercana (Nearest), la distancia a la isla de Santa Cruz (Scruz), y el área de la isla más próxima (Adjacent). El estudio está interesado las condiciones que pueden afectar a un mayor número de especies endémicas de tortuga sin tener en cuenta el número total de especies presentes. Identifica la variable respuesta, las predictoras, y el tipo de cada una de ellas. Realiza e interpreta los gráficos individuales descriptivos entre la respuesta y cada predictora de forma individual. Para tratar de linealizar las relaciones entre respuesta y predictora se transforman tomas las variables con la función logaritmo neperiano. ¿Cómo se interpretan los gráficos individuales descriptivos entre las variables transformadas? ¿Qué tipo de modelo parece el más adecuado para esta situación experimental? Escribe la forma reducida de dicho modelo. Partiendo del modelo más complejo y utilizando el AIC como criterio de selección, escribe la ecuación del modelo resultante del proceso de selección. ¿Qué podemos decir de la bondad del ajuste del modelo obtenido? ¿Cómo interpretamos los gráficos y test de diagnóstico asociados con el modelo obtenido? Realiza un análisis de influencia y si lo consideras necesario ajusta un nuevo modelo y analízalo. En función del modelo obtenido, construye diferentes escenarios para predecir el número de especies en una isla en función de las variables predictoras presentes en él. ¿Cuáles son las condiciones óptimas para la determinación de especies endémicas? Endemics &lt;- c(23, 21, 3, 9, 1, 11, 0, 7, 4, 2, 26, 35, 17, 4, 19, 89, 23, 2, 37, 33, 9, 30, 65, 81, 95, 28, 73, 16, 8, 12) Area &lt;- c(25.09, 1.24, 0.21, 0.1, 0.05, 0.34, 0.08, 2.33, 0.03, 0.18, 58.27, 634.49, 0.57, 0.78, 17.35, 4669.32, 129.49, 0.01, 59.56, 17.95, 0.23, 4.89, 551.62, 572.33, 903.82, 24.08, 170.92, 1.84, 1.24, 2.85) Elevation &lt;- c(346, 109, 114, 46, 77, 119, 93, 168, 71, 112, 198, 1494, 49, 227, 76, 1707, 343, 25, 777, 458, 94, 367, 716, 906, 864, 259, 640, 147, 186, 253) Nearest &lt;- c(0.6, 0.6, 2.8, 1.9, 1.9, 8, 6, 34.1, 0.4, 2.6, 1.1, 4.3, 1.1, 4.6, 47.4, 0.7, 29.1, 3.3, 29.1, 10.7, 0.5, 4.4, 45.2, 0.2, 0.6, 16.5, 2.6, 0.6, 6.8, 34.1) Scruz &lt;- c(0.6, 26.3, 58.7, 47.4, 1.9, 8, 12, 290.2, 0.4, 50.2, 88.3, 95.3, 93.1, 62.2, 92.2, 28.1, 85.9, 45.9, 119.6, 10.7, 0.6, 24.4, 66.6, 19.8, 0, 16.5, 49.2, 9.6, 50.9, 254.7) Adjacent &lt;- c(1.84, 572.33, 0.78, 0.18, 903.82, 1.84, 0.34, 2.85, 17.95, 0.1, 0.57, 4669.32, 58.27, 0.21, 129.49, 634.49, 59.56, 0.1, 129.49, 0.03, 25.09, 572.33, 0.57, 4.89, 0.52, 0.52, 0.1, 25.09, 17.95, 2.33) ejer13 &lt;- data.frame(Endemics, Area, Elevation, Nearest, Scruz, Adjacent) "],
["anova.html", "Unidad 4 Modelos ANOVA 4.1 Bancos de datos 4.2 Modelos ANOVA 1 vía 4.3 Modelos ANOVA dos vías 4.4 Estimación y bondad de ajuste del modelo 4.5 Comparación y selección de modelos 4.6 Diagnóstico 4.7 Predicción 4.8 Ejercicios", " Unidad 4 Modelos ANOVA Los modelos ANOVA surgen cuando la variable o variables predictoras son de tipo categórico, es decir, factores con diferentes niveles de clasificación, de forma que cada sujeto es medido (respecto de la respuesta) para una combinación específica de los factores considerados. La variable respuesta sigue siendo de tipo numérico y el objetivo principal es el estudio de la media para los diferentes niveles del factor o combinaciones de los factores. No solo se está interesado en comparar los diferentes grupos, sino además en cuantificar numéricamente esas diferencias. La principal diferencia con los modelos de regresión es que en este caso no modelizamos toda la respuesta observada sino la media observada del conjunto de observaciones para cada nivel del factor. Además, veremos como estos modelos pueden ser descritos en términos de un modelo lineal de regresión lo que nos permite utilizar parte de los procesos de descripción, análisis y diagnóstico utilizamos en los capítulos anteriores. En cada punto de este capítulo estudiaremos las similitudes y diferencias con los modelos de regresión. 4.1 Bancos de datos Si las variables explicativas son de tipo factor, el análisis preliminar se realiza mediante gráficos de cajas. Estos gráficos sirven para visualizar relaciones entre una variable continua y un factor. En estos casos, hablar de relación / asociación equivale a hablar de diferencias en la variable continua respecto de los niveles (categorías) del factor. Tales diferencias se detectan cuando los diagramas de cajas no muestran solapamientos en la escala de la variable respuesta. En los modelos de dos vías podremos realizar además un gráfico de interacción, que es un gráfico de líneas donde se representan las medias de las diferentes combinaciones de niveles del factor, y que nos permite establecer una primera conclusión sobre el comportamiento conjunto de ambos factores. Veamos los diferentes bancos de datos que iremos analizando a lo largo de la unidad. Ejemplo 1. Datos de Insecticidas. Este ejemplo contiene los datos de un experimento agronómico para conocer el efecto de diferentes insecticidas (spray) sobre el número de insectos vivos (count) tras un periodo de tratamiento. Cargamos los datos y realizamos el gráfico descriptivo: data(InsectSprays) insecticidas &lt;- InsectSprays ggplot(insecticidas,aes(x = spray, y = count)) + geom_boxplot() Se aprecia como el número de insectos vivos es superior cuando usamos los sprays A, B, y F (con medias muy similares). El resto de sprays muestran (medias) una supervivencia inferior. Se observan como dos grupos de sprays, uno con medias altas y otro con medias bajas. Ejemplo 2. Datos de Envasado. Se desea estudiar la fabricación de cuatro tipos de máquinas automáticas (maquina) en el cortado de piezas de embutido para envasado. Para ello se toman datos del número de envases sin defecto (produccion) que produce cada máquina durante periodos de una hora. Cargamos los datos y realizamos el gráfico descriptivo: maquina &lt;- c(rep(&quot;M1&quot;, 4),rep(&quot;M2&quot;, 4),rep(&quot;M3&quot;, 4),rep(&quot;M4&quot;, 4)) produccion &lt;- c(103, 115, 101, 105, 109, 106, 116, 124, 104, 98, 117, 99, 128, 117, 121, 130) envasado &lt;- data.frame(maquina, produccion) ggplot(envasado,aes(x = maquina, y = produccion)) + geom_boxplot() Cada máquina muestra un nivel de envases sin defecto diferente. La máquina M4 es la que muestra mejores resultados y la M3 los peores. Ejemplo 3. Datos de venenos. Se ha realizado un experimento para comprobar la efectividad de diferentes antídotos (AA, AB, AC y AD) frente a diferentes venenos (VA, VB y VC). Para ello se recoge el tiempo de reacción (tiempo) que cada antídoto tarda en hacer efecto para cada veneno. Cargamos los datos y realizamos el gráfico descriptivo: tiempo &lt;- c(0.31, 0.45, 0.46, 0.43, 0.36, 0.29, 0.4, 0.23, 0.22, 0.21, 0.18, 0.23, 0.82, 1.1, 0.88, 0.72, 0.92, 0.61, 0.49, 1.24, 0.3, 0.37, 0.38, 0.29, 0.43, 0.45, 0.63, 0.76, 0.44, 0.35, 0.31, 0.4, 0.23, 0.25, 0.24, 0.22, 0.45, 0.71, 0.66, 0.62, 0.56, 1.02, 0.71, 0.38, 0.3, 0.35, 0.31, 0.33) antidoto &lt;- factor(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4), labels=c(&quot;AA&quot;, &quot;AB&quot;, &quot;AC&quot;, &quot;AD&quot;)) veneno &lt;- factor(c(1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3), labels=c(&quot;VA&quot;, &quot;VB&quot;, &quot;VC&quot;)) venenos &lt;- data.frame(tiempo, antidoto, veneno) # Diagrama de cajas ggplot(venenos,aes(x = antidoto, y = tiempo, color = veneno)) + geom_boxplot() # Gráfico de interacción de medias ggplot(venenos, aes(x = antidoto, y = tiempo, group = veneno, color = veneno)) + stat_summary(fun = mean, geom = &quot;point&quot;) + stat_summary(fun = mean, geom = &quot;line&quot;) En el gráfico de interacción se puede apreciar que antídoto es más efectivo para cada tipo de veneno. demás se puede ver que las curvas del tiempo de reacción (líneas de colores) no tiene el mismo comportamiento lo que indica habitualmente que hay cierto efecto de interacción entre los factores considerados, es decir, existe dos combinaciones antídoto - veneno que presentan resultados significativos (medias distintas). 4.2 Modelos ANOVA 1 vía El modelo ANOVA de una vía se plantea cuando tenemos una única variable predictora de tipo categórico y una respuesta de tipo numérico. Dicho modelo se puede describir de forma general, para una muestra de tamaño \\(n\\), como: Una variable respuesta, \\(Y\\), de tipo numérico con observaciones \\(y_1, \\ldots ,y_n\\). Una variable predictora, \\(F\\), de tipo categórico con \\(I\\) grupos o niveles distintos de tamaños muestrales \\(n_1, n_2, \\ldots , n_I\\), de forma que \\(n = n_1 + n_2 + \\ldots + n_I\\), y el vector de observaciones de la respuesta se puede escribir como: \\[ y_{11},\\ldots,y_{1n_1},y_{21},\\ldots,y_{2n_2},\\ldots,y_{I1},\\ldots,y_{In_I} \\] donde el primer subíndice indica el nivel del factor y el segundo la posición dentro del conjunto de datos de dicho nivel del factor. Conjunto \\(\\mu_i\\) de medias de todas las observaciones de la respuesta asociadas con el nivel \\(i\\) del factor, es decir: \\[ \\mu_i = \\frac{\\sum_{j = 1}^{n_j} y_{ij}}{n_i}; \\text{ i = 1, 2,..., I} \\] Media global de la respuesta, \\(\\mu\\), que se puede obtener como: \\[ \\mu = \\frac{\\sum_{j = 1}^{I} \\mu_{j}}{I} \\] Incrementos, \\(\\alpha_i\\), de cada una de las medias de cada grupo con respecto a la media global, es decir: \\[ \\alpha_i= \\mu - \\mu_i; \\text{ i = 1, 2,..., I} \\] El objetivo básico en este tipo de modelos es la comparación de las medias \\(\\mu_i\\) para detectar posibles diferencias entre os niveles del factor, es decir, plateamos el contraste de comparación de medias: \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\mu_1 = \\mu_2 = \\ldots = \\mu_I\\\\ H_a: &amp; \\mbox{Existen dos grupos al menos con medias distintas} \\end{array} \\right. \\] Este contraste es equivalente al que se puede escribir en términos de los incrementos de las medias: \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_I = 0\\\\ H_a: &amp; \\mbox{Al menos hay un incremento distinto de cero} \\end{array} \\right. \\] que tiene una estructura similar al test \\(F\\) de la regresión, donde queremos contrastar si existe algún coeficiente distinto de cero. Esto implica que el modelo ANOVA se puede escribir en términos de un modelo lineal sin más que considerar un conjunto de variables ficticias \\(X_1,...,X_{I}\\) que toman los valores siguientes: \\[ X_i= \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{si la respuesta pertenece al grupo i}\\\\ 0 &amp; \\mbox{en otro caso} \\end{array} \\right. \\] y un efecto común para todos los niveles del factor que viene dado por \\(\\mu\\) que denotamos por \\(\\alpha_0\\). La única diferencia entre ambas estructuras de comparación es que la referida a las medias \\(\\mu_i\\) tiene \\(I\\) parámetros, mientras que la de los incrementos tiene \\(I+1\\) parámetros. Para hacer equivalentes ambas estructuras añadimos la conocida como restricción de identificabilidad que establece de partida que uno de los incrementos debe ser igual a cero, es decir, que una media de un grupo coincide con la media global. Este es el nivel de referencia que utilizamos como base de comparación con el resto de niveles o grupos del factor. En esta situación si consideramos \\(Y_i = \\{y_{i1},...,y_{in_i}\\}\\), \\(1_{n_i} = \\{1,...,1\\}\\) un vector de \\(n_i\\) unos, \\(0_{n_i} = \\{0,...,0\\}\\) un vector de \\(n_i\\) ceros, para cada uno de los niveles \\(i\\) del factor, \\(\\mu = \\alpha_0\\), y la restricción de identificabilidad \\(\\alpha_I = 0\\) (podríamos elegir cualquier otro \\(\\alpha_j =0\\) y obtendríamos el mismo modelo), podemos escribir el modelo ANOVA como: \\[ Y = \\left( \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\ldots \\\\ Y_I \\\\ \\end{array} \\right) = \\left( \\begin{array}{ccccc} 1_{n_1} &amp; 1_{n_1} &amp; 0_{n_1} &amp; \\ldots &amp; 0_{n_1} \\\\ 1_{n_2} &amp; 0_{n_2} &amp; 1_{n_2} &amp; \\ldots &amp; 0_{n_2} \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 1_{n_I} &amp; 0_{n_I} &amp; 0_{n_I} &amp;\\ldots &amp; 1_{n_I} \\\\ \\end{array} \\right) \\left( \\begin{array}{c} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\ldots \\\\ \\alpha_{I_1} \\\\ \\end{array} \\right) + \\left( \\begin{array}{c} e_1 \\\\ e_2 \\\\ \\ldots \\\\ e_n \\\\ \\end{array} \\right) = X\\beta + \\epsilon \\] o equivalentemente en términos de las medias como: \\[\\begin{equation} \\mu_i = \\alpha_0 + \\alpha_i + \\epsilon_i, \\text{ para } i = 1, 2,...,k, \\text{ con } \\alpha_k = 0 \\tag{4.1} \\end{equation}\\] Las hipótesis de este modelo es que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante \\(\\sigma^2\\) para cada uno de los grupos que determina la variable predictora. En esta situación el contraste sobre los incrementos es equivalente al test \\(F\\) de regresión de los modelos RLM. 4.2.1 Especificacion del modelo en R A continuación se presenta la especificación en R de los modelos anteriores y de todos los anidados que pueden surgir a partir de ellos, así como su interpretación en términos de contraste de hipótesis o coeficientes asociados a cada efecto presente en el modelo. La especificación en R del modelo dado en la ecuación @ref{eq:aovonefac} para su estimación se realiza a través de la expresión reducida del modelo dada por: \\[ Y \\sim F \\] En este tipo de modelos se plantean varias situaciones de modelos anidados que deberemos estudiar: Modelo con efecto de \\(F\\): \\[\\begin{equation} Y \\sim F, \\tag{4.2} \\end{equation}\\] Si \\(F\\) resulta significativo, tenemos que se detectan diferencias entre al menos dos medias para los niveles del factor \\(F\\). Modelo sin efectos: \\[\\begin{equation} Y \\sim 1 \\tag{4.3} \\end{equation}\\] En este caso \\(F\\) no resulta significativo y podmeos conluir que las medias de todos los niveles del factor se pueden considerar iguales. En la práctica el ajuste de este tipo de modelos se puede plantear como una comparación entre los modelos (4.2) y (4.3), dado que están anidados, de forma similar al proceso de comparación de modelos planteado en los modelos RLM y MP. 4.3 Modelos ANOVA dos vías El modelo ANOVA de dos vías se presenta cuando consideramos dos variables predictoras de tipo categórico. Si tenemos dos factores \\(F_1\\) y \\(F_2\\) con \\(I\\) y \\(J\\) niveles respectivamente (y utilizando lo visto en el punto anterior), el modelo expresado en términos de las medias de las combinaciones de los diferentes niveles de los factores considerados viene dado por: \\[\\begin{equation} \\mu_{ij} = \\alpha_0 + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ij}, \\ \\mbox{para} \\ i = 1,\\ldots,I; \\ j = 1,\\ldots,J \\ \\mbox{con} \\ \\alpha_1 = \\beta_1 = \\alpha\\beta_{11} = 0 \\tag{4.4} \\end{equation}\\] donde: \\(\\mu_{ij}\\) es la media de la respuesta para el nivel \\(i\\) de \\(F_1\\) y el nivel \\(j\\) de \\(F_2\\). \\(\\alpha_0\\) es el efecto común a todas las combinaciones de niveles para ambos factores. \\(\\alpha_i\\) son los incrementos asociados a cada uno de los niveles del factor \\(F_1\\). \\(\\beta_j\\) son los incrementos asociados a cada uno de los niveles del factor \\(F_2\\). \\(\\alpha\\beta_{ij}\\) son los incrementos asociados a la combinación de los niveles \\(i\\) de \\(F_1\\) y \\(j\\) de \\(F_2\\). Es lo que se denomina efecto de interacción que valora los cambios en las combinaciones de las medias para ambos factores. Si este efecto no es significativo, la media para una combinación de niveles de los factores se construye sumando los incrementos de cada nivel de forma independiente. Dado que tenemos dos factores se hace necesario añadir más restricciones de identificabildiad. Añadimos una por cada factor y la que aparece de forma inmediata en la interacción de los dos efectos que estamos tomando como cero. En este caso no escribimos la matriz de diseño del modelo pero se puede escribir considerado la matriz de coeficientes de dimensiones \\(IJ \\times IJ\\) que surge para el modelo (4.4). Bastará con sustituir los unos y ceros por vectores de unos y ceros de dimensiones adecuadas en función de los tamaños de cada uno de los niveles de los dos factores considerados. Dicha matriz de coeficientes se puede extraer a partir de las ecuaciones completas del modelo (4.4) (teniendo en cuenta las restricciones de identificabilidad) dadas por: \\[\\begin{array}{lll} \\mu_{11} &amp; = \\alpha_0 + \\alpha_{1} + \\beta_{1} + \\alpha\\beta_{11} + \\epsilon_{11}&amp; = \\alpha_0 + \\epsilon_{11}\\\\ \\mu_{12} &amp; = \\alpha_0 + \\alpha_{1} + \\beta_{2} + \\alpha\\beta_{12} + \\epsilon_{12}&amp; = \\alpha_0 + \\beta_{2} + \\alpha\\beta_{12} + \\epsilon_{12}\\\\ \\ldots &amp; \\ldots &amp;\\ldots\\\\ \\mu_{1J} &amp; = \\alpha_0 + \\alpha_{1} + \\beta_{J} + \\alpha\\beta_{1J} + \\epsilon_{1J}&amp; = \\alpha_0 + \\beta_{J} + \\alpha\\beta_{1J} + \\epsilon_{1J}\\\\ \\mu_{21} &amp; = \\alpha_0 + \\alpha_{2} + \\beta_{1} + \\alpha\\beta_{21} + \\epsilon_{21}&amp; = \\alpha_0 + \\alpha_{2} + \\alpha\\beta_{21} +\\epsilon_{11}\\\\ \\mu_{22} &amp; = \\alpha_0 + \\alpha_{2} + \\beta_{2} + \\alpha\\beta_{22} + \\epsilon_{22}&amp; = \\alpha_0 + \\alpha_{2} + \\beta_{2} + \\alpha\\beta_{22} + \\epsilon_{22}\\\\ \\ldots &amp; \\ldots &amp;\\ldots\\\\ \\mu_{2J} &amp; = \\alpha_0 + \\alpha_{2} + \\beta_{J} + \\alpha\\beta_{2J} + \\epsilon_{2J}&amp; = \\alpha_0 + \\alpha_{2} + \\beta_{J} + \\alpha\\beta_{2J} + \\epsilon_{2J}\\\\ \\ldots &amp; \\ldots &amp;\\ldots\\\\ \\ldots &amp; \\ldots &amp;\\ldots\\\\ \\ldots &amp; \\ldots &amp;\\ldots\\\\ \\mu_{I1} &amp; = \\alpha_0 + \\alpha_{I} + \\beta_{1} + \\alpha\\beta_{I1} + \\epsilon_{I1}&amp; = \\alpha_0 + \\alpha_{I} + \\alpha\\beta_{I1} + \\epsilon_{I1}\\\\ \\mu_{I2} &amp; = \\alpha_0 + \\alpha_{I} + \\beta_{2} + \\alpha\\beta_{I2} + \\epsilon_{I2}&amp; = \\alpha_0 + \\alpha_{I} + \\beta_{2} + \\alpha\\beta_{I2} + \\epsilon_{I2}\\\\ \\ldots &amp; \\ldots &amp;\\ldots\\\\ \\mu_{IJ} &amp; = \\alpha_0 + \\alpha_{I} + \\beta_{J} + \\alpha\\beta_{IJ} + \\epsilon_{IJ}&amp; = \\alpha_0 + \\alpha_{I} + \\beta_{J} + \\alpha\\beta_{IJ} + \\epsilon_{IJ}\\\\ \\end{array}\\] En este caso el objetivo vuelve a ser la comparación de medias pero el contraste a resolver depende de la especificación del modelo considerado. Hay que tener en cuenta que en este caso no tenemos una única variable predictora, y además hay que valorar la posibilidad de la interacción entre ambos factores. 4.3.1 Especificacion del modelo en R La especificación en R del modelo dado en la ecuación @ref{eq:aovtwofac} para su estimación se realiza a través de la expresión reducida del modelo dada por: \\[\\begin{equation} Y \\sim F_1 + F_2 + F_1 : F_2, \\tag{4.5} \\end{equation}\\] donde \\(F_1 : F_2\\) representa el efecto de interacción, y \\(F_1\\) y \\(F_2\\) son los denominados efectos principales asociados con los factores considerados. En este tipo de modelos se plantean varias situaciones de modelos anidados que deberemos estudiar para establecer cual es el tipo de contraste sobre las medias que estamos resolviendo. Alternativamente ese modelo se puede escribir de forma más reducida como \\(Y \\sim F_1*F_2\\), que es un modelo con los efectos principales de cada factor y con la interacción. Modelo saturado o modelo con todos los factores y la interacción dado por: \\[\\begin{equation} Y \\sim F_1 + F_2 + F_1 : F_2 \\tag{4.6} \\end{equation}\\] Si la interacción está presente en el modelo el contraste planteado es: \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\mbox{No hay diferencias entre las medias conjuntas de ambos niveles del factor}\\\\ H_a: &amp; \\mbox{Al menos hay dos combinaciones de ambos factores con medias distintas} \\end{array} \\right. \\] que nos permite concluir sobre si el efecto de interacción es necesario para explicar el comportamiento de la respuesta. En caso de que dicho contraste resulte no significativo podemos plantear el modelo con ambos efectos principales. Modelo con ambos efectos principales: \\[ Y \\sim F_1 + F_2 \\] donde se parte de que la interacción es no significativa y se plantean dos contrastes (uno por cada factor) \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\mbox{No hay diferencias entre las medias de } F_1\\\\ H_a: &amp; \\mbox{Al menos hay dos medias de } F_1 \\mbox{ distintas} \\end{array} \\right. \\] Si no rechazamos este contraste deberemos considerar que no hay diferencias en las medias de los niveles establecidos en \\(F_1\\) y debemos considerar un modelo con efecto principal de \\(F_2\\). \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\mbox{No hay diferencias entre las medias de } F_2\\\\ H_a: &amp; \\mbox{Al menos hay dos medias de } F_2 \\mbox{ distintas} \\end{array} \\right. \\] Si no rechazamos este contraste deberemos considerar que no hay diferencias en las medias de los niveles establecidos en \\(F_2\\) y debemos considerar un modelo con efecto principal de \\(F_1\\). Estos contrastes nos permiten concluir sobre cada factor de forma independiente (no hay interacción entre ellos). Modelo con efecto principal de \\(F_1\\): \\[ Y \\sim F_1 \\] donde se parte de que la interacción y el efecto principal de \\(F_2\\) son no significativos y se plantea el contraste \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\mbox{No hay diferencias entre las medias de } F_1\\\\ H_a: &amp; \\mbox{Al menos hay dos medias de } F_1 \\mbox{ distintas} \\end{array} \\right. \\] Si no rechazamos este contraste deberemos considerar que no hay diferencias en las medias de los niveles establecidos en \\(F_1\\) y nuestro modelo final no contiene ningún tipo de efecto predictivo. Modelo con efecto principal de \\(F_2\\): \\[ Y \\sim F_2 \\] donde se parte de que la interacción y el efecto principal de \\(F_1\\) son no significativos y se plantea el contraste \\[ \\left\\{ \\begin{array}{ll} H_0: &amp; \\mbox{No hay diferencias entre las medias de } F_2\\\\ H_a: &amp; \\mbox{Al menos hay dos medias de } F_2 \\mbox{ distintas} \\end{array} \\right. \\] Si no rechazamos este contraste deberemos considerar que no hay diferencias en las medias de los niveles establecidos en \\(F_2\\) y nuestro modelo final no contiene ningún tipo de efecto predictivo. Modelo sin efectos: \\[ Y \\sim 1 \\] donde concluimos que todas las medias consideradas son iguales, es decir, los niveles de los factores considerados no tienen efecto sobre la respuesta. Esta combinación de modelos nos da una estructura secuencial de modelización (modelos alternativos) que deberemos estudiar para elegir el modelo con una mayor capacidad predictiva. Dicha estructura viene dada por: \\[\\begin{array}{lll} M1: &amp; Y \\sim F_1 + F_2 + F_1:F_2\\\\ M2: &amp; Y \\sim F_1 + F_2 \\\\ M3.1: &amp; Y \\sim F_1 \\\\ M3.2: &amp; Y \\sim F_2 \\\\ M4: &amp; Y \\sim 1\\\\ \\end{array}\\] Más adelante veremos como llevar a cabo esta comparación secuencial y como interpretarla para determinar los efectos que finalmente están presentes en el modelo y los que no. 4.4 Estimación y bondad de ajuste del modelo Dado que los modelos ANOVA se pueden escribir como modelos lineales de regresión con una matriz de diseño que solo contiene unos y ceros, se pueden utilizar los procedimientos de estimación vistos en el capitulo anterior para estimar los par??metros del modelo. Podemos utilizar las ecuaciones normales para obtener los coeficientes del modelo que en este caso representarán los incrementos de cada nivel con respecto al de referencia (que hemos tomado como cero en la restricción de identificabilidad), que nos permiten obtener las estimaciones de las medias asociadas a cada nivel o niveles del factor o factores. Además, podemos obtener los intervalos de confianza de dichos coeficientes. Sin embargo, en este caso no se pueden interpretar de forma directa las significatividades de cada coeficiente en el modelo, ya que al contrario de los que ocurría en los modelos de regresión, donde cada coeficiente representaba el efecto de una predictora, en los modelos ANOVA el efecto de la predictora va asociado con todos los coeficientes estimados con el factor o interacción de factores. Por eso es necesario proporcionar una medida de bondad de ajuste con dichas estimaciones que en este caso es el test F ANOVA que se obtiene como el test F de la regresión de los modelos de unidades anteriores. Este test nos permite extraer conclusiones directas sobre los efectos presentes en el modelo y no sobre los coeficientes estimados. En el punto siguiente veremos como utilizar el test F ANOVA para obtener el mejor modelo. En este caso no utilizaremos el \\(R^2\\) como medida de bondad de ajusta ya que se encuentra diseñada para modelos de regresión donde las predictoras son de tipo numérico. Para resolver la estimación y la bondad de ajuste en este tipo de modelos utilizaremos las funciones utilizadas en la unidad anterior. En algunos casos modificaremos la sintaxis para adaptarnos a las variables predictoras de tipo factor. 4.4.1 Ejemplos Pasamos a estudiar cada uno de los ejemplos ajustando el modelo saturado, es decir, aquel que contiene todos los efectos posibles en el modelo. Por defecto el programa R utiliza como restricción de identificabilidad que el incremento asociado con la primera categoría (por orden numérico o texto de los niveles del factor) es igual a cero, es decir, que la estimación de dicho incremento corresponde con la interceptación o efecto común del modelo. En las tablas de estimaciones eliminamos el p-valor individual asociado a cada incremento, dado que en este caso esos efectos no deben interpretarse de forma individual. 4.4.1.1 Datos de Insecticidas Obtenemos el ajuste del modelo de ANOVA de un factor para el banco de datos de Insecticidas. La expresión del modelo viene dado por: \\[\\mu_{spray,i}=\\alpha_0+\\alpha_{spray,i}+\\epsilon_{spray,i}, \\text{para }i=A,B,C,D,E ,F\\] donde la restricción de identificabilidad es fijar el incremento del spray A igual a cero, es decir, \\(\\alpha_{spray,A} = 0\\). # Ajuste del modelo fit.insecticidas &lt;- lm(count ~ spray, data = insecticidas) # Inferencia sobre los parámetros del modelo tab_model(fit.insecticidas, show.r2 = FALSE, show.p = FALSE) count Predictors Estimates CI (Intercept) 14.50 12.24 – 16.76 spray [B] 0.83 -2.36 – 4.03 spray [C] -12.42 -15.61 – -9.22 spray [D] -9.58 -12.78 – -6.39 spray [E] -11.00 -14.20 – -7.80 spray [F] 2.17 -1.03 – 5.36 Observations 72 Las ecuaciones de estimación de las medias del número de insectos vivos se pueden extraer a partir de la tabla de coeficientes estimados. Sin embargo, podemos obtener las estimaciones y representar las medias de todos los sprays haciendo uso de la función update() quitando el término de interceptación del modelo. De esta forma se prescinde de la restricción de identificabilidad y se obtienen las medias directamente. # Modelo sin interceptación m1 &lt;- update(fit.insecticidas, . ~ spray - 1) # Inferencia sin identificabilidad tab_model(m1, show.r2 = FALSE, show.p = FALSE) count Predictors Estimates CI spray [A] 14.50 12.24 – 16.76 spray [B] 15.33 13.07 – 17.59 spray [C] 2.08 -0.18 – 4.34 spray [D] 4.92 2.66 – 7.18 spray [E] 3.50 1.24 – 5.76 spray [F] 16.67 14.41 – 18.93 Observations 72 En esta tabla podemos ver tanto el valor estimado con el intervalo de confianza al 95% del número de insectos vivos con cada uno de los sprays. De esta forma os sprays A, B, y F son los menos efectivos (medias de insectos vivos más altas), mientras que los otros tres tienen medias significativamente más bajas. El spray C es el que se muestra más efectivo con una media de insectos vivos de 2 (redondeando a entero). Podemos ver gráficamente la solución obtenida mediante: # Gráfico de medias estimadas plot_model(m1, show.values = TRUE, vline.color = &quot;yellow&quot;) En cuanto a las medidas de bondad de ajuste utilizaremos le test F de igual de medias para establecer si los diferentes sprays considerados tienen un comportamiento similar o al menos hay dos de ellos que se comportan de forma distinta. glance(fit.insecticidas) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.724 0.704 3.92 34.7 3.18e-17 6 -197. 409. 425. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Podemos ver que el test F resulta significativo indicando que al menos hay dos medias distintas (algún incremento distinto de cero), lo que implica que al menos hay dos tipos de spray que se comportan de forma diferente con respecto al número de insectos vivos que quedan finalmente. Dado que el factor considerado es significativo resulta necesario conocer que medias son realmente distintas y cuales podemos considerar iguales. Para realizar esta tarea utilizaremos los denominados tests de comparaciones múltiples que nos permiten comparar dos a dos todas las combinaciones del factor para determinar las que son significativamente distintas o no. En R podemos utilizar la función TukeyHSD que realiza el test de comparaciones múltiples de Tukey. La salida proporciona la diferencia estimada entre cada para de niveles del factor considerado, así como el p-valor asociado con dicha comparación. # Test de comparaciones múltiples TukeyHSD(fit.insecticidas) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = x) ## ## $spray ## diff lwr upr p adj ## B-A 0.8333333 -3.866075 5.532742 0.9951810 ## C-A -12.4166667 -17.116075 -7.717258 0.0000000 ## D-A -9.5833333 -14.282742 -4.883925 0.0000014 ## E-A -11.0000000 -15.699409 -6.300591 0.0000000 ## F-A 2.1666667 -2.532742 6.866075 0.7542147 ## C-B -13.2500000 -17.949409 -8.550591 0.0000000 ## D-B -10.4166667 -15.116075 -5.717258 0.0000002 ## E-B -11.8333333 -16.532742 -7.133925 0.0000000 ## F-B 1.3333333 -3.366075 6.032742 0.9603075 ## D-C 2.8333333 -1.866075 7.532742 0.4920707 ## E-C 1.4166667 -3.282742 6.116075 0.9488669 ## F-C 14.5833333 9.883925 19.282742 0.0000000 ## E-D -1.4166667 -6.116075 3.282742 0.9488669 ## F-D 11.7500000 7.050591 16.449409 0.0000000 ## F-E 13.1666667 8.467258 17.866075 0.0000000 Para interpretar la tabla resultante nos debemos fijar en la columna p adj. Todos aquellos inferiores a 0.05 indican que las medias son distintas. Analizando todos los p-valores podemos establecer dos grupos de sprays, el formado por los tipos A, B y F y el formado por los tipos C, D, y E. Dentro de cada grupo no hay diferencias pero sin entres los sprays de grupos distintos. También podemos representar gráficamente las comparaciones realizadas para que resulte más fácil su interpretación: plot(TukeyHSD(fit.insecticidas), las=1, tcl = -.3) Los intervalos de confianza que contienen al cero muestran aquellos pares de sprays que pueden considerarse iguales. Los intervalos de confianza a la izquierda de cero muestran las combinaciones de sprays donde el primer spray tiene un efecto menor que el segundo (media negativa), mientras que en la parte derecha tenemos las combinaciones donde el primer spray es superior al segundo. Por ejemplo, podemos concluir: los sprays A y B tienen comportamiento similar; los sprays A y C tienen comportamientos distintos con la media del número de supervivientes en A superior que en con el spray C; los sprays F y E tienen comportamientos distintos con la media del número de supervivientes en F superior que en con el spray E. ¿Qué otras conclusiones podemos extraer? 4.4.1.2 Datos de Envasado Ajustamos un modelo ANOVA de una vía para el conjunto de datos de envasado. En este caso la restricción de identificabilidad es fijar el incremento de Máquina M1 igual a cero, es decir, \\(\\alpha_{maquina,M1} = 0\\). \\[\\mu_{maquina,i}=\\alpha_0+\\alpha_{maquina,i}+\\epsilon_{maquina,i}, \\text{para }i=M1, M2, M3, M4\\] donde la restricción de identificabilidad es fijar el incremento de Máquina M1 igual a cero, es decir, \\(\\alpha_{maquina,M1} = 0\\). # Ajuste del modelo fit.envasado &lt;- lm( produccion ~ maquina, data = envasado) # Inferencia sobre los parámetros del modelo tab_model(fit.envasado, show.r2 = FALSE, show.p = FALSE) produccion Predictors Estimates CI (Intercept) 106.00 98.00 – 114.00 maquina [M2] 7.75 -3.57 – 19.07 maquina [M3] -1.50 -12.82 – 9.82 maquina [M4] 18.00 6.68 – 29.32 Observations 16 A la vista de los coeficientes la máquina el orden de las máquinas (de mejor a peor, es decir, de mayor número de envases sin defecto a menor) es \\(M4 &gt; M2 &gt; M1 &gt; M3\\), con media estimadas dadas por: # Modelo sin interceptación m1 &lt;- update(fit.envasado, . ~ maquina - 1) # Inferencia sin identificabilidad tab_model(m1, show.r2 = FALSE, show.p = FALSE) produccion Predictors Estimates CI maquina [M1] 106.00 98.00 – 114.00 maquina [M2] 113.75 105.75 – 121.75 maquina [M3] 104.50 96.50 – 112.50 maquina [M4] 124.00 116.00 – 132.00 Observations 16 Vemos la solución gráfica donde se aprecian comportamientos muy diferenciados entre las diferentes máquinas. plot_model(m1, show.values = TRUE, vline.color = &quot;yellow&quot;) Con respecto a la bondad del ajuste podemos ver que el test \\(F\\) resulta significativo indicando que al menos alguna de las medias es distinta del resto (algún incremento distinto de cero). Por tanto, existe un efecto del tipo de máquina en el número de envases sin defecto. glance(fit.envasado) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.596 0.496 7.35 5.91 0.0102 4 -52.3 115. 118. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Realizamos las comparaciones múltiples para comparar todas las máquinas dos a dos. TukeyHSD(fit.envasado) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = x) ## ## $maquina ## diff lwr upr p adj ## M2-M1 7.75 -7.673887 23.173887 0.4716327 ## M3-M1 -1.50 -16.923887 13.923887 0.9911761 ## M4-M1 18.00 2.576113 33.423887 0.0210561 ## M3-M2 -9.25 -24.673887 6.173887 0.3283778 ## M4-M2 10.25 -5.173887 25.673887 0.2508228 ## M4-M3 19.50 4.076113 34.923887 0.0126965 Tan sólo las combinaciones M4-M1 y M4-M3 muestran resultados significativos indicando que la máquina M4 es comparable con la M2, es decir, la media de envases sin defecto es igual para dichas máquinas, pero distintas de la M1 y M3 que también puede considerarse iguales. Veamos los resultados gráficamente: plot(TukeyHSD(fit.envasado), las=1, tcl = -.3) 4.4.1.3 Datos de Venenos Ajustamos un modelo ANOVA de dos vías con interacción entre veneno y antídoto para el conjunto de datos de venenos cuya expresión viene dada por (el índice i hace referencia al antídoto, y el j a los venenos): \\[\\mu_{Antidoto:i,Veneno:j} = \\alpha_0 + \\alpha_{Antidoto:i} +\\alpha_{Veneno:j} + \\alpha\\beta_{Antidoto:i,Veneno:j} + \\epsilon_{Antidoto:i,Veneno:j}\\] para \\(i=AA,AB,AC,AD\\), \\(j=VA,VB,VC\\) con las restricciones de identificabilidad: \\(\\alpha_{antidoto,AA} = 0\\), \\(\\beta_{veneno,VA} = 0\\), \\(\\alpha\\beta_{Antidoto,AA; Veneno,VA} = 0\\), de forma que el ajuste para este modelo viene dado por: # Ajuste del modelo fit.venenos &lt;- lm( tiempo ~ antidoto*veneno, data = venenos) # Inferencia sobre los parámetros del modelo tab_model(fit.venenos, show.r2 = FALSE, show.p = FALSE) tiempo Predictors Estimates CI (Intercept) 0.41 0.26 – 0.56 antidoto [AB] 0.47 0.25 – 0.68 antidoto [AC] 0.15 -0.06 – 0.37 antidoto [AD] 0.20 -0.02 – 0.41 veneno [VB] -0.09 -0.31 – 0.12 veneno [VC] -0.20 -0.42 – 0.01 antidoto [AB] * veneno[VB] 0.03 -0.27 – 0.33 antidoto [AC] * veneno[VB] -0.10 -0.40 – 0.20 antidoto [AD] * veneno[VB] 0.15 -0.15 – 0.45 antidoto [AB] * veneno[VC] -0.34 -0.64 – -0.04 antidoto [AC] * veneno[VC] -0.13 -0.43 – 0.17 antidoto [AD] * veneno[VC] -0.08 -0.39 – 0.22 Observations 48 Desde luego interpretar la tabla de coeficientes en este caso es más complicado, dada la gran cantidad de parámetros involucrados, por lo que resulta más fácil obtener las medias y el gráfico asociado. En este caso no podemos actualizar el modelo eliminando la interceptación dado que tenemos tres restricciones y no una. Modificamos la tabla de estimaciones y el gráfico para obtener todo el conjunto de medias. Además, como no hemos descartado el efecto de interacción, en el gráfico sólo representaremos las medias asociadas con dicho efecto. Para representar el efecto de interacción añadimos el parámetro \"int\" a la función plot_model. # Objeto gráfico p &lt;- plot_model(fit.venenos, &quot;int&quot;, show.stat = TRUE, title =&quot;&quot;) # Tabla de estimaciones p$data ## ## # Predicted values of tiempo ## # x = antidoto ## ## # veneno = VA ## ## x | Predicted | SE | group_col | 95% CI ## ----------------------------------------------- ## 1 | 0.41 | 0.07 | VA | [0.27, 0.56] ## 2 | 0.88 | 0.07 | VA | [0.73, 1.03] ## 3 | 0.57 | 0.07 | VA | [0.42, 0.71] ## 4 | 0.61 | 0.07 | VA | [0.46, 0.76] ## ## # veneno = VB ## ## x | Predicted | SE | group_col | 95% CI ## ----------------------------------------------- ## 1 | 0.32 | 0.07 | VB | [0.17, 0.47] ## 2 | 0.82 | 0.07 | VB | [0.67, 0.96] ## 3 | 0.38 | 0.07 | VB | [0.23, 0.52] ## 4 | 0.67 | 0.07 | VB | [0.52, 0.81] ## ## # veneno = VC ## ## x | Predicted | SE | group_col | 95% CI ## ----------------------------------------------- ## 1 | 0.21 | 0.07 | VC | [0.06, 0.36] ## 2 | 0.33 | 0.07 | VC | [0.19, 0.48] ## 3 | 0.24 | 0.07 | VC | [0.09, 0.38] ## 4 | 0.32 | 0.07 | VC | [0.18, 0.47] # Gráfico de interacción estimado p A la vista de la tabla de estimaciones podemos concluir que: El antídoto AA es el menos tiempo tarda en hacer efecto sobre el veneno VA. El antídoto AA es el menos tiempo tarda en hacer efecto sobre el veneno VB. El antídoto AA es el menos tiempo tarda en hacer efecto sobre el veneno VC. De esta forma parece que el antídoto AA es el mejor funciona con cualquiera de los venenos pero será necesario estudiar la bondad de ajuste y realizar el proceso de selección para determinar que efectos deben aparecer el modelo, para poder concluir más precisamente sobre el comportamiento de los diferentes antídotos. En el gráfico de estimación del efecto de interacción podemos apreciar de forma más sencillo la relación entre ambos factores. Aunque el antídoto AA muestra los tiempos más bajos, también es cierto que los intervalos de confianza no son muy diferentes de los obtenidos para AC y AD. Con respecto a la bondad del ajuste podemos ver que el test \\(F\\) resulta significativo indicando que al menos alguna de las medias es distinta del resto (algún incremento distinto de cero). Por tanto, existe un efecto de interacción entre venenos y antídoto que proporciona un tiempo de reacción distinto. glance(fit.venenos) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.734 0.653 0.149 9.03 1.93e-7 12 30.2 -34.3 -9.98 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; El p-valor asociado con la interacción resulta no significativo, indicando que dicho efecto puede ser considerado nulo y no se debería considerarse en el modelo. Vemos aquí la diferencia entre la interpretación del test F para el modelo ANOVA de una vía y el ANOVA de dos vías. En este último caso es necesario el estudio de la tabla ANOVA para determinar los efectos significativos y no basta solo con la bondad del ajuste. En el punto siguiente vemos como realizar la selección del mejor modelo en este tipo de situaciones. 4.5 Comparación y selección de modelos La comparación de modelos se basa en determinar la capacidad explicativa de cada uno de los efectos presentes en el modelo. Mientras que en los modelos de regresión esos efectos van asociados con un único coeficiente del modelo (pendiente asociada a la predictora), en los modelos ANOVA el efecto de un factor va asociado con todos los incrementos establecidos. En realidad, tendremos un efecto significativo cuando detectemos diferencias entre al menos dos medias de la respuesta para los diferentes niveles del factor. Dado que el número de modelos que podemos construir en estas situaciones es limitado, podríamos construirlos todos ellos y compararlos de golpe para decidir el mejor modelo. Esto se puede hacer con el test F de comparación de modelos que es una versión del test F para un contraste especifico, o bien utilizar el criterio del AIC o BIC y quedarnos con aquel modelo que de un valor más pequeño. En modelos más complejos optaremos por un procedimiento automático por pasos empezando por el modelo saturado. En los modelos ANOVA de una vía la selección del modelo se basa únicamente en el análisis de bondad de ajuste, ya que el test F que obtenemos es realmente la comparación entre los modelos que contienen el factor y el que no lo contiene, mientras que los modelos de más de una vía es necesario realizar el proceso de selección de efectos para determinar el modelo final. A continuación presentamos las diferentes opciones con el banco de datos de venenos ya que en los otros dos casos ya hemos resulto la selección mediante el análisis de la bondad del ajuste. 4.5.1 Ejemplos Realizamos la selección le mejor modelo par el banco de datos de venenos. En primer lugar procederemos de forma manual considerando todos los posibles modelos con las combinaciones de los factores considerados: \\(tiempo ∼veneno∗antidoto\\): Modelo con interacción, donde hay al menos una combinación de veneno y antídoto con tiempo de reacción medio distinto de otras combinaciones. \\(tiempo∼veneno+antidoto\\): Modelo sin interacción con efectos principales, donde el efecto del veneno y el antídoto tiene un efecto aditivo en e tiempo medio de reacción. \\(tiempo∼veneno\\): Modelo con efecto principal marginal del veneno, donde hay almenos un tipo de veneno con un tiempo medio de reacción distinto pero los antidotos no tienen efecto. \\(tiempo∼antidoto\\): Modelo con efecto principal marginal del antídoto, donde hay al menos un tipo de antídoto con un tiempo medio de reacción distinto pero los venenos no tienen efecto. \\(tiempo∼1\\): Modelo sin efectos, donde el tiempo de reacción no cambia con el antídoto y el tipo de veneno. En este caso la especificación de todos los posibles modelos así como su comparación resulta más costosa ya que están involucrados dos factores de clasificación. Los posibles modelos son: \\(tiempo \\sim veneno * antidoto\\): Modelo con interacción, donde hay al menos una combinación de veneno y antídoto con tiempo de reacción medio distinto de otras combinaciones. \\(tiempo \\sim veneno + antidoto\\): Modelo sin interacción con efectos principales, donde el efecto del veneno y el antídoto tiene un efecto aditivo en e tiempo medio de reacción. \\(tiempo \\sim veneno\\): Modelo con efecto principal marginal del veneno, donde hay almenos un tipo de veneno con un tiempo medio de reacción distinto pero los antidotos no tienen efecto. \\(tiempo \\sim antidoto\\): Modelo con efecto principal marginal del antídoto, donde hay al menos un tipo de antídoto con un tiempo medio de reacción distinto pero los venenos no tienen efecto. \\(tiempo \\sim 1\\): Modelo sin efectos, donde el tiempo de reacción no cambia con el antídoto y el tipo de veneno. Consideramos en primer lugar la comparación entre los modelos con y sin interacción para determinar si dicho efecto es relevante o no para explicar el comportamiento del tiempo medio de reacción. Obtenemos el contraste F parcial mediante: # Ajuste del modelo con interacción fit1 &lt;- lm(tiempo ~ antidoto + veneno + antidoto:veneno, data = venenos) # Ajuste del modelo sin interacción fit2 &lt;- lm(tiempo ~ antidoto + veneno, data = venenos) # Comparación entre modelos anova(fit2, fit1) ## Analysis of Variance Table ## ## Model 1: tiempo ~ antidoto + veneno ## Model 2: tiempo ~ antidoto + veneno + antidoto:veneno ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 42 1.0504 ## 2 36 0.8001 6 0.25027 1.8768 0.1118 Dado que el p-valor no es significativo (p-valor &gt; 0.05) podemos concluir que ambos modelos pueden ser considerarse como iguales, es decir, el efecto de interacción no resulta significativo. Vamos a comparar las estimaciones de ambos modelos y representamos gráficamente el modelo sin interacción. En este caso obtendremos un gráfico por cada factor dado que no actúan de forma conjunta sino de forma aditiva. Utilizamos el parámetro “pred” para obtener las estimaciones de las medias asociadas a cada factor. # Estimaciones de ambos modelos tab_model(fit1, fit2, show.r2 = FALSE) tiempo tiempo Predictors Estimates CI p Estimates CI p (Intercept) 0.41 0.26 – 0.56 &lt;0.001 0.45 0.34 – 0.57 &lt;0.001 antidoto [AB] 0.47 0.25 – 0.68 &lt;0.001 0.36 0.23 – 0.49 &lt;0.001 antidoto [AC] 0.15 -0.06 – 0.37 0.150 0.08 -0.05 – 0.21 0.232 antidoto [AD] 0.20 -0.02 – 0.41 0.069 0.22 0.09 – 0.35 0.002 veneno [VB] -0.09 -0.31 – 0.12 0.386 -0.07 -0.19 – 0.04 0.198 veneno [VC] -0.20 -0.42 – 0.01 0.063 -0.34 -0.45 – -0.23 &lt;0.001 antidoto [AB] * veneno[VB] 0.03 -0.27 – 0.33 0.855 antidoto [AC] * veneno[VB] -0.10 -0.40 – 0.20 0.507 antidoto [AD] * veneno[VB] 0.15 -0.15 – 0.45 0.321 antidoto [AB] * veneno[VC] -0.34 -0.64 – -0.04 0.028 antidoto [AC] * veneno[VC] -0.13 -0.43 – 0.17 0.389 antidoto [AD] * veneno[VC] -0.08 -0.39 – 0.22 0.572 Observations 48 48 # Gráfico del modelo sin interacción plot_model(fit2, &quot;pred&quot;) ## $antidoto ## ## $veneno Como era de esperar, dado que la interacción no es significativa, los coeficientes estimados (incrementos respecto de las medias de referencia dadas por el antídoto AA y el veneno VA) no varían sustancialmente entre ambos modelos. En el gráfico podemos ver que el veneno con un tiempo de reacción más pequeño es el asociado con el veneno VC, mientras que si nos fijamos en el antídoto, el menor tiempo de reacción se da para el AA. Dado que no hay interacción el modelo sugiere que independientemente del veneno utilizado, el antídoto más efectivo es el AA, seguido por el AC, AD, y por último el AB. Se pueden obtener las estimaciones asociadas con el modelo sin interacción para verificar este hecho. Este proceso manual resulta complicado de llevar a cabo cuando tenemos modelos más complejos (con más de dos vías de clasificación), por lo que es necesario utilizar procedimientos secuenciales automáticos para alcanzar el mejor modelo. A continuación se muestra como realizar esta selección: # Selección basada en test F ols_step_backward_p(fit.venenos, prem = 0.05) ## ## ## Elimination Summary ## ------------------------------------------------------------------------------- ## Variable Adj. ## Step Removed R-Square R-Square C(p) AIC RMSE ## ------------------------------------------------------------------------------- ## 1 antidoto:veneno 0.6508 0.6092 5.2608 -33.2407 0.1581 ## ------------------------------------------------------------------------------- Como era de esperar el proceso determina que podemos prescindir del efecto de interacción. Para utilizar el criterio AIC utilizamos la función step() porque la función ols_step_backward_aic esta diseñada específicamente para trabajar con preditores numéricos y no con interacciones de factores lo que puede llevar a soluciones erróneas. Por tanto, la única opción pasa por evaluar todos los modelos con la función ols_step_all_possible y elegir el mejor de ellos teniendo en cuenta la construcción del modelo. Sin embargo, esta forma de proceder no será práctica en modelos más complejos. # Selección con función step y AIC stats::step(fit.venenos, direction = &quot;backward&quot;) ## Start: AIC=-172.52 ## tiempo ~ antidoto * veneno ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 0.8001 -172.52 ## - antidoto:veneno 6 0.25027 1.0504 -171.46 ## ## Call: ## lm(formula = tiempo ~ antidoto * veneno, data = venenos) ## ## Coefficients: ## (Intercept) antidotoAB antidotoAC ## 0.4125 0.4675 0.1550 ## antidotoAD venenoVB venenoVC ## 0.1975 -0.0925 -0.2025 ## antidotoAB:venenoVB antidotoAC:venenoVB antidotoAD:venenoVB ## 0.0275 -0.1000 0.1500 ## antidotoAB:venenoVC antidotoAC:venenoVC antidotoAD:venenoVC ## -0.3425 -0.1300 -0.0850 El proceso indica que no podemos descartar el efecto de interacción contradiciendo los resultados obtenidos con el test F, aunque la diferencia entre los valores de AIC es muy pequeña. Habitualmente para este tipo de modelos se suele utilizar el criterio basado en el test F, dado que ambos pueden llevar a soluciones distintas como ocurre en este caso. 4.6 Diagnóstico Los procedimientos de diagnóstico son los mismos que en los modelos de regresión salvo por el hecho de que la igualdad de varianzas de los residuos se debe cumplir para cada uno de los grupos determinados por los niveles de los factores, es decir, varianzas residuales iguales. Para este tipo de modelos utilizaremos el test de Kolmogorov-Smirnov para el test de normalidad y el de Levene para la homogeneidad de varianzas También se pueden realizar los diagnósticos gráficos de residuos para cada hipótesis pero en estos modelos sólo tiene sentido realizar los diagramas de cajas de residuos versus niveles del factor para valorar linealidad y varianza constante, dado que en este tipo de modelos tenemos un único valor ajustado (media) para todos los sujetos de un mismo nivel del factor, y por tanto un mismo residuo para todos los sujetos de un mismo grupo. En cuanto al estudio de influencia, nos centraremos principalmente en la distancia de Cook dado que no tiene sentido considerar la influencia sobre los coeficientes del modelo ni sobre los valores ajustados dada la estructura agrupada del banco de datos a analizar, es decir, cada coeficiente no representa una variable sino que debemos considerar el conjunto de ellos que representan el efecto de un factor. Para realizar el diagnóstico partiremos del modelo ajustado en el apartado anterior para cada banco de datos. 4.6.1 Ejemplos Para realizar el diagnóstico partiremos del modelo ajustado en el apartado anterior para cada banco de datos. En caso de detectar algún problema con las hipótesis del modelo deberemos ajustar un nuevo modelo y verificar que cumple las hipótesis. 4.6.1.1 Datos de insecticidas Obtenemos los valores de diagnóstico y realizamos los correspondientes tests de hipótesis y análisis de influencia para el banco de datos de insecticidas. En primer lugar realizamos el gráfico cajas de residuos versus niveles del factor para valorar la hipótesis de linealidad y varianza constante. # Valores de diagnóstico diagnostico &lt;- fortify(fit.insecticidas) # Gráfico de residuos vs factor ggplot(diagnostico,aes(x = spray, y = .stdresid)) + geom_boxplot() + geom_hline(yintercept = 0, col = &quot;red&quot;) En el gráfico se observa como la variabilidad dentro de cada nivel del factor parece diferente, aunque los residuos parecen tener un comportamiento lineal alrededor del cero. # Tests de normalidad ols_test_normality(fit.insecticidas) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9601 0.0223 ## Kolmogorov-Smirnov 0.1301 0.1747 ## Cramer-von Mises 6.6076 0.0000 ## Anderson-Darling 1.2015 0.0037 ## ----------------------------------------------- El test de Kolmogorov-Smirnov resulta no significativo indicando el cumplimiento de la hipótesis de Normalidad de los residuos. # Tests de homogeneidad de varianzas leveneTest(.stdresid ~ spray, data = diagnostico) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 3.8214 0.004223 ** ## 66 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El test resulta significativo (p-valor inferior a 0.05) indicando que no se verifica la hipótesis de varianza constante, como ya sospechábamos del gráfico de los residuos. Podemos ver si el incumplimiento es debido a algún valor influyente, pero sino es así deberemos plantear alguna transformación de la respuesta para tratar de corregir los problemas de diagnóstico. Se puede verificar que el análisis de la distancia de Cook no presenta ninguna observación influyente, por lo que lo ideal sería plantear la transformación de Box-Cox para encontrar una solución a esta situación pero dado que tenemos valores de cero en la respuesta esto no resulta posible. Sin embargo puesto que la respuesta son datos de conteos podemos utilizar la función logaritmo añadiendo un uno a los valores de la respuesta para evitar problemas en la transformación. # Calculamos la nueva variable insecticidas &lt;- insecticidas %&gt;% mutate(lcount = log(count+1)) ¿qué ajuste tenemos con esa nueva variable? ¿Cómo es la bondad del ajuste?¿se verifican ahora las hipótesis del modelo? 4.6.1.2 Datos de envasado Obtenemos los valores de diagnóstico y realizamos los correspondientes tests de hipótesis y análisis de influencia. # Valores de diagnóstico diagnostico &lt;- fortify(fit.envasado) # Gráfico ggplot(diagnostico,aes(x = maquina, y = .stdresid)) + geom_boxplot() + geom_hline(yintercept = 0, col = &quot;red&quot;) # Tests de hipótesis ols_test_normality(fit.envasado) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9074 0.1058 ## Kolmogorov-Smirnov 0.176 0.6420 ## Cramer-von Mises 1.5833 0.0000 ## Anderson-Darling 0.5552 0.1269 ## ----------------------------------------------- leveneTest(.stdresid ~ maquina, data = diagnostico) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 0.2091 0.8882 ## 12 # Análisis de influencia ols_plot_cooksd_chart(fit.envasado) Los tests de hipótesis resultan no significativos indicando que el modelo propuesto verifica dichas hipótesis. No se detectan además observaciones influyentes, con lo que el modelo propuesto y que fue analizado en la sección de estimación es el modelo final para este banco de datos. 4.6.1.3 Datos de venenos Utilizamos el modelo sin interacción que hemos obtenido tras el proceso de selección de efectos tratada en la sección anterior. # Modelo fit.venenos &lt;- lm (tiempo ~ veneno + antidoto, data = venenos) # Valores de diagnóstico diagnostico &lt;- fortify(fit.venenos) # Tests de hipótesis ols_test_normality(fit.venenos) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9221 0.0035 ## Kolmogorov-Smirnov 0.113 0.5353 ## Cramer-von Mises 12.0813 0.0000 ## Anderson-Darling 0.8952 0.0205 ## ----------------------------------------------- El test de Kolmogorov-Smirnov resulta no significativo indicando el cumplimiento de la hipótesis de normalidad. leveneTest(.stdresid ~ veneno*antidoto, data = diagnostico) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 11 4.1582 0.0005539 *** ## 36 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El test de Leven resulta significativo indicando que no se cumple la hipótesis de varianza constante. Se plantea utilizar Box-Cox para obtener una posible transformación de la respuesta que permita verificar las hipótesis del modelo. MASS::boxcox(fit.venenos) Se puede ver que el intervalo de confianza para la transformación contiene el valor de -1. Esto nos da indicios de que podríamos utilizar la transformación inversa para corregir los problemas detectados de homogeneidad de varianzas. En ese caso habrá que tener en cuenta que todas nuestras conclusiones se basarán en dicha transformación. Comenzamos de nuevo con la construcción del modelo. # Creamos la variable transformada venenos &lt;- venenos %&gt;% mutate(tiempoinv = 1/tiempo) # Ajustamos el modelo de nuevo fit.venenos.inv &lt;- lm(tiempoinv ~ antidoto*veneno, data = venenos) # Selección utilizando el test F ols_step_backward_p(fit.venenos.inv, prem = 0.05) ## ## ## Elimination Summary ## ------------------------------------------------------------------------------ ## Variable Adj. ## Step Removed R-Square R-Square C(p) AIC RMSE ## ------------------------------------------------------------------------------ ## 1 antidoto:veneno 0.8454 0.827 0.4181 75.5483 0.4911 ## ------------------------------------------------------------------------------ El modelo resultante es aquel que no contiene la interacción. Obtenemos los coeficientes de ese modelo, así como las medias estimadas para la combinación de veneno-antídoto. Expresamos las medias con la transformación propuesta. # Modelo sin interacción fit.venenos.inv &lt;- lm(tiempoinv ~ antidoto + veneno, data = venenos) # Estimaciones de ambos modelos tab_model(fit.venenos.inv, show.r2 = FALSE) tiempoinv Predictors Estimates CI p (Intercept) 2.70 2.35 – 3.05 &lt;0.001 antidoto [AB] -1.66 -2.06 – -1.25 &lt;0.001 antidoto [AC] -0.57 -0.98 – -0.17 0.007 antidoto [AD] -1.35 -1.76 – -0.95 &lt;0.001 veneno [VB] 0.47 0.12 – 0.82 0.010 veneno [VC] 2.00 1.65 – 2.35 &lt;0.001 Observations 48 Las ecuaciones de estimación son: \\[\\begin{array}{lll} 1/\\mu_{AA,VA} &amp; = \\alpha_0 + \\alpha_{AA} + \\beta_{VA} &amp; = 2.70 + 0 + 0 = 2.70\\\\ 1/\\mu_{AA,VB} &amp; = \\alpha_0 + \\alpha_{AA} + \\beta_{VB} &amp; = 2.70 + 0 + 0.47 = 3.17\\\\ 1/\\mu_{AA,VC} &amp; = \\alpha_0 + \\alpha_{AA} + \\beta_{VC} &amp; = 2.70 + 0 + 2.00 = 4.70\\\\ 1/\\mu_{AB,VA} &amp; = \\alpha_0 + \\alpha_{AB} + \\beta_{VA} &amp; = 2.70 - 1.66 + 0 = 1.04\\\\ 1/\\mu_{AB,VB} &amp; = \\alpha_0 + \\alpha_{AB} + \\beta_{VB} &amp; = 2.70 - 1.66 + 0.47 = 1.51\\\\ 1/\\mu_{AB,VC} &amp; = \\alpha_0 + \\alpha_{AB} + \\beta_{VC} &amp; = 2.70 - 1.66 + 2.00 = 3.04\\\\ 1/\\mu_{AC,VA} &amp; = \\alpha_0 + \\alpha_{AC} + \\beta_{VA} &amp; = 2.70 - 0.57 + 0 = 2.13\\\\ 1/\\mu_{AC,VB} &amp; = \\alpha_0 + \\alpha_{AC} + \\beta_{VB} &amp; = 2.70 - 0.57 + 0.47 = 2.60\\\\ 1/\\mu_{AC,VC} &amp; = \\alpha_0 + \\alpha_{AC} + \\beta_{VC} &amp; = 2.70 - 0.57 + 2.00 = 4.13\\\\ 1/\\mu_{AD,VA} &amp; = \\alpha_0 + \\alpha_{AD} + \\beta_{VA} &amp; = 2.70 - 1.35 + 0 = 1.35\\\\ 1/\\mu_{AD,VB} &amp; = \\alpha_0 + \\alpha_{AD} + \\beta_{VB} &amp; = 2.70 - 1.35 + 0.47 = 1.82\\\\ 1/\\mu_{AD,VC} &amp; = \\alpha_0 + \\alpha_{AD} + \\beta_{VC} &amp; = 2.70 - 1.35 + 2.00 = 3.35\\\\ \\end{array}\\] Veamos si se verifican las hipótesis del modelo: # Valores de diagnóstico diagnostico &lt;- fortify(fit.venenos.inv) # Tests de hipótesis ols_test_normality(fit.venenos.inv) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9789 0.5339 ## Kolmogorov-Smirnov 0.1057 0.6192 ## Cramer-von Mises 6.0617 0.0000 ## Anderson-Darling 0.2948 0.5834 ## ----------------------------------------------- leveneTest(.stdresid ~ veneno*antidoto, data = diagnostico) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 11 1.1672 0.3429 ## 36 # Análisis de influencia ols_plot_cooksd_chart(fit.venenos.inv) Se verifican las hipótesis del modelo. La transformación planteada permite obtener un modelo válido para la fase de predicción, y nos permite extraer conclusiones sobre la relación entre veneno y antídoto con el tiempo de reacción. 4.7 Predicción El proceso de predicción en este tipo de modelos es muy simple ya que los posibles valores de las predictoras coincide con las medias de los posibles niveles o combinación de los niveles del factor o factores que aparezcan en el modelo final. En este caso solo tiene sentido predecir los valores de la media ya que nuestro objetivo es el estudio de dichas medias. Veremos además como representar gráficamente los resultados de la predicción. 4.7.1 Ejemplos Obtenemos las tablas de medias estimadas para cada uno de los ejemplos tratados en esta unidad. 4.7.1.1 Datos de insecticidas Estamos interesados en predcir el número de insectos muertos en función del tipo de insecticida utilizado. Podemos obtener la tabla de predicción y el gráfico asociado con el código siguiente: # Objeto gráfico p &lt;- plot_model(fit.insecticidas, &quot;pred&quot;, terms = &quot;spray&quot;, show.stat = TRUE) # Tabla de estimaciones p$data ## ## # Predicted values of count ## # x = spray ## ## x | Predicted | SE | group_col | 95% CI ## ------------------------------------------------- ## 1 | 14.50 | 1.13 | 1 | [12.28, 16.72] ## 2 | 15.33 | 1.13 | 1 | [13.11, 17.55] ## 3 | 2.08 | 1.13 | 1 | [-0.14, 4.30] ## 4 | 4.92 | 1.13 | 1 | [ 2.70, 7.14] ## 5 | 3.50 | 1.13 | 1 | [ 1.28, 5.72] ## 6 | 16.67 | 1.13 | 1 | [14.45, 18.89] Para obtener el verdadero valor de la media debemos deshacer la transformación utilizada para el ajuste del modelo. Representamos la media predicha así como un intervalo de predicción para ella. Ordenamos los valores obtenidos de mayor a menor predicción para una mejor visualización. # Secuencia de valores de predicción newdata &lt;- data.frame(spray = unique(insecticidas$spray)) # Predicción para la media de la respuesta newdata &lt;- data.frame(newdata, predict(fit.insecticidas, newdata, interval = &quot;confidence&quot;)) # Deshacemos la transformación newdata &lt;- newdata %&gt;% mutate(prediccion = exp(fit) - 1, lower = exp(lwr) - 1, upper = exp(upr) - 1) # Gráfico ordenado del menor al mayor valor en función del tipo de spray ggplot(newdata, aes(x = fct_reorder(spray, prediccion), y = prediccion)) + geom_errorbar(aes(ymin = lower, ymax = upper), width = .1) + geom_line() + geom_point() + labs(x = &quot;Spray&quot;, y = &quot;Conteo&quot;) + coord_flip() ¿Qué podemos decir del gráfico de predicción de medias obtenido? ¿ Qué insecticida es más efectivo y cual menos? 4.7.1.2 Datos de envasado Podemos obtener la tabla de predicción y el gráfico asociado con el código siguiente: # Objeto gráfico p &lt;- plot_model(fit.envasado, &quot;pred&quot;, terms = &quot;maquina&quot;, show.stat = TRUE, title = &quot; &quot;) # Tabla de estimaciones p$data ## ## # Predicted values of produccion ## # x = maquina ## ## x | Predicted | SE | group_col | 95% CI ## --------------------------------------------------- ## 1 | 106.00 | 3.67 | 1 | [ 98.80, 113.20] ## 2 | 113.75 | 3.67 | 1 | [106.55, 120.95] ## 3 | 104.50 | 3.67 | 1 | [ 97.30, 111.70] ## 4 | 124.00 | 3.67 | 1 | [116.80, 131.20] # Gráfico p Podemos ver como la predicción para la máquina 4 es diferente de las predicciones para las máquinas 1 y 3 (sus intervalos de confianza no se solapan). No hay diferencia entre la M4 y la M2, ni entre la M2 y la M1 o M3. La máquina que podemos indicar como más efectiva es la M4 ya que es la que produce un mayor número de envases sin defectos. 4.7.1.3 Datos de venenos Comenzaremos obteniendo la predicción para el modelo estimado y posteriormente lo haremos para la variable respuesta en la escala original. # Objeto gráfico p &lt;- plot_model(fit.venenos.inv, &quot;pred&quot;, terms = c(&quot;veneno&quot;,&quot;antidoto&quot;), show.stat = TRUE, title =&quot;&quot;) # Tabla de estimaciones en términos de la inversa p$data ## ## # Predicted values of tiempoinv ## # x = veneno ## ## # antidoto = AA ## ## x | Predicted | SE | group_col | 95% CI ## ----------------------------------------------- ## 1 | 2.70 | 0.17 | AA | [2.36, 3.04] ## 2 | 3.16 | 0.17 | AA | [2.82, 3.50] ## 3 | 4.70 | 0.17 | AA | [4.36, 5.04] ## ## # antidoto = AB ## ## x | Predicted | SE | group_col | 95% CI ## ----------------------------------------------- ## 1 | 1.04 | 0.17 | AB | [0.70, 1.38] ## 2 | 1.51 | 0.17 | AB | [1.17, 1.85] ## 3 | 3.04 | 0.17 | AB | [2.70, 3.38] ## ## # antidoto = AC ## ## x | Predicted | SE | group_col | 95% CI ## ----------------------------------------------- ## 1 | 2.12 | 0.17 | AC | [1.78, 2.46] ## 2 | 2.59 | 0.17 | AC | [2.25, 2.93] ## 3 | 4.13 | 0.17 | AC | [3.78, 4.47] ## ## # antidoto = AD ## ## x | Predicted | SE | group_col | 95% CI ## ----------------------------------------------- ## 1 | 1.34 | 0.17 | AD | [1.00, 1.68] ## 2 | 1.81 | 0.17 | AD | [1.47, 2.15] ## 3 | 3.35 | 0.17 | AD | [3.01, 3.69] # Gráfico de interacción estimado p Dado que hemos utilizado la transformación inversa para encontrar el antídoto que mejor funciona para cada veneno debemos buscar el valor más alto de valor predicho. El antídoto AA es que le muestra valores predichos más altos para cualquier tipo de venenos por lo que resulta el más efectivo. Podemos ver el resultado gráficamente: # Gráfico de interacción estimado p ¿Cómo podemos interpretar los intervalos de confianza para la predicción que hemos obtenido? ¿Cómo se comportan los antídotos con cada veneno? ¿Este resultado es esperable en función del modelo ajustado? Obtenemos y representamos los valores de predicción en la escala original de la variable respuesta. # Creamos la combinación de grupos con ambos factores f1 &lt;- unique(venenos$antidoto) f2 &lt;- unique(venenos$veneno) # Generamos todas las posibles combinaciones de niveles newdata &lt;- data.frame(expand.grid(f1, f2)) colnames(newdata) &lt;- c(&quot;antidoto&quot;, &quot;veneno&quot;) # Predicción para la media de la respuesta newdata &lt;- data.frame(newdata, predict(fit.venenos.inv, newdata, interval=&quot;confidence&quot;)) # Deshacemos la transformación inversa newdata &lt;- newdata %&gt;% mutate(prediccion = 1/fit, lower = 1/lwr, upper = 1/upr) # Presentamos los datos en la escala transformada y la original newdata ## antidoto veneno fit lwr upr prediccion lower upper ## 1 AA VA 2.696003 2.3455815 3.046425 0.3709194 0.4263335 0.3282536 ## 2 AB VA 1.038601 0.6881791 1.389022 0.9628339 1.4531102 0.7199308 ## 3 AC VA 2.123868 1.7734460 2.474289 0.4708391 0.5638739 0.4041564 ## 4 AD VA 1.344279 0.9938569 1.694700 0.7438934 1.0061810 0.5900748 ## 5 AA VB 3.164644 2.8142227 3.515066 0.3159913 0.3553379 0.2844897 ## 6 AB VB 1.507242 1.1568203 1.857664 0.6634635 0.8644385 0.5383106 ## 7 AC VB 2.592509 2.2420873 2.942931 0.3857267 0.4460130 0.3397973 ## 8 AD VB 1.812920 1.4624982 2.163342 0.5515964 0.6837615 0.4622479 ## 9 AA VC 4.697388 4.3469667 5.047810 0.2128842 0.2300455 0.1981057 ## 10 AB VC 3.039986 2.6895643 3.390408 0.3289489 0.3718074 0.2949498 ## 11 AC VC 4.125253 3.7748312 4.475675 0.2424094 0.2649125 0.2234300 ## 12 AD VC 3.345664 2.9952422 3.696086 0.2988943 0.3338628 0.2705565 Realizamos el gráfico de predicción en la escala original de la variable respuesta ordenando los intervalos de confianza desde el tiempo de reacción más pequeño al más grande, para así determinar las combinaciones antídoto-veneno más efectivas. # Variable de combinaciones de factores newdata &lt;- newdata %&gt;% mutate(grupo = paste(antidoto,veneno)) # Gráfico ggplot(newdata, aes(x = fct_reorder(grupo,fit), y = fit, colour = veneno)) + geom_errorbar(aes(ymin = lwr, ymax = upr), width=.1) + geom_line() + geom_point() + coord_flip() + labs(y = &quot;Tiempo&quot;, x = &quot;Antidoto-Veneno&quot;) Las conclusiones que podemos extraer son: Para el veneno VA los mejores antídotos son el AA y el AC con tiempos de reacción similares (intervalos de predicción no disjuntos), y distintos de los que proporcionan el antídoto AD y el AB. Para el veneno VB los mejores antídotos son el AA y el AC con tiempos de reacción similares, y distintos de los que proporcionan el antídoto AD y el AB. Para el veneno VC los mejores antídotos son el AA y el AC con tiempos de reacción similares, y distintos de los que proporcionan el antídoto AD y el AB. A la hora de elegir deberíamos hacerlo entre AA y AC para cualquiera de los venenos. Además, podemos ver que el veneno VC es el más resistente a los antídotos ya que tiene mayores tiempos de reacción (sitúa más intervalos de predicción en la zona alta). El menos resistente sería el veneno VA (mayor número de intervalos de predicción en la parte inferior). 4.8 Ejercicios Ejercicio 1. Se realiza una investigación para conocer los niveles de fosfato inorgánico en plasma (mg / dl) una hora después de una prueba de tolerancia a la glucosa estándar para sujetos obesos, con o sin hiperinsulinemia, y controles. Los datos corresponden con la tabla 6.18 de Dobson (2002). # Lectura de datos ejer01 &lt;- read_csv(&quot;https://goo.gl/3L4EtK&quot;, col_types = &quot;cd&quot;) Ejercicio 2. Se lleva a cabo un estudio sobre el contenido promedio de grasa (en porcentaje) en la leche del ganado de cinco razas distintas canadienses. Para ello se consideran veinte ejemplares de pura raza (diez de dos años y diez maduras de más de cuatro años de cada una de cinco razas. # Lectura de datos ejer02 &lt;- read_csv(&quot;https://goo.gl/J2ZKWK&quot;, col_types = &quot;dcc&quot;) Ejercicio 3. Los datos que se presentan a continuación (Dobson 2002) corresponden a un estudio en el que semillas genéticamente iguales son asignadas aleatoriamente, bien a un entorno enriquecido nutricionalmente (tratamiento), bien a condiciones estándar (control). Una vez han crecido todas las plantas, se recolectan, secan y pesan. El interés es investigar el efecto del tratamiento utilizado sobre le peso seco (en gramos) de las plantas en cuestión. peso &lt;- c(4.17, 5.58, 5.18, 6.11, 4.50, 4.61, 5.17, 4.53, 5.33, 5.14, 4.81, 4.17, 4.41, 3.59, 5.87, 3.83, 6.03, 4.89, 4.32, 4.69) entorno &lt;- c(rep(&quot;tratamiento&quot;,10),rep(&quot;control&quot;,10)) ejercicio03 &lt;- data.frame(peso,entorno) Ejercicio 4. Se realiza un estudio experimental para estudiar el rendimiento de un proceso químico en función de la concentración del compuesto base (A, B, C y D), el catalizador usado (C1, C2, y C3), y la temperatura usada en el proceso (T1, T2). Se quiere estudiar como afectan estos factores en el rendimiento del proceso. ejercicio04 &lt;- read_csv(&quot;https://bit.ly/2GhFsl7&quot;, col_types = &quot;dccc&quot;) ejercicio04 &lt;- ejercicio04 %&gt;% mutate_if(sapply(ejercicio04,is.character),as.factor) Ejercicio 5. Se quiere evaluar la eficacia de distintas dosis de un fármaco contra la hipertensión arterial, comparándola con la de una dieta sin sal. Para ello se seleccionan al azar 25 hipertensos y se distribuyen aleatoriamente en 5 grupos. Al primero de ellos no se le suministra ningún tratamiento (T1), al segundo una dieta con un contenido pobre en sal (T2), al tercero una dieta sin sal (T3), al cuarto el fármaco a una dosis determinada (T4) y al quinto el mismo fármaco a otra dosis (T5). Las presiones arteriales (Presion) sistólicas de los 25 sujetos al finalizar los tratamientos son: presion &lt;- c(180, 173, 175, 182, 181, 172, 158, 167, 160, 175, 163, 170, 158, 162, 170, 158, 146, 160, 171, 155, 147, 152, 143, 155, 160) tratamiento &lt;- c(rep(&quot;T1&quot;,5),rep(&quot;T2&quot;,5),rep(&quot;T3&quot;,5), rep(&quot;T4&quot;,5),rep(&quot;T5&quot;,5)) ejercicio05 &lt;- data.frame(presion, tratamiento) Ejercicio 6. La convección es una forma de transferencia de calor por los fluidos debido a sus variaciones de densidad por la temperatura; las partes calientes ascienden y las frías descienden formando las corrientes de convección que hacen uniforme la temperatura del fluido. Se ha realizado un experimento para determinar las modificaciones de la densidad de fluido al elevar la temperatura en una determinada zona. Los resultados obtenidos han sido los siguientes: densidad &lt;- c(21.8, 21.9, 21.7, 21.6, 21.7, 21.7, 21.4, 21.5, 21.4, 21.9, 21.8, 21.8, 21.6, 21.5, 21.9, 22.1, 21.85, 21.9) temperatura &lt;- c(rep(&quot;T100&quot;,5),rep(&quot;T125&quot;,4),rep(&quot;T150&quot;,5), rep(&quot;T175&quot;,4)) ejercicio06 &lt;- data.frame(densidad, temperatura) Ejercicio 7. Un laboratorio de reciclaje controla la calidad de los plásticos utilizados en bolsas. Se desea contrastar si existe variabilidad en la calidad de los plásticos que hay en el mercado. Para ello, se eligen al azar cuatro plásticos y se les somete a una prueba para medir el grado de resistencia a la degradación ambiental. De cada plástico elegido se han seleccionado ocho muestras y los resultados de la variable que mide la resistencia son los de la tabla adjunta. resistencia &lt;- c(135, 175, 97, 169, 213, 171, 115, 143, 275, 170, 154, 133, 219, 187, 220, 185, 169, 239, 184, 222, 253, 179, 280, 193, 115, 105, 93, 85, 120, 74, 87, 63) plastico &lt;- c(rep(&quot;PA&quot;,8),rep(&quot;PB&quot;,8),rep(&quot;PC&quot;,8), rep(&quot;PD&quot;,8)) ejercicio07 &lt;- data.frame(resistencia, plastico) Ejercicio 8. Se realiza un estudio sobre el efecto que produce la descarga de aguas residuales de un planta sobre la ecología del agua natural de un río. En el estudio se utilizaron dos lugares de muestreo. Un lugar está aguas arriba del punto en el que la planta introduce aguas residuales en la corriente; el otro está aguas abajo. Se tomaron muestras durante un periodo de cuatro semanas y se obtuvieron los datos sobre el número de diatomeas halladas. Los datos se muestran en la tabla adjunta: diatomeas &lt;- c(78, 94, 43, 58, 620, 760, 420, 913, 204, 333, 98, 89, 890, 655, 763, 562, 79, 87, 145, 522, 546, 652, 76, 94, 45, 69, 59, 62, 254, 86, 789, 267) semana &lt;- c(rep(&quot;S1&quot;,4), rep(&quot;S2&quot;,4), rep(&quot;S3&quot;,4), rep(&quot;S4&quot;,4), rep(&quot;S1&quot;,4), rep(&quot;S2&quot;,4), rep(&quot;S3&quot;,4), rep(&quot;S4&quot;,4)) lugar &lt;- c(rep(&quot;Aguas arriba&quot;,16), rep(&quot;Aguas abajo&quot;,16)) ejercicio08 &lt;- data.frame(diatomeas, semana, lugar) Ejercicio 9. (Estos datos corresponden a modelos ANOVA) Un fabricante de ropa que suministra uniformes militares debe cortar chaquetas, camisas, pantalones (variable Prenda) y otros complementos (en muchas tallas diferentes), de rollos de tela. La tela es cara, de modo que el desperdicio (Desperdicio) tiene un efecto muy grande en los beneficios. El fabricante tiene que elegir entre tres máquinas (Maquina) cortadoras asistidas por computadora: A, B y C. El fabricante decide experimentar haciendo que cada máquina corte varios lotes de chaquetas, varios más de camisas otros más de pantalones y complementos para determinar que máquina es más eficiente en cada caso, es decir, tratamos de conocer el desperdicio que se producirá para cada prenda y máquina. # Lectura de datos ejer09 &lt;- read_csv(&quot;https://bit.ly/2GcVn3R&quot;, col_types = &quot;ccd&quot;) ejer09 &lt;- ejer09 %&gt;% mutate_if(sapply(ejer09,is.character),as.factor) References "],
["ancova.html", "Unidad 5 Modelos ANCOVA 5.1 Bancos de datos 5.2 Modelo ANCOVA básico 5.3 Análisis preliminar 5.4 Ajuste y Selección del modelo 5.5 Diagnóstico del modelo 5.6 Predicción 5.7 Ejercicios modelos ANCOVA", " Unidad 5 Modelos ANCOVA En este tema se presentan los modelos ANCOVA o modelos de análisis de la covarianza. Esto modelos surgen cuando entre las posibles variables predictoras de la respuesta (de tipo numérico) consideramos tanto variables numéricas como factores. El objetivo principal de este tipo de modelos es estudiar si la relación entre las predictoras numéricas y la respuesta viene condicionada por el factor o factores de clasificación considerados, es decir, si debemos construir: un único modelo entre la respuesta y las predictoras de tipo numérico, un único modelo entre la respuesta y las predictoras de tipo categórico (factores), un modelo diferente entre la respuesta y las predictoras numéricas para cada nivel o combinaciones de niveles de los factores. Este tipo de modelos permiten una versatilidad que nos posibilita el estudio de situaciones experimentales más complejas. Sin embargo, no están exentos de dificultades sobre todo en lo que tiene que ver con el cumplimiento de las hipótesis del modelo. En función del modelo final las hipótesis de normalidad y homogeneidad varían en su aplicación. Además, el proceso de selección del mejor modelo requiere de un proceso de análisis más profundo debido a la inclusión de diferentes tipos de variables en el conjunto de posibles predictoras. Para introducir los conceptos básicos de este tipo de modelos y mostrar todas sus posibilidades de análisis comenzaremos con el modelo ANCOVA más sencillo donde únicamente consideramos dos variables predictoras, una de tipo numérico y la otra un factor. La formulación presentada se puede generalizar rápidamente a situaciones más complejas donde el número de predictoras sea mayor. 5.1 Bancos de datos Veamos los diferentes bancos de datos que iremos analizando a lo largo de la unidad. Ejemplo 1. Datos de tiempo de vida. Se desea estudiar el tiempo de vida de una pieza (vida) cortadora de dos tipos, A y B (herramienta), en función de la velocidad del torno (velocidad) en el que está integrada (en revoluciones por segundo). El objetivo del análisis es describir la relación entre el tiempo de vida de la pieza y la velocidad del torno, teniendo en cuenta de qué tipo es la pieza. Cargamos los datos y realizamos el gráfico descriptivo: # Carga de datos velocidad &lt;- c(610, 950, 720, 840, 980, 530, 680, 540, 980, 730, 670, 770, 880, 1000, 760, 590, 910, 650, 810, 500) vida &lt;- c(18.73, 14.52, 17.43, 14.54, 13.44, 25.39, 13.34, 22.71, 12.68, 19.32, 30.16, 27.09, 25.40, 26.05, 33.49, 35.62, 26.07, 36.78, 34.95, 43.67) herramienta &lt;- gl(2, 10, 20, labels=c(&quot;A&quot;, &quot;B&quot;)) tiempovida &lt;- data.frame(velocidad, vida, herramienta) # Gráfico ggplot(tiempovida, aes(x = velocidad, y = vida, color = herramienta)) + geom_point() A la vista de la Figura anterior se puede apreciar que el comportamiento del tiempo de vida con respecto a la velocidad disminuye al aumentar esta última, y además ese descenso es distinto en función de la herramienta utilizada. A la misma velocidad la herramienta A proporciona un tiempo de vida inferior que la herramienta B. Para ambas máquinas se aprecia una tendencia lineal bastante clara que deberá ser objeto de estudio. Esto implicaría que nuestro modelo se desdoblaría en dos rectas de regresión lineal simple en función de la herramienta utilizada. Ejemplo 2. Datos de longevidad. Partridge y Farquhar realizan un experimento para relacionar la vida útil (longevidad) de las moscas de la fruta con su actividad sexual (actividad). La información recogida es la longevidad en días de 125 moscas macho, divididas en cinco grupos bajo diferentes condiciones ambientales para medir su actividad sexual. Asimismo, se recoge la longitud del tórax (thorax) ya que se sospecha que afecta directamente a la longevidad de las moscas. Cargamos los datos y realizamos el gráfico descriptivo: # Carga de datos thorax &lt;- c(0.68, 0.68, 0.72, 0.72, 0.76, 0.76, 0.76, 0.76, 0.76, 0.8, 0.8, 0.8, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.88, 0.88, 0.92, 0.92, 0.92, 0.94, 0.64, 0.7, 0.72, 0.72, 0.72, 0.76, 0.78, 0.8, 0.84, 0.84, 0.84, 0.84, 0.84, 0.88, 0.88, 0.88, 0.88, 0.88, 0.92, 0.92, 0.92, 0.92, 0.92, 0.92, 0.94, 0.64, 0.68, 0.72, 0.76, 0.76, 0.8, 0.8, 0.8, 0.82, 0.82, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.92, 0.92, 0.68, 0.68, 0.72, 0.76, 0.78, 0.8, 0.8, 0.8, 0.84, 0.84, 0.84, 0.84, 0.84, 0.84, 0.88, 0.88, 0.88, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.92, 0.92, 0.64, 0.64, 0.68, 0.72, 0.72, 0.74, 0.76, 0.76, 0.76, 0.78, 0.8, 0.8, 0.82, 0.82, 0.84, 0.84, 0.84, 0.84, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.92) longevidad &lt;- c(37, 49, 46, 63, 39, 46, 56, 63, 65, 56, 65, 70, 63, 65, 70, 77, 81, 86, 70, 70, 77, 77, 81, 77, 40, 37, 44, 47, 47, 47, 68, 47, 54, 61, 71, 75, 89, 58, 59, 62, 79, 96, 58, 62, 70, 72, 75, 96, 75, 46, 42, 65, 46, 58, 42, 48, 58, 50, 80, 63, 65, 70, 70, 72, 97, 46, 56, 70, 70, 72, 76, 90, 76, 92, 21, 40, 44, 54, 36, 40, 56, 60, 48, 53, 60, 60, 65, 68, 60, 81, 81, 48, 48, 56, 68, 75, 81, 48, 68, 16, 19, 19, 32, 33, 33, 30, 42, 42, 33, 26, 30, 40, 54, 34, 34, 47, 47, 42, 47, 54, 54, 56, 60, 44) actividad &lt;- c(rep(&quot;G1&quot;,24),rep(&quot;G2&quot;,25),rep(&quot;G3&quot;,25),rep(&quot;G4&quot;,25),rep(&quot;G5&quot;,25)) longevidad &lt;- data.frame(thorax,longevidad,actividad) # Gráfico ggplot(longevidad, aes(x = thorax, y = longevidad, color = actividad)) + geom_point() Se puede ver como la longevidad aumenta cuando aumenta la longitud del thorax pero ese crecimiento no parece distinto según actividad, dado que las nubes de puntos están bastante mezcladas. En este caso no parece adecuado un modelo lineal para cada grupo de actividad. 5.2 Modelo ANCOVA básico Consideramos el modelo ANCOVA más sencillo donde consideramos dos variables predictoras: una numérica y otra un factor. Consideramos una muestra de tamaño \\(n\\) donde tenemos: Una variable respuesta, \\(Y\\), de tipo numérico con observaciones \\(y_1,...,y_n\\). Una variable predictora, \\(X\\), de tipo numérico con observaciones \\(x_1,...,x_n\\). Una variable predictora, \\(F\\), de tipo categórico con \\(I\\) grupos o niveles distintos de tamaños muestrales \\(n_1,n_2,...,n_I\\), de forma que \\(n = n_1 + n_2 + ... + n_I\\), de forma que el vector de observaciones de la respuesta y de la predictora numérica se pueden escribir como: \\[(Y_1, Y_2,...,Y_I) = y_{11},\\ldots,y_{1n_1},y_{21},\\ldots,y_{2n_2},\\ldots, y_{I1},\\ldots,y_{In_I}\\] \\[ (X_1, X_2,...,X_I) = x_{11},\\ldots,x_{1n_1},x_{21},\\ldots,x_{2n_2},\\ldots,x_{I1},\\ldots,x_{In_I} \\] donde el primer subíndice indica el nivel del factor y el segundo la posición dentro del conjunto de datos de dicho nivel del factor. * Conjunto \\(\\mu_i\\) de medias de todas las observaciones de la respuesta asociadas con el nivel \\(i\\) del factor, es decir: \\[ \\mu_i = \\frac{\\sum_{j = 1}^{n_j} y_{ij}}{n_i}; \\text{ i = 1, 2,..., I} \\] Media global de la respuesta, \\(\\mu\\), que se puede obtener como: \\[ \\mu = \\frac{\\sum_{j = 1}^{I} \\mu_{j}}{I} \\] Incrementos, \\(\\alpha_i\\), de cada una de las medias de cada grupo con respecto a la media global, es decir: \\[ \\alpha_i= \\mu - \\mu_i; \\text{ i = 1, 2,..., I} \\] Pendiente común, \\(\\beta\\), que representa la posible relación entre las variables de tipo numérico. Pendientes diferentes entre las predictoras numéricas asociadas a cada nivel del factor, \\(\\gamma_i\\) con \\(i = 1,...,I\\). En esta situación el modelo que describe la posible relación entre respuesta y predictoras se puede escribir como: \\[\\begin{equation} y_{ij} = \\alpha_0 + \\alpha_i + \\beta x_{ij} + \\gamma_i x_{ij} + \\epsilon_{ij};\\quad i=1,...,I \\quad \\text{con} \\quad \\alpha_I = 0 \\tag{5.1} \\end{equation}\\] que en forma matricial se puede escribir fácilmente (de forma análoga al ANOVA de un vía) sin más que considerar \\(1_(n_i )={1,...,1}\\) un vector de \\(n_i\\) unos, \\(0_(n_i )={0,...,0}\\) un vector de \\(n_i\\) ceros, para cada uno de los niveles i del factor la matriz de diseño viene dada por: \\[ Y = \\left( \\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\ldots \\\\ Y_I \\\\ \\end{array} \\right) = \\left( \\begin{array}{ccccccccc} 1_{n_1} &amp; 1_{n_1} &amp; 0_{n_1} &amp; \\ldots &amp; 0_{n_1} &amp; X_1 &amp; X_1 &amp; \\ldots &amp; 0_{n_1}\\\\ 1_{n_2} &amp; 0_{n_2} &amp; 1_{n_2} &amp; \\ldots &amp; 0_{n_2} &amp; X_2 &amp; 0_{n_2} &amp; \\ldots &amp; 0_{n_2}\\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; 0_{n_3}\\\\ 1_{n_I} &amp; 0_{n_I} &amp; 0_{n_I} &amp;\\ldots &amp; 1_{n_I} &amp; X_I &amp; 0_{n_I} &amp; \\ldots &amp; X_I\\\\ \\end{array} \\right) \\left( \\begin{array}{c} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\\\ \\ldots \\\\ \\alpha_{I_1} \\\\ \\beta \\\\ \\gamma_1\\\\ \\ldots\\\\ \\gamma_I\\\\ \\end{array} \\right) + \\left( \\begin{array}{c} e_1 \\\\ e_2 \\\\ \\ldots \\\\ e_n \\\\ \\end{array} \\right) = X\\beta + \\epsilon \\] donde las primeras \\(I\\) columnas representan el efecto del factor (ANOVA de una vía), la siguiente columna representa el efecto común entre predictoras numéricas (Regresión lineal simple), y las últimas \\(I\\) columnas representan el efecto distinto de la predictora numérica para cada nivel del factor. Estas columnas se obtienen fácilmente multiplicando la columna de \\(X\\) por cada de las columnas desde la \\(2\\) a la \\(I\\). Las posibles modelos anidados que se pueden obtener a partir de la ecuación (5.1), así como sus interpretaciones se presentan a continuación. Además, se ofrece la interpretación de dichos modelos en términos de los coeficientes que resulten significativos. 5.2.1 I rectas de regresión perpendiculares Este modelo es el más general y considera una pendiente distinta para la predictora numérica en función de los niveles del factor, que resulta equivalente a plantear el contraste \\[\\begin{equation} \\left\\{ \\begin{array}{ll} H_0: &amp; \\gamma_1 = \\gamma_2 = \\ldots = \\gamma_I = 0\\\\ H_a: &amp; \\mbox{Al menos hay una pendiente distinto de cero}\\\\ \\end{array} \\right. \\tag{5.2} \\end{equation}\\] Si rechazamos el contraste (5.2) tendremos un modelo de regresión entre la respuesta y la predictora numérica para cada nivel del factor, es decir, I rectas de regresión distintas con ecuaciones: \\[\\begin{equation} \\begin{array}{ll} y_{1j} &amp;= (\\alpha_0 + \\alpha_1) + (\\beta + \\gamma_1) x_{1j} + \\epsilon_{1j}\\\\ y_{2j} &amp;= (\\alpha_0 + \\alpha_2) + (\\beta + \\gamma_2) x_{2j} + \\epsilon_{2j}\\\\ \\ldots &amp;= \\ldots \\\\ y_{Ij} &amp;= (\\alpha_0 + \\alpha_I) + (\\beta + \\gamma_I) x_{Ij} + \\epsilon_{Ij}\\\\ \\end{array} \\tag{5.3} \\end{equation}\\] con interceptaciones \\(\\alpha_0 + \\alpha_i\\) y pendientes \\(\\beta + \\gamma_i\\) para cada uno de los \\(I\\) niveles del factor. Tenemos I modelos de regresión distintos (uno por cada nivel del factor), es decir, tenemos I rectas perpendiculares o que se cortan. Ejemplo. Para el banco de datos de tiempo de vida podemos representar gráficamente el modelo asumiendo dos rectas de regresión perpendiculares de vida versus velocidad del torno en función de la herramienta utilizada. Se puede ver como las rectas tienen interceptaciones distintas y pendientes algo diferentes. Las tendencias no son perpendiculares en el sentido estricto sino que existe algún punto de corte entre ellas. La resolución del contraste nos indicara si el modelo es adecuado o si deberíamos optar por un modelo más sencillo. Ejemplo. Para el banco de datos de longevidad representamos gráficamente el modelo que asume 5 rectas de regresión perpendiculares de longevidad versus longitud del thorax en función del grupo de actividad. Se puede ver como las rectas tienen interceptaciones distintas y pendientes muy similares lo que puede ser un indicativo de que este modelo de rectas perpendiculares no es adecuado. 5.2.2 I rectas de regresión paralelas Si no rechazamos el contraste (5.2) tendremos un único modelo de regresión entre la respuesta y la predictora pero con diferentes interceptaciones (la pendiente es la misma), es decir, I rectas paralelas cuyas ecuaciones vienen dadas por: \\[\\begin{equation} \\begin{array}{ll} y_{1j} &amp;= (\\alpha_0 + \\alpha_1) + \\beta x_{1j} + \\epsilon_{1j}\\\\ y_{2j} &amp;= (\\alpha_0 + \\alpha_2) + \\beta x_{2j} + \\epsilon_{2j}\\\\ \\ldots &amp;= \\ldots \\\\ y_{Ij} &amp;= (\\alpha_0 + \\alpha_I) + \\beta x_{Ij} + \\epsilon_{Ij}\\\\ \\end{array} \\tag{5.4} \\end{equation}\\] Ejemplo. Modelo de rectas paralelas para el banco de datos de tiempo de vida. E ¿Se diferencia mucho este gráfico del de rectas perpendiculares visto antes? Ejemplo. Modelos de rectas paralelas para el banco de datos de longevidad. En este caso el gráfico es prácticamente idéntico indicando que para estos datos el modelo de rectas paralelas tiene la misma capacidad explicativa que el de rectas perpendiculares. En la situación donde el contraste (5.2) es no significativo podemos definir diferentes modelos anidados en función de los incrementos del factor (`s) y la pendiente \\(\\beta\\). Los contrastes son: \\[\\begin{equation} \\left\\{ \\begin{array}{ll} H_0: &amp; \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_I = 0\\\\ H_a: &amp; \\mbox{Al menos hay un incremento distinto de cero}\\\\ \\end{array} \\right. \\tag{5.5} \\end{equation}\\] y \\[\\begin{equation} \\left\\{ \\begin{array}{ll} H_0: &amp; \\beta = 0\\\\ H_a: &amp; \\beta \\neq 0 \\end{array} \\right. \\tag{5.6} \\end{equation}\\] La resolución de estos contrastes nos lleva a nuevos modelos que pasamos a describir. 5.2.3 Una recta de regresión Si no rechazamos la hipótesis nula de (5.5) pero si rechazamos la hipótesis nula de (5.6) diríamos que no hay efecto del factor pero si que podemos establecer un modelo de regresión entre la respuesta y la predictora numérica (Modelo de Regresión Lineal Simple). Ejemplo. Modelo de recta de regresión para el banco de datos de tiempo de vida. ¿Qué podemos decir de este modelo frente a los anteriores? Ejemplo. Modelo de recta de regresión para el banco de datos de longevidad. ¿Podríamos considerar este modelo como válido frente a los considerados antes? Recuerda que siempre buscamos modelos con capacidades explicativas similares pero con menos efectos o variables. 5.2.4 ANOVA de una vía Si rechazamos la hipótesis nula de (5.5) pero no rechazamos la hipótesis nula de (5.6) diríamos que no hay efecto de la predictora numérica pero que si podemos establecer diferencias entre las medias de la respuesta dadas por los diferentes niveles del factor. Nos encontramos ante un modelo ANOVA de una vía. En esta situación asumimos que no existe rectas de regresión que relacionen el comportamiento de la respuesta con la predictora numérica, es decir, ya no tendremos una nube de puntos sino datos agrupados por el factor. Ejemplo. Modelo ANOVA de una vía para el banco de datos de tiempo de vida. Se observa claramente como la herramienta de tipo B produce un tiempo de vida superior al del tipo A. Al menos el efecto del factor parece relevante para explicar el comportamiento del tiempo de vida. Ejemplo. Modelo ANOVA de una vía para el banco de datos de longevidad. El gráfico de cajas asociado con el factor muestra que los primeros cuatro grupos tienen un comportamiento muy similar, y tan sólo el grupo cinco parecer tener una longevidad inferior al resto. 5.2.5 Modelos sin efectos Si no rechazamos la hipótesis nula de (5.5) y no rechazamos la hipótesis nula de (5.6) estaríamos ante un modelo nulo donde el comportamiento de la respuesta no viene explicado por las predictoras consideradas. 5.2.6 Especificación del modelo en R El modelo ANCOVA planteado para un factor \\(F\\) y una variable predictora numérica \\(X\\), se puede escribir en R en su formato reducido como: \\[Y \\sim F + X + F:X\\] donde: \\(F\\) representa el efecto del factor, es decir, comparamos si las medias de la respuesta para cada grupo pueden considerarse iguales. \\(X\\) representa el efecto de regresión asociado con la variable numérica, es decir, la respuesta y X están relacionadas mediante una única pendiente que deberemos estimar. \\(F:X\\) representa el efecto de interacción entre predictoras, es decir, que la respuesta se relaciona con la predictora numérica mediante tantas curvas (generalmente líneas) como niveles tenga el factor \\(F\\). A continuación, se presentan los modelos reducidos para diferentes situaciones experimentales en el número y tipo de variables predictoras: Modelo para dos factores (\\(F_1\\) y \\(F_2\\)) y una numérica (\\(X\\)) \\[Y \\sim F_1 + F_2 + F_1:F_2 + X + F_1:X + F_2:X + F_1:F_2:X\\] o en forma más simplificada \\(Y \\sim F_1*F_2*X\\) Modelo para un factor (\\(F\\)) y dos numéricas (\\(X_1\\) y \\(X_2\\)) \\[Y \\sim F + X_1 + X_2 + F:X_1 + F:X_2\\] o en forma más simplificada \\(Y \\sim F*(X_1 + X_2)\\) Modelo para dos factores (\\(F_1\\) y \\(F_2\\)) y dos numéricas (\\(X_1\\) y \\(X_2\\)) \\[Y \\sim F_1*F_2*(X_1 + X_2)\\] Como se puede ver la complejidad del modelo aumenta sustancialmente con la consideración de más variables predictoras. La forma de expresar el modelo saturado debe contemplar tanto los efectos principales asociados a cada predictora, como los efectos de interacción entre factores y entre factores y numéricas. 5.3 Análisis preliminar En este tipo de modelos el análisis preliminar pasa por una representación de los diferentes modelos que pueden surgir desde el modelo saturado. En este caso se muestra como especificar los diferentes modelos y como obtener los gráficos correspondientes. Ejemplo. (Tiempo de vida). El objetivo del análisis es describir la relación entre el tiempo de vida de la pieza y la velocidad del torno, teniendo en cuenta de qué tipo es la pieza. Tenemos entonces los posibles modelos que involucran a la velocidad: \\[ \\left\\{ \\begin{array}{ll} \\mbox{M0}:&amp; Vida \\sim velocidad\\\\ \\mbox{M1}:&amp; Vida \\sim velocidad + herramienta\\\\ \\mbox{M2}:&amp; Vida \\sim velocidad + herramienta + velocidad:herramienta\\\\ \\end{array} \\right. \\] A continuación se presenta el código para poder representar todos estos modleos en un único gráfico. # Comenzamos con el modelo más sencillo # Modelo con una única recta M0 &lt;- lm(vida ~ velocidad, data = tiempovida) # M1: modelo con rectas paralelas M1 &lt;- lm(vida ~ herramienta + velocidad, data = tiempovida) # M2: modelo con rectas no paralelas M2 &lt;- lm(vida ~ herramienta + velocidad + herramienta:velocidad, data = tiempovida) # grid de valores para construir los modelos grid &lt;- tiempovida %&gt;% data_grid(herramienta, velocidad) %&gt;% gather_predictions(M0, M1, M2) # Gráfico ggplot(tiempovida,aes(velocidad, vida, colour = herramienta)) + geom_point() + geom_line(data = grid, aes(y = pred)) + facet_wrap(~ model) + labs(x = &quot;Velocidad del torno&quot;, y = &quot;Tiempo de vida&quot;) También representamos el modelo donde únicamente tenemos el factor (diagrama de cajas): \\[ \\begin{array}{ll} \\mbox{M}:&amp; Vida \\sim herramienta\\\\ \\end{array} \\] ggplot(tiempovida, aes(x = herramienta, y = vida)) + geom_boxplot() Ejemplo. Planterasmos los diferentes modelos para los datos de longevidad y los representamos gráficamente. \\[ \\left\\{ \\begin{array}{ll} \\mbox{M0}:&amp; longevidad \\sim thorax\\\\ \\mbox{M1}:&amp; longevidad \\sim thorax + actividad\\\\ \\mbox{M2}:&amp; longevidad \\sim thorax + herramienta + thorax:actividad\\\\ \\end{array} \\right. \\] # Comenzamos con el modelo más sencillo # Modelo con una única recta M0 &lt;- lm(longevidad ~ thorax, data = longevidad) # M1: modelo con rectas paralelas M1 &lt;- lm(longevidad ~ actividad + thorax, data = longevidad) # M2: modelo con rectas no paralelas M2 &lt;- lm(longevidad ~ actividad + thorax + actividad:thorax, data = longevidad) # grid de valores para construir los modelos grid &lt;- longevidad %&gt;% data_grid(actividad, thorax) %&gt;% gather_predictions(M0, M1, M2) # Gráfico ggplot(longevidad,aes(thorax, longevidad, colour = actividad)) + geom_point() + geom_line(data = grid, aes(y = pred)) + facet_wrap(~ model) + labs(x = &quot;Longitud del tórax&quot;, y = &quot;Longevidad&quot;) También representamos el modelo donde únicamente tenemos el factor (diagrama de cajas): \\[ \\begin{array}{ll} \\mbox{M}:&amp; longevidad \\sim actividad\\\\ \\end{array} \\] ggplot(longevidad, aes(x = actividad, y = longevidad)) + geom_boxplot() 5.4 Ajuste y Selección del modelo Las hipótesis del modelo ANCOVA son que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante σ^2 para cada uno de los grupos que determina la variable predictora de tipo factor. Estas hipótesis se adaptarán en función del tipo de modelo que finalmente alcancemos en el proceso de selección (I rectas, una recta, o un ANOVA de una vía). Dado que hemos expresado el modelo ANCOVA como un modelo de tipo lineal con una ecuación similar a los modelos de regresión múltiple, la estimación de los parámetros del modelo se puede realizar utilizando las ecuaciones normales. En el proceso de selección del mejor modelo actuaremos como en los modelos ANOVA, es decir partiremos del modelo saturado y veremos que efectos puede ser considerados como irrelevantes, y por tanto deben desaparecer del modelo. Esta selección nos permitirá elegir el modelo final resultante. En este caso más sencillo podemos escribir todos los modelos posibles y elegir el mejor de ellos, bien mediante la comparación con el test F o con el AIC, pero en la práctica se recurre a los procedimientos secuenciales automáticos que son los que presentaremos aquí. Ejemplo. Realizamos la selección del mejor modleo apra el conjunto de datos de tiempo de vida. Para ello construimos el modelo saturado y utilizamos el procedimiento automático basado en el test F parcial. # Modelo saturado fit.vida &lt;- lm(vida ~ velocidad * herramienta, data = tiempovida) # Selección del modelo ols_step_backward_p(fit.vida, prem = 0.05) ## ## ## Elimination Summary ## ------------------------------------------------------------------------------------- ## Variable Adj. ## Step Removed R-Square R-Square C(p) AIC RMSE ## ------------------------------------------------------------------------------------- ## 1 velocidad:herramienta 0.8969 0.8847 3.9652 106.6591 3.0919 ## ------------------------------------------------------------------------------------- El proceso de selección identifica el efecto de interacción entre velocidad y herramienta como no significativo, de forma que el modelo final viene dado por: \\[vida \\sim velocidad + herramienta\\] Ajustamos el modelo con rectas paralelas, es decir, la pendiente de la recta entre el tiempo de vida y la velocidad stiene la misma pendiente, variando únicamenyte la interceptación en función del tipo de herramienta utilizada. Tenemos por tanto un modelo con dos rectas paralelas (una por cada tipo de herramienta). # Modelo saturado fit.vida &lt;- lm(vida ~ velocidad + herramienta, data = tiempovida) # Parámetros estimados tab_model(fit.vida, show.r2 = FALSE, show.p = FALSE) vida Predictors Estimates CI (Intercept) 36.93 29.56 – 44.30 velocidad -0.03 -0.04 – -0.02 herramienta [B] 14.67 11.75 – 17.58 Observations 20 Las ecuaciones de estimación para este modelo vienen dadas por: \\[ \\left\\{ \\begin{array}{lll} \\mbox{Herramienta A}:&amp; \\widehat{Vida_{A}} = 36.93 + 0 - 0.03*velocidad &amp;= 36.93 -0.03*velocidad\\\\ \\mbox{Herramienta B}:&amp; \\widehat{Vida_{B}} = 36.93 + 14.67 - 0.03*velocidad &amp;= 51.60 - 0.03*velocidad \\\\ \\end{array} \\right. \\] Tenemos una interceptación mayor para la herramienta B indicando que la recta asociada con dicha herramienta está por encima de la de la herramienta A, lo que en térinos prácticos nos indica que el tiempo de vida con la herramienta B siempre será superior al del tipo A para cualquier velocidad considerada. Además, la pendiente negativa asociada con la velocidad indica que conforme aumenta esta disminuye el tiempo de vida. Ejemplo. Analizamos ahora los datos de longevidad. Construimos el modelo saturado y seleccionamos mediante el test \\(F\\). # Modelos fit.longevidad &lt;- lm(longevidad ~ thorax * actividad, data = longevidad) # Selección del modelo ols_step_backward_p(fit.longevidad, prem = 0.05) ## ## ## Elimination Summary ## ---------------------------------------------------------------------------------- ## Variable Adj. ## Step Removed R-Square R-Square C(p) AIC RMSE ## ---------------------------------------------------------------------------------- ## 1 thorax:actividad 0.6527 0.638 -3.7881 943.8165 10.5394 ## ---------------------------------------------------------------------------------- El proceso de selección identifica el efecto de interacción entre thorax y actividad como no significativo, de forma que el modelo final viene dado por: \\[longevidad \\sim thorax + actividad\\] Ajustamos el modelo y estudiamos los parámetros obtenidos: # Modelos fit.longevidad &lt;- lm(longevidad ~ thorax + actividad, data = longevidad) # Parámetros estimados tab_model(fit.longevidad, show.r2 = FALSE, show.p = FALSE) longevidad Predictors Estimates CI (Intercept) -44.61 -65.53 – -23.69 thorax 134.34 109.13 – 159.55 actividad [G2] -4.14 -10.13 – 1.86 actividad [G3] -1.50 -7.48 – 4.47 actividad [G4] -11.15 -17.15 – -5.16 actividad [G5] -24.14 -30.12 – -18.17 Observations 124 Todas las rectas tienen la misma pendiente (coeficiente asociado a tórax) pero con distinto punto de origen debido al tratamiento al que son sometidos los sujetos. La pendiente positiva indica que la longevidad aumenta cuando lo hace la longitud del tórax, mientras que podemos ver que el grupo G1 es el de mayor longevidad por tener la interceptación más grande. Las ecuaciones para cada uno de los grupos viene dada por: \\[ \\left\\{ \\begin{array}{ll} \\mbox{G1}:&amp; \\widehat{logevidad_{G1}} = - 44.61 + 134.34*thorax\\\\ \\mbox{G2}:&amp; \\widehat{logevidad_{G2}} = - 48.75 + 134.34*thorax \\\\ \\mbox{G3}:&amp; \\widehat{logevidad_{G3}} = - 46.11 + 134.34*thorax \\\\ \\mbox{G4}:&amp; \\widehat{logevidad_{G4}} = - 55.76 + 134.34*thorax \\\\ \\mbox{G5}:&amp; \\widehat{logevidad_{G5}} = - 68.75 + 134.34*thorax \\\\ \\end{array} \\right. \\] El grupo con mayor longevidad es el G1, ya que tiene la interceptación más grande, mientras que el que tiene menor longevidad es G5. El orden vendría dado por \\(G1 &gt; G3 &gt; G2 &gt; G4 &gt; G5\\). 5.5 Diagnóstico del modelo En este caso el diagnóstico es similar al de los modelos de regresión pero teniendo en cuenta que las hipótesis se deben verificar para los residuos asociados a cada nivel del factor (si este está presente en el modelo). Las hipótesis son linealidad, normalidad y varianza constante. Para verificar la hipótesis de linealidad utilizamos el gráfico de residuos vs ajustados y residuos vs predictora numérica, mientras que usamos los tests de normalidad y homogeneidad para el resto de hipótesis. Utilizamos los mismos ejemplos de los puntos anteriores para mostrar como realizar el diagnóstico en este tipo de modelos. Ejemplo A continuación, se presenta el diagnóstico para el modelo de tiempos de vida. Para realizar el diagnóstico partimos del modelo obtenido en la sección anterior. # Valores de diagnóstico diagnostico &lt;- fortify(fit.vida) # Gráfico ggplot(diagnostico,aes(x = velocidad, y = .stdresid, colour = herramienta)) + geom_point() + geom_hline(yintercept = 0, col = &quot;red&quot;) + facet_wrap(. ~ herramienta) En la figura no se observan problemas con los residuos, aunque sí se puede ver que para el tipo A la variabilidad de los residuos aumenta cuando aumenta el valor ajustado, indicando posibles problemas con la homogeneidad de varianzas. Además, no se observa ningún tipo de tendencia que pueda indicar falta de linealidad. Procedemos con los tests de hipótesis. # Tests de hipótesis ols_test_normality(fit.vida) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9715 0.7858 ## Kolmogorov-Smirnov 0.1232 0.8859 ## Cramer-von Mises 1.4412 2e-04 ## Anderson-Darling 0.2652 0.6555 ## ----------------------------------------------- leveneTest(.stdresid ~ herramienta, data = diagnostico) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 1.3888 0.254 ## 18 Puesto que ambos tests resultan no significativo se verifican las hipótesis del modelo, por lo que estamos en condiciones de afirmar que el modelo resultante es adecuado para explicar el comportamiento del tiempo de vida en función del tipo de pieza y de la velocidad considerada. # Análisis de influencia ols_plot_cooksd_chart(fit.vida) La distancia de Cook no muestra ninguna observación influyente (valor mayor que 1). Dado que se verifican las hipótesis el modelo obtenido parece adecuado para estudiar el tiempo de vida en función de la velocidad y la herramienta utilizada. Ejemplo. Analizamos ahora el modelo correspondiente a los datos de longevidad. En primer lugar, realizamos el análisis gráfico de los residuos. En este caso no hay interacción presente en el modelo y todos los gráficos deben identificar cada uno de los niveles de actividad. # Valores de diagnóstico diagnostico &lt;- fortify(fit.longevidad) # Gráfico ggplot(diagnostico,aes(x = thorax, y = .stdresid, colour = actividad)) + geom_point() + geom_hline(yintercept = 0, col = &quot;red&quot;)+ facet_wrap(. ~ actividad) En la figura se puede observar que para algún nivel del factor los residuos aumentan o disminuyen en función del valor ajustado, por ejemplo para el grupo G3 (mayor variabilidad en el centro que en los extremos) indicando posibles problemas con la homogeneidad de varianzas. Pasamos a valorar las hipótesis del modelo. # Tests de hipótesis ols_test_normality(fit.longevidad) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9916 0.6607 ## Kolmogorov-Smirnov 0.0538 0.8654 ## Cramer-von Mises 10.2413 0.0000 ## Anderson-Darling 0.3224 0.5241 ## ----------------------------------------------- leveneTest(.stdresid ~ actividad, data = diagnostico) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 4 1.2925 0.2769 ## 119 # Análisis de influencia ols_plot_cooksd_chart(fit.longevidad) Aunque se verifican las hipótesis del modelo y no se detectan observaciones influyentes, si que es cierto que los gráficos de residuos muestran cierto comportamiento de embudo con variabilidades más pequeñas en valores más pequeños de thorax, y mayor dispersión al aumentar la longitud del thorax. Sería recomendable probar Box-Cox para tratar de obtener una transformación de la respuesta que nos permita obtener gráficos sin esos efectos indeseables. MASS::boxcox(fit.longevidad) La transformación raíz cuadrada parece adecuada en esta situación. Obtenemos la nueva variable y ajustamos de nuevo el modelo. # Transformación longevidad &lt;- longevidad %&gt;% mutate(rlongevidad = sqrt(longevidad)) # Modelo saturado fit.longevidad &lt;- lm(rlongevidad ~ thorax * actividad, data = longevidad) # Selección del modelo ols_step_backward_p(fit.longevidad, prem = 0.05) ## ## ## Elimination Summary ## --------------------------------------------------------------------------------- ## Variable Adj. ## Step Removed R-Square R-Square C(p) AIC RMSE ## --------------------------------------------------------------------------------- ## 1 thorax:actividad 0.6868 0.6736 -3.0694 267.2943 0.6888 ## --------------------------------------------------------------------------------- De nuevo el modelo seleccionado prescinde del efecto de interacción. Ajustamos y estudiamos el nuevo modelo. # Modelos fit.longevidad &lt;- lm(rlongevidad ~ thorax + actividad, data = longevidad) # Parámetros estimados tab_model(fit.longevidad, show.r2 = FALSE, show.p = FALSE) rlongevidad Predictors Estimates CI (Intercept) 0.34 -1.03 – 1.71 thorax 9.41 7.77 – 11.06 actividad [G2] -0.30 -0.69 – 0.09 actividad [G3] -0.12 -0.51 – 0.27 actividad [G4] -0.76 -1.15 – -0.37 actividad [G5] -1.73 -2.12 – -1.34 Observations 124 ¿Cuáles son las ecuaciones de estimación en este caso? El proceso de diagnóstico para el nuevo modelo permite verificar el cumplimiento de las hipótesis y la leve mejora de los gráficos de residuos. # Valores de diagnóstico diagnostico &lt;- fortify(fit.longevidad) # Gráfico ggplot(diagnostico,aes(x = thorax, y = .stdresid, colour = actividad)) + geom_point() + geom_hline(yintercept = 0, col = &quot;red&quot;)+ facet_wrap(. ~ actividad) # Tests de hipótesis ols_test_normality(fit.longevidad) ## ----------------------------------------------- ## Test Statistic pvalue ## ----------------------------------------------- ## Shapiro-Wilk 0.9951 0.9465 ## Kolmogorov-Smirnov 0.0474 0.9432 ## Cramer-von Mises 10.7431 0.0000 ## Anderson-Darling 0.2137 0.8484 ## ----------------------------------------------- leveneTest(.stdresid ~ actividad, data = diagnostico) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 4 0.6856 0.6033 ## 119 # Análisis de influencia ols_plot_cooksd_chart(fit.longevidad) 5.6 Predicción El proceso de predicción en este tipo de modelos es muy simple a partir de las ecuaciones de los modelos obtenidos. De hecho, en las secciones anteriores ya hemos visto gráficamente la predicción para todos estos modelos en los ejemplos que hemos ido trabajando. Básicamente, si queremos obtener una predicción especifica deberemos dar un valor del factor y otro de la predictora numérica para calcular el valor de predicción y su correspondiente intervalo. En este caso nos imitamos a representar las bandas de predicción que podemos obtener para cada modelo. Ejemplo. A continuación, se presentan las rectas de predicción para el modelo ajustado en el banco de datos de tiempo de vida. plot_model(fit.vida, &quot;pred&quot;, terms = c(&quot;velocidad&quot;, &quot;herramienta&quot;), title =&quot;Predicción de la media del tiempo de vida&quot;) En la figura se presentan los resultados obtenidos donde queda patente el comportamiento distinto para tipo de herramienta, mostrando que la de tipo B tiene un tiempo de vida superior, pero que se va reduciendo cuando aumentamos la velocidad. Ejemplo. A continuación, se presentan las rectas de predicción para el modelo ajustado en el banco de datos de longevidad. En la figura se presentan los resultados obtenidos donde queda patente el comportamiento paralelo para cada tipo de actividad, mostrando que el grupo G1 es el que muestra una mayor longevidad que aumenta además con la longitud del tórax. Además, podemos ver como el único grupo que muestra un comportamiento distinto es el del grupo G5, mientras que los otros grupos muestran predicciones muy similares. plot_model(fit.longevidad, &quot;pred&quot;, terms = c(&quot;thorax&quot;, &quot;actividad&quot;), title =&quot;Predicción de la raíz cuadrada de la media de longevidad&quot;) Obtenemos ahora el gráfico de predicción en la escala original. Deshacemos el cambio de raíz cuadrada y representamos de nuevo las bandas de confianza. # Creamos grid de predicción newdata &lt;- data.frame(thorax = rep(seq(min(longevidad$thorax), max(longevidad$thorax), .01), each=5), actividad = factor(c(&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;, &quot;G4&quot;, &quot;G5&quot;))) # Obtenemos la predicción para el modelo ajusatdo newdata &lt;- data.frame(newdata, predict(fit.longevidad, newdata, interval=&quot;confidence&quot;)) # Eliminamos la raíz cuadrada de las predicciones newdata$fit &lt;- newdata$fit^2 newdata$lwr &lt;- newdata$lwr^2 newdata$upr &lt;- newdata$upr^2 # Gráfico de la predicción ggplot(newdata, aes(x = thorax, y = fit, color = actividad)) + geom_line() + geom_ribbon(aes(ymax = upr, ymin = lwr, fill = actividad), alpha = 1/5) + labs(x = &quot;Longitud del thorax&quot;, y = &quot;Longevidad&quot;, title = &quot;Bandas de predicción&quot;) En la figura siguiente se presentan los resultados obtenidos donde se aprecia cierta curvatura en las bandas de predicción debida al cambio en la escala de la raíz cuadrada a la escala original de la variable. 5.7 Ejercicios modelos ANCOVA A continuación se presenta la colección de ejercicios de esta unidad. Ejercicio 1. Disponemos de los datos de peso de 24 niños recién nacidos (peso), su sexo (sexo; “H” = Hombres y “M” = Mujeres) y la edad de sus madres (edad). Nos gustaría ser capaces de determinar un modelo que explique el peso de los niños recién nacidos en función de su sexo y de la edad de sus madres. # Lectura de datos edad &lt;- c(40, 38, 40, 35, 36, 37, 41, 40, 37, 38, 40, 38, 40, 36, 40, 38, 42, 39, 40, 37, 36, 38, 39, 40) peso &lt;- c(2968, 2795, 3163, 2925, 2625, 2847, 3292, 3473, 2628, 3176, 3421, 2975, 3317, 2729, 2935, 2754, 3210, 2817, 3126, 2539, 2412, 2991, 2875, 3231) sexo &lt;- gl(2,12, labels=c(&quot;H&quot;, &quot;M&quot;)) ejer01 &lt;- data.frame(edad, peso, sexo) Ejercicio 2. Se lleva a cabo una investigación sobre diversas malformaciones del sistema nervioso central registradas en nacidos vivos en Gales del Sur, Reino Unido. El estudio fue diseñado para determinar el efecto de la dureza del agua sobre la incidencia de tales malformaciones. La información registrada son: NoCNS = recuento de nacimientos sin problema CNS; An = conteo de nacimientos de Anencephalus; Sp = conteo de nacimientos de espina bífida; Otro = recuento de otros nacimientos del SNC; Agua = endurecimiento del agua; Trabajo = un factor con niveles Manual no manual en función del tipo de trabajo realizado por los padres Se está interesado en predecir el número total de malformaciones en función de la calidad del agua y el trabajo realizado por los padres. # Lectura de datos previo &lt;- read_csv(&quot;https://goo.gl/bNOSxt&quot;, col_types = &quot;cdddddc&quot;) # Calculamos el número total de malformaciones ejer02 &lt;- previo %&gt;% mutate(CNS = An + Sp + Other) Ejercicio 3. Se ha realizado un estudio para establecer la calidad de los vinos de la variedad Pino Noir en función de un conjunto de características analizadas. Las características analizadas son claridad, aroma, cuerpo, olor y matiz. Para medir la calidad se organiza una cata ciega a un conjunto de expertos y se calcula la puntuación final de cada vino a partir de la información de todos ellos. Además se registra la región (region) de procedencia del vino por si puede influir en la calidad del vino. # Lectura de datos ejer03 &lt;- read_csv(&quot;https://goo.gl/OX9wgM&quot;, col_types = &quot;ddddddc&quot;) Ejercicio 4. Una empresa recibe cargamentos de material para procesar en sus almacenes. El objetivo básico del estudio es determinar el tiempo de procesado de los cargamentos recibidos como función del tamaño del cargamento y el tipo de almacén. # Carga de datos ejer04 &lt;- read.table(&quot;https://goo.gl/kuMNpD&quot;, header = TRUE) ejer04 &lt;- as_tibble(ejer04) Ejercicio 5. Una empresa dedicada a la fabricación de aislantes térmicos y acústicos establece un experimento que mide la pérdida de calor (Calor) a través de cuatro tipos diferentes de cristal para ventanas (Cristal) utilizando cinco graduaciones diferentes de temperatura exterior (TempExt). Se prueban tres hojas de cristal en cada graduación de temperatura, y se registra la pérdida de calor para cada hoja. # Lectura de datos ejer05 &lt;- read_csv(&quot;https://goo.gl/V6hyVW&quot;, col_types = &quot;ddc&quot;) ejer05 &lt;- ejer05 %&gt;% mutate_if(sapply(ejer05,is.character),as.factor) Ejercicio 6. El grupo de asesores LearnStatistics ha realizado un estudio para comprobar si las empresas destinan parte de los beneficios de sus ventas en la formación de sus empleados para mejorar su competitividad. Para ellos se recoge la información sobre ventas (Ventas) en miles de euros, capital invertido en formación (Capital) en miles de euros, y el nivel de productividad de la empresa establecido por un asesor externo (Productividad). # Lectura de datos ejer06 &lt;- read_csv(&quot;https://bit.ly/2rCATaO&quot;, col_types = &quot;dcd&quot;) ejer06 &lt;- ejer06 %&gt;% mutate_if(sapply(ejer06,is.character),as.factor) "],
["smooth.html", "Unidad 6 Modelos aditivos lineales 6.1 Bancos de datos 6.2 Funciones de suavizado 6.3 Bondad del ajuste y selección del modelo 6.4 Diagnóstico 6.5 Predicción 6.6 Ejercicios", " Unidad 6 Modelos aditivos lineales En esta unidad se presentan los modelos aditivos lineales. Esto modelos surgen cuando la relación entre la predictora y la respuesta (en el caso de variables numéricas) no se puede escribir de forma lineal, sino más bien a través de una función desconocida. En unidades anteriores utilizamos los modelos polinómicos para poder capturar comportamientos no lineales entre predictora y respuesta, pero en este caso utilizaremos funciones de suavizado que permiten capturar todo tipo de comportamiento entre ambas. La mayor dificultad en este tipo de modelos es que no tenemos una forma explícita para la función de suavizado, y por tanto es necesario utilizar las funciones específicas de predicción proporcionadas por la librería de ajuste para obtener el modelo resultante. En este tema sólo se pretende dar una versión introductoria de los modelos de suavizado por o que se recomienda la lectura de textos más avanzados para completar lo visto en esta unidad. El modelo aditivo más básico con una variable predictora y una respuesta Normal viene dado por: \\[Y = f(X) + \\epsilon\\] donde \\(f()\\) se denomina función suave o de suavizado para la variable \\(X\\). Las ventajas de este tipo de modelos es que son muy flexibles ya que permiten modelizar, a través de dichas funciones suaves, relaciones de tipo no lineal entre la variable respuesta y las predictoras. Sin embargo, no todo son ventajas ya que el proceso de selección del mejor modelo se complica al añadir la elección de la función de suavizado a utilizar. En situaciones con dos variables predictoras, \\(X_1\\) y \\(X_2\\), de tipo numérico se podrían plantear los modelos saturados siguientes: \\[ \\begin{array}{ll} M0: &amp; Y \\sim X_1 + X_2\\\\ M1: &amp; Y \\sim f(X_1) + X_2\\\\ M2: &amp; Y \\sim f(X_1) + f(X_2)\\\\ \\end{array} \\] También resulta posible plantear este tipo de modelos donde se incluyen variables predictoras de tipo factor. En este caso debemos plantear una ecuación de suavizado para cada uno de los grupos determinados por el factor al igual que ocurría con el efecto de interacción en los modelos ANCOVA. Para modelizar este tipo de datos es necesario instalar y cargar la librería mgcv. 6.1 Bancos de datos Veamos los diferentes ejemplos con los que vamos a trabajar. Muchos de ellos ya los hemos utilizado en modelos anteriores. Ejemplo 1. Datos de calidad del aire. Este diseño experimental contiene la información recogida sobre el estudio de calidad del aire que ya presentamos en la Unidad 2. El objetivo de este estudio era tratar de predecir la calidad del aire, medida en términos del nivel de ozono (Ozone), en función de la radiación solar (Solar.R), velocidad del viento (Wind), y temperatura (Temp). Además, se recogen las variables mes (Month) y día (Day) de la recogida de datos. En base a las variables experimentales recogidas cabría pensar que un modelo de regresión lineal múltiple de la forma \\[Ozone \\sim Solar.R + Wind + Temp\\] A continuación, se presenta el código para la carga de datos y el gráfico de la respuesta versus cada predictora. En la figura se pueden ver los modelos lineales ajustados, y la aparente falta de ajuste de estos. Tan solo la relación entre Ozono y Wind parece de tipo lineal, mientras que en los otros dos parece necesario realizar algún tipo de transformación para linealizar la relación. Podemos intentar encontrar dicha transformación de respuesta o predictoras pero podemos ver que los métodos de suavizado nos proporcionan una solución rápida a esta situación sin necesidad de perder el tiempo buscando transformaciones adecuadas. # Carga de datos data(&quot;airquality&quot;) # Seleccionamos variables de interés datos &lt;- airquality[,c(&quot;Ozone&quot;, &quot;Solar.R&quot;, &quot;Wind&quot;, &quot;Temp&quot;)] datacomp = melt(datos, id.vars=&#39;Ozone&#39;) # Representamos respuesta vs predictoras ggplot(datacomp) + geom_jitter(aes(value,Ozone, colour=variable),) + facet_wrap(~variable, scales=&quot;free_x&quot;) + labs(x = &quot;&quot;, y = &quot;Ozono&quot;) Como veremos más adelante la solución en términos de modelo de suavizado se puede apreciar en el gráfico siguiente: ggplot(datacomp) + geom_jitter(aes(value,Ozone, colour=variable),) + geom_smooth(aes(value,Ozone, colour=variable), method=loess, se=FALSE) + facet_wrap(~variable, scales=&quot;free_x&quot;) + labs(x = &quot;&quot;, y = &quot;Ozono&quot;) donde podemos ver el cambio en la asociación entre las diferentes predictoras y el nivel de ozono. Las tendencias de suavizado obtenidas reflejan el comportamiento a gran escala de la predictora versus cada respuesta. ¿Qué conclusiones podemos extraer de cada uno de los suavizados obtenidos? Ejemplo 2. Datos de producción. Se realiza un ensayo agrícola para estudiar la producción de cierto tipo de planta en dos localidades en función de la densidad de plantas en la parcela de producción. Las variables recogidas en el experimento son la densidad de plantas (Densidad), la producción global obtenida (Produccion), y la localidad donde se encuentra la parcela de producción (Localidad). El banco de datos obtenido se presenta a continuación: Densidad &lt;- c(23.48, 26.22, 27.79, 32.88, 33.27, 36.79, 37.58, 37.58, 41.49, 42.66, 44.23, 44.23, 51.67, 55.58, 55.58, 57.93, 58.71, 59.5, 60.67, 62.63, 67.71, 70.06, 70.45, 73.98, 73.98, 78.67, 95.9, 96.68, 96.68,101.38, 103.72, 104.51, 105.68, 108.03,117.82, 127.21, 134.26, 137.39, 151.87, 163.61, 166.35, 184.75, 18.78, 21.25, 23.23, 27.18, 30.15, 31.63, 32.12, 32.62, 32.62, 33.61, 37.07, 38.55, 39.54, 39.54, 41.02, 42.5, 43.98, 45.47, 49.92, 50.9, 53.87, 57.82, 61.78, 61.78, 63.75, 67.71, 71.66, 77.59, 80.56, 86.49, 88.46, 89.45, 90.93, 92.91, 101.81, 103.78, 115.15, 123.06, 144.31, 155.68, 158.15, 180.39) Produccion &lt;- c(5.41, 5.46, 5.4, 5.4, 5.29, 5.25, 5.35, 5.25, 5.05, 5.12, 5.29, 5.04, 5.03, 4.96, 4.84, 5.12, 4.97, 5.02, 4.87, 4.83, 4.74, 4.76, 4.79, 4.9, 4.74, 4.51, 4.62, 4.58, 4.62, 4.58, 4.47, 4.4, 4.34, 4.47, 4.44, 4.24, 4.17, 4.2, 4.14, 4.02, 4.14, 4, 5.61, 5.46, 5.2, 5.18, 4.95, 5.13, 4.93, 5.15, 4.72, 5.05, 4.92, 5.04, 4.82, 4.99, 4.66, 4.94, 5, 4.7, 4.51, 4.63, 4.68, 4.53, 4.57, 4.55, 4.6, 4.54, 4.5, 4.24, 4.3, 4.32, 4.29, 4.38, 4.37, 4.26, 4.11, 4.31, 3.9, 4.04, 3.87, 3.69, 3.66, 3.37) Localidad &lt;- c(rep(&quot;A&quot;, 42), rep(&quot;B&quot;, 42)) plantas &lt;- data.frame(Densidad, Produccion, Localidad) A la vista de la información recogida se podría plantear un modelo ANCOVA, por lo que realizamos un gráfico de dispersión identificando cada punto según la localidad de procedencia. ggplot(plantas, aes(x = Densidad, y = Produccion, colour = Localidad)) + geom_point() + labs(x = &quot;Densidad&quot;, y = &quot;Producción&quot;) Se puede ver como la producción en la localidad A queda por encima de la de la localidad B, con un descenso asociado con el aumento de la densidad de plantas. La única diferencia con los modelos ANCOVA clásicos es que el descenso no parece ajustarse a un modelo lineal, sino más bien a un modelo con una caída curvilínea. Aunque existe la posibilidad de plantear una transformación de la respuesta y/o predictora, o incluso un modelo polinómico, resulta difícil plantear un modelo tan rígido dado que ambas localidades parecen comportarse de forma distinta. Vemos el resultado del modelo de suavizado (asumiendo que cada localidad puede tener un comportamiento distinto) El gráfico muestra las tendencias ajustadas para cada localidad reflejando una curva distinta para cada localidad, lo que permite obtener un modelo muy flexible que se adapta al comportamiento global de la producción versus la densidad de plantas en función de la localidad de procedencia. Más tarde presentaremos todas las posibilidades de modelización para este conjunto de datos. Ejemplo 3. Datos de infiltración. Se conoce como infiltración el proceso por el cual el agua (riego o lluvia) se va introduciendo bajo la superficie de un terreno cultivado. Este proceso es vital para determinar las cantidades de agua de riego necesarias, para mantener el terreno en condiciones óptimas. Un parámetro habitual que sirve para estudiar dicho proceso es la carga hidráulica. Este depende tanto de la profundidad de la infiltración (profundidad) como del procedimiento de riego usado. Se diseña un experimento para estudiar la carga hidráulica (cargahid) de un terreno bajo diferentes condiciones de riego (denominados tratamiento). Los datos recogidos en el experimento se presentan a continuación: tratamiento &lt;- c(rep(&quot;A&quot;, 15), rep(&quot;B&quot;, 15), rep(&quot;C&quot;, 15)) profundidad &lt;-c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150) cargahid &lt;- c(-406.90, -345.70, -335.50, -315.10, -304.90, -315.10, -323.26, -335.50, -345.70, -362.02, -374.26, -386.50, -421.18, -435.46, -447.70, -896.50, -737.38, -653.74, -470.14, -406.90, -388.54, -396.70, -396.70, -396.70, -406.90, -419.14, -437.50, -468.10, -466.06, -492.58, -896.50, -855.70, -818.98, -788.38, -678.22, -590.50, -545.62, -515.02, -498.70, -496.66, -517.06, -555.82, -619.06, -623.14, -623.14) infiltracion &lt;- data.frame(tratamiento, profundidad, cargahid) Representamos los datos mediante un gráfico de dispersión identificando cada uno de los tratamientos: ggplot(infiltracion, aes(x = profundidad, y = cargahid, colour = tratamiento)) + geom_point() + labs(x = &quot;Profundidad&quot;, y = &quot;Carga Hidráulica&quot;) En este caso las tendencias observadas son bastante diferentes entre tratamientos y claramente no lineales. Un posible modelo de suavizado para este conjunto de datos vendría dado por: En este caso los ajustes de suavizado se asemejan a modelos polinómicos de grado 3 o 4, de forma que se podrían plantear ambas modelizaciones y compararlas para determinar el modelo que mejor ajusta la tendencia observada en los datos. 6.2 Funciones de suavizado Las funciones de suavizado son los denominados splines que consisten en funciones definidas sobre bases de polinomios. En nuestro caso utilizaremos los denominados splines penalizados o p-splines. Para el ajuste de este tipo de mosdelos utilizaremos la función gam de la libreria mgcv. La función de suavizado tiene la estructura siguiente: s(variable, k = , m = , bs =\" \", by = factor) donde \\(k\\) es el tamaño de la base de polinomios, \\(m\\) es el orden de los polinomios, \\(bs\\) es el tipo de la base de splines utilizados y \\(by\\) identifica un factor para el ajuste de las curvas de suavizado (efecto de interacción). 6.2.1 Splines de regresión Las funciones de suavizado con las que se empieza a trabajar asumen que la función de suavizado \\(f(X)\\) se puede escribir como: \\[ f(X) = \\sum_{i=1}^q \\beta_j b_j(X) \\] siendo \\(\\beta_j\\) parámetros desconocidos y \\(b_1(X),\\ldots,b_q(X)\\) una base de funciones de polinomios de dimensión \\(q\\) (modelo polinómico de orden \\(q\\)), de forma que el modelo lineal para la respuesta \\(Y\\) se podría expresar como: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\ldots + \\beta_q x_i^q + \\epsilon_i, \\quad i=1,...,n \\] El problema principal con esta propuesta es que la base de polinomios nos llevaría a modelos donde aparecían problemas de multicolinealidad, y que exigen un grado bastante alto para poder adaptarse a los cambios entre respuesta y predictora. Una generalización de estos modelos son los denominados splines de regresión que son curvas definidas a trozos mediante polinomios, es decir, dividimos el rango de \\(X\\) en trozos y sobre cada trozo ajustamos un modelo polinómico con la única restricción que loas funciones obtenidas en cada trozo deben unirse formando una curva suave. La ventaja principal es que podemos ajustar utilizando polinomios de grado bajo en cada trozo, consiguiendo representar curvas con formas complicadas, evitando grandes oscilaciones en la tendencia que aparecen cuando utilizamos polinomios de grados muy altos. La desventaja principal es que debemos elegir el número de trozos o knots y el orden de la base de polinomios que debemos utilizar. 6.2.2 Splines cúbicos Un `spline cúbico`` es una curva construida a partir de trozos de polinomios de grado 3 que se ensamblan perfectamente de forma que la curva que forma es continua hasta la segunda derivada. Son los más utilizados dentro de los splines de regresión porque utilizan polinomios muy sencillos que se adaptan perfectamente a cambios en el comportamiento entre respuesta y predictora en los diferentes trozos en que se divide el rango de \\(X\\). 6.2.3 Splines penalizados La elección del grado de suavización de la función que ajusta la tendencia entre respuesta y predictora es un tema muy importante, y está asociado al grado de la base de polinomios utilizada. Las posibilidades que tenemos a la hora de elegir el grado de suavizado pasan por utilizar los denominados splines penalizados que son splines de regresión en los que se introduce una penalización al realizar el ajuste del modelo. Dicha penalización viene controlada por el parámetro de suavizado \\(\\lambda\\). Si \\(\\lambda = 0\\) estamos en el caso particular en el que no hay penalización y a medida que \\(\\lambda\\) aumenta, aumentamos la intensidad de la penalización. Cuando\\(\\lambda\\) tiende a 1 el modelo se convierte prácticamente en un modelo de regresión lineal simple. Se recomienda la lectura de la bibliografía recomendada para completar la información sobre el parámetro de suavizado y la penalización utilizada. Para ajustar este tipo de modelos en R utilizaremos la función anterior donde se toman valores: \\(k = 10\\) cuando nuestra muestra es pequeña y \\(k = 20\\) cuando nuestra muestra es grande (aunque se puede variar en función de los datos analizados). \\(m = 2\\) como el orden de los polinomios. Se toma como base de splines los splines penalizados (\\(bs = ps\\)). 6.3 Bondad del ajuste y selección del modelo La bondad del ajuste de este tipo de modelos se basa en los estadísticos AIC y GCV. El segundo de estos es específico de este tipo de modelos ya que se utiliza para estimar la curva de suavizado asociada con la variable predictora. En ambos casos cuanto menor es el valor mejor será el modelo obtenido. En la mayoría de situaciones se utilizan ambos criterios para comparar el modelo lineal con el suavizado y determinar cual es el mejor de los dos. Para este tipo de modelos se puede obtener además la capacidad explicativa del modelo construido a través de la desvianza explicada, que representa el porcentaje de variabilidad de la respuesta que viene explicada por el modelo, de forma similar al \\(R^2\\) en los modelos de regresión. En cuanto a la selección del modelo, la principal diferencia con respecto a los modelos tratados hasta ahora es que no existen procedimientos automáticos, con lo que la construcción y validación de los modelos requiere la construcción de todos los que consideremos que pueden ser adecuados. Las posibilidades más habituales de modelización y comparación consisten en: Comparar diferentes tipos de modelos lineales (RLS, RLM, o MP) con el modelo suavizado. Comparar las componentes del modelo aditivo, es decir, comprobar si podemos eliminar algunos de los efectos presentes en el modelo. En este sentido, cuando tenemos más de dos variables predictoras se pueden trabajar con todos los modelos que presentamos al inicio de esta unidad. Para la selección del mejor modelo se usan como referencia el AIC. El criterio basado en GCV se utiliza para seleccionar la función de suavizado (en cuanto a los parámetros que se usan para su construcción). Pasamos a realizar un análisis completo de los tres ejemplos presentados al inicio de la unidad. En este tipo de modelos utilizaremos la función summary en lugar de tab_model para obtener la información del modelo ajustado. Ejemplo. Estudaimos las diferentes posibilidades de modelización para el banco de datos de calidad del aire. Para construir el mejor modelo para este banco de datos vamos a proceder ajustando individualmente cada posible predictora para proceder posteriormente con modelos más complejos. Para cada modelo individual probaremos un modelo polinómico (MP) de orden 3 (para poder captar los cambios de tendencia) y el correspondiente modelo de suavizado. Los ajustaremos todos y valoraremos la capacidad explicativa de cada uno. Se proponen los modelos siguientes para la asociación entre calidad del aire y la radiación solar: \\[ \\begin{array}{ll} Ozone &amp; \\sim Solar.R + Solar.R^2 + Solar.R^3\\\\ Ozone &amp; \\sim f(Solar.R)\\\\ \\end{array} \\] con \\(f()\\) una función de suavidado basada en p-splines penalizdos con \\(k = 10\\) nodos y polinomios de orden \\(2\\). Realizamos el ajuste de ambos modelos utilizando la función gam(), y comparamos ambos modelos con el estadístico AIC. # M1: modelo polinómico fit1.solarr &lt;- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) , data = airquality) # M1: modelo suavizado fit2.solarr &lt;- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) Resultados para el modelo polinómico summary(fit1.solarr) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.432e+01 1.244e+01 1.151 0.2522 ## Solar.R -1.245e-01 3.265e-01 -0.381 0.7038 ## I(Solar.R^2) 3.594e-03 2.212e-03 1.625 0.1072 ## I(Solar.R^3) -9.661e-06 4.292e-06 -2.251 0.0264 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## R-sq.(adj) = 0.225 Deviance explained = 24.6% ## GCV = 890.23 Scale est. = 858.15 n = 111 La tabla de coeficientes muestra que el grado 3 del polinomio es necesario (p-valor asociado significativo), aunque la capacidad explicativa (Deviance explained) es muy baja (24.6%). En cuanto al modelo suavizado tendríamos: summary(fit2.solarr) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 2.783 15.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Solar.R) 2.943 3.569 9.074 7.17e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.224 Deviance explained = 24.4% ## GCV = 891.28 Scale est. = 859.62 n = 111 La función de suavizado es relevante (p-valor de s(Solar.R) significativo), pero la capacidad explicativa también es muy baja (24.6%), aunque del mismo orden que la obtenida con el modelo polinómico. Realizamos la comparación de ambos modelos mediante AIC: # Bondad de ajuste de cada modelo (AIC) g1 &lt;- glance(fit1.solarr) g2 &lt;- glance(fit2.solarr) as_tibble(rbind(g1, g2)) ## # A tibble: 2 x 6 ## df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4. -530. 1071. 1084. 91822. 107. ## 2 3.94 -530. 1071. 1084. 92029. 107. Ambos modelos tienen un AIC del mismo orden por lo que resulta difícil establecer cual de ellos resultaría más conveniente. Veamos gráficamente los ajustes obtenidos donde se aprecia el comportamiento de ambos modelos. ## $Solar.R ## $Solar.R Analizamos ahora el efecto de la velocidad del viento en el nivel de ozono. Se proponen los modelos siguientes: \\[ \\begin{array}{ll} Ozone &amp; \\sim Wind + Wind^2 + Wind^3\\\\ Ozone &amp; \\sim f(Wind)\\\\ \\end{array} \\] Ajustamos ambos modelos # M1: modelo polinómico fit1.wind &lt;- gam(Ozone ~ Wind + I(Wind^2) + I(Wind^3) , data = airquality) # M1: modelo suavizado fit2.wind &lt;- gam(Ozone ~ s(Wind, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) Resultados para el modelo polinómico summary(fit1.wind) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ Wind + I(Wind^2) + I(Wind^3) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 200.28778 27.01403 7.414 2.5e-11 *** ## Wind -32.26471 8.29374 -3.890 0.00017 *** ## I(Wind^2) 1.92084 0.78733 2.440 0.01627 * ## I(Wind^3) -0.03771 0.02305 -1.636 0.10465 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## R-sq.(adj) = 0.484 Deviance explained = 49.8% ## GCV = 581.27 Scale est. = 561.23 n = 116 La tabla de coeficientes muestra que es suficiente con considera el grado 2 en el polinomio. La capacidad explicativa de este modelo se sitúa casi en el 50% (49.8%), indicando que individualmente está variable contribuye más a la explicación del nivel de ozono. En cuanto al modelo de suavizado: summary(fit2.wind) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ s(Wind, k = 10, m = 2, bs = &quot;ps&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.129 2.193 19.21 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Wind) 2.889 3.534 30.77 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.487 Deviance explained = 50% ## GCV = 577.41 Scale est. = 558.05 n = 116 La función se suavizado es relevante (p-valor significativo), y la capacidad explicativa también es del 50%. Si comparamos con el AIC podemos ver que ambos modelos son muy similares. En realidad deberíamos ajustar de nuevo el modelo polinómico considerando el grado 2, pero los resultados son muy similares. # Bondad de ajuste de cada modelo (AIC) g1 &lt;- glance(fit1.wind) g2 &lt;- glance(fit2.wind) as_tibble(rbind(g1,g2)) ## # A tibble: 2 x 6 ## df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4. -530. 1069. 1083. 62858. 112. ## 2 3.89 -529. 1069. 1082. 62564. 112. En cuanto a los ajustes obtenidos con cada modelo podemos ver lo que se parecen las dos soluciones propuestas. ## $Wind ## $Wind Analizamos ahora el efecto de la tempreratura sobre el nivel de ozono. Proponemos los modelos: \\[ \\begin{array}{ll} Ozone &amp; \\sim Temp + Temp^2 + Temp^3\\\\ Ozone &amp; \\sim f(Temp)\\\\ \\end{array} \\] Ajustamos ambos modelos # M1: modelo polinómico fit1.temp &lt;- gam(Ozone ~ Temp + I(Temp^2) + I(Temp^3) , data = airquality) # M1: modelo suavizado fit2.temp &lt;- gam(Ozone ~ s(Temp, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) Resultados para el modelo polinómico summary(fit1.temp) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ Temp + I(Temp^2) + I(Temp^3) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0910285 0.0663384 1.372 0.1727 ## Temp 2.2972754 1.6743100 1.372 0.1728 ## I(Temp^2) -0.0732824 0.0430854 -1.701 0.0917 . ## I(Temp^3) 0.0006372 0.0002747 2.319 0.0222 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Rank: 3/4 ## R-sq.(adj) = 0.533 Deviance explained = 54.1% ## GCV = 522.02 Scale est. = 508.52 n = 116 La tabla de coeficientes muestra que es necesario el grado 3 en el polinomio. La capacidad explicativa de este modelo se sitúa casi en 54.1% (la más alta de forma individual). En cuanto al modelo de suavizado: summary(fit2.temp) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ s(Temp, k = 10, m = 2, bs = &quot;ps&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.129 2.043 20.62 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Temp) 3.771 4.524 32.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.555 Deviance explained = 57% ## GCV = 505.06 Scale est. = 484.29 n = 116 La función se suavizado es relevante (p-valor significativo), y la capacidad explicativa también es algo superior que en el modelo polinómico al alcanzar el 57%. En este caso el AIC para el modelo de suavizado es algo inferior al polinómico pero ambos proporcionan soluciones muy similares. # Bondad de ajuste de cada modelo (AIC) g1 &lt;- glance(fit1.temp) g2 &lt;- glance(fit2.temp) as_tibble(rbind(g1,g2)) ## # A tibble: 2 x 6 ## df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3. -525. 1057. 1068. 57462. 113. ## 2 4.77 -521. 1053. 1069. 53867. 111. La solución con cada modelo es: ## $Temp ## $Temp A la vista de los análisis individuales las variables predictoras más relvantes por orden de importancia serían temperatura, velocidad del viento, y por último la radiación solar. En cuanto a la elección de modelos polinómicos o suavizados los resultados son muy similares en los tres casos ya que únicamente se observa cierta preferencia del suavizado con la variable temperatura. Pasamos a analizar los modelos más complejos con dos o tres predictoras. En primer lugar comparamos modelos con estructuras polinómicas frente a modelos de suavizado. Veamos todos los modelos: \\[ \\begin{array}{lll} M1:&amp; Ozone &amp; \\sim Solar.R + Solar.R^2 + Solar.R^3 + Wind + Wind^2\\\\ M2:&amp; Ozone &amp; \\sim Solar.R + Solar.R^2 + Solar.R^3 + Temp + Temp^2 + Temp^3\\\\ M3:&amp; Ozone &amp; \\sim Wind + Wind^2 + Temp + Temp^2 + Temp^3\\\\ M4:&amp; Ozone &amp; \\sim Solar.R + Solar.R^2 + Solar.R^3 + Wind + Wind^2 + Temp + Temp^2 + Temp^3\\\\ M5:&amp; Ozone &amp; \\sim f(Solar.R) + f(Wind)\\\\ M6:&amp; Ozone &amp; \\sim f(Solar.R) + f(Temp)\\\\ M7:&amp; Ozone &amp; \\sim f(Wind) + f(Temp)\\\\ M8:&amp; Ozone &amp; \\sim f(Solar.R) + f(Wind) + f(Temp)\\\\ \\end{array} \\] En primer lugar ajustamos todos los modelos (con el indicador de la tabla anterior): fit.M1 &lt;- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + Wind +I(Wind^2), data = airquality) fit.M2 &lt;- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + Temp +I(Temp^2) + I(Temp^3), data = airquality) fit.M3 &lt;- gam(Ozone ~ Wind +I(Wind^2) + Temp +I(Temp^2) + I(Temp^3), data = airquality) fit.M4 &lt;- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + Wind +I(Wind^2) + Temp +I(Temp^2) + I(Temp^3), data = airquality) fit.M5 &lt;- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;) + s(Wind, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) fit.M6 &lt;- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;) + s(Temp, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) fit.M7 &lt;- gam(Ozone ~ s(Wind, k = 10, m = 2, bs = &quot;ps&quot;) + s(Temp, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) fit.M8 &lt;- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;) + s(Wind, k = 10, m = 2, bs = &quot;ps&quot;) + s(Temp, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) En lugar de estudiar con detalle todos los modelos propuestos, utilizamos el AIC para ordenarlos (de mejor a peor) y analizamos los dos más relevantes. # Bondad de ajuste de cada modelo (AIC) g1 &lt;- glance(fit.M1) g2 &lt;- glance(fit.M2) g3 &lt;- glance(fit.M3) g4 &lt;- glance(fit.M4) g5 &lt;- glance(fit.M5) g6 &lt;- glance(fit.M6) g7 &lt;- glance(fit.M7) g8 &lt;- glance(fit.M8) as_tibble(rbind(g1, g2, g3, g4, g5, g6, g7, g8)) ## # A tibble: 8 x 6 ## df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5. -523. 1057. 1074. 79944. 106. ## 2 6.00 -497. 1008. 1027. 50331. 105. ## 3 5.00 -506. 1024. 1041. 41853. 111. ## 4 8.00 -477. 971. 996. 34858. 103. ## 5 6.06 -495. 1003. 1022. 48176. 105. ## 6 7.51 -493. 1003. 1026. 46669. 103. ## 7 7.69 -501. 1018. 1042. 38028. 108. ## 8 9.97 -470. 963. 992. 31176. 101. Los dos mejores modelos (menor valor del AIC) son el M4 y el M8 que corresponden con los modelos que incorporan las tres predictoras. El M4 expresado como modelo polinómico y el M8 como modelo aditivo. De hecho, el modelo aditivo con las tres predictoras es el mejor de todos. Dado que individualmente tanto para la variable Solar.R y Wind el modelo polinómico se comportaba al mismo nivel que el suavizado, se propone una última alternativa de modelización que consiste en asumir polinomios en estas dos variables y suavizado en Temp: \\[ \\begin{array}{lll} M9:&amp; Ozone &amp; \\sim Solar.R + Solar.R^2 + Solar.R^3 + Wind + Wind^2 + f(Temp)\\\\ \\end{array} \\] fit.M9 &lt;- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + Wind +I(Wind^2) + s(Temp, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) En lugar de estudiar con detalle todos los modelos propuestos, utilizamos el AIC para ordenarlos (de mejor a peor) y analizamos los dos más relevantes. # Bondad de ajuste de cada modelo (AIC) glance(fit.M9) ## # A tibble: 1 x 6 ## df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.30 -495. 1008. 1033. 48296. 103. El AIC para este modelo es superior al obtenido para los modelos M4 y M8, de forma que el modelo preferido sería el M8. En primer lugar estudiamos el modelo obtenido: summary(fit.M8) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;) + s(Wind, k = 10, ## m = 2, bs = &quot;ps&quot;) + s(Temp, k = 10, m = 2, bs = &quot;ps&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.667 25.25 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Solar.R) 2.622 3.209 4.142 0.00666 ** ## s(Wind) 2.747 3.374 14.773 1.24e-08 *** ## s(Temp) 3.600 4.327 12.738 5.38e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.721 Deviance explained = 74.4% ## GCV = 339.03 Scale est. = 308.58 n = 111 Los tres suavizados resultan significativos y la capacidad explicativa del modelo alcanza el 74.4%. Como alternativa a este modelo podríamos considerar modelos aditivos donde aumentamos el número de nodos. Probamos este nuevo modelo duplicando el número de nodos (pasamos de 10 a 20). En este caso dado que se trata de comparar diferentes opciones de suavizado utilizaremos el criterio GCV para decidirnos entre los dos. fit.M10 &lt;- gam(Ozone ~ s(Solar.R, k = 20, m = 2, bs = &quot;ps&quot;) + s(Wind, k = 20, m = 2, bs = &quot;ps&quot;) + s(Temp, k = 20, m = 2, bs = &quot;ps&quot;), data = airquality) summary(fit.M10) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Ozone ~ s(Solar.R, k = 20, m = 2, bs = &quot;ps&quot;) + s(Wind, k = 20, ## m = 2, bs = &quot;ps&quot;) + s(Temp, k = 20, m = 2, bs = &quot;ps&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.099 1.445 29.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Solar.R) 1.068 1.132 10.118 0.00173 ** ## s(Wind) 3.588 4.416 20.900 5.49e-14 *** ## s(Temp) 16.392 17.430 7.082 3.05e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.791 Deviance explained = 83.1% ## GCV = 289.38 Scale est. = 231.9 n = 111 El valor de GCV con 10 nodos es 339.03 mientras que con 20 nodos dicho estadístico es 289.38. Por tanto, el modelo con 20 nodos es preferido. De hecho, la capacidad explicada ha crecido hasta el 83.1% (un 10% superior). El valor de AIC para este modelo es 941 mostrando ser mejor que el modelo polinómico. El modelo final es un suavizado independiente para cada una de las predictoras consideradas. Ejemplo. Analizamos ahora el banco de datos de producción. La estructura de este banco de datos nos hace pensar que un modelo ANCOVA podría ser adecuado pero el gráfico descriptivo no deja muy claro si el comportamiento es de tipo lineal o si hay que considerar efectos de interacción entre localidad y densidad de plantas de tipo no lineal. ggplot(plantas, aes(x = Densidad, y = Produccion, colour = Localidad)) + geom_point() + labs(x = &quot;Densidad&quot;, y = &quot;Producción&quot;) El conjunto de modelos que se pueden plantear son: M1: Modelo lineal sin efecto de interacción. M2: Modelo lineal sin efecto de interacción. M3: Modelo polinómico de grado 2 sin interacción. M4: Modelo polinómico de grado 2 con interacción. M5: Modelo suavizado sin interacción. M6: Modelo suavizado con interacción. La representación gráfica de estos modelos se puede obteenr mediante: Veamos el código necesario para ajustar cada uno de los modelos propuestos y valoremos el AIC y el GCV para seleccionar el mejor modelo. fit.plantas.M1 &lt;- gam(Produccion ~ Localidad + Densidad, data = plantas) fit.plantas.M2 &lt;- gam(Produccion ~ Localidad * Densidad, data = plantas) fit.plantas.M3 &lt;- gam(Produccion ~ Localidad + Densidad + I(Densidad^2), data = plantas) fit.plantas.M4 &lt;- gam(Produccion ~ Localidad * (Densidad + I(Densidad^2)), data = plantas) fit.plantas.M5 &lt;- gam(Produccion ~ Localidad + s(Densidad, k = 10, m = 2, bs = &quot;ps&quot;), data = plantas) fit.plantas.M6 &lt;- gam(Produccion ~ Localidad + s(Densidad, k = 10, m = 2, bs = &quot;ps&quot;, by = Localidad), data = plantas) ### Valores de AIC rbind(glance(fit.plantas.M1),glance(fit.plantas.M2),glance(fit.plantas.M3), glance(fit.plantas.M4),glance(fit.plantas.M5),glance(fit.plantas.M6)) ## # A tibble: 6 x 6 ## df logLik AIC BIC deviance df.residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3. 46.6 -85.2 -75.4 1.62 81 ## 2 4 49.5 -89.0 -76.9 1.51 80 ## 3 4. 63.7 -117. -105. 1.08 80.0 ## 4 6.00 66.0 -118. -101. 1.02 78. ## 5 6.34 70.9 -127. -109. 0.909 77.7 ## 6 9.09 77.0 -134. -109. 0.786 74.9 ### Valores de GCV cbind(fit.plantas.M1$gcv.ubre, fit.plantas.M2$gcv.ubre, fit.plantas.M3$gcv.ubre, fit.plantas.M4$gcv.ubre, fit.plantas.M5$gcv.ubre, fit.plantas.M6$gcv.ubre) ## [,1] [,2] [,3] [,4] [,5] [,6] ## GCV.Cp 0.02077199 0.01985587 0.01415785 0.01410118 0.01266822 0.01177339 Con ambos criterios de selección el modelo preferido es el M6, es decir, el modelo de suavizado con interacción con el factor localidad. Estudiamos con más detalle dicho modelo: summary(fit.plantas.M6) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## Produccion ~ Localidad + s(Densidad, k = 10, m = 2, bs = &quot;ps&quot;, ## by = Localidad) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.84394 0.01605 301.88 &lt;2e-16 *** ## LocalidadB -0.32929 0.02272 -14.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Densidad):LocalidadA 2.541 3.101 216.4 &lt;2e-16 *** ## s(Densidad):LocalidadB 4.550 5.333 165.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.951 Deviance explained = 95.6% ## GCV = 0.011773 Scale est. = 0.010499 n = 84 Todos los efectos del modelo resultan significativos y la capacidad explicativa alcanza el 95%, indicando que el modelo obtenido es muy adecuado para estudiar la producción en las dos localidades. Ejemplo. Análisis de los datos de infoltración. El gráfico descriptivo de este conjunto de datos muestra relaciones no lineales distintas entre carga hidráulica y profundidad en función del tratamiento, por lo que un modelo ANCOVA con interacción podría resultar demasiado rígido. Utilizamos un modelo suavizado para estos datos. fit.cargahid &lt;- gam(cargahid ~ tratamiento + s(profundidad, k = 10, m = 2, bs = &quot;ps&quot;, by = tratamiento), data = infiltracion) summary(fit.cargahid) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## cargahid ~ tratamiento + s(profundidad, k = 10, m = 2, bs = &quot;ps&quot;, ## by = tratamiento) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -363.652 3.238 -112.32 &lt;2e-16 *** ## tratamientoB -131.920 4.579 -28.81 &lt;2e-16 *** ## tratamientoC -277.848 4.579 -60.68 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(profundidad):tratamientoA 4.031 4.797 38.27 &lt;2e-16 *** ## s(profundidad):tratamientoB 7.224 7.928 248.40 &lt;2e-16 *** ## s(profundidad):tratamientoC 6.862 7.584 215.45 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.994 Deviance explained = 99.7% ## GCV = 296.27 Scale est. = 157.24 n = 45 Los suavizados para cada tratamiento resultan significativos y la capacidad explicativa alcanza el 99%, indicando que el modelo a parece capturar adecuadamente la tendencia observada en los datos. 6.4 Diagnóstico Una vez ajustado un modelo el estudio de los residuos nos permite realizar su diagnóstico en las mismas condiciones que los modelos lineales habituales. Las hipótesis de este modelo son las mismas que las del modelo de regresión, y por tanto, el diagnóstico se centra en los mismos procedimientos gráficos. Recordemos que debemos verificar la normalidad y varianza constante de los residuos del modelo. Sin embargo, en este tipo de modelos se añade el diagnóstico sobre el grado de suavizado del modelo considerado para saber si es necesario modificarlo. Para este tipo de modelos resulta posible obtener todos los gráficos de interés con la función gam.check(). Esta función ofrece dos tipos de salidas: Una tabla que permite contrastar los parámetros del suavizado utilizado. Los p-valores obtenidos deben resultar no significativos. gráficos de diagnóstico (qq, residuos versus ajustados, histograma de los residuos, valores observados versus valores ajustados). El gráfico qq y el histograma nos permiten verificar la hipótesis de normalidad, mientras que los residuos versus ajustados nos permite verificar la hipótesis de varianza constante. El último gráfico nos permite conocer lo bueno que es el ajuste realizado, ya que si los punto se distribuyen a lo largo de la diagonal significará que el modelo predice adecuadamente la respuesta. Ejemplo. Realizamos el diagnóstico el modelo ajustado para los datos de calidad del aire. gam.check(fit.M10) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 17 iterations. ## The RMS GCV score gradient at convergence was 5.831016e-05 . ## The Hessian was positive definite. ## Model rank = 58 / 58 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(Solar.R) 19.00 1.07 1.06 0.71 ## s(Wind) 19.00 3.59 1.22 1.00 ## s(Temp) 19.00 16.39 1.09 0.79 Los test de suavizado indican que el modelo es adecuado, pero mientras que los gráficos para validar normalidad parecen indicar que no hay problema con dicha hipótesis, si parece haberlo con la de homogeneidad de varianzas. Se aprecia un efecto de embudo (aumenta la variabilidad cuando aumenta el valor ajustado) en el gráfico de residuos versus ajustados. En este caso no podemos utilizar las transformaciones de Box-Cox que están diseñadas para los modelos estudiados en unidades anteriores, pero si podemos probar alguna transformación habitual para ver si corregimos ese defecto observado. Probamos con la raíz cuadrada. Dado que vamos a modificar la escala de la respuesta es necesario reajustar el número de nodos (ya que hemos comprimido la escala). Asumimos 10 nodos y verificamos dicho modelo. # Modelo fit.M11 &lt;- gam(sqrt(Ozone) ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;) + s(Wind, k = 10, m = 2, bs = &quot;ps&quot;) + s(Temp, k = 10, m = 2, bs = &quot;ps&quot;), data = airquality) # Bondad del ajuste summary(fit.M11) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sqrt(Ozone) ~ s(Solar.R, k = 10, m = 2, bs = &quot;ps&quot;) + s(Wind, ## k = 10, m = 2, bs = &quot;ps&quot;) + s(Temp, k = 10, m = 2, bs = &quot;ps&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.0165 0.1165 51.62 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Solar.R) 2.145 2.655 7.684 0.000265 *** ## s(Wind) 2.461 3.049 12.871 2.40e-07 *** ## s(Temp) 3.862 4.622 15.929 1.74e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.747 Deviance explained = 76.6% ## GCV = 1.6482 Scale est. = 1.5076 n = 111 # Diagnóstico gam.check(fit.M11) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 7 iterations. ## The RMS GCV score gradient at convergence was 1.329944e-06 . ## The Hessian was positive definite. ## Model rank = 28 / 28 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(Solar.R) 9.00 2.15 0.97 0.355 ## s(Wind) 9.00 2.46 0.98 0.370 ## s(Temp) 9.00 3.86 0.84 0.025 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El modelo ajustado resulta significativo con una capacidad explicativa del 76.6%. En cuanto a las hipótesis del modelo podemos ver como se ha corregido el efecto de embudo obteniendo un modelo que verifica las hipótesis de partida. Ejemplo. Realizamos el diagnóstico del modelo obtenido para los datos de producción. gam.check(fit.plantas.M6) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 5 iterations. ## The RMS GCV score gradient at convergence was 3.261881e-07 . ## The Hessian was positive definite. ## Model rank = 20 / 20 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(Densidad):LocalidadA 9.00 2.54 1.08 0.76 ## s(Densidad):LocalidadB 9.00 4.55 1.08 0.71 No se detecta ningún problema con las hipótesis del modelo y podemos pasar a establecer la predicción para el modelo ajustado. Ejemplo. Realizamos el diagnóstico del modelo para los datos de infiltración. gam.check(fit.cargahid) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 7 iterations. ## The RMS GCV score gradient at convergence was 1.786611e-05 . ## The Hessian was positive definite. ## Model rank = 30 / 30 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(profundidad):tratamientoA 9.00 4.03 1.1 0.74 ## s(profundidad):tratamientoB 9.00 7.22 1.1 0.70 ## s(profundidad):tratamientoC 9.00 6.86 1.1 0.71 Todos los gráficos de diagnóstico parecen indicar que los residuos cumplen con las hipótesis del modelo (qq-plot, Resids vs linear pred., histogram of residuals, Response vs. ftted values). Por otro lado, el análisis del parámetro de suavizado indica que el ajuste obtenido es adecuado, no encontrando problemas importantes. 6.5 Predicción Una vez analizados y elegido el mejor modelo nos resta la fase de predicción. En este caso al no disponer de una función paramétrica que relaciona la respuesta con las predictoras no podemos obtener las ecauciones de predicción mediante una ecuación específica. Utilizamos las herramientas gráficas para ver los diferentes tipos de predicciones posibles. Ejemplo. Obtenemos la predicción para el modelo de calidad del aire. En primer lugar obtenemos las predicciones marginales asociadas a cada una de las predictoras presentes en el modelo. plot_model(fit.M11, &quot;pred&quot;, title =&quot;Predicción de la media de la raiz cuadarada de ozono&quot;) ## $Solar.R ## ## $Wind ## ## $Temp En estos gráficos podemos ver el efecto de cada predictora con respecto a la respuesta a partir del suavizado estimado. En ellos podemos ver: el nivel de ozono aumenta con la radiación solar (hasta el valor de 250 aproximadamente), y luego empieza a caer lentamente. El nivel de ozon disminuye con el aumento de la velocidad del viento hasta casi hacerse constante. El nivel de ozono se mantiene casi constante hasta una temperatura de 70, momento en el que empieza a crecer hasta la temperatura de 90, caundo comienza a ser casi constante de nuevo. Además, de los gráficos marginales podemos obtener gráficos conjuntos para dos variables sin más que estudiar los efectos combinados de ambas variables. En estos gráficos se crean escenarios con respecto a una de las variables numéricas (valor bajo, valor intermedio, y valor alto) y se representa la evolución de la otra predictora. plot_model(fit.M11, &quot;pred&quot;, terms = c(&quot;Solar.R&quot;, &quot;Wind&quot;), title =&quot;Predicción de la media de la raiz cuadarada de ozono&quot;) plot_model(fit.M11, &quot;pred&quot;, terms = c(&quot;Solar.R&quot;, &quot;Temp&quot;), title =&quot;Predicción de la media de la raiz cuadarada de ozono&quot;) plot_model(fit.M11, &quot;pred&quot;, terms = c(&quot;Wind&quot;, &quot;Temp&quot;), title =&quot;Predicción de la media de la raiz cuadarada de ozono&quot;) Podemos ver cada uno de los escenarios obtenidos lo que nos permite estudiar claramente el comportamiento de la respuesta. En realidad se reproducen los resultados independientes pero para diferentes valores (escenarios) de una de las predictoras numéricas consideradas. ¿Qué información obtenemos de dichos gráficos? Ejemplo. Para los datos de producciónos nos limitamos a representar gráficamente la predicción del modelo ajustado, junto con sus intervalos de confianza. Se puede ver como la producción disminuye conforme aumenta la densidad de plantas, siendo el comportamiento no lineal y distinto en cada localidad. De hecho, la producción siempre es inferior en la localidad B frente a la A en todo el rango de densidades de plantas. Ejemplo. Obtenemos y analizamos las funciones de predicción para los datos de infiltración. Los suavizados obtenidos se comportan adecuadamente al reproducir la tendencia observada. Se pueden ver las diferentes curvas de predicción en función del tratamiento utilizado. 6.6 Ejercicios En todos los ejercicios se debe proponer un modelo alterantivo basado en modelos de suavizado frente al modelo que sería de uso habitual. Se deberan comaparar ambos modelos para establecer si el modelo de suavizado es necesario. Ejercicio 1. Una empresa dedicada a la fabricación de aislantes térmicos y acústicos establece un experimento que mide la pérdida de calor (Calor) a través de cuatro tipos diferentes de cristal para ventanas (Cristal) utilizando cinco graduaciones diferentes de temperatura exterior (TempExt). Se prueban tres hojas de cristal en cada graduación de temperatura, y se registra la pérdida de calor para cada hoja. # Lectura de datos ejer01 &lt;- read_csv(&quot;https://goo.gl/V6hyVW&quot;, col_types = &quot;ddc&quot;) ejer01 &lt;- ejer01 %&gt;% mutate_if(sapply(ejer05,is.character),as.factor) Ejercicio 2. Treinta aleaciones del tipo 90/10 Cu-Ni, cada una con un contenido específico de hierro son estudiadas bajo un proceso de corrosión. Tras un período de 60 días se obtiene la pérdida de peso (en miligramos al cuadrado por decímetro y día) de cada una de las aleaciones debido al proceso de corrosión. El objetivo es estudiar el nivel de corrosión en función del contenido de hierro. A continuación se presenta el banco de datos y se realiza la primera inspección gráfica. hierro &lt;- c(0.01, 0.48, 0.71, 0.95, 1.19, 0.01, 0.48, 1.44, 0.71, 1.96, 0.01, 1.44, 1.96) peso &lt;- c(127.6, 124, 110.8, 103.9, 101.5, 130.1, 122, 92.3, 113.1, 83.7, 128, 91.4, 86.2) ejer02 &lt;- data.frame(hierro,peso) Ejercicio 3. Se ha realizado un experimento para tratar de conocer la viscosidad de cierto compuesto en función de la cantidad de un tipo der aceite que se usa en su fabricación. Se asume una relación de tipo lineal entre la viscosidad y la cantidad de aceite utilizada. aceite &lt;- c(0, 12, 24, 36, 48, 60, 0, 12, 24, 36, 48, 60, 0, 12, 24, 36, 48, 60, 12, 24, 36, 48, 60) viscosidad &lt;- c(26, 38, 50, 76, 108, 157, 17, 26, 37, 53, 83, 124, 13, 20, 27, 37, 57, 87, 15, 22, 27, 41, 63) ejer03 &lt;- data.frame(aceite, viscosidad) Ejercicio 4. Se realiza un estudio de campo para conocer el desarrollo de cierta especie de pez del lago lakemary en EEUU. Para medir el desarrollo se establece la edad de cada pez capturado mediante un procedimiento proporcionado por los biólogos. Además se mide la longitud del pez para tratar de establecer el estado de maduración de cada ejemplar. La investigación trata de relacionar la edad el pez a partir de su longitud para determinar el número de capturas permitidas. Las variables recogidas son: “Age” (edad del pez), y “Length” (longitud del pez en mm). data(&quot;lakemary&quot;) ejer04 &lt;-lakemary Ejercicio 5. Es bien sabido que la concentración de colesterol en el suero sanguíneo aumenta con la edad, pero es menos claro si el nivel de colesterol también está asociado con el peso corporal. Los datos muestran para una treinta de mujeres el colesterol sérico (milimoles por litro), la edad (años) y el índice de masa corporal (peso dividido por la altura al cuadrado, donde el peso se midió en kilogramos y la altura en metros). Se trata de construir un modelo que explique el nivel de colesterol en función de la edad y del índice de masas corporal. Los datos corresponden con la tabla 6.17 de Dobson (2002). ejer05 &lt;- read_csv(&quot;https://goo.gl/EKXWRc&quot;, col_types = &quot;ddd&quot;) Ejercicio 6. Los datos muestran el porcentaje de calorías totales obtenidas de carbohidratos complejos, para veinte diabéticos dependientes de insulina que habían seguido una dieta alta en carbohidratos durante seis meses. Se consideró que el cumplimiento del régimen estaba relacionado con la edad (en años), age, el peso corporal (relativo al peso “ideal” para la altura), weight, y otros componentes de la dieta como el porcentaje de proteínas ingeridas. Los datos corresponden con la tabla 6.3 de Dobson (2002). ejer06 &lt;- read_csv(&quot;https://goo.gl/Grm8xM&quot;, col_types = &quot;dddd&quot;) "]
]
