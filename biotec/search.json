[{"path":"index.html","id":"bienvenidos","chapter":"Bienvenidos","heading":"Bienvenidos","text":"Este compendio de unidades sobre modelización estadística trata de mostrar una versión aplicada para el tratamiento de los modelos estadísticos más básicos. El objetivo es mostrar una versión teórica de estos modelos sino una versión aplicada. Se recomienda los lectores interesado que complementen la teoría de estos modelos con manuales más específicos.Para poder seguir los contenidos aquí expuestos se recomiendan conocimientos básicos de estadística descriptiva, probabilidad e inferencia estadística, así como las funciones de R necesarias para llevar cabo dichos análisis.Los modelos contenidos en este manual son:Modelos de regresión lineal simpleModelos de regresión lineal múltiple y polinómicosModelos ANOVAModelos ANCOVAModelos aditivos linealesModelos lineales generalizadosRespuesta BinomialRespuesta PoissonTablas de contingenciaSupervivencia","code":""},{"path":"introlibro.html","id":"introlibro","chapter":"Unidad 1 Comenzamos","heading":"Unidad 1 Comenzamos","text":"La importancia de la estadística dentro del campo experimental siempre ha sido muy relevante, ya que para poder extraer conclusiones de un conjunto de datos experimentales se hace necesaria la utilización de procedimientos estadísticos más o menos sofisticados. Con la irrupción de los ordenadores personales y de los programas estadísticos para legos en la materia, así como la explosión tecnológica que estamos viviendo en los últimos años, la importancia de un correcto estudio estadístico de los datos experimentales se hace más necesaria que nunca. Se siguen publicando trabajos de investigación basados en datos experimentales donde el tratamiento estadístico de la información allí recogida puede considerarse como decepcionante. Con esta materia pretendemos guiar al estudiante en un correcto uso y análisis de las técnicas estadísticas más habituales en los diseños experimentales.El tratamiento estadístico de datos experimentales se puede caracterizar en dos grandes áreas: estudios descriptivos y análisis y modelización. Los estudios descriptivos se centran en el procesado de los datos experimentales obtenidos con el objetivo de establecer o reflejar posibles patrones o tendencias en su comportamiento. Se engloban dentro de este ámbito todas la técnicas estadísticas que permiten los resúmenes numéricos y gráficos de la información observada, así como la detección de observaciones anómalas, la transformación y el filtrado de los datos experimentales. Sin embargo, los estudios descriptivos tienen la gran limitación de que sus resultados están circunscritos los datos observados, y por tanto se pueden generalizar la población más general de la que se han obtenido. En el análisis y modelización se pretende generalizar los posibles patrones de comportamiento observados, en la fase descriptiva, mediante la construcción de modelos que nos permiten aproximar el comportamiento de datos experimentales observados. Evidentemente la construcción de dichos modelos estadísticos es una tarea rutinaria que debe tomarse la ligera. La propia naturaleza de los datos observados puede dar una idea de los posibles modelos que se pueden utilizar, pero el modelo final obtenido es el resultados de un proceso iterativo de construcción, verificación y validación que puede resultar costoso en algunas situaciones.La modelización estadística resulta relevante para representar el comportamiento de los datos experimentales de la forma más sencilla posible mediante modelos matemáticos donde se introduce de forma natural la incertidumbre de cualquier diseño experimental. Esta asignatura se centrará en la fase de modelización pero para poder llegar comprender su naturaleza es necesario introducir primero los conceptos básicos de cualquier estudio estadístico, así como los procedimientos de estadística descriptiva y el estudio de la aleatoriedad en los diseños experimentales.Este tema establece las definiciones básicas de cualquier estudio estadístico sobre diferentes ejemplos e introduce la nomenclatura básica de los modelos estadísticos que estudiaremos más adelante.Usar la estadística necesariamente es sinónimo de utilizar palabras raras o de hacer cálculos complicados. Significa que deseamos ver la realidad de forma objetiva, través de datos que reflejen de la mejor manera posible qué es lo que está ocurriendo. Una vez se tienen los datos hay que saber sacarles la información y saberla plasmar de forma clara y convincente.","code":""},{"path":"introlibro.html","id":"conceptos-básicos-del-diseño-experimental","chapter":"Unidad 1 Comenzamos","heading":"1.1 Conceptos básicos del diseño experimental","text":"En esta sección presentamos los conceptos básicos que utilizaremos lo largo de la materia. Se trata únicamente de un resumen muy esquemática, pero nos sirve para sentar las bases de los temas siguientes.","code":""},{"path":"introlibro.html","id":"objetivo-del-diseño-experimental","chapter":"Unidad 1 Comenzamos","heading":"1.1.1 Objetivo del diseño experimental","text":"El objetivo de cualquier diseño experimental es aquellos que pretendemos estudiar en función del tipo de información que se ha recogido y del tipo de premisas establecidas antes de la recolección de los datos. Además es importante establecer el número de repeticiones del experimento que vamos realizar, ya que eso condicionará el análisis de dichos datos. Si nuestro diseño experimental es muy complejo puede ocurrir que plantemos más de un objetivo.Ejemplo 1 (Degradación compuesto orgánico). Se va realizar un experimento para conocer el tiempo que tarda en degradarse un compuesto orgánico. En este caso nuestro objetivo es el tiempo hasta la degradación. Si el experimneto considera diferentes tipos de compuestos nuestro objetivo podría ser comparar el tiempo de degradación en función del tipo de compuesto.","code":""},{"path":"introlibro.html","id":"población-y-muestra","chapter":"Unidad 1 Comenzamos","heading":"1.1.2 Población y muestra","text":"Se define la población como el conjunto de sujetos u objetos que son de interés para el objetivo u objetivos planteados en nuestro diseño experimental. EL problema principal es que la población de sujetos u objetos suele ser demasiado grande para poder analizarla de forma completa, y por tanto debemos acudir un subconjunto de dicha población para llevar cabo nuestro diseño experimental.Se define la muestra como el subconjunto de la población la que accedemos para obtener la información necesaria de cara responder de la forma más precisa posible al objetivo u objetivos planteados.","code":""},{"path":"introlibro.html","id":"medidas-y-escalas-de-medida","chapter":"Unidad 1 Comenzamos","heading":"1.1.3 Medidas y escalas de medida","text":"Una medida es un número o atributo que se puede calcular para cada uno de los miembros de la población que está relacionado directamente con el objetivo de interés de la investigación. El conjunto de medidas obtenidas para cada uno de los elementos muestrales se denominan datos muestrales.EL conjunto de medidas que se pueden observar y registrar para un conjunto de sujetos u objetos bajo investigación se denominan variables. Por tanto, una variable es el conjunto de valores que puede tomar cierta característica de la población sobre la que se realiza el estudio estadístico. Se distinguen dos tipos que pasamos describir continuación.","code":""},{"path":"introlibro.html","id":"variables-cualitativas","chapter":"Unidad 1 Comenzamos","heading":"1.1.3.1 Variables cualitativas","text":"Son el tipo de variables que como su nombre lo indica expresan distintas cualidades, características o modalidad. Cada modalidad que se presenta se denomina atributo o categoría, y la medición consiste en una clasificación de dichos atributos. Las variables cualitativas pueden ser dicotómicas cuando sólo pueden tomar dos valores posibles, como sí y , hombre y mujer o ser politómicas cuando pueden adquirir tres o más valores. Dentro de ellas podemos distinguir:Variable cualitativa ordinal: La variable puede tomar distintos valores ordenados siguiendo una escala establecida, aunque es necesario que el intervalo entre mediciones sea uniforme, por ejemplo: leve, moderado, fuerte.Variable cualitativa nominal: En esta variable los valores pueden ser sometidos un criterio de orden, como por ejemplo los colores.","code":""},{"path":"introlibro.html","id":"variables-cuantitativas","chapter":"Unidad 1 Comenzamos","heading":"1.1.3.2 Variables cuantitativas","text":"Son las variables que toman como argumento cantidades numéricas. Las variables cuantitativas además pueden ser:Variable discreta: Es la variable que presenta separaciones o interrupciones en la escala de valores que puede tomar. Estas separaciones o interrupciones indican la ausencia de valores entre los distintos valores específicos que la variable pueda asumir. Ejemplo: El número de hijos (1, 2, 3, 4, 5). En muchas ocasiones una variable cualitativa ordinal puede ser interpretada como una variable discreta asociando las categorías de la variable valores numéricos respetando el orden o escala establecida. Por ejemplo la escala leve, moderado y fuerte le podríamos asociar la escala 1, 2 y 3 para mantener el orden.Variable continua: Es la variable que puede adquirir cualquier valor dentro de un intervalo especificado de valores. Por ejemplo el peso (2,3 kg, 2,4 kg, 2,5 kg,…), la altura (1,64 m, 1,65 m, 1,66 m,…), o el salario. Solamente se está limitado por la precisión del aparato medidor, en teoría permiten que existan valores infinitos entre dos valores observados.De forma habitual, la estructura de cualquier banco de datos (asociado un diseño experimental) tiene una estructura matricial donde en las filas se colocan los sujetos bajo estudio y en las columnas se sitúan las variables medidas para cada uno de ellos.Asociada cada variable de nuestro banco de datos se puede establecer lo que conocemos como parámetro o parámetros de interés de la variable.Ejemplo 2 (Variable de interés). Para el diseño experimental del estudio de la degradación del compuesto orgánico presentado en el ejemplo ??, la variable de interés es de tipo continuo y viene dada por el tiempo de degradación asociado cada repetición del experimento. Sin embargo, la hora de extraer conclusiones podemos presentar todo el conjunto de datos sino que recurrimos un resumen de dichos datos.","code":""},{"path":"introlibro.html","id":"parámetros-poblacionales-y-estadísticos","chapter":"Unidad 1 Comenzamos","heading":"1.1.4 Parámetros poblacionales y estadísticos","text":"Asociado cada variable se puede establecer lo que conocemos como parámetro o parámetros de interés de la variable. En el ejemplo anterior el parámetro de interés es el tiempo medio de degradación. Dado que generalmente es posible examinar toda la población y debemos recurrir una muestra de dicha población, es imposible conocer el verdadero valor del parámetro asociado con dicha variable. Para sortear este problema definimos el estadístico como una realización del parámetro para los datos muestrales observados. Por tanto el valor del estadístico (denominado estimación) varia entre dos muestras de las misma población. Cuanto mayor es la muestra más se parecerá el valor del estadístico al del parámetro.En ocasiones ocurrirá que el número de parámetros asociado con una variable es único, ya que se pueden establecer varios parámetros para estudiar el comportamiento de una variable. En el caso de variables de tipo cuantitativo siempre existen dos parámetros de interés: la media y la desviación típica. El primero nos indica como se sitúan los datos mientras que el segundo nos indica como se reparten los datos muestrales alrededor de la media.Ejemplo 3 (Parámetro de interés). Para el diseño experimental del estudio de la degradación del compuesto orgánico presentado en el ejemplo ??, el parámetro poblacional de interés es el tiempo medio de degradación, mientras que el estadístico es la media del tiempo de degradación observado para los sujetos de la muestra. Distinguimos entonces entre media poblacional (parámetro) y media muestral (estadístico).","code":""},{"path":"introlibro.html","id":"primeros-pasos-con-r-y-rstudio","chapter":"Unidad 1 Comenzamos","heading":"1.2 Primeros pasos con R y RStudio","text":"Para poder utilizar el código expuesto en estos materiales es necesario la instalación del programa R, del programa RStudio y de diferentes librerías de R. En los puntos siguientes se describe brevemente cada uno de estos puntos. Se recomienda además la consulta de los materiales electrónicos siguientes para complmentar la formación.Childs, D. Z. (2017). APS 135: Introduction Exploratory Data Analysis R. Versión electrónica.Grosser, M. 2017. Tidyverse Cookbook. Versión electrónica incompleta.Wickham, H. 2015. Advanced R. CRC Press. Versión electrónica resumida.Wickham, H. 2010. ggplot2. Third Edition. Springer. Recursos electrónicos.Wickham, H. & Grolemund, G. 2016. R Data Science. O’Reilly. Versión electrónica resumida.","code":""},{"path":"introlibro.html","id":"instalación-y-puesta-en-marcha","chapter":"Unidad 1 Comenzamos","heading":"1.2.1 Instalación y puesta en marcha","text":"Instalación de R y RStudio. Leer el capítulo de introducción del libro de Childs (2017) e instalar ambos programas descargándolos de sus correspondientes webs:\nPara instalar el programa R: https://cran.r-project.org/\nPara instalar RStudio: https://rstudio.com/\nInstalación de R y RStudio. Leer el capítulo de introducción del libro de Childs (2017) e instalar ambos programas descargándolos de sus correspondientes webs:Para instalar el programa R: https://cran.r-project.org/Para instalar RStudio: https://rstudio.com/Primeros pasos en R y RStudio. Leer los capítulos 1, 2, y 3 de Childs (2017), el capítulo 4 de Wickham (2015), y los capítulos 4 y 6 de Wickham (2016) para un desarrollo más amplio. Realizar los ejercicios que van apareciendo lo largo de los capítulos.Primeros pasos en R y RStudio. Leer los capítulos 1, 2, y 3 de Childs (2017), el capítulo 4 de Wickham (2015), y los capítulos 4 y 6 de Wickham (2016) para un desarrollo más amplio. Realizar los ejercicios que van apareciendo lo largo de los capítulos.Estructuras de datos. Leer los capítulos 4, 5, 6 y 9 de Childs (2017) para una breve introducción y los capítulos 2 y 3 de Wickham (2015) para completar la información. Realiza los ejercicios que van apareciendo.Estructuras de datos. Leer los capítulos 4, 5, 6 y 9 de Childs (2017) para una breve introducción y los capítulos 2 y 3 de Wickham (2015) para completar la información. Realiza los ejercicios que van apareciendo.Instalación y uso de librerías en RStudio. Leer el capítulo 8 de Childs (2017) e instala las librerías tidyverse, stringr, forcats, lubridate, magrittr, broom y datasets mediante la consola de RStudio. Carga las librerías con el comando library para comprobar que se han instalando de forma correcta.Instalación y uso de librerías en RStudio. Leer el capítulo 8 de Childs (2017) e instala las librerías tidyverse, stringr, forcats, lubridate, magrittr, broom y datasets mediante la consola de RStudio. Carga las librerías con el comando library para comprobar que se han instalando de forma correcta.Creación de proyectos y entornos de trabajo. Leer el capítulo 8 de Wickham (2016). Crea un proyecto para esta unidad, selecciona el directorio de trabajo donde se encuentra situado el proyecto, y guarda el entorno de trabajo.Creación de proyectos y entornos de trabajo. Leer el capítulo 8 de Wickham (2016). Crea un proyecto para esta unidad, selecciona el directorio de trabajo donde se encuentra situado el proyecto, y guarda el entorno de trabajo.","code":""},{"path":"introlibro.html","id":"informes-prediseñados","chapter":"Unidad 1 Comenzamos","heading":"1.3 Informes prediseñados","text":"En los últimos tiempos se ha puesto de modo la creación de informes directos apartir del código utilizado en Rstudio mediante la creación de documentos específicos. Su puede consulyar una guía sencilla de uso en enlace. Un desaroollo más completo se puede ver en este enlace. También se puede consultar este vídeo.","code":""},{"path":"introlibro.html","id":"librerías-de-r-necesarías","chapter":"Unidad 1 Comenzamos","heading":"1.4 Librerías de R necesarías","text":"Para poder utilizar el código expuesto en estos materiales es necesario la instalación de diferentes librerías de R. continuación se encuentra el código para cargar dichas librerías. Para algún análisis especifíco se utilizará alguna librería accesoria que será cargada con el comando require().Configuramos además el tema de los gráficos para que tengan un aspecto más limpio y más fácil de exportar en formato pdf o word. Para ellos utilizamos la función theme_set().","code":"\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(tidymodels)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(lubridate)\nlibrary(magrittr)\nlibrary(broom)\nlibrary(pubh)\nlibrary(datasets)\nlibrary(lmtest)\nlibrary(MASS)\nlibrary(kableExtra)\nlibrary(mosaic)\nlibrary(latex2exp)\nlibrary(moonBook)\nlibrary(sjlabelled)\nlibrary(sjPlot)\nlibrary(reshape2)\nlibrary(olsrr)\nlibrary(ggfortify)\nlibrary(mgcv)\nlibrary(modelr)\nlibrary(alr4)\nlibrary(equatiomatic) \nlibrary(survival)\nlibrary(survminer)\nlibrary(knitr)\ntheme_set(theme_sjplot2())"},{"path":"aed.html","id":"aed","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"Unidad 2 Análisis exploratorio de datos","text":"Esta unidad mostrará cómo utilizar la visualización y la transformación para explorar los datos de un diseño experimental de una manera sistemática, una tarea que los estadísticos llaman análisis exploratorio de datos, o AED (EDA en inglés) para abreviar. Los contenidos para este tema se han obtenido de @HadleyGrolemund16.EDA es un ciclo iterativo en el que el investigador debe: este caso se irán mezclando los contenidos teóricos con los prácticos para ir mostrando el funcionamiento de las diferentes funciones y procedimientos para el análisis inicial de nuestro banco de datos.Generar preguntas sobre tus datos.Buscar respuestas visualizando, transformando y modelando sus datos.Usar lo que aprende para refinar sus preguntas y / o generar nuevas preguntas.EDA es un proceso formal con un conjunto estricto de reglas. Más que nada, EDA es un estado mental. Durante las fases iniciales de EDA, debe sentirse libre de investigar cada idea que se le ocurra. Algunas de estas ideas funcionarán, y algunas serán callejones sin salida. medida que continúe su exploración, se dirigirá algunas áreas particularmente productivas que eventualmente escribirá y comunicará otros.El EDA es una parte importante de cualquier análisis de datos porque siempre debe investigar la calidad de sus datos. La limpieza de datos es solo una aplicación de EDA: el investigador debe hacer preguntas sobre si sus datos cumplen con sus expectativas o . Para realizar la limpieza de datos, deberá implementar todas las herramientas de EDA: visualización, transformación y modelado.","code":""},{"path":"aed.html","id":"objetivos","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.1 Objetivos","text":"El objetivo durante EDA es desarrollar una comprensión de los datos experimentales recogidos. La forma más fácil de hacerlo es utilizar preguntas como herramientas para guiar su investigación. Cuando se hace una pregunta, la pregunta centra la atención del investigador en una parte específica del conjunto de datos y le ayuda decidir qué gráficos, modelos o transformaciones realizar.EDA es fundamentalmente un proceso creativo. Como la mayoría de los procesos creativos, la clave para hacer preguntas de calidad es generar una gran cantidad de preguntas. Es difícil hacer preguntas reveladoras al comienzo del análisis porque el investigador sabe qué información contiene su conjunto de datos. Por otro lado, cada nueva pregunta que haga le expondrá un nuevo aspecto de sus datos y aumentará sus posibilidades de hacer un descubrimiento. Se puede profundizar rápidamente en las partes más interesantes de los datos experimentales recogidos y desarrollar una serie de preguntas que invitan la reflexión, si se realiza un seguimiento de cada pregunta con una nueva pregunta basada en lo que se encuentre.hay una regla sobre qué preguntas se deben hacer para guiar la investigación, ya que debe ser el investigador en función de los objetivos del experimento planteado el que desarrolle dichas preguntas. Sin embargo, dos tipos de preguntas siempre serán útiles para hacer descubrimientos dentro de los datos. Estas preguntas son:¿Qué tipo de variabilidad ocurre dentro de las variables recogidas?¿Qué tipo de covariación (o variabilidad conjunta entre dos o más variables) ocurre entre las variables recogidas?","code":""},{"path":"aed.html","id":"variabilidad","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.2 Variabilidad","text":"La variabilidad es la tendencia de los valores de una variable cambiar de medición medición. Dicha variabilidad se parecía claramente en la vida real; si se mide cualquier variable continua dos veces, se obtendrán dos resultados diferentes. Esto es cierto incluso si se miden cantidades que son constantes, como la velocidad de la luz. Cada una de sus medidas incluirá una pequeña cantidad de error que varía de una medida otra. Las variables categóricas también pueden variar si se miden diferentes sujetos (por ejemplo, los colores de los ojos de diferentes personas) o en diferentes momentos (por ejemplo, los niveles de energía de un electrón en diferentes momentos). Cada variable tiene su propio patrón de variación, que puede revelar información interesante. La mejor manera de entender ese patrón es visualizar la distribución de los valores de la variable mediante descriptores numéricos o gráficos.Si la variación describe el comportamiento dentro de una variable, la covariación describe el comportamiento entre las variables. La covariación es la tendencia de los valores de dos o más variables variar juntas de una manera relacionada. La mejor forma de detectar la covariación es visualizar la relación entre dos o más variables. Cómo hacer eso nuevamente debería depender del tipo de variables involucradas.","code":""},{"path":"aed.html","id":"procesado-inicial","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3 Procesado inicial","text":"La descripción numérica y la visualización gráfica son las herramientas más importantes en los pasos iniciales para la generación de conocimiento sobre los datos experimentales, pero en ocasiones los datos son recogidos en la forma más efectiva para realizar dichos análisis. menudo se necesitara crear algunas variables o resúmenes nuevos, o tal vez solo se quiera cambiar el nombre de las variables o reordenar las observaciones para facilitar el trabajo de los datos. En este tema aprenderemos cómo hacer todo eso. Para ejemplificar los procedimientos utilizaremos el conjunto de datos flights contenido en la librería nycflights13, que contiene toda la información sobre los vuelos que salieron desde la ciudad de Nueva York en 2013.Las variables que contiene este banco de datos (336776 observaciones = vuelos) son:year: Fecha de salida (año).month: Fecha de salida (mes).year,month,day: Fecha de salida (día).dep_time: Hora real de salida.arr_time: Hora real de llegada (en horario de la ciudad de llegada).sched_dep_time: Hora programada de salida. (Esta variable debe coincidir con la información de las variables hour y minute)sched_arr_time: Hora programada de llegada (en horario de la ciudad de llegada).dep_delay: Demora de salida (en minutos). Los tiempos negativos representan salidas tempranas.arr_delay: Demora de llegada (en minutos). Los tiempos negativos representan llegadas tempranas.hour: Hora de partida programada.minute: Minuto de partida programada.carrier: Aerolínea encargada del vuelotailnum: Identificador del aviónflight: Identificador del vueloorigin: Origen del vuelodest: Destino del vueloair_time: Tiempo de vuelo (en minutos)distance: Distancia entre los dos aeropuertos (en millas)time_hour: Fecha y hora programadas del vuelo como una fecha POSIXct. Junto con el origen, se puede usar para unir datos de vuelos datos meteorológicos.Recuerda que debes instalar dicha librería antes de poder reproducir todo los procedimientos que mostramos en las secciones siguientes.Instalamos la librería (junto con todas las necesarias para la asignatura) y cargamos los datos para poder visualizarlos:Código para cargar y visualizar los datosTambien podemos ver la estructura (tipo de variables) del banco de datos. Los tipos de variables que se admiten en R son: int para enteros, dbl para números reales, chr para vectores de caracteres o cadenas, dttm para fechas-tiempos (una fecha + una hora), lgl para vectores lógicos que solo contienen VERDADERO o FALSO, fctr para factores (que R usa para representar variables categóricas con valores posibles fijos), y date para fechas.","code":"\nlibrary(nycflights13)\n# Carga de datos\ndata(flights)\n# Visualización de los 10 primeros casos\nhead(flights,10)\n# Visualización de la estructura del banco de datos\nstr(flights)## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ..."},{"path":"aed.html","id":"operaciones-con-sujetos","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.1 Operaciones con sujetos","text":"Los procedimientos para el trabajo con los sujetos de nuestra muestra se reducen al filtrado u ordenación, para quedarnos con un subconjunto de sujetos o para organizar su visualización en otra forma.","code":""},{"path":"aed.html","id":"filtrado","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.1.1 Filtrado","text":"Usamos el filtrado para seleccionar un subconjunto de observaciones del data.frame que contiene nuestros datos. Esto se hace menudo cuando queremos limitar un análisis un subconjunto de observaciones. El uso básico del filtro se hace mediante la función filter():donde data_set es el nombre del objeto que contiene nuestros datos y <expression1>, <expression2>,...son uno o más argumentos adicionales, donde cada uno de estos es una expresión de R válida que implica una o más condiciones aplicar sobre las variables del conjunto de datos. Cada expresión se interpreta como una condición lógica (verdadero o falso).Para usar el filtrado de manera efectiva, se debe saber cómo seleccionar las observaciones que se desea utilizando los operadores de comparación. R proporciona el paquete estándar: > (mayor que), >= (mayor o igual que), < (menor que), <= (menor o igual que), != (igual ), y == (igual ).Veamos diferentes posibilidades de filtrado sobre el banco de datos flights. En primer lugar filtramos todos los vuelos cuya día de origen sea el 1 de enero de 2013. El resultado es un conjunto de datos con 842 observaciones donde aparece la información de dichos vuelos. Almacenamos el resultado eb nuevo objeto y calculamos el tamaño (nñumero de sujetos) con la función dim.Podemos combinar diferentes condiciones de filtrado mediante los operadores lógicos & es “y” (condición 1 y condición 2), | es “o” (condición 1 o condición 2). Seleccionamos ahora todos los vuelos con mes de origen igual Noviembre o Diciembre.Los datos filtrados contienen la información completa de 55403 vuelos. Otra forma de conseguir el mismo resultado es con el operador %%veces se pueden simplificar condiciones de filtrado más complicadas sin más que recordar la ley de De Morgan: !(x & y) es lo mismo que!x | !y,  y!(x | y)es lo mismo que!x & !y`. Por ejemplo, si se desean obtener todos vuelos que se retrasaron (en llegada o partida) en más de dos horas, se pueden usar cualquiera de los dos filtros siguientes:","code":"filter(data_set, <expression1>, <expression2>, ...)\njan1 <- dplyr::filter(flights, month == 1, day == 1)\ndim(jan1)## [1] 842  19\nnov_dec <- dplyr::filter(flights, month == 11 | month == 12)\ndim(nov_dec) # Para saber cuantas observaciones contiene el banco de datos filtrado ## [1] 55403    19\nnov_dec <- dplyr::filter(flights, month %in% c(11, 12))\ndim(nov_dec)## [1] 55403    19\ndb_sel1 <- dplyr::filter(flights, !(arr_delay > 120 | dep_delay > 120))\ndim(db_sel1)## [1] 316050     19\ndb_sel2 <- dplyr::filter(flights, arr_delay <= 120, dep_delay <= 120)\ndim(db_sel2)## [1] 316050     19"},{"path":"aed.html","id":"ordenación","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.1.2 Ordenación","text":"Otro procesamiento muy habitual con los sujetos es reordenar las filas de un objeto que contiene nuestros datos. Esto se usa cuando queremos inspeccionar un conjunto de datos para buscar asociaciones entre las diferentes variables, lo que resulta difícil de hacer si están ordenados. Para realizar la ordenación se utiliza la función arrange(). El uso básico de la función es:donde data_set es el nombre del objeto que contiene nuestros datos y varname1, varname2,... son las variables que vamos utilizar para la ordenación. Por ejemplo deseamos ordenar nuestro datos siguiendo el orden año, mes y día:Podemos introducir la función desc() para ordenar de forma descendente por la variable seleccionada. Ordenamos nuestros datos (de mayor menor) por la demora en el tiempo de llegada:En el resumen de los datos presentados se pueden apreciar las diferencia entre los datos ordenados de una u otra forma.","code":"\narrange(data_set, varname1, varname2, ...)\ndbf_ord1 <- dplyr::arrange(flights, year, month, day)\nstr(dbf_ord1)## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\ndbf_ord2 <- dplyr::arrange(flights, desc(arr_delay))\nstr(dbf_ord2)## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 6 1 9 7 4 3 7 12 5 ...\n##  $ day           : int [1:336776] 9 15 10 20 22 10 17 22 5 3 ...\n##  $ dep_time      : int [1:336776] 641 1432 1121 1139 845 1100 2321 2257 756 1133 ...\n##  $ sched_dep_time: int [1:336776] 900 1935 1635 1845 1600 1900 810 759 1700 2055 ...\n##  $ dep_delay     : num [1:336776] 1301 1137 1126 1014 1005 ...\n##  $ arr_time      : int [1:336776] 1242 1607 1239 1457 1044 1342 135 121 1058 1250 ...\n##  $ sched_arr_time: int [1:336776] 1530 2120 1810 2210 1815 2211 1020 1026 2020 2215 ...\n##  $ arr_delay     : num [1:336776] 1272 1127 1109 1007 989 ...\n##  $ carrier       : chr [1:336776] \"HA\" \"MQ\" \"MQ\" \"AA\" ...\n##  $ flight        : int [1:336776] 51 3535 3695 177 3075 2391 2119 2047 172 3744 ...\n##  $ tailnum       : chr [1:336776] \"N384HA\" \"N504MQ\" \"N517MQ\" \"N338AA\" ...\n##  $ origin        : chr [1:336776] \"JFK\" \"JFK\" \"EWR\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"HNL\" \"CMH\" \"ORD\" \"SFO\" ...\n##  $ air_time      : num [1:336776] 640 74 111 354 96 139 167 109 149 112 ...\n##  $ distance      : num [1:336776] 4983 483 719 2586 589 ...\n##  $ hour          : num [1:336776] 9 19 16 18 16 19 8 7 17 20 ...\n##  $ minute        : num [1:336776] 0 35 35 45 0 0 10 59 0 55 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-09 09:00:00\" \"2013-06-15 19:00:00\" \"2013-01-10 16:00:00\" ..."},{"path":"aed.html","id":"trabajando-con-variables","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.2 Trabajando con variables","text":"Los procedimientos para el trabajo con las variables de nuestra muestra se reducen la selección de un subconjunto de variables, la creación de nuevas variables, el renombrado de variables, y la recodificación en nuevas variables.","code":""},{"path":"aed.html","id":"selección","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.2.1 Selección","text":"Usamos la función select() para seleccionar un subconjunto de variables de nuestro banco de datos. Esto función se usa cuando tenemos un conjunto de datos con muchas variables, pero solo necesitamos trabajar con un subconjunto de ellas. La función tiene la estructura:El primer argumento, data_set, es el nombre del objeto que contiene nuestros datos. continuación incluimos una serie de uno o más argumentos adicionales, donde cada uno es el nombre de una o más variables en el conjunto de datos. Estas son las variables que aparecerán en el nuevo banco de datos.Para el conjunto de datos flights vamos seleccionar las variables year, month, y day.veces es más conveniente establecer la selección especificando aquellas que necesitamos, en lugar de especificar cuáles guardar. Usamos el operador - para indicar que variables deben ser eliminadas.Cuando las variables que deseamos eliminar se muestran de forma consecutiva en nuestro banco de datos podemos utilizar una expresión equivalente","code":"\nselect(data_set, varname1, varname2, ...)\ndbf_sel1 <- dplyr::select(flights, year, month, day)\n# indicamos la libreria por la coincidencia de la función select()\n# en otra libreria cargada\nstr(dbf_sel1) ## tibble [336,776 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month: int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day  : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\ndbf_sel2 <- dplyr::select(flights, -(year:day)) \n# No seleccionamos las varaibles que se encuentran entre las variables year y day\nstr(dbf_sel2)## tibble [336,776 × 16] (S3: tbl_df/tbl/data.frame)\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\ndbf_sel3 <- dplyr::select(flights, -c(year,month,day)) \nstr(dbf_sel3)## tibble [336,776 × 16] (S3: tbl_df/tbl/data.frame)\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ..."},{"path":"aed.html","id":"creación","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.2.2 Creación","text":"En la creación de variables partir de las originales en nuestros datos tenemos dos opciones: ) crear una nueva variable sin eliminar las variables originales, ii) crear una nueva variable eliminando las variables originales.Usamos la función mutate() para crear nuevas variables en nuestro banco de datos sin eliminar las variables que forman parte de la nueva variable. La función tiene la estructura:El primer argumento, data_set, es el nombre del objeto que contiene nuestros datos. continuación incluimos una serie de uno o más argumentos adicionales, donde cada uno es la expresión para la nueva o nuevas variables.Veamos un ejemplo de uso de la funciónPodemos ver que se han añadido las dos variables en el nuevo banco de datos que se ha creado. Una ventaja de esta función es que resulta posible crear nuevas variables partir de las nuevas creadasUsamos la función transmute() para crear un banco de datos donde solo aparecen las nuevas variables creadas. La estructura de la función es idéntica la de la función mutate().El listado de funciones que podemos usar con las funciones mutate() y transmute() son:Operadores aritméticos: +, -, *, /, ^Funciones logaritmo: log(), log2(), log10()Funciones de agregación: cumsum() (suma acumulada), cumprod() (producto acumulado), cummin() (mínimo acumulado), cummax() (máximo acumulado), cummean() (media acumulada).Comparaciones lógicas: <, <=, >, >=, !=","code":"mutate(data_set, <expression1>, <expression2>, ...)\ndbf_sel4 <- dplyr::select(flights, c(year,month,day,dep_delay,arr_delay,distance,air_time)) \n# Seleccionamos un subconjunto de las variables originales\nstr(dbf_sel4)## tibble [336,776 × 7] (S3: tbl_df/tbl/data.frame)\n##  $ year     : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month    : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day      : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_delay: num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_delay: num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ distance : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ air_time : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n# Creamos una varaible que indica la reducción de demora entre salida y llegada\n# Calculamos la velocidad del viaje\ndbf_new <- dplyr::mutate(dbf_sel4,\n  gain = arr_delay - dep_delay,\n  speed = distance / air_time * 60\n) \nstr(dbf_new)## tibble [336,776 × 9] (S3: tbl_df/tbl/data.frame)\n##  $ year     : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month    : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day      : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_delay: num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_delay: num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ distance : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ air_time : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ gain     : num [1:336776] 9 16 31 -17 -19 16 24 -11 -5 10 ...\n##  $ speed    : num [1:336776] 370 374 408 517 394 ...\ndbf_new2 <- dplyr::mutate(dbf_sel4,\n  gain = arr_delay - dep_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n) \nstr(dbf_new2)## tibble [336,776 × 10] (S3: tbl_df/tbl/data.frame)\n##  $ year         : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month        : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day          : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_delay    : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_delay    : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ distance     : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ air_time     : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ gain         : num [1:336776] 9 16 31 -17 -19 16 24 -11 -5 10 ...\n##  $ hours        : num [1:336776] 3.78 3.78 2.67 3.05 1.93 ...\n##  $ gain_per_hour: num [1:336776] 2.38 4.23 11.62 -5.57 -9.83 ...\ndbf_new3 <- dplyr::transmute(dbf_sel4,\n  gain = arr_delay - dep_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n) \nstr(dbf_new3)## tibble [336,776 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ gain         : num [1:336776] 9 16 31 -17 -19 16 24 -11 -5 10 ...\n##  $ hours        : num [1:336776] 3.78 3.78 2.67 3.05 1.93 ...\n##  $ gain_per_hour: num [1:336776] 2.38 4.23 11.62 -5.57 -9.83 ..."},{"path":"aed.html","id":"creación-de-factores","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.2.3 Creación de factores","text":"La creación de variables tipo factor es un aspecto muy importante en el análisis de datos. Existen tres formas principales de conseguir variables de tipo factor:partir de variables tipo carácterA partir de variables de tipo entero que pueden identificar niveles de una variableA partir de una variable de tipo numérico.Por el momento solo mostraremos las opciones 1 y 2. La función utilizada para estas operaciones es fct_recode() cuya estructura viene dada por:donde varfactor es la variable factor original, levelnew son los niveles del factor recodificados y levelold son los niveles del factor en la variable original.Vamos ver un ejemplo de su uso sobre el banco de datos NCBIRTH800 que presentamos en la unidad anterior. Cargamos los datos desde el repositorio y vemos su estructura:En esta base de datos hay varias variables que se han recogido como carácter (aunque se les ha asignado un código numérico). continuación se presentan dichas variables así como la asignación de valor cada uno de los códigos numéricos:Realizamos la asignación de los valoresEn el resultado se aprecia la modificación de los diferentes factores.","code":"\nfct_recode(varfactor, levelnew1=levelold1, levelnew2=levelold2, ...)\nNCBIRTH800 = readr::read_csv(\"https://goo.gl/mB9Jcn\", col_types = \"dcddcccdccddcc\")sex: \"male\" = 1,\"female\" = 2\nmarital: \"married\" = 1,\"not married\" = 2\nracemom: \"other non white\" = 0, \"White\" = 1,\"Black\" = 2,\n         \"America indian\" = 3,\"Chinese\" = 4,\"Hawaiian\" = 5,\n         \"Filipino\" = 6,\"Other asian\" = 7, \"Other\" = 8\nhispmom: \"Cuban\" = C, \"Mexican\" = M, \"Non-Hispanic\" = N ,\n         \"Other\" = O,\"Puerto Rican\" = P, \"Central/South american\" = S,\n         \"U\" = Not classificable\nsmoke: \"Yes\"=1, \"No\" = 0\ndrink: \"Yes\" = 1 ,\"No\" = 0\nlow: \"Yes\" = 1, \"No\" = 0\npremie: \"Yes\"= 1, \"No\" = 0\nNCBIRTHnew <- dplyr::mutate(NCBIRTH800,      \n              sex = forcats::fct_recode(sex,\"male\" = \"1\",\"female\" = \"2\"),\n              marital = forcats::fct_recode(marital,\"married\" = \"1\",\"not married\" = \"2\"),\n              racemom = forcats::fct_recode(racemom,\"other non white\" = \"0\",\"White\" = \"1\",\n                                   \"Black\" = \"2\",\"America  indian\" = \"3\",\n                                   \"Chinese\" = \"4\",\"Hawaiian\" = \"5\",\"Filipino\" = \"6\",\n                                   \"Other asian\" = \"7\",\"Other\" = \"8\"),\n              hispmom = forcats::fct_recode(hispmom,\"Cuban\" = \"C\",\"Mexican\" = \"M\",\n                                   \"Non-Hispanic\" = \"N\",\"Other\" = \"O\",\n                                   \"Puerto Rican\" = \"P\",\"Central/South american\" = \"S\",\n                                   \"U\" = \"Not classificable\"),\n              smoke = forcats::fct_recode(smoke,\"Yes\" = \"1\",\"No\" = \"0\"),\n              drink = forcats::fct_recode(drink,\"Yes\" = \"1\",\"No\" = \"0\"),\n              low = forcats::fct_recode(low,\"Yes\" = \"1\",\"No\" = \"0\"),\n              premie = forcats::fct_recode(premie,\"Yes\" = \"1\",\"No\" = \"0\"))\nstr(NCBIRTHnew)## spec_tbl_df [800 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ plural : num [1:800] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ sex    : Factor w/ 2 levels \"male\",\"female\": 1 2 1 1 1 1 2 2 2 2 ...\n##  $ mage   : num [1:800] 32 32 27 27 25 28 25 15 37 21 ...\n##  $ weeks  : num [1:800] 40 37 39 39 39 43 39 42 41 39 ...\n##  $ marital: Factor w/ 2 levels \"married\",\"not married\": 1 1 1 1 1 1 1 2 1 1 ...\n##  $ racemom: Factor w/ 6 levels \"White\",\"Black\",..: 1 1 1 1 1 1 1 1 6 1 ...\n##  $ hispmom: Factor w/ 6 levels \"Cuban\",\"Mexican\",..: 3 3 3 3 3 3 3 3 3 3 ...\n##  $ gained : num [1:800] 38 34 12 15 32 32 75 25 31 28 ...\n##  $ smoke  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ drink  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ tounces: num [1:800] 111 116 138 136 121 117 143 113 139 120 ...\n##  $ tgrams : num [1:800] 3147 3289 3912 3856 3430 ...\n##  $ low    : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ premie : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   plural = col_double(),\n##   ..   sex = col_character(),\n##   ..   mage = col_double(),\n##   ..   weeks = col_double(),\n##   ..   marital = col_character(),\n##   ..   racemom = col_character(),\n##   ..   hispmom = col_character(),\n##   ..   gained = col_double(),\n##   ..   smoke = col_character(),\n##   ..   drink = col_character(),\n##   ..   tounces = col_double(),\n##   ..   tgrams = col_double(),\n##   ..   low = col_character(),\n##   ..   premie = col_character()\n##   .. )\n##  - attr(*, \"problems\")=<externalptr>"},{"path":"aed.html","id":"reuniendo-los-datos","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.3.3 Reuniendo los datos","text":"Un problema bastante común que parece en la recogida de datos experimentales es que los nombres de las columnas de la base de datos es una variable en si, sino los valores de una variable. Si tomamos como ejemplo el banco de datos PCKDATA.Como se puede ver aparecen dos columnas indicando el tratamiento del sujeto, es decir, cada fila identifica un único sujeto. Por ese motivo en cada una de ellas aparece el valor del nivel de creatinina. Para poder trabajar de forma óptima cada fila debe contener la información única de cada sujeto. En este caso cada fila contiene la información de dos sujetos: el identificado como 1 en el tratamiento y el identificado como 1 en el tratamiento B.La función gather()nos permite reorganizar los datos de un banco de datos de forma muy sencilla. La estructura básica de la función es:donde data_set es el banco de datos, var es el conjunto de variables que reorganizamos, key es el nombre de la variable donde reorganizamos las variables anteriores, y value es el nombre de la variable donde almacenamos los valores de respuesta. Para el conjunto de datos anterior tenemos:Podemos ver que se han reorganizado las filas de la base de datos. De hecho se han duplicado el número de filas (hemos ampliado dos variables) y ahora cada columna identifica claramente la información de un sujeto.","code":"\nPCKDATA = readr::read_csv(\"https://goo.gl/W8Bfgv\", col_types = \"idd\")\nPCKDATA## # A tibble: 1,005 × 3\n##     SUBJ     A     B\n##    <int> <dbl> <dbl>\n##  1     1   193   250\n##  2     2    90   173\n##  3     3   120   135\n##  4     4   154    49\n##  5     5   149    83\n##  6     6   146   123\n##  7     7   180   126\n##  8     8   128   177\n##  9     9   180   164\n## 10    10    66   121\n## # … with 995 more rows\ngather(data_set,var, key = \"key\", value = \"value\", ...)\ngather(PCKDATA,`A`,`B`, key = \"Grupo\", value = creatine)## # A tibble: 2,010 × 3\n##     SUBJ Grupo creatine\n##    <int> <chr>    <dbl>\n##  1     1 A          193\n##  2     2 A           90\n##  3     3 A          120\n##  4     4 A          154\n##  5     5 A          149\n##  6     6 A          146\n##  7     7 A          180\n##  8     8 A          128\n##  9     9 A          180\n## 10    10 A           66\n## # … with 2,000 more rows"},{"path":"aed.html","id":"análisis-descriptivo-inicial","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4 Análisis Descriptivo inicial","text":"Esta sección muestra los procedimientos de análisis estadístico descriptivo para el estudio de una o dos variables de tipo numérico y/o categórico. Las situaciones que se plantean son:Una variable de tipo factor.Una variable de tipo numérico.Dos variables categóricas.Dos variables numéricas.Una variable categórica y una variable numérica.Este es el primer paso del EDA y sirve al investigador para plantear o responder las primeras preguntas de interés sobre sus datos. Para ilustrar los procedimientos utilizaremos el conjunto de datos storms de la librería nasaweather. Estos datos son un subconjunto de la base de datos de la NASA sobre los huracanes en el Atlántico Norte (NOAA). Los datos incluyen las posiciones y atributos de 198 tormentas tropicales, medidas cada seis horas durante la vida de la tormenta. Las variables registradas son:name: Nombre de la tormentayear, month, day: Año, mes y día del informe de la tormentahour: Hora del informe (en UTC)lat, long: Latitud y longitud de la tormentapressure: Presión atmosférica en el centro de la tormenta (en milibares)wind: Máxima velocidad sostenida de la tormenta (en nudos)type: Clasificación de la tormenta (Tropical Depression, Tropical Storm, Hurricane)seasday: día de la temporada de tormentasDado que las variables year y month vienen definidas como factores, el primer paso es convertirlas en factores. En este caso vamos utilizar una versión diferente de la vista en el tema anterior para convertir varaibles enteras factores.lo largo de cada una de las secciones siguientes se irán introduciendo las funciones y procedimientos necesarias para cada análisis en función de la variable o variables que se desean analizar.","code":"\nlibrary(nasaweather)\n# Guardamos los datos en un nuevo objeto\nstorm <- nasaweather::storms \n# Estructura de los datos\nstr(storm)\n# Primero creamos el factor año\nstorm$year_f <- factor(storm$year)\n# Asignamos los niveles\nlevels(storm$year_f) <- as.character(1995:2000)\n# Ahora el factor mes\nstorm$month_f <- factor(storm$month)\n# Asignamos los niveles\nlevels(storm$month_f) <- c(\"June\", \"July\", \"August\", \"September\", \n                           \"October\", \"November\", \"December\")\n# Veamos como queda el banco de datos\nstr(storm)## tibble [2,747 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ name    : chr [1:2747] \"Allison\" \"Allison\" \"Allison\" \"Allison\" ...\n##  $ year    : int [1:2747] 1995 1995 1995 1995 1995 1995 1995 1995 1995 1995 ...\n##  $ month   : int [1:2747] 6 6 6 6 6 6 6 6 6 6 ...\n##  $ day     : int [1:2747] 3 3 3 3 4 4 4 4 5 5 ...\n##  $ hour    : int [1:2747] 0 6 12 18 0 6 12 18 0 6 ...\n##  $ lat     : num [1:2747] 17.4 18.3 19.3 20.6 22 23.3 24.7 26.2 27.6 28.5 ...\n##  $ long    : num [1:2747] -84.3 -84.9 -85.7 -85.8 -86 -86.3 -86.2 -86.2 -86.1 -85.6 ...\n##  $ pressure: int [1:2747] 1005 1004 1003 1001 997 995 987 988 988 990 ...\n##  $ wind    : int [1:2747] 30 30 35 40 50 60 65 65 65 60 ...\n##  $ type    : chr [1:2747] \"Tropical Depression\" \"Tropical Depression\" \"Tropical Storm\" \"Tropical Storm\" ...\n##  $ seasday : int [1:2747] 3 3 3 3 4 4 4 4 5 5 ...\n##  $ year_f  : Factor w/ 6 levels \"1995\",\"1996\",..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ month_f : Factor w/ 7 levels \"June\",\"July\",..: 1 1 1 1 1 1 1 1 1 1 ..."},{"path":"aed.html","id":"una-variable-factor","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.1 Una variable factor","text":"En esta sección se considera cómo explorar la distribución de una variable categórica. Se presentan las descriptivas básicas y visualizaciones que son apropiadas para las variables categóricas. Para ejemplificar los procedimientos utilizaremos la variable type que identifica el tipo de tormenta.Explorar variables categóricas es generalmente más simple que trabajar con variables numéricas porque tenemos menos respuestas posibles. La primera pregunta que nos debemos plantear es la variables categórica es de tipo nominal u ordinal. Esto tiene un efecto relevante la hora de presentar y visualizar la información. La variable type es una variable categórica de tipo ordinal debido que las tormentas son clasificadas según su virulencia.","code":""},{"path":"aed.html","id":"resúmenes-numéricos","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.1.1 Resúmenes numéricos","text":"Cuando calculamos resúmenes de variables categóricas, intentamos describir la distribución muestral de la variable mediante el recuento de ocurrencias de cada una de las posibles respuestas de la variable. Estos recuentos es lo que denominamos en estadística frecuencias absolutas. Necesitamos entender qué categorías son comunes y cuáles son raras. Asociado dichas frecuencias absolutas podemos obtener las frecuencias relativas, o más frecuentemente los porcentajes de cada una de las posibles categorías de la variable.Si \\(F_i\\) es la frecuencia absoluta de la categoría \\(\\) de la variable y \\(n\\) es el tamaño de la muestra, se define la frecuencia relativa de la categoría \\(\\) (\\(f_i\\)) como:El porcentaje de la categoría \\(\\) es simplemente \\(100 * f_i\\). La forma más sencilla para obtener la tabla de frecuencias absoluta asociada con una variable categórica es la función table().Para obtener los porcentajes en lugar de los conteos podemos utilizar le código siguienteSin embargo, hay otra opción un poco más compleja pero que nos servirá de utilidad para el resto de esta unidad y que pasamos mostrar continuación. Es una combinación del operador de anidamiento %>%, de la función de agrupación (group_by()), de la función de resumen (summarise()), y de la función de conteos (n()).Aunque esta forma es más costosa cuando tenemos únicamente una variable nos resultará de más utilidad cuando deseemos realizar análisis que involucren un mayor número de variables.Podemos ver que mayoritariamente se han producido Tormentas Tropicales (33.71%) y Huracanes (32.62%). Estos dos tipos suman más de dos tercios de los ecentos registrados. Por contra los eventos menos abundantes han sido las Depresiones Tropicales (18.67%) y las Extratropicales (15.00%).Las funciones más habituales que se utilizan con summarise() son:Localización: mean() (media), median() (mediana)Escala: sd() (desviación típica), IQR() (rango intercuartílico)Rango: min() (mínimo), max() (máximo), quantile() (cuantil)Posición: first() (primero), last() (último), nth() (posición n-ésima)Conteo: n() (número de casos), n_distinct() (número de casos distintos)La mayoría se usan exclusivamente para variables de tipo numérico.","code":"\ntable(storm$type)## \n##       Extratropical           Hurricane Tropical Depression      Tropical Storm \n##                 412                 896                 513                 926\ntype_counts <- table(storm$type)\nround(type_counts / sum(type_counts),2) # Redondeamos a dos decimales## \n##       Extratropical           Hurricane Tropical Depression      Tropical Storm \n##                0.15                0.33                0.19                0.34\n# banco de datos\ntabla_tipo <- storm %>%  # agrupamos por la variable factor\n  group_by(type) %>%     # resumimos contando el número de casos de cada nivel del factor\n  summarise(n=n())       # Para calcular los porcentajes\ndplyr::mutate(tabla_tipo,percent=round(100*n/sum(n),2))## # A tibble: 4 × 3\n##   type                    n percent\n##   <chr>               <int>   <dbl>\n## 1 Extratropical         412    15  \n## 2 Hurricane             896    32.6\n## 3 Tropical Depression   513    18.7\n## 4 Tropical Storm        926    33.7"},{"path":"aed.html","id":"visualización-gráfica","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.1.2 Visualización gráfica","text":"En este apartado veremos como representar los datos de los conteos de una variable categórica mediante la función ’ggplot`. Esta función permite realizar casi cualquier tipo de gráfico que podamos imaginar. En este punto vamos ir presentando diferentes parámetros de dicha función para ir familiarizándonos con su uso.","code":""},{"path":"aed.html","id":"gráfico-barras","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.1.2.1 Gráfico barras","text":"La herramienta gráfica más común utilizada para resumir una variable categórica es un gráfico de barras. Un gráfico de barras (o gráfico de barras) es un gráfico que presenta resúmenes de datos agrupados con barras rectangulares. La longitud de las barras es proporcional los valores que representan. Al resumir una sola variable categórica, la longitud de las barras debe mostrar los recuentos brutos o las proporciones de cada categoría.Para realizar este gráfico con la función ggplotnecesitamos identificar el conjunto de datos sobre el que vamos trabajar y la variable que queremos representar:\nFigura 2.1: Gráfico de barras del tipo de tormenta.\nPodemos personalizar este gráfico de barras si es necesario con funciones como xlab y ylab, y configurando varias propiedades dentro de geom_bar. Por ejemplo:\nFigura 2.2: Gráfico de barras del tipo de tormenta (versión 2).\nComo podemos ver tanto en las tablas obtenidas como en los dos gráficos precedentes la escala del tipo de tormenta está ordenada, es decir, la tenemos graduada por la relevancia de la tormenta. Veamos como podemos hacer esto e integrarlo en el gráfico:\nFigura 2.3: Gráfico de barras del tipo de tormenta (versión 3).\nAhora el gráfico si está ordenado con la escala adecuada y resulta más fácil cuantificar la relevancia de las tormentas más importantes. También podemos intercambiar las filas por las columnas para una mejor visualización de las etiquetas de la variable categórica. Para ello utilizamos el parámetro coord_flip():\nFigura 2.4: Gráfico de barras del tipo de tormenta (versión 4).\nPor último utilizamos la función theme_bw() para configurar un fondo blanco para el gráfico. Otras posibilidades para los temas son theme_classic(), theme_dark(), theme_grey(), theme_light().\nFigura 2.5: Gráfico de barras del tipo de tormenta (versión 5).\nEn lugar de representar los contesos podemos visualizar los porcentajes asociados cada categoría en lugar de los conteos haciendo uso de la variable ..prop.. en la configuración de geom_bar(). Se debe modificar la escala de la varaible para indicar que estamos representando porcentajes (labels = scales::percent):\nFigura 2.6: Gráfico de barras del porcentaje de cada tipo de tormenta.\n","code":"\n# Configuramos el gráfico identificando los datos y la variable de interés\nbar_plt <- ggplot(storm, aes(x = type)) \nbar_plt <- bar_plt + geom_bar() # Seleccionamos el tipo gráfico\nbar_plt                         # Representamos el gráfico\n# Retocamos las barras para que aparezcan en azul y con un ancho inferior\n# En este caso no almacenamos el gráfico sino que lo ejecutamos directamente\nggplot(storm, aes(x = type)) + \n  geom_bar(fill = \"blue\", width = 0.7) + \n  xlab(\"Tipo de Tormenta\") + ylab(\"Número de observaciones\")\n# Creamos un vector con el orden predefinido\nords <- c(\"Tropical Depression\", \"Extratropical\", \"Tropical Storm\", \"Hurricane\")\n# Generamos el gráfico indica que el eje x tiene escala dada por el vector ordenado\nggplot(storm, aes(x = type)) + \n  geom_bar(fill = \"blue\", width = 0.7) + \n  scale_x_discrete(limits = ords) +\n  xlab(\"Tipo de Tormenta\") + ylab(\"Número de observaciones\")\nggplot(storm, aes(x = type)) + \n  geom_bar(fill = \"blue\", width = 0.7) + \n  scale_x_discrete(limits = ords) +\n  coord_flip() + \n  xlab(\"Tipo de Tormenta\") + ylab(\"Número de observaciones\")\nggplot(storm, aes(x = type)) + \n  geom_bar(fill = \"blue\", width = 0.7) + \n  scale_x_discrete(limits = ords) +\n  coord_flip() + \n  xlab(\"Tipo de Tormenta\") + ylab(\"Número de observaciones\")+\n  theme_bw() \nggplot(storm, aes(x = type)) + \n  geom_bar(aes(y = ..prop.. , group = 1),fill = \"blue\", width = 0.7) + \n  scale_y_continuous(labels = scales::percent) +\n  coord_flip() + \n  xlab(\"Tipo de Tormenta\") + ylab(\"Porcentaje\")+\n  theme_bw() "},{"path":"aed.html","id":"una-variable-numérica","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.2 Una variable numérica","text":"En esta sección se considera cómo explorar la distribución de una variable numérica Se presentan las descriptivas básicas y visualizaciones que son apropiadas para las variables de este tipo. Para ejemplificar los procedimientos utilizaremos las variables wind y pressure.","code":""},{"path":"aed.html","id":"resúmenes-numéricos-1","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.2.1 Resúmenes numéricos","text":"Hasta ahora hemos estado describiendo las propiedades de las distribuciones de muestra en términos muy generales, usando frases como “valores más comunes” y “el rango de los datos” sin decir realmente lo que queremos decir. Los estadísticos han ideado términos específicos para describir este tipo de propiedades, así como diferentes estadísticas descriptivas para cuantificarlas. Los dos que más importan son la tendencia central y la dispersión:Una medida de tendencia central describe un valor típico (‘central’) de una distribución de datos. La medida de localización más extendida es la media aritmética de una muestra. Hay muchas medidas diferentes de tendencia central, cada una con sus propios pros y contras. Entre estos, la mediana es la que se usa con mayor frecuencia en los análisis exploratorios ya que es le valor que nos divide la muestra de dos partes iguales situando el 50% de los datos cada lado de ese valor.Una medida de dispersión describe cómo se distribuye una distribución. Las medidas de dispersión cuantifican la variabilidad o dispersión de una variable con respecto al promedio de los datos. Si una distribución está más dispersa que otra, significa que, en cierto sentido, abarca una gama más amplia de valores. Lo que esto significa en la práctica depende del tipo de medida con la que estamos trabajando. Las medidas de dispersión más habituales son la varianza y su raíz cuadrada, la desviación estándar.","code":""},{"path":"aed.html","id":"medidas-de-tendencia-central","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.2.1.1 Medidas de tendencia central","text":"Hay dos estadísticos que se utilizan generalmente para describir la tendencia central de la distribución de los datos muestrales de un variable numérica. De ahora en adelante denotamos por \\(n\\) al tamaño muestral y \\(x_1, x_2,...,x_n\\) los valores muestrales de la variable que deseamos estudiar.La media muestral es la medida de tendencia muestral por excelencia. La definición matemática de la media muestral viene dada por: \\[\\begin{equation} \n  \\bar{x} = \\frac{\\sum_{=1}^n x_i}{n}\n  (\\#eq:samplemean)\n\\end{equation}\\]Para obtener la media utilizamos la función mean()Esto nos dice que la media de la velocidad del viento es de 55 mph y que la media de la presión es de 989.82 milibares. ¿Como podemos interpretar esos resultados?Una limitación de la media aritmética es que se ve afectada por la forma de la distribución de los datos. Es muy sensible los extremos de una muestra. Esta es la razón por la cual, por ejemplo, tiene mucho sentido mirar el ingreso medio de los trabajadores en un país para tener una idea de lo que gana una persona “típica”. La distribución del ingreso es muy asimétrica, y los pocos que tienen la suerte de ganar salarios muy buenos tienden cambiar la media hacia arriba y superar cualquier cosa que sea realmente “típica”. La media de la muestra también se ve fuertemente afectada por la presencia de ‘valores atípicos’ o valores extremos, es decir, valores inusualmente grandes o pequeños en una muestra.Debido que la media muestral es sensible la forma de una distribución y la presencia de valores atípicos, menudo se prefiere una segunda medida de tendencia central: la mediana de la muestra. La mediana de una muestra es el número que separa los datos en dos subgrupos (la mitad superior de la mitad inferior). Podemos calcular la mediana muestral en R con la función median():Estos resultados indican que el 50% de de los registros muestran una valor de viento inferior 50 mph. De la misma forma el 50% de los datos muestran una valor de la presión inferior 995 milibares.Otras medidas de localización son el mínimo, el máximo y los percentiles. Los percentiles son los valores que dividen en la muestra según el valor del percentil solicitado. Si solicitamos el percentil 20, se separa la muestra en dos subconjuntos dejando el 20% en un grupo y e 80% en el otro. Los percentiles más habituales son lo denominados primer y tercer cuartil que corresponden los percentiles 25 y 75 respectivamente. Para obtener el percentil asociado una variable numérica podemos hacer uso de la función quantile(). Para ver como se debe utilizar es útil consultar la ayuda de dicha función help(quantile).","code":"\nmean(storm$wind)## [1] 54.68329\nmean(storm$pressure)## [1] 989.8238\nmedian(storm$wind)## [1] 50\nmedian(storm$pressure)## [1] 995"},{"path":"aed.html","id":"medidas-de-dispersión","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.2.1.2 Medidas de dispersión","text":"Hay muchas maneras de cuantificar la dispersión de un conjunto de datos muestrales de una variable numérica. Los valores más importantes desde el punto de vista estadístico son la varianza muestral y la desviación estándar. La varianza muestral \\(s^2\\) es “la suma de las desviaciones cuadradas” (es decir, las diferencias) de cada observación con respecto la media de la muestra, dividida por el tamaño de la muestra menos uno. La desviación típica es la raíz cuadrada de la varianza muestral. Las definiciones matemáticas de ambas cantidades son: \\[\\begin{equation} \n  s^2 = \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}\n  (\\#eq:var)\n\\end{equation}\\]Las funciones de R para calcular ambas cantidades son var() para la varianza y sd() para la desviación típica.¿Qué significa ese número en realidad? Las variaciones son siempre negativas. Una pequeña varianza indica que las observaciones tienden ser cercanas la media (y una la otra), mientras que una alta varianza indica que las observaciones están muy dispersas. Una varianza de cero solo ocurre si todos los valores son idénticos. Sin embargo, es difícil interpretar si una varianza muestral es realmente “pequeña” o “grande” porque el cálculo involucra desviaciones al cuadrado. Por ejemplo, cambiar la escala de medición de una variable por 10 implica un cambio de 100 veces (102) en la varianza.La varianza es una cantidad importante en las estadísticas que aparece una y otra vez. Muchas herramientas estadísticas comunes usan cambios en la varianza para comparar formalmente qué tan bien diferentes modelos describen un conjunto de datos. Sin embargo, es muy difícil interpretar las variaciones, por lo que rara vez las utilizamos en el trabajo exploratorio. Para expresar la variabilidad en la misma escala de la variable original utilizamos la desviación típica. En este caso se puede observar una mayor variabilidad en la variable presión atmosférica (desviación típica de 25.84 milibares) que en la variable de viento (18.69 mph).La desviación estándar de la muestra está exenta de problemas. Al igual que la media muestral, es sensible la forma de la distribución de los datos y la presencia de valores atípicos. Una medida de dispersión más robusta para este tipo de características es el rango intercuartílico, definida como la diferencia entre el percentil 75 (tercer cuartil) y el percentil 25 (primer cuartil):Obviamente, cuanto más dispersos estén los datos, mayor será el IQR. La razón por la que preferimos usar IQR para medir la dispersión es que solo depende de los datos en el “medio” de una distribución de muestra. Esto lo hace robusto la presencia de valores atípicos. Podemos usar la función IQR() para calcular el rango intercuartílico:La última medidad de dispersión que veremos es el coeficiente de variación. Esta medida es una de las más habituales y se obtiene partir de la media y desviación típica muestral como: \\[\\begin{equation} \n CV = \\frac{s}{\\bar{x}}\n  (\\#eq:cv)\n\\end{equation}\\]Su fórmula expresa la desviación estándar como porcentaje de la media aritmética, mostrando una interpretación relativa del grado de variabilidad, independiente de la escala de la variable, diferencia de la desviación típica o estándar. De esta forma, valores bajos del coeficiente de variación expresan menor variabilidad, lo que resulta de utilidad cuando deseamos comparar la variabilidad de dos muestras independeintemente de su media. El mayor problema es que sólo puede ser usado cuando la media muestral es positiva.","code":"\nvar(storm$wind);sd(storms$wind)## [1] 668.1444## [1] 25.84849\nvar(storm$pressure);sd(storms$pressure)## [1] 349.4912## [1] 18.69468\nIQR(storm$wind)## [1] 35\nIQR(storm$pressure)## [1] 24"},{"path":"aed.html","id":"resúmenes-conjuntos","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.2.1.3 Resúmenes conjuntos","text":"Aunque podemos ir calculando cada una de las medidas de localización y dispersión vistas anteriormente, en la práctica nos resulta más útil obtenerlas todas de una vez. Existen diferentes funciones que nos permiten obterner estos análisis descriptivos. La primera de ellas es la función summary() que nos porporciona el mínimo, máximo, media, mediana y los percentiles 25 y 75. Sin embargo, nos porporciona ninguna de las medidas de varaibilidad usuales. En nuestro ejemploSe pude ver que el 25% de las observaciones muestran una valor de viento inferior 35mph y un 25% un valor de viento superior 70mph. Con respecto la presión atmosférica tenemos que el 25% de las observaciones tienen un valor inferior 980 milibares, y un 25% tienen un valor superior 1004 milibares.Otra función que nos porporciona medidas conjuntas es estat() de la librería pubh. Esta función nos porporciona el número de casos, el mínimo, el máximo, la media, la mediana, la desviación típica, y el coeficiente de variación.","code":"\nsummary(storm$wind)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   15.00   35.00   50.00   54.68   70.00  155.00\nsummary(storm$pressure)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   905.0   980.0   995.0   989.8  1004.0  1019.0\n# Etiquetamos las variables\nstorm = storm %>% \n  var_labels(wind = 'Wind speed (in knots)', \n             pressure = 'Air pressure (in mbar)')\n# Análisis descriptivos\nestat(~ wind, data = storm)##                            N Min. Max.  Mean Median    SD   CV\n## 1 Wind speed (in knots) 2747   15  155 54.68     50 25.85 0.47\nestat(~ pressure, data = storm)##                             N Min. Max.   Mean Median    SD   CV\n## 1 Air pressure (in mbar) 2747  905 1019 989.82    995 18.69 0.02"},{"path":"aed.html","id":"visualización-gráfica-1","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.2.2 Visualización gráfica","text":"En este apartado veremos como representar los datos de los conteos de una variable categórica mediante la función ’ggplot`. Esta función permite realizar casi cualquier tipo de gráfico que podamos imaginar. En este punto vamos ir presentando diferentes parámetros de dicha función para ir familiarizándonos con su uso.","code":""},{"path":"aed.html","id":"gráfico-barras-1","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.2.2.1 Gráfico barras","text":"El gráfico por excelencia para una variable de tipo numérico es el denominado histograma. El histograma es una representación mediante barras de la distribución de los datos. Para construir las barras se divide el rango de la variable numérica en un conjunto fijo de intervalos disjuntos y se contabiliza el número de datos que quedan dentro de cada uno de ellos (altura del gráfico de barras). Es un gráfico muy interesante porque representa de forma bastante precisa, ajustando el número de intervalos, la distribución del conjunto de datos pudiéndose observar su dispersión y/o asimetria.Para realizar este gráfico utilizamos parámetros específicos dentro de la función ggplot() partir de geom_histogram().\nFigura 2.7: Histograma de la presión atmosférica.\nPodemos ver que la mayoría de los datos se sitúan por encima de 915 milibares, hay una clara asimetría hacia los valores más grandes y existe una gran dispersión entre en el conjunto de valores observados.Veamos como introducir diferentes parámetros en el gráfico anterior. el más importante es el que hace referencia los intervalos asociado con el histograma (parámetro binwith)\nFigura 2.8: Histograma de la presión atmosférica (modificando binwidth).\nVeamos ahora el histograma de la variable wind\nFigura 2.9: Histograma de la velocidad del viento.\nEl otro gráfico habitual para una variable de tipo numérico es el llamado gráfico de cajas. En este gráfico se representa mediante una caja la media (linea central de la caja), el percentil 75 (línea superior de la caja), y el percentil 25 (linea inferior de la caja). También se representan el valor máximo (percentil 75 + 1.5 IQR) y mínimo (percentil 25 - 1.5 IQR), así como los caracterizados como valores extremos (punto fuera de la caja). Veamos como realizar este gráfico mediante el parámetro geom_boxplot().\nFigura 2.10: Gráfico de cajas de la presión atmosférica.\nAñadimos parámetros (título de ejes, color caja, selección de valores extremos, y fondo blanco)\nFigura 2.11: Gráfico de cajas de la presión atmosférica (versión 2).\nPodemos ver que la media se sitúa muy próxima los 1000 mbar y la gran cantidad de valores extremos en la parte baja de la distribución. Además la caja es muy estrecha indicando la poca variabilidad en los datos, lo que viene corroborado también por la proximidad de los percentiles 25 y 75.Ahora el gráfico para la variable wind\nFigura 2.12: Gráfico de cajas de la velocidad del viento.\n¿Cómo interpretamos este gráfico?","code":"\nggplot(storm, aes(x = pressure)) +\n   geom_histogram()\nggplot(storm, aes(x = pressure)) + \n  geom_histogram(binwidth = 8, fill = \"steelblue\") + \n  xlab(\"Air pressure (in mbar)\") + ylab(\"Frequency\")\nggplot(storm, aes(x = wind)) + \n  geom_histogram(binwidth = 8, fill = \"steelblue\") + \n  xlab(\"Wind speed (in knots)\") + ylab(\"Frequency\")\nggplot(storm, aes(x = factor(1),y = pressure)) +\n   geom_boxplot()\nggplot(storm, aes(x = factor(1), y = pressure)) +\n   geom_boxplot(fill = \"orange\", outlier.colour = \"red\", outlier.shape = 1) +\n   scale_x_discrete(name = \" \") + scale_y_continuous(name = \"Air pressure (in mbar)\") +\n   theme_bw()\nggplot(storm, aes(x = factor(1),y = wind)) +\n   geom_boxplot(fill = \"orange\",outlier.colour = \"red\", outlier.shape = 1) +\n   scale_x_discrete(name = \" \") + scale_y_continuous(name = \"Wind speed (in knots)\") +\n   theme_bw()"},{"path":"aed.html","id":"dos-variables-categóricas","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.3 Dos variables categóricas","text":"Explorar numéricamente las asociaciones entre pares de variables categóricas es tan simple como el caso de una variable. La pregunta general que debemos abordar es, “¿las diferentes combinaciones de categorías parecen estar sub o sobre representadas?”. Necesitamos entender qué combinaciones son comunes y cuáles son raras. Lo más simple que podemos hacer es construir una tabulación cruzada del número de ocurrencias de cada combinación de niveles de ambas variables. La tabla resultante se llama tabla de contingencia.En cuanto la representación gráfica la opción más habitual pasa por representar los conteos mediante gráficos de barras que representan de forma conjunta la información de ambas variables.Para ejemplificar los cálculos y gráficos utilizaremos las variables month_f (mes como factor) y type. En primer lugar etiquetamos las variables.","code":"\n# Etiquetamos las variables\nstorm = storm %>% \n  var_labels(month_f = 'Month', \n             type = 'Storm category')"},{"path":"aed.html","id":"resúmenes-numéricos-2","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.3.1 Resúmenes numéricos","text":"El resumen numérico habitual para este tipo de situación es la tabla de contigencia (tabla de doble entrada) que nos porporciona los conteos o coincidencias entre los niveles de cada factor. Obtener la tabla completa (frecuencias y porcentajes) puede ser una faena algo pesada utilizando las funciones habituales. En nuestro caso utilizaremos la función cross_tab de la librería pubh. Dicha función nos porpociona la tbala de doble entrada con los conteos y los porcentajes marginales por columnas. Veamos su uso en el banco de datos.Esta tabla nos permite analizar las concidencias entres los fatores estudiados (conteos), así como la relevancia de cada tipo de tormenta los largo de los meses estudiados (porcentajes marginales por columnas). Por ejemplo, podemos ver que el perido más activo de huracanes es el comprendido entre los meses de agosto octubre.En este caso los porcentajes marginales nos revelan que tipo de tormenta es más habitual dentro de cada mes de los analizados. Por ejemplo, en el mes de agosto el tipo de tormenta más habitual son los huracanes con un 40.1%, lo que representa casi la mitad de los observado durante ese mes.","code":"\ntable(storm$type, storm$month_f)##                      \n##                       June July August September October November December\n##   Extratropical         27   38     23       149     129       42        4\n##   Hurricane              3   31    300       383     152       25        2\n##   Tropical Depression   22   59    150       156      84       42        0\n##   Tropical Storm        31  123    247       259     204       61        1"},{"path":"aed.html","id":"visualización-gráfica-2","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.3.2 Visualización gráfica","text":"Los gráficos de barras se pueden usar para resumir la relación entre dos variables categóricas. La idea básica es producir una barra separada para cada combinación de categorías en las dos variables. La longitud de estas barras es proporcional los valores que representan, que son los recuentos brutos o las proporciones en cada combinación de categorías. Esta es la misma información que se muestra en una tabla de contingencia. El uso de ggplot2 para mostrar esta información es muy diferente de producir un gráfico de barras para resumir una única variable categórica.Tomamos las variables type y year_f para mostrar su funcionamiento. Ordenamos la variable type para mostrar los gráficos por orden de importancia de la tormenta. En primer lugar realizamos el gráfico de barras apiladas.\nFigura 2.13: Gráfico de barras apiladas para tipo de tormenta versus año.\nEn este caso cada tipo de tormenta tiene su propia barra, y cada barra se ha dividido en diferentes segmentos de colores, cuya longitud está determinada por el número de observaciones asociadas con cada año. Podemos apreciar si para un mismo tipo de tormenta la ocurrencia de un tipo de tormenta es similar o . Por ejemplo, para los huracanes podemos apreciar que los años 1996, 1998, 1999, y 2000 tienen un número similar de ocurrencias.Un problema con este tipo de gráfico es que puede ser difícil detectar asociaciones entre las dos variables categóricas. Si queremos saber cómo están asociados, menudo es mejor trazar los recuentos para cada combinación de categorías una al lado de la otra. Este gráfico se denomina gráfico de barras agrupado. Utilizamos la opción dodge en la función geom_bar para poder realizar esta versión:\nFigura 2.14: Gráfico de barras agrupado para el tipo de tormenta versus año.\n¿Qué ventajas o desventajas aprecias en este gráfico frente al de barras apiladas?Alternativamente podríamos realizar el gráfico de porcentahjes en lugar de los conteos. Hacemos uso de la función ..prop.. e indicamos la agrupación por la variable year_f:\nFigura 2.15: Gráfico de barras agrupado (porcentjes) para el tipo de tormenta versus año.\nOtra opción es realizar un mapa de intensidad de cada una de las combinaciones e ambos factores. Utilizamos la función geom_tile() partir los conteos conjuntos para ambas variables con la opción count() que se almacenan en la variable temporal n.\nFigura 2.16: Mapa de intensidad para el tipo de tormenta versus año.\nEn este caso las casillas en tonos más claros corresponden con las combinaciones de niveles con una mayor ocurrencia. Las más abundantes se concentran en 1995 y los tipos de tormenta más graves. Por otro lado el año 1997 es el que ha registrado menor número de tormentas.","code":"\n# Creamos un vector con el orden predefinido\nords <- c(\"Tropical Depression\", \"Extratropical\", \"Tropical Storm\", \"Hurricane\")\n# Generamos el gráfico indica que el eje x tiene escala dada por el vector ordenado\nggplot(storm, aes(x = type, fill = year_f)) + \n  geom_bar() + \n  scale_x_discrete(limits = ords) +\n  xlab(\"Storm category\") + ylab(\"Frequancy\") \nggplot(storm, aes(x = type, fill = year_f)) + \n  geom_bar(position = \"dodge\") + \n  scale_x_discrete(limits = ords) +\n  labs(x = \"Storm category\", y = \"Frequancy\", fill = \"Storm category\")\nggplot(storm, aes(x = type, fill = year_f)) + \n  geom_bar(aes(y = ..prop.. , group = year_f),position = \"dodge\") + \n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Storm category\", y = \"Percent\", fill = \"Storm category\")\nstorm %>%\n    count(type,year_f) %>%\n    ggplot(mapping = aes(x = type, y = year_f)) + \n        geom_tile(mapping = aes(fill = n)) + \n        scale_x_discrete(limits = ords) +\n        labs(x = \"Storm type\", y = \"Year\", fill = \"n\")"},{"path":"aed.html","id":"categórica-vs-numérica","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.4 Categórica vs Numérica","text":"El objetivo en este tipo de situaciones es comparar cada una de las distribuciones de la variable numérica que surgen al segmentar los valores por cada uno de los niveles de la variable categórica. Tenemos tantas conjuntos de datos como niveles del facgor hemos observado.","code":""},{"path":"aed.html","id":"resúmenes-numéricos-3","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.4.1 Resúmenes numéricos","text":"Los resúmenes numéricos se pueden construir tomando las diversas ideas que hemos explorado para las variables numéricas (medias, medianas, etc.) y aplicándolas subconjuntos de datos definidos por los valores de la variable categórica. Sin embargo, podemos hacer uso de la función estat() para simplicar este trabajo. para ejemplificar su uso utilizamos la variable type como categórica y las variables wind y pressure como numéricas.Se puede ver como el tipo Huracanes tiene la media más alta de velocidad del viento y la más baja de presión atmosférica. Además en ambos casos la variabilidad observada es la mayor de todos los tipos de tormenta. La depresión tropical es justo el caso contrario. Evidentemente los datos observados corresponden con la clasificación de tormenta establecida desde el inicio. ¿Qué otras conclusiones podemos obtener? ¿qué tipo de tormenta muestra una mayor variabilidad?","code":"\nstorm$type <- as.factor(storm$type)\n# Simplificamos las etiquetas de las variables\nstorm = storm %>% \n  var_labels(wind = 'Wind', \n             pressure = 'Air pressure')\n# Análisis descriptivos\nestat(~ wind|type, data = storm)##                       type   N Min. Max.  Mean Median    SD   CV\n## 1 Wind       Extratropical 412   15   80 40.06     40 13.25 0.33\n## 2                Hurricane 896   65  155 84.66     80 18.79 0.22\n## 3      Tropical Depression 513   20   30 27.36     30  3.52 0.13\n## 4           Tropical Storm 926   35  120 47.32     45 11.09 0.23\nestat(~ pressure|type, data = storm)##                               type   N Min. Max.    Mean Median    SD   CV\n## 1 Air pressure       Extratropical 412  950 1019  993.95    996 14.19 0.01\n## 2                        Hurricane 896  905 1005  970.44    974 16.95 0.02\n## 3              Tropical Depression 513  982 1015 1006.16   1007  3.90 0.00\n## 4                   Tropical Storm 926  935 1013  997.70   1000  8.95 0.01"},{"path":"aed.html","id":"visualización-gráfica-3","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.4.2 Visualización gráfica","text":"Para la visualización gráfica de las relaciones entre una variable categórica y una numérica tenemos diferentes opciones: gráfico de densidad, gráfico de cajas, y gráficos comparativos matriciales.El gráfico de densidad representa la distribución del conjunto de datos muestrales. Con este gráfico se puede apreciar claramente el rango de valores y su concentración (altura de la curva de densidad). Para su obtención utilizamos la opción geom_density() donde sólo debemos fijar el parámetro de suavizado, bw, que nos indica el grado de información que debemos utilizar para obtener dicha densidad. Valores pequeños dan curvas poco suavizadas y valores grandes dan curva suavizadas. Siempre es necesario un pequeño ajuste para obtener el valor más adecuado. Con este gráfico resulta muy fácil comparar el comportamiento de los diferentes grupos.Comenzamos con la variable wind:\nFigura 2.17: Gráficos de densidades de la velocidad del viento por tipo de tormenta.\nEl resultado son cuatro curvas de densidad (una por cada top de tormenta) donde los más destacable es que el rango de valores posibles para la velocidad del viento es sólo diferente para el tipo huracanes, ya que su curva de densidad se encuentra desplazada respecto del resto de densidades. También se puede ver que las depresiones tropicales tienen una menor dispersión (menor rango de valores) los que provoca que su curva sea más puntiaguda. Cuanto mayor sea la dispersión mas amplia sera la densidad obtenida. Si la densidad es simétrica la media se sitúa en el punto medio, mientras que el valor asociado con el punto más alto de la densidad es lo que denominamos moda.Ahora con la variable presión atmosférica:\nFigura 2.18: Gráficos de densidades de la presión atmosférica por tipo de tormenta.\nEl comportamiento de la variable presión atmosférica es muy similar al de la velocidad del viento pero en la parte izquierda del rango de valores. Los huracanes tienen la menor presión atmosférica y se distinguen del resto de tipos de tormenta. También muestra una mayor variabilidad que el resto de tipos.La visualización más común para explorar las relaciones entre una variable categórica y otra numérica es el diagrama de cajas. Cada diagrama de caja consiste en:Una casilla que se extiende desde el percentil 25 de la distribución hasta el percentil 75, una distancia conocida como rango intercuartílico (IQR). En el medio del recuadro hay una línea que muestra la mediana, es decir, el percentil 50 de la distribución. Estas tres líneas le dan una idea de la extensión de la distribución y si la distribución es simétrica o respecto la mediana o sesgada hacia un lado.Puntos visuales que muestran observaciones que caen más de 1,5 veces el IQR desde cualquier borde de la caja. Estos puntos remotos son inusuales, por lo que se representan de forma individual.Una línea (o bigote) que se extiende desde cada extremo de la caja y va al el punto más lejano atípico en la distribución.Veamos los ejemplos:\nFigura 2.19: Gráficos de cajas de la velocidad del viento por tipo de tormenta.\nEn este tipo de gráficos se esta interesado en dos aspecto fundamentales: * Estudiar la variabilidad dentro de cada grupo viendo la altura de la caja (IQR). * Comparar el comportamiento de cada grupo observando si las cajas quedan alturas superpuestas.En este caso podemos ver que la variabilidad más grande se produce en el tipo huracanes y la más pequeña en las depresiones tropicales. Se aprecian diferencias entre las variabilidades de los grupos. Por otro lado, las cajas correspondientes las tormentas extra tropicales y tormentas tropicales quedan una misma altura indicando que tienen valores similares para la velocidad del viento. Este gráfico nos da unas primeras indicaciones claras para los procedimientos de comparaciones de medias que estudiaremos más adelante.\nFigura 2.20: Gráficos de cajas de la presión atmosférica por tipo de tormenta.\nEn este caso observamos que los tipos extra tropical y Huracanes muestran variabilidades similares, mientras que los otros dos tipos también muestran variabilidades similares. En cuanto la comparación de los grupos se observa que el único con un comportamiento diferente son los huracanes. Los otros tres tipos muestran cajas que se podrías solapar mostrando una mayor igualdad en los valores de presión atmosférica.Los gráficos matriciales o de facetas permiten representar mediante múltiples gráficos la información de una variable numéricas con respecto los niveles de la variable categórica. Se pretende de esta forma comprobar la forma de la distribución de los datos de forma similar al gráfico de densidad pero realzando un gráfico para cada nivel. Aunque resultan más útiles cuando trabajamos con más de dos variables, se introducen aquí para ir conociendo su funcionamiento en ejemplos sencillos.Comenzamos realizando un histograma independiente. Para que resulte más fácil visualizar el gráfico introducimos un orden asociado con los valores de la variable numérica, comenzando con el nivel que tiene valores en esa variable más pequeños, y finalizando con la que tiene los valores más grandes.\nFigura 2.21: Gráficos matricial de velocidad del viento por tipo de tormenta\nTambién podríamos realizar el gráfico de densidad\nFigura 2.22: Gráficos matricial de velocidad del viento por tipo de tormenta\nEn ambos casos las interpretaciones son similares las que se hicieron con el gráfico conjunto de densidad.Podemos cambiar la configuración cambiando el número de columnas:\nFigura 2.23: Gráficos matricial de velocidad del viento por tipo de tormenta\nAunque el gráfico se visualiza mejor también resulta más difícil la comparación entre todos los niveles.Si deseamos cambiar la situación de las etiquetas del factor podemos utilizar la opción facet_grid.\nFigura 2.24: Gráficos matricial (grid) de velocidad del viento por tipo de tormenta\nEsta opción nos resultará de mayor utilidad cuando tengamos que representar dos factores ya que se podrá situar en la filas uno de los factores y el otro en las columnas con la opción facet_grid(factor1 ~ factor2).","code":"\nggplot(storm, aes(x = wind)) + \n  geom_density(aes(colour = type), bw = 3, na.rm = TRUE) + \n  labs(x = \"Wind speed (in knots)\", y = \"Density\") \n# la opción na.rm elimina los valores pérdidos.\nggplot(storm, aes(x = pressure)) + \n  geom_density(aes(colour = type), bw = 3, na.rm = TRUE) + \n  labs(x = \"Air pressure (in knots)\", y = \"Density\") \nggplot(storm, aes(x = type, y = wind)) + \n  geom_boxplot() + \n  scale_x_discrete(limits = ords) +\n  labs(x = \"Storm category\", y = \"Wind speed (in knots)\") \nggplot(storm, aes(x = type, y = pressure)) + \n  geom_boxplot() + \n  scale_x_discrete(limits = ords) +\n  labs(x = \"Storm category\", y = \"Air pressure (in mbar)\") \n# Creamos un nuevo factor ordenado de acuerdo a la variable que estamos midiendo\nstorm$type2 <- reorder(storm$type, storm$wind)\n# Creamos el gráfico\nggplot(storm, aes(x = wind))  +\n  geom_histogram(binwidth = 5) + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Frequency\") +\n  facet_wrap(~ type2, ncol = 1)\n# Creamos el gráfico\nggplot(storm, aes(x = wind))  +\n  geom_density(bw = 3) + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Density\") +\n  facet_wrap(~ type2, ncol = 1)\n# Creamos el gráfico\nggplot(storm, aes(x = wind))  +\n  geom_density(bw = 3) + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Density\") +\n  facet_wrap(~ type2, ncol = 2)\n# Creamos el gráfico\nggplot(storm, aes(x = wind))  +\n  geom_density(bw = 3) + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Density\") +\n  facet_grid(type2 ~ .)"},{"path":"aed.html","id":"dos-variables-numéricas","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.5 Dos variables numéricas","text":"Los estadísticos han ideado varias formas diferentes de cuantificar la asociación entre dos variables numéricas en un banco de datos. Las medidas más comunes medidas buscan calcular algún tipo de coeficiente de asociación Los términos “asociación” y “correlación” están estrechamente relacionados; tanto que menudo se usan indistintamente. La más habitual es la correlación lineal que cuantifica el grado de asociación lineal entre dos variables de tipo numérico. Para ejemplificar nuestros cálculos y gráficos utilizaremos las variables wind y preassure.","code":""},{"path":"aed.html","id":"resúmenes-numéricos-4","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.5.1 Resúmenes numéricos","text":"La medida de correlación más utilizada es el coeficiente de correlación lineal de Pearson. El coeficiente de correlación de Pearson cuantifica el grado de asociación entre las variables en la escala -1 1, donde -1 indica una relación inversa (cuando una variable crece la otra decrece) y 1 indica una relación directa (cuando una crece la otra también lo hace. Valores próximo cero indican que hay asociación lineal entre las variables analizadas. Este coeficiente es la base para plantear lo que denominaremos más adelante los modelos de regresión lineal simple.La definición formal del coeficiente de correlación de Pearson (\\(\\rho\\)) viene dada por: \\[\\begin{equation} \n  \\rho = \\frac{1}{n-1}\\sum_{=1}^n \\frac{(x_i - \\bar{x})(y_i - \\bar{y})}{s_x s_y}\n  (\\#eq:coefcorrel)\n\\end{equation}\\]donde \\(x_i\\), \\(y_i\\) son las observaciones de la variable \\(x\\) e \\(y\\) respectivamente, \\(\\bar{x}\\), \\(\\bar{y}\\) son las medias muestrales de cada variable, y \\(s_x\\), \\(s_y\\) son las desviaciones típica de cada variable. El coeficiente trata de valorar la “covaraición” entre ambas variables, es decir, como afectan los cambios de valores en una variable en los valores de la otra, teniendo en cuenta la propia variabilidad de cada una de ellas.Para obtener el coeficiente de correlación utilizamos la función cor().El coeficiente de correlación resulta negativo, lo que indica que la velocidad del viento tiende disminuir al aumentar la presión. Al estar próximo -1 se puede intuir que dicha asociación es muy fuerte. Sin embargo, el coeficiente de correlación de Pearson debe interpretarse con cuidado debido que está diseñado para medir una relación de tipo lineal, lo que implica que dicho coeficiente será engañoso cuando esta relación sea curva, o incluso peor, en forma de joroba.¿Qué deberíamos hacer si pensamos que la relación entre dos variables es lineal? deberíamos usar el coeficiente de correlación de Pearson para medir la asociación en este caso. En cambio, podemos calcular lo que denominamos correlación de rango. La idea es muy simple. En lugar de trabajar con los valores reales de cada variable, los “clasificamos”, es decir, ordenamos cada variable de menor mayor y asignamos las etiquetas “primero”, “segundo”, “tercero”, etc. diferentes observaciones. Las medidas de correlación de rangos se basan en una comparación de los rangos resultantes. Los dos más populares son Spearman’s y Kendall’s. Ambos coeficientes se comportan de una manera muy similar al coeficiente de correlación de Pearson. Toman un valor de 0 si los rangos están correlacionados, y un valor de +1 o -1 si están perfectamente relacionados.Podemos calcular ambos coeficientes de correlación de rangos en R usando nuevamente la función cor. Esta vez necesitamos establecer el argumento del método en el valor apropiado: method = \"kendall\" o method = \"spearman\".Los resultados obtenidos son compatibles con el del coeficiente de Pearson.","code":"\ncor(storm$wind,storm$pressure)## [1] -0.9254911\ncor(storm$wind,storm$pressure,method = \"kendall\")## [1] -0.7627645\ncor(storm$wind,storm$pressure,method = \"spearman\")## [1] -0.9025831"},{"path":"aed.html","id":"visualización-gráfica-4","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.4.5.2 Visualización gráfica","text":"Los coeficientes de correlación nos dan una forma simple de resumir las asociaciones entre variables numéricas. Sin embargo, son limitados, porque un solo número nunca puede resumir todos los aspectos de la relación entre dos variables. Es por eso que siempre visualizamos la relación entre dos variables. El gráfico estándar para mostrar asociaciones entre variables numéricas es un diagrama de dispersión, usando ejes horizontales y verticales para trazar dos variables como una serie de puntos. Para realizar este gráfico usamos la opción geom_point()\nFigura 2.25: Gráfico de dispersión de velocidad del viento vs presión atmosférica\nEn el gráfico se puede apreciar la relación de tipo lineal en orden o pendiente decreciente (cuando aumenta el viento disminuye la presión).El problema de este gráfico es que podemos apreciar todos los puntos, ya que si tenemos dos observaciones con los mismo valores en ambas variables, estos quedarían superpuestos. Para solucionar esta deficiencia podemos optar por otra versión del gráfico de dispersión que nos permita contabilizar el número de repeticiones cuando estas existan. Este gráfico se obtiene con la opción geom_hex(). Para poder realizarlo es necesario instalar la librería hexbin:\nFigura 2.26: Gráfico de dispersión de velocidad del viento vs presión atmosférica (versión dos).\nEl parámetro bins segmenta el rango de cada variable en intervalos disjuntos. Lo que se representa es una gráfico de dispersión por intervalos, de forma que cada casilla representa todos los valores que quedan dentro del intervalo conjunto que obtenemos con ambas variables. Se observa que la tendencia se mantiene pero resulta posible ver que valores muestran una mayor o menor ocurrencia. La combinación de valores bajos de viento (< 40 mph) con altos de presión (> 990 mb) son los que más aparecen en el banco de datos.Otra opción es agrupar una variable continua para que actúe como una variable categórica. Luego se puede usar un gráfico combinado de cajas para representar ambas variables Veamos un ejemplo:\nFigura 2.27: Gráfico de cajas discretizando la velocidad del viento vs presión atmosférica.\nLa interpretación es similar la que se realizaba cuando trabajamos con una variable factor y otra numérica. Lo que resulta interesante es que podemos observar los intervalos con un mayor volumen de valores extremos o anómalos (valores de viento entre 30 y 90 mph).","code":"\nggplot(storm, aes(x = wind, y = pressure)) + \n  geom_point() + \n  labs(x = \"Wind speed (in knots)\", y = \"Air pressure (in mbar)\")\nggplot(storm, aes(x = wind, y = pressure)) + \n  geom_hex(bins = 25) + \n        labs(x = \"Wind speed (in knots)\", y = \"Air pressure (in mbar)\", fill = \"n\")\nggplot(data = storm, aes(x = wind, y = pressure)) + \n  geom_boxplot(mapping = aes(group = cut_width(wind, 10))) + \n        labs(x = \"Wind speed (in knots)\", y = \"Air pressure (in mbar)\", fill = \"n\")"},{"path":"aed.html","id":"análisis-descriptivo-avanzado","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.5 Análisis Descriptivo avanzado","text":"En esta unidad se amplían los procedimientos de análisis descriptivos vistos en el tema anterior para estudiar una o dos variables de tipo numérico o categórico al caso de más de dos variables de este tipo. se hace un barrido cualquier situación que pueda aparecer sino que se pretende mostrar los casos más habituales. Dichos casos son:Tres variables categóricas.Dos variables categóricas y una variable numérica.Una variable categórica y dos variables numéricas.Dos variables categóricas y dos variables numéricas.Tres variables categóricas y dos variables numéricas.De nuevo utilizaremos el conjunto de datos storms de la librería nasaweather.","code":""},{"path":"aed.html","id":"tres-factores","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.5.1 Tres factores","text":"Los procedimientos numéricos se restringen en este caso la obtención de la tabla de frecuencias conjunta de las tres variables, mientras que los gráficos se basan en gráficos matriciales donde se consideran gráficos de barras.modo de ejemplo vamos realizar el análisis conjunto de las variables year_f, month_f y type. Para poder realizar estos análisis utilizamos la función mytable de la librería moonBook. Para conocer todas las caracter´siticas de esta función se recomienda ver la ayuda help(mytable). EL problema principal con esta función es que si el número de nivles de los factores es demasiado grande resulta muy complicado visualizar todos los resultados en una única página. De hecho solo se pueden visulaizar resulatdos si el número de niveles del factor es 5 como máximo. Para poder ver los resultados en esta situación procedemos reando un conjunto de datos para cada tipo de tormenta. En este caso seleccionamos los meses centrales y los últimos cinco años para poder visualizar los resultados.#{r aed045,error=FALSE,warning=FALSE,message=FALSE} #stormTD <- dplyr::filter(storm , month %% c(7,8,9,10,11),  #                         year %% c(1996,1997,1998,1999,2000)) #mytable(year_f + type ~ month_f, data = stormTD) #Para realizar le gráfico combinado de las tres variables categóricas utilizamos un gráfico matricial con dos factores y representamos dentro de cada combinación el gráfico de barras de la otra variable.\nFigura 2.28: Gráfico de barras para tipo de tormenta para los diferentes meses y años.\n¿Qué conclusiones podemos extraer de estos resultados? El gráfico resulta revelador, ya que se aprecian de forma directa las combinaciones de año - mes en al que hay datos, y en aquellas donde si los hay se puede ver claramente cual es el tipo de tormenta más predominante.Dado que la mayoría de los datos se producen entre los meses de agosto y octubre vamos filtrar los datos para estudiar esas combinaciones únicamente.\nFigura 2.29: Gráfico de barras para tipo de tormenta para los diferentes meses y años (versión 2).\nEste gráfico nos permite estudiar con más detalle los meses que concentran un mayor número de tormentas.","code":"\nords <- c(\"Tropical Depression\", \"Extratropical\", \"Tropical Storm\", \"Hurricane\")\nggplot(storm, aes(x = type))  +\n  geom_bar() + \n  scale_x_discrete(limits = ords) +\n  xlab(\"Storm category\") +\n  ylab(\"Frequency\") +\n  facet_grid(year_f ~ month_f)+\n  theme(axis.text.x = element_text(angle = 90)) \nstorm_meses <- storm %>%\n  filter(month_f == c(\"August\",\"September\",\"October\"))\nords <- c(\"Tropical Depression\", \"Extratropical\", \"Tropical Storm\", \"Hurricane\")\nggplot(storm_meses, aes(x = type))  +\n  geom_bar() + \n  scale_x_discrete(limits = ords) +\n  xlab(\"Storm category\") +\n  ylab(\"Frequency\") +\n  facet_grid(year_f ~ month_f) +\n  theme(axis.text.x = element_text(angle = 90)) "},{"path":"aed.html","id":"dos-factores-una-numérica","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.5.2 Dos factores, Una numérica","text":"En este caso generalizamos el cálculo de medidas de localización y dispersión esta situación, y analizamos los diferentes gráficos que podemos realizar en esta situación. Utilizamos las variables year_f, type, y wind. Mostraremos solo los datos para los años 1999 y 2000.Como antes el análisis de estas tablas es más complejo que tratar de representar los datos de forma que se puedan extraer conclusiones de forma más efectiva. Empezamos con el gráfico matricial mezclado con los gráficos de densidad.\nFigura 2.30: Gráfico de densidad de la velocidad del viento para cada tipo de tormenta y año.\nAhora realizamos el gráfico de cajas con una estructura similar\nFigura 2.31: Gráfico de cajas de la velocidad del viento para cada tipo de tormenta y año.\n¿Qué conclusiones podemos extraer de este gráfico?Otra versión de este gráfico podría ser:\nFigura 2.32: Gráfico de cajas de la velocidad del viento para cada tipo de tormenta y año (versión 2).\n","code":"\nstormTD <- dplyr::filter(storm , year %in% c(1999,2000))\nmytable(year_f + type ~ wind, data = stormTD)## \n##                                          Descriptive Statistics stratified by 'year_f' and 'type'                                         \n## ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— \n##                                       1999                                                                2000                                \n##       —————————————————————————————————————————————————————————————————— —————————————————————————————————————————————————————————————————— \n##       Extratropical  Hurricane  Tropical Depression Tropical Storm   p   Extratropical  Hurricane  Tropical Depression Tropical Storm   p  \n##          (N=22)       (N=164)         (N=75)           (N=150)             (N=63)       (N=130)         (N=76)           (N=138)         \n## ——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— \n##  Wind  38.9 ± 20.3  90.5 ± 20.7     27.1 ±  3.9      47.7 ±  8.1   0.000  39.7 ± 13.2  79.5 ± 14.7     27.8 ±  2.6      46.2 ±  8.5   0.000\n## ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n# Creamos un nuevo factor ordenado de acurdo a la variable que estamos midiendo\nstorm$type2 <- reorder(storm$type, storm$wind)\n# Creamos el gráfico\nggplot(storm, aes(x = wind, color = type2))  +\n  geom_density(bw = 3) + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Density\") +\n  facet_grid(year_f ~ .)\n# Creamos el gráfico\nggplot(storm, aes(x = type2, y = wind))  +\n  geom_boxplot() + \n  xlab(\"Storm category\") +\n  ylab(\"Wind speed (in knots)\") +\n  facet_grid(. ~ year_f) +\n  theme(axis.text.x = element_text(angle = 90))\n# Creamos el gráfico\nggplot(storm, aes(x = year_f, y = wind, color = type2))  +\n  geom_boxplot() + \n  xlab(\"Year\") +\n  ylab(\"Wind speed (in knots)\") "},{"path":"aed.html","id":"un-factor-dos-numéricas","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.5.3 Un factor, Dos numéricas","text":"En este caso generalizamos el análisis de correlación y el gráfico de dispersión con la inclusión del factor. Consideramos las variables type, wind y pressure. En primer lugar realizamos el estudio descriptivo numérico.El resultado muestra gran asociación entre la velocidad del viento y la presión atmosférica en todas la categorías salvo para las Depresiones tropicales.Veamos ahora el gráfico de dispersión conjunto. En primer lugar realizamos un único gráfico marcando con colores los tipos de tormenta\nFigura 2.33: Gráfico de dispersion de presión vs velocidad para cada tipo de tormenta.\nPodemos ver como cada punto viene identificado según el tipo de tormenta. Los huracanes en la parte inferior donde se dan las relaciones entre velocidades del viento más altas y presiones atmosféricas más bajas. ¿Qué más podemos decir?Podemos distinguir cada grupo introduciendo un gráfico matricial\nFigura 2.34: Gráfico de dispersion de presión vs velocidad para cada tipo de tormenta.\nEn este gráfico se aprecia mejor el comportamiento de ambas variables en cada uno de los niveles del factor. Salvo en las depresiones tropicales donde se aprecia asociación, en el resto de niveles se aprecia un relación de orden inverso. Lo que si podemos ver es que hay observaciones en algunas categorías que podrían corresponder otras. En la categoría de tormentas tropicales tenemos combinaciones de velocidad y presión que parecen corresponder más un huracán que una tormenta tropical. Esto puede ser debido al protocolo de clasificación establecido o la propia evolución de las tormentas.","code":"\nstorm %>% \n  group_by(type) %>% # Segmentamos por tipo de tormenta\n  summarise(cor = cor(wind,pressure)) # Obtenemos coeficientes de correlación## # A tibble: 4 × 2\n##   type                   cor\n##   <fct>                <dbl>\n## 1 Extratropical       -0.816\n## 2 Hurricane           -0.914\n## 3 Tropical Depression -0.157\n## 4 Tropical Storm      -0.819\nggplot(storm, aes(x = wind, y = pressure, color = type )) + \n  geom_point() + \n  labs(x = \"Wind speed (in knots)\", y = \"Air pressure (in mbar)\")\n# Creamos el gráfico\nggplot(storm, aes(x = wind, y = pressure))  +\n  geom_point() + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Air pressure (in mbar)\") +\n  facet_grid(. ~ type2) "},{"path":"aed.html","id":"dos-factores-dos-numéricas","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.5.4 Dos factores, Dos numéricas","text":"Este caso es una generalización directa del caso anterior, ya que únicamente debemos añadir una nueva variable categórica. Consideramos las variables type, year_f, wind y pressure. Comenzamos con el análisis numérico:En esta tabla aparecen representados los coeficientes de correlación entre viento y presión para las diferentes combinaciones de niveles de las variables tipo y año. Se aprecian valores muy bajos en todas las combinaciones de la depresión tropical, mientras que en el resto hay asociaciones que pueden resultar interesantes de estudiar posteriormente. En el caso de los huracanes esas asociaciones son muy fuertes ya que muestran valores muy próximos -1.En cuanto los procedimientos gráficos optamos por una combinación de los gráficos que utilizamos en la sección anterior. Representamos el gráfico de dispersión coloreando por tipo de tormenta, y usamos un diagrama matricial por año.\nFigura 2.35: Gráfico de dispersion de presión vs velocidad para cada tipo de tormenta y año.\nEn todos los años se observa un comportamiento similar del resto de variables, indicando que el año es una factor que pueda ser considerado como relevante. Si utilizamos la variable mes en su lugar el gráfico resultante es:\nFigura 2.36: Gráfico de dispersion de presión vs velocidad para cada tipo de tormenta y mes\nEn este caso el comportamiento de los meses es tan parecido. Si bien es cierto que en todas las combinaciones se aprecia una tendencia negativa (sube viento - baja presión), también se puede ver que los huracanes aparecen mayoritariamente en los meses de agosto octubre. ¿Qué otra información podemos extraer de este gráfico?","code":"\ntabla_cor <- storm %>% \n  group_by(year_f,type) %>% \n  summarise(cor = cor(wind,pressure))\n# Visulaizamos la tabla de resultados de forma óptima\ntabla_resumen <- dplyr::select(tabla_cor,year_f,type,cor)\n# Arreglamos la tabla para una mejor visualización\nspread(tabla_resumen, key = type, value = cor)## # A tibble: 6 × 5\n## # Groups:   year_f [6]\n##   year_f Extratropical Hurricane `Tropical Depression` `Tropical Storm`\n##   <fct>          <dbl>     <dbl>                 <dbl>            <dbl>\n## 1 1995          -0.848    -0.885                 0.113           -0.731\n## 2 1996          -0.826    -0.941                -0.290           -0.917\n## 3 1997          -0.781    -0.973                -0.455           -0.748\n## 4 1998          -0.726    -0.947                -0.271           -0.613\n## 5 1999          -0.961    -0.916                -0.170           -0.661\n## 6 2000          -0.915    -0.940                -0.127           -0.791\n# Creamos el gráfico\nggplot(storm, aes(x = wind, y = pressure, color = type2))  +\n  geom_point() + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Air pressure (in mbar)\") +\n  facet_grid(. ~ year_f) \n# Creamos el gráfico\nggplot(storm, aes(x = wind, y = pressure, color = type2))  +\n  geom_point() + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Air pressure (in mbar)\") +\n  facet_grid(. ~ month_f) +\n  theme(axis.text.x = element_text(angle = 90))"},{"path":"aed.html","id":"tres-factores-dos-numéricas","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.5.5 Tres factores, Dos numéricas","text":"Es una generalización directa de los dos casos anteriores. Se presenta únicamente el código para obtener los resultados. Añadimos la variable month_f las del caso anterior.¿Qué podemos decir de los resultados obtenidos?Veamos ahora el gráfico matricial. Seleccionamos los meses de agosto octubre para poder visualizarlo mejor.\nFigura 2.37: Gráfico de dispersion de presión vs velocidad para cada tipo de tormenta, año y mes.\n¿Qué podemos decir de este gráfico?","code":"\ntabla_cor <- storm %>% \n  group_by(year_f,month_f,type) %>% \n  summarise(cor = cor(wind,pressure))\n# Visulaizamos la tabla de resultados de forma óptima\ntabla_resumen <- dplyr::select(tabla_cor,year_f,month_f,type,cor)\nspread(tabla_resumen, key = type, value = cor)## # A tibble: 30 × 6\n## # Groups:   year_f, month_f [30]\n##    year_f month_f   Extratropical Hurricane `Tropical Depression` `Tropical Storm`\n##    <fct>  <fct>             <dbl>     <dbl>                 <dbl>            <dbl>\n##  1 1995   June             -0.688    NA                   NA                -0.849\n##  2 1995   July             -0.816    NA                    0.121            -0.701\n##  3 1995   August           -0.926    -0.750                0.294            -0.805\n##  4 1995   September        -0.881    -0.931               -0.203            -0.937\n##  5 1995   October          -0.954    -0.918               -0.0585           -0.727\n##  6 1995   November         -0.968    NA                   NA                 1    \n##  7 1996   June             -0.591    NA                   -0.828             0    \n##  8 1996   July             -0.165    -0.779               -0.612            -0.955\n##  9 1996   August           NA        -0.976               -0.0301           -0.830\n## 10 1996   September        -0.887    -0.852                0.0622           -0.927\n## # … with 20 more rows\nstorm_meses <- storm %>%\n  filter(month_f == c(\"August\",\"September\",\"October\"))\n# Creamos un nuevo factor ordenado de acurdo a la variable que estamos midiendo\nstorm_meses$type2 <- reorder(storm_meses$type, storm_meses$wind)\n# Creamos el gráfico\nggplot(storm_meses, aes(x = wind, y = pressure, color = type2))  +\n  geom_point() + \n  xlab(\"Wind speed (in knots)\") +\n  ylab(\"Air pressure (in mbar)\") +\n  facet_grid(year_f ~ month_f) +\n  theme(axis.text.x = element_text(angle = 90))"},{"path":"aed.html","id":"librería-de-interés","chapter":"Unidad 2 Análisis exploratorio de datos","heading":"2.6 Librería de interés","text":"La libreria ggplotgui través de la función ggplot_shiny nos permite generar una aplicación con la que se puede obtener el código correspondiente un gráfico. Para utilizar dicha función basta con escribir ggplot_shiny(dataframe).","code":""},{"path":"prob.html","id":"prob","chapter":"Unidad 3 Probabilidad","heading":"Unidad 3 Probabilidad","text":"En esta unidad introducimos los conceptos básicos de varaible aleatoria, distribución de probabilidad, y las variables aleatorias más habituales en la investigación científica. se trata de una unidad sobre probabilidad (para lo que se recomienda acudir libros con mayores desarrollos), sino una unidad práctica de conocimiento básico de las distribuciones de probabilidad que juegan un papel relevante en los procesos de inferencia y en la cnstruccion de modelos estadísticos que veremos en las unidades siguientes.","code":""},{"path":"prob.html","id":"prob-intro","chapter":"Unidad 3 Probabilidad","heading":"3.1 Introducción","text":"La probabilidad o el azar juega un papel muy importante en el razonamiento científico. Ejemplos de procesos biológicos donde la probabilidad juega un papel relevante son: ) la segregación de cromosomas en la formación de gametos o la ocurrencia de mutaciones genéticas. En otras ocasiones es el propio diseño experimental el que introduce la aleatoriedad como por ejemplo cuando dividimos un grupo de sujetos en función del tratamiento al que se van ver sometidos.Las conclusiones del análisis estadístico de datos se expresan en muchas ocasiones en términos de probabilidad, ya que implícitamente se está introduciendo la aleatoriedad debida la muestra de sujetos con el que estamos trabajando, y que generalmente coincide con toda la población bajo estudio.En las unidades anteriores hemos visto que el estudio estadístico se centra en la información recogida sobre alguna variable relacionada directamente con el objetivo u objetivos del diseño experimental planteado. Un hecho cierto es que debido la aleatoriedad e los sujetos resulta imposible saber con certeza el valor de dicha variable para un sujeto en particular. Se introduce de esta forma el concepto de variable aleatoria que hace referencia todas aquellas en las que intrínsecamente se reconoce variabilidad en la respuesta de los sujetos.En este punto introducimos algo de notación que nos resultará de utilidad de aquí en adelante. Las variables aleatorias siempre se denotan en mayúsculas, \\(Y\\), mientras que los valores observados para un conjunto de sujetos en esa variable (muestra) se denotan por minúsculas e indicando la posición que el sujeto ocupa en el banco de datos \\(y_1,y_2,...,y_n\\).Por razones obvias se definen entonces las variables aleatorias discretas y las variables aleatorias continuas. Una variable discreta es aquella que sólo puede tomar un número finito o contable de posibles resultados, de forma que es posible conocer de antemano cuales son los posibles resultados que se pueden observar. Una variable continua es aquella que puede tomar infinitos valores numéricos, y por tanto es imposible identificar cada uno de los posibles valores de la variable, aunque si es posible conocer el rango de posibles resultados que se pueden observar. De forma natural se puede establecer una equivalencia entre la definición de variables categóricas y numéricas introducidas en unidades anteriores con las variables discretas y continuas.Se puede describir de forma completa una variable aleatoria sin más que especificar la probabilidad asociada cada uno de sus posibles valores. Esta especificación se conoce con el nombre de distribución de probabilidad. Sin embargo, la forma en que se puede especificar dicha distribución de probabilidad depende del tipo de variable aleatoria con la que estemos trabajando. En el caso de variables discretas basta con determinar la probabilidad de cada uno de los posibles resultados observables de la variable, pero ocurre así en las variables continuas donde es imposible conocer todos sus posibles valores.","code":""},{"path":"prob.html","id":"distribuciones-modelos","chapter":"Unidad 3 Probabilidad","heading":"3.2 Distribuciones-Modelos","text":"En la Unidad anterior se ha visto que el objetivo principal de muchos análisis estadísticos es estudiar el comportamiento de los sujetos de una población, partir de la información recogida en una muestra de sujetos seleccionados de dicha población. Sin embargo, cuando hablamos de una población lo hacemos teniendo en mente las variables que se han medido sobre ellos. Imaginemos que estamos interesados en conocer la nota media de acceso de todos los estudiantes de primero de grado en la Universidad Miguel Hernández de Elche (UMH). En este caso es evidente que la población objetivo son todos los estudiantes de primero de grado que han accedido la UMH, aunque esa población contiene información sobre muchas otras variables (sexo, edad, localidad de residencia,…).Por lo tanto, todas las poblaciones se considerarán poblaciones de valores de alguna variable especificada. Si la población es infinita, nunca podremos obtener todos sus valores, e incluso si la población es finita, generalmente queremos medir todos sus valores. En cualquier caso, deseamos obtener información sobre características particulares de la población partir de un número restringido de valores de muestra. Por ejemplo, en el estudio de la nota de acceso podríamos querer utilizar la media de la nota media de todos los estudiantes como referencia de la población, aunque podríamos utilizar otros como el percentil 80, etc…Para proceder de esta forma, necesitamos formular un modelo adecuado para los valores \\(x\\) que componen la población, relacionar las características de interés con los aspectos apropiados del modelo y luego usar los datos de la muestra para proporcionar estimaciones de estos aspectos.La característica esencial de tales valores \\(x\\) en su imprevisibilidad, y lo mejor que podemos hacer es especificar la probabilidad de obtener un valor dado o un valor en un rango dado. Por lo tanto, las distribuciones de probabilidad proporcionan los modelos más apropiados para las poblaciones variables de respuesta. Más específicamente, la función de densidad de probabilidad \\(f(x)\\) o la función de distribución de probabilidad \\(F(x)\\) proporcionan la información necesaria, por lo que constituye el modelo de población para la variable aleatoria \\(X\\). Por supuesto, dado que nunca conocemos esta distribución con una certeza total, debemos suponer una forma funcional específica (modelo matemático) para \\(f(x)\\) y \\(F(x)\\). Esta forma funcional usualmente involucra uno o más parámetros que pueden ser variados, y leso que se espera es que habrá algunos valores específicos de estos parámetros para los cuales la distribución resultante se ajusta adecuadamente nuestros datos observados. Tal modelo se llama modelo paramétrico para \\(X\\). En el tema siguiente se presentarán los modelos paramétricos más frecuentes para una variable aleatoria \\(X\\).Por el momento, supongamos que hemos especificado alguna función adecuada \\(f(x)\\) como la densidad de probabilidad de nuestra población. ¿Qué valores resumen de esta densidad se relacionan con las características de la población que generalmente son de interés? Para responder esta situación, centrémonos en las variables discretas e interpretemos la probabilidad como una frecuencia relativa. Si la variable aleatoria \\(X\\) tiene valores posibles \\(x_1, x_2, x_3, ... ,x_n\\) y si \\(p_i = P(X = x_i)\\), entonces podemos pensar en \\(p_i\\) como la frecuencia relativa con la que el valor \\(x_i\\) ocurre en toda la población.Definimos el valor esperado de \\(X\\) por la ecuación:\\[\\begin{equation} \n  E(X) = \\sum_{=1}^n  x_i p_i\n  \\tag{3.1}\n\\end{equation}\\]Utilizando la interpretación de frecuencia relativa de \\(p_i\\) dada anteriormente, \\(E(X)\\) puede interpretarse como el promedio de los valores \\(X\\) en toda la población, por lo que es la media poblacional. Este es claramente uno de los principales valores de resumen para la población. menudo se denota \\(\\mu\\) y como mide el “centro” de la población también se lo conoce como el parámetro de localización de la población.De forma similar se define la varianza de la población, \\(\\sigma2\\), como:\\[\\begin{equation} \n  \\sigma^2 = V(X) = E\\{(X-\\mu)^2\\} = \\sum_{=1}^n  (x_i - \\mu)^2 p_i\n  \\tag{3.2}\n\\end{equation}\\]La desviación típica poblacional, \\(\\sigma\\), se obtiene partir de la raíz cuadrada de la varianza poblacional:\\[\\begin{equation} \n  \\sigma = DT(X) = \\sqrt{V(X)}\n  \\tag{3.3}\n\\end{equation}\\]Siguiendo con el ejemplo de la variable aleatoria que representa la suma del lanzamiento de dos dados ¿Cuál es el valor esperado de la suma de ambos lanzamientos? ¿Cuál es la desviación típica? En este caso ¿la distribución es aproximada o la distribución poblacional?En el caso de variables aleatorias continuas, donde denotamos por \\(R\\) el rango de todos los valores posibles, se define la media, varianza y desviación típica poblacional como\\[\\begin{equation} \n  \\mu = E(X) = \\int_R x f(x) dx\n  \\tag{3.4}\n\\end{equation}\\]\\[\\begin{equation} \n  \\sigma^2 = V(X) = \\int_R (x - \\mu)^2 f(x) dx\n  \\tag{3.5}\n\\end{equation}\\]\\[\\begin{equation} \n  \\sigma = DT(X) = \\sqrt{V(X)}\n  \\tag{3.6}\n\\end{equation}\\]Para las variables de tipo continuo más habituales la esperanza, varianza y desviación típica poblacionales se aproximan de forma precisa (si la muestra de trabajo es adecuada) por la media, varianza y desviación típica muestral.","code":""},{"path":"prob.html","id":"distribuciones-notables","chapter":"Unidad 3 Probabilidad","heading":"3.3 Distribuciones notables","text":"En la práctica resultaría complicado y farragoso si para cada experimento aleatorio ligeramente diferente de otro ya realizado, tuviéramos que determinar las distribuciones de probabilidad desde cero. Afortunadamente, podemos hacer uso de las similitudes que existen entre ciertos tipos o familias de experimentos y ciertas distribuciones de probabilidad, conocidas con el nombre de distribuciones notables, que nos permiten el desarrollo de las funciones de distribución que representen las características generales del experimento.Por ejemplo, muchos experimentos comparten el elemento común de que sus resultados se pueden clasificar en uno de dos eventos: una moneda puede salir cara o cruz; un niño puede ser hombre o mujer; una persona puede morir o morir; una persona puede ser empleada o desempleada. Estos resultados menudo se etiquetan como “éxito” o “fracaso”, teniendo en cuenta que aquí hay connotación de “bondad”; por ejemplo, al observar los nacimientos, el estadístico podría calificar el nacimiento de un niño como un “éxito” y la el nacimiento de una niña como un “fracaso”, pero los padres necesariamente verían las cosas de esa manera. Esta situación experimental se conoce como experimento Bernouilli, asignando probabilidad \\(\\theta\\) al suceso calificado como éxito y probabilidad \\(1-\\theta\\) al suceso calificado como de fracaso. Dichas probabilidades son diferentes para cada situación y se pueden aproximar mediante la obtención de datos experimentales. La distribución de probabilidad que surge en este experimento se conoce como distribución Bernouilli y es la primera distribución de probabilidad notable conocida.","code":""},{"path":"prob.html","id":"prob-binomial","chapter":"Unidad 3 Probabilidad","heading":"3.3.1 Distribución Binomial","text":"menudo nos interesa el resultado de los ensayos independientes y repetidos de Bernoulli, es decir, el número de éxitos en ensayos repetidos. En esta situación se considera:sucesos independientes: el resultado de un ensayo afecta el resultado de otro ensayo.repeticiones: las condiciones son las mismas para cada prueba, es decir, la probabilidad de éxito y fracaso permanecen constantes través de los diferentes ensayos realizados.Una distribución binomial nos da las probabilidades asociadas con los ensayos independientes y repetidos de Bernoulli. En una distribución binomial, las probabilidades de interés son las de observar un cierto número de éxitos, \\(r\\), en \\(n\\) ensayos independientes, cada uno de los cuales tiene solo dos resultados posibles y la misma probabilidad, \\(\\theta\\), de éxito. Por ejemplo, usando una distribución binomial, podemos determinar la probabilidad de obtener 4 caras en 10 lanzamientos de una misma moneda.La distribución de probabilidad para este experimento queda completamente determinado si conocemos \\(n\\) (número de repeticiones realizadas) y \\(\\theta\\) (probabilidad de éxito). Si \\(X\\) denota la variable aleatoria asociada con este experimento, la función de densidad de probabilidad se define como:\\[\\begin{equation} \n  P(X = x) = {n \\choose x} \\theta^x (1-\\theta)^{n-x}\n  \\tag{3.7}\n\\end{equation}\\]donde \\({n \\choose x}\\) representa el número combinatorio de \\(n\\) sobre \\(x\\), y \\(x\\) el número de éxitos observados. Dicha distribución se denota habitualmente:\\[\\begin{equation} \n  X \\sim Bi(n, \\theta)\n  \\tag{3.8}\n\\end{equation}\\]Esta distribución se puede evaluar para cualquier valor entre 0 y el número de repeticiones del experimento \\(n\\). En R la función dbinom permite obtener cualquier probabilidad de la distribución binomial una vez fijamos el valor que deseamos evaluar (\\(x\\)), el número de repeticiones (\\(n\\)), y la probabilidad de éxito asociada (\\(\\theta\\)).Una vez establecida la función de densidad de probabilidad resulta posible obtener el valor esperado del número de éxitos, así como conocer su variabilidad haciendo uso de las definiciones expuestas en el tema anterior. En concreto, para una variable que sigue una distribución de probabilidad Binomial ((3.8)) tenemos que:\\[\\begin{equation} \n  E(X) = n \\theta\n  \\tag{3.9}\n\\end{equation}\\]\\[\\begin{equation} \n  V(X) = n \\theta (1-\\theta)\n  \\tag{3.8}\n\\end{equation}\\]\\[\\begin{equation} \n  DT(X) = \\sqrt{n \\theta (1-\\theta)}\n  \\tag{3.10}\n\\end{equation}\\]En la página web http://www.artofstat.com/ se encuentra disponible una aplicación de trabajo de la distribución Binomial (https://istats.shinyapps.io/BinomialDist/). Dicha aplicación nos permite calcular tanto probabilidades como cuantiles para un experimento binomial. Utiliza dicha aplicación para responder las cuestiones siguientes:En una población de sujetos se conoce que el 39% de ellos sufre algún tipo de mutación genética. Si se obtiene una muestra de 20 sujetos ¿cuál es la probabilidad de observar tres sujetos con esa mutación genética?En una población de moscas de la fruta se conoce que el 30% son de color negro y el 70% son de color gris. Si se extrae una muestra de 15 moscas ¿cuál es la probabilidad de observar al menos cuatro moscas de color negro? ¿y de color gris?Una cierta droga causa daños en el hígado en el 1% de los pacientes. Se van realizar estudios completos sobre 50 pacientes que están tomando dicha droga para detectar daños en el hígado. ¿Cuál es la probabilidad de que ninguno de lo pacientes muestre daños en el hígado? ¿Cuál es la probabilidad de que al menos uno de los pacientes muestre daños en el hígado?Los estudios realizados concluyen que el 10% de las adolescentes de EEUU tienen deficiencia de hierro. Se obtiene una muestra de 14 adolescentes y se desea conocer ¿cuál es la probabilidad de que al menos el 50% de ellas tengan una deficiencia de hierro?En un experimento se comprobó que la aplicación de un tratamiento químico aumentaba la resistencia la corrosión de un material en un 80 % de los casos. Si se tratan ocho piezas, determina: () probabilidad de que el tratamiento sea efectivo para más de cinco piezas, (ii) probabilidad de que el tratamiento sea efectivo para al menos tres piezas, (iii) número de piezas para las que espera que el tratamiento sea efectivo.Se dispone de un cristal que tiene dos tipos de impurezas que absorben radiación de la misma longitud de onda. Una de ellas emite un electrón tras la absorción de un fotón, mientras que la segunda emite electrones. Las impurezas están en igual concentración y distribuidas homogéneamente en el cristal. Sin embargo, la sección eficaz de absorción, que es una medida de la probabilidad de absorber un fotón, es 90 veces mayor para la impureza que emite electrones que el de la impureza que los emite. Suponiendo que sobre el cristal inciden 200 fotones y que este es lo suficientemente grande para absorber todos, calcula la probabilidad de que al menos se emitan tres electrones.","code":""},{"path":"prob.html","id":"prob-poisson","chapter":"Unidad 3 Probabilidad","heading":"3.3.2 Distribución Poisson","text":"La distribución de Poisson se emplea como un modelo para variables aleatorias de tipo discreto cuando se quieren obtener las probabilidades de ocurrencia de un evento que se distribuye al azar en el espacio o el tiempo. Algunos ejemplos de esta distribución son:En el estudio de cierto organismo acuático, se toman un gran número de muestras de un lago y se cuentan el número de organismos que aparecen en cada muestra. El interés principal radica en conocer cuál es la probabilidad de encontrar algún organismo en una muestra próxima si la media observada en el conjunto de nuestras es de 2 organismos.En un estudio sobre la efectividad de un insecticida sobre cierto tipo de insecto, se fumiga una gran región. Posteriormente se crea una cuadrícula sobre el terreno, se selecciona de forma aleatoria un conjunto de ellas, y se cuenta el número de insectos vivos dentro de cada una. Estamos interesados en conocer cuál es la probabilidad de que encontremos ningún insecto vivo en una cuadrícula próxima si se sabe que que la media de insectos vivos en las cuadriculas analizadas es de 0.5.Un grupo de investigadores observó la ocurrencia de hemangioma capilar retiniano (RCH) en pacientes con la enfermedad de von Hippel-Lindau (VHL). RCH es un tumor vascular benigno de la retina. Usando una revisión retrospectiva de series de casos consecutivos, los investigadores encontraron que el número de medio de tumores RCH por ojo para pacientes con VHL era de 4. Están interesados en conocer cuál es la probabilidad de que se detecten más de cuatro tumores por ojo.Como se puede ver en los ejemplos la distribución de Poisson es muy habitual en los campos de la biología y la medicina. En una distribución de Poisson, las probabilidades de interés son las de observar un número de eventos, \\(x\\), en un tiempo o espacio determinado. La distribución de probabilidad para este experimento queda completamente determinada si conocemos el número de eventos y \\(\\lambda\\) la tasa o media del número de eventos que ocurren por unidad de tiempo o espacio. Si \\(X\\) denota la variable aleatoria asociada con este experimento, la función de densidad de probabilidad se define como:\\[\\begin{equation} \n  P(X = x) = \\frac{e^{-\\lambda} \\lambda^x}{x!}\n  \\tag{3.11}\n\\end{equation}\\]Dicha distribución se denota habitualmente: \\[ X \\sim Po(\\lambda)\\]. Esta distribución se puede evaluar para cualquier valor entre 0 y el número de máximo de ocurrencias que pueden ocurrir. Dado que este valor se conoce de antemano se debe prefijar un valor máximo que asegure que la probabilidad de ese valor sea cero.En R la función dpois permite obtener cualquier probabilidad de la distribución de Poisson una vez fijamos el valor que deseamos evaluar (\\(x\\)), y la tasa o media del número de eventos (\\(\\lambda\\)).Una vez establecida la función de densidad de probabilidad resulta posible obtener el valor esperado del número de eventos ocurridos, así como conocer su variabilidad haciendo uso de las definiciones expuestas en el tema anterior. En concreto, para una variable que sigue una distribución de probabilidad Poisson (\\(X \\sim Po(\\lambda)\\)) tenemos que:\\[\\begin{equation} \n  E(X) = \\lambda\n  \\tag{3.12}\n\\end{equation}\\]\\[\\begin{equation} \n  V(X) = \\lambda\n  \\tag{3.13}\n\\end{equation}\\]\\[\\begin{equation} \n  DT(X) = \\sqrt{\\lambda}\n  \\tag{3.14}\n\\end{equation}\\]En la página web http://www.artofstat.com/ se encuentra disponible una aplicación de trabajo de la distribución Binomial (https://istats.shinyapps.io/PoissonDist/). Dicha aplicación nos permite calcular tanto probabilidades como cuantiles para un experimento poisson Utiliza dicha aplicación para responder las cuestiones siguientes:(Hint: Recuerda que la tasa de la poisson se debe modificar si las cuestiones de interés están referenciadas la misma escala que la escala proporcionada).Un laboratorio es capaz de realizar 20 análisis de cierto tipo en un hora. ¿Cuál es la probabilidad de realizar entre 30 y 36 análisis en las próximas dos horas?¿y la probabilidad de realizar más de 36? (Hint: En este caso concemos directamente la media de la poisson ya que la escala medida es la misma sobre la que se este interesado, por lo que es necesario ajustar la tasa cada pregunta)Consideremos que el número de trozos de chocolate en una determinada galleta sigue una distribución de Poisson. Queremos que la probabilidad de que una galleta seleccionada al azar tenga por lo menos tres trozos de chocolate sea mayor que 0.8. Encontrar el valor entero más pequeño de la media de la distribución que asegura esta probabilidad.Un fabricante de maquinaria pesada tiene instalados en el campo 3840 generadores de gran tamaño con garantía. Sí la probabilidad de que cualquiera de ellos falle durante el año dado es de 1/1200 determine la probabilidad de que ) 4 generadores fallen durante el año en cuestión, b) que más 1 de un generador falle durante el año en cuestión.En un proceso de manufactura, en el cual se producen piezas de vidrio, ocurren defectos o burbujas, ocasionando que la pieza sea indeseable para la venta. Se sabe que en promedio 1 de cada 1000 piezas tiene una o más burbujas. ¿Cuál es la probabilidad de que en una muestra aleatoria de 8000 piezas, menos de 3 de ellas tengan burbujas?Se sabe que el número de microorganismos por gramo de una cierta muestra de suelo diluida en agua destilada se distribuye según una variable aleatoria de media 0.08. Si una preparación con un gramo de esta disolución se vuelve turbia, este gramo contiene al menos un microorganismo. Hallar la probabilidad de que una preparación que se ha vuelto turbia contenga:\nUn solo microorganismo\nMenos de tres microorganismos\nMás de dos microorganismos\nUn solo microorganismoMenos de tres microorganismosMás de dos microorganismosEn un monte, el número de plantas de romero por círculo de un metro de radio, se distribuye según una variable aleatoria. Se eligen 1o plantas al azar y se mide la distancia la planta que tiene más próxima. Las distancias observadas fueron (medidas en metros): 0.7, 1.7, 1.2, 0.4, 0.2, 0.5, 0.7, 0.8, 1.5, y 2.1. partir de estos datos estima los parámetros de interés de la distribución y calcula cuantas plantas es de esperar que habrá en una zona del monte de unos 12500 m2 de superficie.","code":""},{"path":"prob.html","id":"prob-normal","chapter":"Unidad 3 Probabilidad","heading":"3.3.3 Distribución Normal","text":"Hasta ahora todas las distribuciones consideradas eran de tipo de discreto. En este punto tratamos la distribución de probabilidad de tipo continuo y más concreta mente la más famosa y utilizada de todas ellas: la distribución de probabilidad Normal. Una variable de tipo continuo es aquella que puede tomar cualquier valor dentro de un rango de valores, es decir, existe un infinito número de valores posibles para la variable aleatoria. Para representar gráficamente una distribución de una variable aleatoria continua se debe construir un subconjunto de clases o intervalos consecutivos para el rango de valores de la variable considerada y considerar el histograma resultante. Cuando consideramos un número muy grande de clases o intervalos podríamos obtener la curva suavizada que representa la función de densidad de probabilidad de la variable aleatoria.La función de densidad de probabilidad para una variable aleatoria \\(X\\) que sigue una distribución Normal con parámetros \\(\\mu\\) y \\(\\sigma\\), denotada por \\(N(\\mu, \\sigma^2)\\) viene dada por:\n\\[\\begin{equation} \n  f(x) = \\frac{1}{2\\pi\\sigma^2} exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n  \\tag{3.15}\n\\end{equation}\\]Los dos parámetros de la distribución son \\(\\mu\\) que representa la medida de localización y \\(\\sigma\\) la medida de dispersión. Habitualmente los conocemos por sus nombres más habituales como son la media y la desviación típica. En concreto, para una variable que sigue una distribución de probabilidad Normal (\\(X \\sim N(\\mu, \\sigma^2)\\)) tenemos que:\\[\\begin{equation} \n  E(X) = \\mu\n  \\tag{3.16}\n\\end{equation}\\]\\[\\begin{equation} \n  V(X) = \\sigma^2\n  \\tag{3.17}\n\\end{equation}\\]\\[\\begin{equation} \n  DT(X) = \\sigma\n  \\tag{3.18}\n\\end{equation}\\]Las características más importantes de la distribución Normal:Es una distribución simétrica alrededor de la media, \\(\\mu\\).La media, la mediana y la moda son iguales.La distribución Normal queda completamente especificada partir de los valores de \\(\\mu\\) y \\(\\sigma\\). Los diferentes valores de la media y la desviación típica desplazan hacia un lado o el otro, o consiguen una distribución más puntiaguda (desviaciones típicas más pequeñas) o más achatada (desviaciones típicas más grandes).El intervalo definido por \\(\\mu - 5 * \\sigma, \\mu + 5 * \\sigma\\) tiene probabilidad 1, es decir, la probabilidad de los extremos inferior y superior del rango de valores se puede considerar despreciable.Dada una variable aleatoria Normal con media \\(\\mu\\) y desviación típica \\(\\sigma\\) y tenemos un escalar \\(\\) tenemos que:\\[ X \\sim N(\\mu, \\sigma^2) \\Longrightarrow aX \\sim N(\\mu, ^2\\sigma^2)\\]\n* Dada dos variables aleatorias Normales, \\(X e Y\\), con medias y desviaciones típicas respectivas: \\(\\mu_1\\), \\(\\sigma_1\\) y \\(\\mu_2\\), \\(\\sigma_2\\) y dos escalares \\(\\) y \\(b\\) tenemos que\\[ X \\sim N(\\mu_1, \\sigma^2_1), Y \\sim N(\\mu_2, \\sigma^2_2) \\Longrightarrow aX + \\sim N(\\mu_1+b\\mu_2, ^2\\sigma^2_1+b^2\\sigma^2_2)\\]Esta última propiedad se puede generalizar para el caso de \\(m\\) variables aleatorias en lugar de 2.","code":""},{"path":"prob.html","id":"teorema-central-de-límite","chapter":"Unidad 3 Probabilidad","heading":"3.3.3.1 Teorema Central de Límite","text":"Si las variables aleatorias \\(X_1,...X_n\\) son una muestra aleatoria de una distribución con media \\(\\mu\\) y desviación típica \\(\\sigma\\) entonces las variables aleatorias suma (\\(T = X_1+...+X_n\\)) y media (\\(M = (X_1+...+X_n)/n\\)) tienen distribuciones:\n\\[ T \\sim N\\left(n\\mu,n\\sigma^2\\right)\\]\n\\[ M \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\\]En la página web http://www.artofstat.com/ se encuentra disponible una aplicación de trabajo de la distribución Binomial (https://istats.shinyapps.io/NormalDist/). Dicha aplicación nos permite calcular tanto probabilidades como cuantiles para un experimento normal Utiliza dicha aplicación para responder las cuestiones siguientes:La longitud L (en micras) de ciertas larvas parásitas de moluscos, sigue la distribución Normal, pero de media y desviación típica distinats, según la larva se encuentre en estadio 1 o en estadio 2 de su crecimiento. La media y la desviación típica son: 219 y 20 para el estadio 1, y 241 y 14 para el estadio 2. Para identificar el estadio en que se encuentra la larva, se adopta el criterio siguiente: Si L≤230 pertenece al estadio 1, en otro caso pertenece al estadio 2. Resolver las siguientes cuestiones:\nCalcular la probabilidad de la presencia del suceso L≤230 para larvas del estadio 1. Idem. para larvas en el esatdio 2.\nDesde que nace una larva hasta que llega al estadio 3 de su desarrollo (que es fácilmente identificable), trasncurren 12 horas, permaneciendo 8 horas en el estadio 1 y 4 horas en el estadio 2. La observación de una larva cualquiera se hace en un instante que puede considerarse elegido al azar durante estas 12 horas (se desconoce cuando ha nacido la larva). Entonces, si se ha observado que una larva mide L = 225 y, por lo tanto, se considera que pertenece al estadio 1, calcular la probabilidad de equivocarnos al tomar esta decisión.\nCalcular la probabilidad de la presencia del suceso L≤230 para larvas del estadio 1. Idem. para larvas en el esatdio 2.Desde que nace una larva hasta que llega al estadio 3 de su desarrollo (que es fácilmente identificable), trasncurren 12 horas, permaneciendo 8 horas en el estadio 1 y 4 horas en el estadio 2. La observación de una larva cualquiera se hace en un instante que puede considerarse elegido al azar durante estas 12 horas (se desconoce cuando ha nacido la larva). Entonces, si se ha observado que una larva mide L = 225 y, por lo tanto, se considera que pertenece al estadio 1, calcular la probabilidad de equivocarnos al tomar esta decisión.Supongamos que el diámetro de las aceitunas rellenas de anchoa varía entre 15 y 17 mm. Debido al sequía del último año, la producción de aceituna verde se ha distribuido según una normal de media 15.5 mm. y desviación típica 1.2 mm. ¿Qué porcentaje de aceitunas podría ser aprovechado para rellenar de anchoa?La duración media de un lavavajillas es de 15 años y su desviación típica 0.5. Sabiendo que la vida útil del lavavajillas se distribuye normalmente, hallar la probabilidad de que al adquirir un lavavajillas, este dure más de 15 años.Una normativa europea obliga que en los envases de yogur debe haber menos de 120 gr. La máquina dosificadora de una empresa láctea hace los envases de yogur según una distribución normal de desviación típica de 2 gr. y media 122 gr.\n¿Qué tanto por ciento de los envases de yogur de esa empresa cumplirá la normativa?\n¿Cuál deberá ser la media de la distribución normal con la cual la máquina dosificadora debe hacer los envases para que el 98% de la producción de yogures de la empresa cumpla la normativa?. (La desviación típica sigue siendo de 2 gr.).\n¿Qué tanto por ciento de los envases de yogur de esa empresa cumplirá la normativa?¿Cuál deberá ser la media de la distribución normal con la cual la máquina dosificadora debe hacer los envases para que el 98% de la producción de yogures de la empresa cumpla la normativa?. (La desviación típica sigue siendo de 2 gr.).","code":""},{"path":"prob.html","id":"otras-distribuciones","chapter":"Unidad 3 Probabilidad","heading":"3.3.4 Otras distribuciones","text":"partir de la distribución de probabilidad Normal se pueden obtener otras distribuciones de probabilidad que resultan de gran utilidad para los procesos de generalización de resultados de un diseño experimental una población que veremos en la unidad siguiente.","code":""},{"path":"prob.html","id":"distribución-chi-cuadrado","chapter":"Unidad 3 Probabilidad","heading":"3.3.4.1 Distribución Chi-cuadrado","text":"Sean n variables aleatorias \\(X_1,...,X_n\\) independientes entre sí cuya distribución de probabilidad es idéntica para todas ellas e igual una Normal estándar (\\(N(0,1)\\)). Si definimos la variable aleatoria suma como:\n\\[X = X_1 + ... + X_n,\\] entonces decimos que \\(X\\) se distribuye según una distribución Chi-cuadrado con \\(n\\) grados de libertad y la denotamos por \\[X \\sim \\chi^2_n.\\]","code":""},{"path":"prob.html","id":"t-se-student","chapter":"Unidad 3 Probabilidad","heading":"3.3.4.2 T se Student","text":"Si tenemos dos variables aleatorias independientes \\(Y\\) y \\(Z\\) con distribuciones de probabilidad\n\\[Z \\sim N(0,1); Y \\sim \\chi^2_n,\\] y consideramos la variable aleatoria \\(X\\) dada por:\n\\[X = \\frac{Z}{\\sqrt{Y/n}},\\] entonces decimos que dicha variable sigue una distribución \\(t\\) de Student con \\(n\\) grados de libertad, y se denota por\n\\[X \\sim t_n.\\]","code":""},{"path":"prob.html","id":"f-de-snedecor","chapter":"Unidad 3 Probabilidad","heading":"3.3.4.3 F de Snedecor","text":"Si tenemos dos variables aleatorias independientes \\(Y\\) y \\(Z\\) con distribuciones de probabilidad\n\\[Z \\sim \\chi^2_n; Y \\sim \\chi^2_m,\\] y consideramos la variable aleatoria \\(X\\) dada por:\n\\[X = \\frac{Z/n}{Y/m},\\] entonces decimos que dicha variable sigue una distribución \\(F\\) de Snedecor con \\(n\\) y \\(m\\) grados de libertad, y se denota por\n\\[X \\sim F_{n,m}.\\]","code":""},{"path":"inferencia-básica.html","id":"inferencia-básica","chapter":"Unidad 4 Inferencia básica","heading":"Unidad 4 Inferencia básica","text":"En la unidad anterior se han expuesto brevemente la teoría y los métodos de la probabilidad. En este unidad se expondrán la teoría y los métodos de la inferencia estadística que servirán como base para los modelos estadísticos que estudiaremos en la unidad siguiente. Un problema de inferencia estadística es un problema en el cual se han de analizar datos que han sido generados de acuerdo con una distribución de probabilidad desconocida y en la que se debe realizar algún tipo de inferencia (“conocer su comportamiento”) acerca de tal distribución. En la mayoría de situaciones reales, existe un número infinito de distribuciones posibles distintas que podrían haber generado los datos. En la práctica, dado el tipo de variable aleatoria considerada, se suele asumir un modelo de distribución de probabilidad que es completamente conocida salvo excepto por los valores de los parámetros que la especifican completamente. Utilizamos los datos de la muestra para obtener información sobre dichos parámetros desconocidos y poder asumir de esta forma una distribución de probabilidad completa para todos los datos de la población.Por ejemplo, se podría saber que la duración de cierto tipo de marcapasos tiene una distribución exponencial con parámetro \\(\\lambda\\) pero desconocer el valor exacto de dicho parámetro. Si se puede observar la duración de varios marcapasos de este tipo, entonces, partir de estos valores observados y de cualquier otra información relevante de la que se pudiera disponer, es posible producir una inferencia acerca de ese valor desconocido \\(\\lambda\\). Podría interesar producir la mejor estimación del valor de dicho parámetro o especificar un intervalo en el cual se piensa que pueda estar incluido el verdadero valor de \\(\\lambda\\), o decidir si dicho parámetro es menor que un valor específico, ya que en ningún caso es posible obtener el verdadero valor de \\(\\lambda\\) ya que sería necesario obtener la información de todos los sujetos de la población y sólo los de la muestra.En un problema de inferencia estadística, cualquier característica de la distribución que genera los datos experimentales que tenga un valor desconocido, como \\(\\lambda\\) en el ejemplo anterior, se llama parámetro de la distribución. El conjunto \\(\\Omega\\) de todos los valores posibles de dicho parámetro se llama espacio parámetrico. Cuando nuestra distribución de probabilidad tiene dos parámetros (por ejemplo la distribución Normal) el espacio paramétrico vendrá dado por todo el conjunto de parejas de valores de \\(\\mu\\) y \\(\\sigma^2\\).","code":""},{"path":"inferencia-básica.html","id":"estimador-y-estimación","chapter":"Unidad 4 Inferencia básica","heading":"4.1 Estimador y estimación","text":"Si tenemos una variable aleatoria \\(X\\) cuya función de distribución viene caracterizada por un parámetro o conjunto de parámetros \\(\\delta\\), y una muestra aleatoria de tamaño \\(n\\), \\(X_1,...,X_n\\), partir de la cual se observan el conjunto de datos muestrales \\(x_1,...,x_n\\) vamos definir dos conceptos relevantes en todo proceso de inferencia: estimador y estimación.Un estimador del parámetro \\(\\delta\\), basado en las variables aleatorias \\(X_1,...,X_n\\), es una función \\(\\delta(X_1,...,X_n)\\) que especifica una valor para \\(\\delta\\). Se trata de una función matemática que tiene la misma forma independientemente de la muestra utilizada. Por ejemplo, los estimadores para la media, varianza (y cuasivarainza), y proporción poblacionales vienen dados por:Media muestral\\[\\delta_1(X_1,...,X_n) = \\bar{X} = \\frac{X_1+X_2+...+X_n}{n}\\]Varianza muestral\\[\\delta_2(X_1,...,X_n) = S^2 = \\frac{\\sum_{=1}^n (X_i-\\bar{X})^2}{n}\\]Cuasi-Varianza muestral\\[\\delta_3(X_1,...,X_n) = S_q^2 = \\frac{n}{n-1} S^2\\]Para una variable de tipo discreto se aplican las mismas definiciones si consideramos cada \\(X_i\\) como una realización de dicha variable. Para una variable que mide “éxito” o “fracaso” los valores de \\(X_i\\) serán 1 o 0 respectivamente, de forma que la media muestral coincide con la proporción muestral ya que tendríamos la suma de 1 en el numerador dividido por el tamaño de muestra en el denominador.Una estimación es el valor numérico del estimador para unos datos dados (\\(x_1,...,x_n\\)), es decir, sustituimos en \\(\\delta(X_1,...,X_n)\\) por sus valores observados y obtendríamos la estimación del parámetro. De forma habitual se identifica con el “gorro” encima del parámetro indica la estimación de un parámetro:\n\\[\\hat{\\mu} = \\delta_1(x_1,...,x_n) =\\bar{x}\\]\n\\[\\hat{\\sigma}^2 = \\delta_2(x_1,...,x_n)= s^2\\]","code":""},{"path":"inferencia-básica.html","id":"información-contenida-en-la-muestra","chapter":"Unidad 4 Inferencia básica","heading":"4.2 Información contenida en la muestra","text":"Antes de comenzar describir los procesos de inferencia estadística es necesario conocer como podemos relacionar la información contenida en una muestra con la distribución de probabilidad de la variable aleatoria de interés. Para ello una vez fijada la población del estudio y el objetivo principal es necesario:Establecer la variable de interés, \\(X\\) (y su tipo), en función del objetivo principalEstablecer una distribución de probabilidad para la variable aleatoria, \\(f(X | \\theta)\\), acorde con el tipo de dicha variable y lo más sencilla posible, donde \\(\\theta\\) representa el parámetro o parámetros que especifican dicha distribución.Muestra aleatoria de tamaño \\(n\\) de la variable de interés, \\(X_1, X_2,...,X_n\\) de forma que cada observación tiene probabilidad \\(f(X_1 | \\theta), f(X_2 | \\theta),...,f(X_n | \\theta)\\) respectivamente y sus valores observados vienen dados por \\(x_1,...,x_n\\).Obtener los descriptivos muestrales habituales: media, varianza, desviación típica, o proporción.Se define entonces la función de verosimilitud \\(L(X|\\theta)\\) como:\n\\[L(X|\\theta) = \\prod_{=1}^n f(X_i|\\theta) = f(X_1|\\theta)f(X_2|\\theta)\\cdots f(X_n|\\theta)\\]La función de verosimilitud contiene toda la información sobre la muestra y la relaciona con el parámetro o parámetros de interés través de la distribución de probabilidad establecida. Se convierte por tanto en la herramienta más importante, desde el punto de vista de la estadística frecuentista, para los procesos de inferencia estadística que estudiaremos continuación. Además, se utiliza como base para la caracterización de la distribución en el muestreo de los diferentes estimadores, aproximaciones numéricas del verdadero valor del parámetro o parámetros de la distribución de probabilidad asumida, que se utilizan en los procesos inferenciales.Para evitar la complejidad matemática que supone trabajar con productos se suele definir el logaritmo de la función de verosimilitud, log-verosimilitud, \\(LL(X|\\theta)\\) como:\n\\[LL(X|\\theta) = log\\left(\\prod_{=1}^n f(X_i|\\theta)\\right) = \\sum_{=1}^n log(f(X_i|\\theta)) = log(f(X_1|\\theta))+log(f(X_2|\\theta))+\\cdots +log(f(X_n|\\theta))\\]","code":""},{"path":"inferencia-básica.html","id":"procedimientos-de-inferencia-estadística","chapter":"Unidad 4 Inferencia básica","heading":"4.3 Procedimientos de inferencia estadística","text":"Estudiamos en este punto los distintos procedimientos de inferencia que tratamos en esta unidad. Realizamos una pequeña introducción de cada uno de ellos y ejemplificaremos su uso en el análisis de una población.","code":""},{"path":"inferencia-básica.html","id":"estimación-puntual","chapter":"Unidad 4 Inferencia básica","heading":"4.3.1 Estimación puntual","text":"Es el procedimiento de inferencia estadística más sencillo y consiste en obtener un único valor aproximado del parámetro de interés partir de un estimador de dicho parámetro y de los datos observados de una muestra. Aunque en las situaciones más sencillas (media, varianza y proporción) tanto el estimador como la estimación son fáciles de obtener, hay que establecer un procedimiento general para obtener dichas estimaciones.Dicho procedimiento consiste en encontrar el máximo de la función de verosimilitud (o más concretamente de la log-verosimilitud) para el parámetro o parámetros involucrados en dicha función, es decir, el valor o valores que se obtienen al igualar cero la derivada de la función de log-verosimilitud:\n\\[\\frac{d}{d\\theta} LL(X | \\theta) = 0\\]","code":""},{"path":"inferencia-básica.html","id":"estimador-de-una-proporción","chapter":"Unidad 4 Inferencia básica","heading":"4.3.1.1 Estimador de una proporción","text":"La estimación de proporciones surge de forma natural en poblaciones Bernouilli cuya función de log-verosimilitud viene dada por:\n\\[LL(X |\\theta) = s*log(\\theta) + (n-s)*log(1-\\theta)\\]\nDerivando e igualando cero:\n\\[\\frac{d}{d\\theta} LL(X | \\theta) = \\frac{s}{\\theta}+\\frac{-(n-s)}{1-\\theta} = 0\\]\nDespejando obtenemos:\n\\[\\hat{\\theta} = \\frac{s}{n}\\]\nque es el estimador habitual para obtener la proporción muestral.","code":""},{"path":"inferencia-básica.html","id":"estimador-de-una-media","chapter":"Unidad 4 Inferencia básica","heading":"4.3.1.2 Estimador de una media","text":"Si asumimos que la variable aleatoria es \\(N(\\mu,\\sigma^2)\\) la función de log verosimilitud viene dada por:\\[LL(X |\\mu,\\sigma^2) = -\\frac{n}{2}*log(2\\pi\\sigma^2)+\\frac{nS^2 + n(\\mu-\\bar{X})^2}{2\\sigma^2}\\]Si derivamos respecto de \\(\\mu\\) e igualamos cero:\\[\\frac{d}{d\\mu}LL(X |\\mu,\\sigma^2) = -\\frac{2n(\\mu-\\bar{X})}{2\\sigma^2} = 0\\]\nDespejando obtenemos el estimador para la media:\n\\[\\hat\\mu = \\bar X\\]","code":""},{"path":"inferencia-básica.html","id":"estimador-de-una-varianza","chapter":"Unidad 4 Inferencia básica","heading":"4.3.1.3 Estimador de una varianza","text":"De forma similar, derivando respecto de \\(\\sigma^2\\) e igualando cero la función de log-verosimilitud tenemos que el estimador para la varianza viene dado por:\n\\[\\hat \\sigma^2 = S^2\\]Podemos ver que en las situaciones habituales los estimadores de los parámetros poblacionales coinciden con las funciones que nos permiten obtener las medidas de localización y dispersión muestrales.El problema con la estimación puntual es que únicamente nos da un valor posible para el parámetro poblacional pero sin ninguna medida del posible error que estamos cometiendo, dado que la estimación obtenida depende de la muestra seleccionada. Dos muestras distintas podrían dar dos estimaciones distintas y por tanto dos valores para el parámetro poblacional. Para introducir una medida del error cometido con la muestra seleccionada se utiliza los procedimiento de inferencia basados en intervalos de confianza, pero antes de pasar con ellos es necesario describir un poco más el comportamiento aleatorio de los estimadores que acabamos de obtener.","code":""},{"path":"inferencia-básica.html","id":"distribución-en-el-muestreo","chapter":"Unidad 4 Inferencia básica","heading":"4.3.2 Distribución en el muestreo","text":"Dado que todos los estimadores se construyen partir de una colección de realizaciones aleatorias \\(X_1,...X_n\\) de la variable aleatoria \\(X\\), resulta que dicho estimador es una nueva variable aleatoria (que toma valores según los datos observados de la muestra) y por tanto su distribución puede ser deducida partir de las distribuciones de cada una de las \\(X_i\\), utilizando la función de verosimilitud. En algunos casos (variables discretas) será necesario utilizar el Teorema Central del Límite para determinar una forma aproximada de dicha distribución de probabilidad que nos permita realizar los procesos inferenciales sin demasiada complejidad matemática. La distribución de los estimadores se conoce con el nombre de distribución en el muestreoEn este punto siguiente se muestran las distribuciones en el muestro para los estimadores obtenidos en este apartado. Se eliminan todos los desarrollos matemáticos y simplemente se muestran las distribuciones obtenidas.","code":""},{"path":"inferencia-básica.html","id":"distribución-en-el-muestreo-de-una-media-poblacional-con-varianza-poblacional-conocida","chapter":"Unidad 4 Inferencia básica","heading":"4.3.2.1 Distribución en el muestreo de una media poblacional con varianza poblacional conocida","text":"Tenemos una población de \\(N\\) sujetos sobre la que se desea estudiar una variable aleatoria de tipo continuo (\\(X\\)) que sigue una distribución \\(N(\\mu,\\sigma^2)\\) y cuyo parámetro de interés es la media (\\(\\mu\\)) pero donde conocemos el valor de \\(\\sigma^2\\). Por la aplicación directa del Teorema Central del Límite la distribución en el muestreo de \\(\\bar X\\) viene dada por:\n\\[\\bar X \\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right)\\]\ny por tanto la variable tipificada tiene distribución Normal estándar, es decir:\n\\[\\frac{\\bar X - \\mu}{\\sigma/\\sqrt n} \\sim N(0,1)\\]","code":""},{"path":"inferencia-básica.html","id":"distribución-en-el-muestreo-de-una-media-poblacional-con-varianza-poblacional-desconocida","chapter":"Unidad 4 Inferencia básica","heading":"4.3.2.2 Distribución en el muestreo de una media poblacional con varianza poblacional desconocida","text":"Cuando la varianza es desconocida también se puede obtener la distribución en el muestreo de la media muestral sin más que sustituir la varianza poblacional por un estimador. Tipificando tenemos que:\n\\[T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n-1}} = \\frac{\\bar{X} - \\mu}{S_q/\\sqrt{n}}\\]\nse distribuye según una distribución \\(t\\) de Student con \\(n-1\\) grados de libertad,\n\\[ T \\sim t_{n-1}\\]","code":""},{"path":"inferencia-básica.html","id":"distribución-en-el-muestreo-de-una-varianza-poblacional","chapter":"Unidad 4 Inferencia básica","heading":"4.3.2.3 Distribución en el muestreo de una varianza poblacional","text":"En la situación poblacional anterior la distribución en el muestreo de la varianza si consideramos la variable aleatoria:\n\\[\\chi^2 = \\frac{nS^2}{\\sigma^2} = \\frac{(n-1)S_q^2}{\\sigma^2}\\] que se distribuye según una distribución Chi cuadrado con \\(n-1\\) grados de libertad,\n\\[ \\chi^2 \\sim \\chi^2_{n-1}\\]La obtención de esta distribución se hace partir de la forma de la función de verosimilitud para una población Normal con ambos parámetros desconocidos.","code":""},{"path":"inferencia-básica.html","id":"distribución-en-el-muestreo-de-una-proporción-poblacional","chapter":"Unidad 4 Inferencia básica","heading":"4.3.2.4 Distribución en el muestreo de una proporción poblacional","text":"Tenemos una población de \\(N\\) sujetos sobre la que se desea estudiar una variable aleatoria de tipo discreto (\\(X\\)) cuyo parámetro de interés es la proporción de sujetos (\\(\\theta\\)) que cumplen con cierta condición. En esta situación si obtenemos una muestra de tamaño \\(n\\) de dicha variable \\(X_1,...,X_n\\) (donde cada uno de ellos toma el valor 1 si cumple con la condición y = si cumple), la proporción de éxito muestral (\\(\\hat{\\theta}\\)) viene dada por:\n\\[\\hat{\\theta} = \\frac{\\sum_{=1}^n X_i}{n}\\]En esta situación la distribución en el muestreo para \\(\\hat{\\theta}\\), cuando \\(n\\) es grande (\\(n \\geq 30\\)), viene dada por:\n\\[\\hat{\\theta} \\sim N \\left(\\theta, \\frac{\\theta (1-\\theta)}{n}\\right) \\]\nde forma que la variable aleatoria tipificada \\(Z\\) cumple que:\\[Z = \\frac{\\hat{\\theta}-\\theta}{\\sqrt{\\frac{\\theta (1-\\theta)}{n}}} \\sim N(0,1)\\]La obtención de dichas distribuciones en el muestreo nos permite realizar cálculos de probabilidades sobre dichas cantidades aleatorias, con lo que resulta posible conocer cuál es la probabilidad de que la media muestral supere cierto valor, y por tanto, podamos tener una mayor certeza del verdadero valor del parámetro poblacional.","code":""},{"path":"inferencia-básica.html","id":"error-estándar","chapter":"Unidad 4 Inferencia básica","heading":"4.3.2.5 Error estándar","text":"Para cuantificar la bondad de la estimación obtenida se define el error estándar (\\(es()\\)) como la desviación típica de la distribución en el muestreo del estimador. De esta forma:Error estándar de la media en una población Normal con varianza conocida\\[es(\\bar X) = \\frac{\\sigma}{\\sqrt n}\\]Error estándar de la media en una población Normal con varianza desconocida\\[es(\\bar X) = \\frac{S}{\\sqrt{n-1}}\\]Error estándar de una proporción en una población Bernouilli\\[es(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\theta} (1-\\hat{\\theta}))}{n}}\\]","code":""},{"path":"inferencia-básica.html","id":"estimación-por-intervalos-de-confianza","chapter":"Unidad 4 Inferencia básica","heading":"4.3.3 Estimación por intervalos de confianza","text":"Un intervalo de confianza al nivel \\(100*(1 - \\alpha)\\%\\) representa la confianza que tenemos en que el verdadero valor del parámetro de la población se encuentre contenido entre los limites de dicho intervalo, que se construye partir de la información muestral y la distribución en el muestreo del estimador. Si tenemos un intervalo de confianza al 95%, es decir \\(\\alpha = 0.05\\), lo que estamos indicando es que si obtuviéramos 100 muestras y calculáramos los 100 intervalos asociados, solo en 95 de ellos contendrían al verdadero valor del parámetro poblacional. Dado que habitualmente tomamos una única muestra debemos confiar en que el intervalo producido sea uno de los 95 que contiene al verdadero valor del parámetro poblacional. El valor de \\(\\alpha\\) se conoce como significatividad.Supongamos que tenemos una población sobre la deseamos estudiar una variable aleatoria \\(X\\) cuya función de distribución es conocida y viene caracterizada por el parámetro \\(\\theta\\), \\(f(X|\\theta)\\). Vamos obtener una muestra de tamaño \\(n\\), \\(X_1,...,X_n\\) y consideramos el estimador \\(\\hat\\theta\\) de \\(\\theta\\) del que conocemos su distribución en el muestreo, \\(f_n\\),y podemos especificar su error estándar, \\(es(\\hat{\\theta})\\). Si fijamos el valor de \\(\\alpha\\) el intervalo de confianza para el parámetro poblacional viene dado por la expresión:\\[IC_{1-\\alpha}(\\theta) = (\\hat{\\theta} - q_{\\alpha/2}*es(\\hat{\\theta}),\\hat{\\theta} + q_{1-\\alpha/2}*es(\\hat{\\theta}))\\]\ndonde \\(q_{\\alpha/2}\\) y \\(q_{1-\\alpha/2}\\) son los cuantiles \\(\\alpha/2\\) y \\(1-\\alpha/2\\) de la distribución en el muestreo, es decir,\n\\[P(\\hat{\\theta} \\leq q_{\\alpha/2}) = \\alpha/2\\]\n\\[P(\\hat{\\theta} \\leq q_{1-\\alpha/2}) = 1-\\alpha/2\\]Si la distribución en el muestreo es simétrica \\(q_{1-\\alpha/2} = - q_{\\alpha/2}\\)Antes de estudiar la forma explicita de los intervalos de confianza para una proporción, una media, y una varianza vamos ver una herramienta de simulación que nos permite comprobar el funcionamiento de dichos intervalos. El funcionamiento de la aplicación es:Fijar valor del parámetro poblacionalFijar el tamaño de la muestraEstablecer el límite de confianzaEstablecer el número de muestras de trabajo y simularlas representando los intervalos de confianza obtenidos.Podemos ver entonces cuantos de esos intervalos contienen al verdadero valor del parámetro.Acceder la aplicación","code":""},{"path":"inferencia-básica.html","id":"ic-para-una-proporción","chapter":"Unidad 4 Inferencia básica","heading":"4.3.3.1 IC para una proporción","text":"El intervalo de confianza al nivel \\(100*(1 - \\alpha)\\%\\) para una proporción poblacional viene dado por:\n\\[IC_{1-\\alpha}(\\theta) = \\left(\\hat{\\theta} - q_{1-\\alpha/2}*\\sqrt{\\frac{\\hat{\\theta} (1-\\hat{\\theta}))}{n}},\\hat{\\theta} + q_{1-\\alpha/2}*\\sqrt{\\frac{\\hat{\\theta} (1-\\hat{\\theta}))}{n}}\\right)\\]\ndonde \\(q_{1-\\alpha/2}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución Normal estándar, \\(\\hat{\\theta}\\) es la proporción muestral y \\(n\\) es el tamaño muestral.","code":""},{"path":"inferencia-básica.html","id":"ic-para-la-media","chapter":"Unidad 4 Inferencia básica","heading":"4.3.3.2 IC para la media","text":"El intervalo de confianza al nivel \\(100*(1 - \\alpha)\\%\\) para la media de una población Normal con varianza desconocida viene dado por:\\[IC_{1-\\alpha}(\\mu) = \\left(\\bar{x} - q_{1-\\alpha/2}*\\frac{s}{\\sqrt{n-1}},\\bar{x} + q_{1-\\alpha/2}*\\frac{s}{\\sqrt{n-1}}\\right)\\]\ndonde \\(q_{1-\\alpha/2}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución \\(t\\) se Student con \\(n-1\\) grados de libertad, \\(\\bar x\\) es la media muestral, \\(s\\) es la desviación típica muestral, y \\(n\\) es el tamaño de muestra.","code":""},{"path":"inferencia-básica.html","id":"intervalo-de-confianza-para-la-varianza","chapter":"Unidad 4 Inferencia básica","heading":"4.3.3.3 Intervalo de confianza para la varianza","text":"El intervalo de confianza al nivel \\(100*(1 - \\alpha)\\%\\) para la varianza de una población Normal viene dado por:\\[IC_{1-\\alpha}(\\sigma^2) = \\left(\\frac{(n-1)s^2}{q_{1-\\alpha/2}}, \\frac{(n-1)s^2}{q_{\\alpha/2}}\\right)\\]\ndonde \\(q_{1-\\alpha/2}\\) y \\(q_{\\alpha/2}\\) son los cuantiles \\(1-\\alpha/2\\) y \\(\\alpha/2\\) de una distribución \\(Chi^2\\) con \\(n-1\\) grados de libertad, \\(s^2\\) es la varianza muestral, y \\(n\\) es el tamaño de muestra.","code":""},{"path":"inferencia-básica.html","id":"contraste-de-hipótesis","chapter":"Unidad 4 Inferencia básica","heading":"4.3.4 Contraste de hipótesis","text":"Un procedimiento de contraste de hipótesis tiene por objetivo valorar la evidencia proporcionada por los datos favor de alguna hipótesis planteada sobre el parámetro o parámetros que identifican la población bajo estudio. En el caso más sencillo, imaginemos que tenemos un parámetro poblacional \\(\\theta\\) que puede tomar valores en el conjunto \\(\\Theta\\)Ejemplo: Estamos interesados en conocer si la proporción de alumnos que superan la asignatura de Estadística en la convocatoria de junio es mayor o igual al 50%. El parámetro de interés, \\(\\theta\\), es entonces la proporción de aprobados en la convocatoria de junio, y el conjunto de posibles valores de dicho parámetro puede tomar valores entre 0 y 100. El contraste de interés viene dado por:\\[\\theta \\geq 0.5\\]Para resolver cualquier procedimiento de contraste debemos establecer dos subconjuntos disjuntos del espacio paramétrico, \\(\\Theta_0\\) y \\(\\Theta_1\\) cumpliendo que \\(\\Theta = \\Theta_0 \\cup \\Theta_1\\), con los posibles valores del parámetro de interés que denominamos hipótesis:Hipótesis nula, que se denota por \\(H_0\\), y que generalmente expresa el valor o conjunto de valores del parámetro,\\(\\Theta_0\\), que corresponde con la idea que deseamos verificar.Hipótesis alternativa, que se denota por \\(H_a\\), y que generalmente expresa el valor o conjunto de valores complementarios, \\(\\Theta_1\\), los dados en la hipótesis nula. Formalmente, el problema de contraste de hipótesis se plantea como:\n\\[\\left\\{\\begin{array}{ll} H_0: & \\theta \\\\Theta_0\\\\ H_a: & \\theta \\\\Theta_1\\end{array}\\right.\\]Estas hipótesis se deben establecer antes de obtener la información muestral ya que deben reflejar conocimiento previo sobre el parámetro poblacional de interés. La muestra recogida aportará las evidencias suficientes para rechazar o rechazar la hipótesis nula planteada.En nuestro ejemplo\\[\\left\\{\\begin{array}{ll} H_0: & \\theta \\geq 0.5\\\\ H_a: & \\theta < 0.5\\end{array}\\right.\\]","code":""},{"path":"inferencia-básica.html","id":"posibles-contrastes","chapter":"Unidad 4 Inferencia básica","heading":"4.3.4.1 Posibles contrastes","text":"Dada la estructura del procedimiento de contraste se plantean únicamente dos tipos de posibilidades:Contraste bilateral: El conjunto de posibles valores del parámetro de interés establecidos en la hipótesis nula se concentra en un único valor, \\(\\theta_0\\), es decir,\n\\[\\left\\{\\begin{array}{ll} H_0: & \\theta = \\theta_0\\\\ H_a: & \\text{ } \\theta_0 \\end{array}\\right.\\]Contraste bilateral: El conjunto de posibles valores del parámetro de interés establecidos en la hipótesis nula se concentra en un único valor, \\(\\theta_0\\), es decir,\n\\[\\left\\{\\begin{array}{ll} H_0: & \\theta = \\theta_0\\\\ H_a: & \\text{ } \\theta_0 \\end{array}\\right.\\]Contraste unilateral: El conjunto de posibles valores del parámetro de interés establecidos en la hipótesis nula se concentra en un conjunto de valores dado por una desigualdad con respecto \\(\\theta_0\\), es decir,Contraste unilateral: El conjunto de posibles valores del parámetro de interés establecidos en la hipótesis nula se concentra en un conjunto de valores dado por una desigualdad con respecto \\(\\theta_0\\), es decir,\\[\\left\\{\\begin{array}{ll} H_0: & \\theta \\geq \\theta_0\\\\ H_a: & \\text{ } \\theta_0 \\end{array}\\right.\\]\n\\[\\left\\{\\begin{array}{ll} H_0: & \\theta \\leq \\theta_0\\\\ H_a: & \\text{ } \\theta_0 \\end{array}\\right.\\]","code":""},{"path":"inferencia-básica.html","id":"resolución-de-contraste","chapter":"Unidad 4 Inferencia básica","heading":"4.3.4.2 Resolución de contraste","text":"Para establecer evidencias favor de una hipótesis u otra se debe:Elegir un nivel de significación, \\(\\alpha\\), que refleja el riesgo que tomamos cuando rechazamos la hipótesis nula o probabilidad de rechazar la hipótesis nula cuando es cierta, es decir:\n\\[\\alpha = P(\\text{ Rechazar } H_0 | H_0 \\text{ cierta})\\]Normalmente se eligen valores pequeños, \\(\\alpha=0,1, 0.05, y 0.01\\) que resultan los equivalentes del 90%, 95%, y 99% al nivel de confianza en el proceso de estimación.Ejemplo. Si tomamos una significación del 5%, tendremos una probabilidad de 0.05 de rechazar la hipótesis nula cuando en realidad es cierta.Establecer estadístico de contraste (\\(EC\\)) que nos permita, utilizando la información muestral, estudiar la compatibilidad de dicha información con la hipótesis nula planteada. La forma habitual suele ser:\\[EC = \\frac{\\theta - \\hat{\\theta}}{es(\\hat{\\theta})}\\]\ndonde \\(\\hat{\\theta}\\) es un estimador puntual del parámetro poblacional y \\(es(\\hat{\\theta})\\) es una medida del error estándar que cometemos con el estimador utilizado.Para decidir sobre el contraste planteado utilizaremos el \\(p-valor\\), que representa la probabilidad de que el valor del EC sea superior al valor de dicho estadístico evaluado en los datos muestrales, \\(EC_{obs}\\),atendiendo la distribución en el muestreo, es decir,\\[p-valor = P(EC \\geq EC_{obs})\\]\nPara tomar una decisión sobre el contraste miramos si el p-valor obtenido y concluimos que:Si \\(p-valor < \\alpha\\), tenemos evidencias estadísticas suficientes para rechazar la hipótesis nula favor de la hipótesis alternativa.Si \\(p-valor > \\alpha\\), tenemos evidencias estadísticas suficientes para rechazar la hipótesis nula.continuación vemos como ampliar el procedimiento de contraste para una proporción y una media. El proceso de contraste sobre una varianza suele tener interés práctico ya que resulta más habitual obtener un intervalo de confianza. Como veremos en el tema siguiente, si que resulta relevante el proceso de contraste cuando se involucran dos poblaciones y estamos interesados en comparar la variabilidad de ambas.","code":""},{"path":"inferencia-básica.html","id":"una-proporción","chapter":"Unidad 4 Inferencia básica","heading":"4.3.4.3 Una proporción","text":"Tenemos una población de tipo Bernouilli que viene especificada partir de la proporción de sujetos que alcanzan el “éxito”, \\(\\theta\\), y una estimación de dicho parámetro dada por \\(\\hat{\\theta}\\). Anteriormente ya hemos visto que para una muestra de tamaño \\(n\\) el error estándar viene dado por:\\[es(\\hat{\\theta}) = \\sqrt{\\frac{\\hat{\\theta} (1-\\hat{\\theta}))}{n}}.\\]Tanto para el contraste unilateral como el bilateral el estadístico de contraste viene dado por:\n\\[EC = \\frac{\\theta - \\hat{\\theta}}{\\sqrt{\\frac{\\hat{\\theta} (1-\\hat{\\theta}))}{n}}}\\]\ncuya distribución en el muestreo es \\(N(0,1)\\). Dicho estadístico valora lo cerca que queda el estimador respecto del valor poblacional teniendo en cuenta el error cometido en el proceso de estimación.Ejemplo. Se está estudiando el efecto de los rayos X sobre la viabilidad huevo-larva en Tribolium casteneum. Se plantean tres situaciones: ) la proporción de viabilidad es del 50%, b) la proporción de viabilidad es superior al 60%, c) la proporción de viabilidad es inferior al 55%. Los contrastes asociados con cada situación son:\n\\[\\left\\{\\begin{array}{ll} H_0: & \\theta = 0.5\\\\ H_a: &  \\theta \\neq 0,5 \\end{array}\\right.\\]\n\\[\\left\\{\\begin{array}{ll} H_0: & \\theta \\geq 0.6\\\\ H_a: & \\theta < 0.6 \\end{array}\\right.\\]\n\\[\\left\\{\\begin{array}{ll} H_0: & \\theta \\leq 0.55\\\\ H_a: & \\theta > 0.55 \\end{array}\\right.\\]Para verificar dichios contarstes se irradiaron 1000 huevos de los que resultaron 572 larvas.Para resolver contrastes en estas situaciones utiliamos la función prop.test()El p-valor resultante es 6.124e-06 que resulta inferior al nivel de significación prefijado de 0.05. Por lo tanto hay evidencias para rechazar la hipótesis nula y concluir que la proporción de viabilidad una vez irradiamos los huevos con rayos X es distinta del 50%. El p-valor resultante es 0.03794 que resulta inferior al nivel de significación prefijado de 0.05. Por lo tanto hay evidencias para rechazar la hipótesis nula y concluir que la proporción de viabilidad una vez irradiamos los huevos es mayor o igual al 60%. El p-valor resultante es 0.08587 que resulta superior al nivel de significación prefijado de 0.05. Por lo tanto hay evidencias para rechazar la hipótesis nula y concluir que la proporción de viabilidad una vez irradiamos los huevos puede ser menor o igual al 55%. ","code":"\n# Situación 1\nn <- 1000\nlarvas <- 572\n# Debemos fijar el valor del contraste, el tipo (two.sided), y el nivel de significación o su análogo como el nivel de confianza\nprop.test(larvas, n, p = 0.5, alternative = \"two.sided\", conf.level = 0.95)## \n##  1-sample proportions test with continuity correction\n## \n## data:  larvas out of n\n## X-squared = 20.449, df = 1, p-value = 6.124e-06\n## alternative hypothesis: true p is not equal to 0.5\n## 95 percent confidence interval:\n##  0.5406126 0.6028273\n## sample estimates:\n##     p \n## 0.572\n# Situación 2\nn <- 1000\nlarvas <- 572\n# Debemos fijar el valor del contraste, el tipo (two.sided), y el nivel de significación o su análogo como el nivel de confianza\nprop.test(larvas, n, p = 0.6, alternative = \"less\", conf.level = 0.95)## \n##  1-sample proportions test with continuity correction\n## \n## data:  larvas out of n\n## X-squared = 3.151, df = 1, p-value = 0.03794\n## alternative hypothesis: true p is less than 0.6\n## 95 percent confidence interval:\n##  0.0000000 0.5980029\n## sample estimates:\n##     p \n## 0.572\n# Situación 2\nn <- 1000\nlarvas <- 572\n# Debemos fijar el valor del contraste, el tipo (two.sided), y el nivel de significación o su análogo como el nivel de confianza\nprop.test(larvas, n, p = 0.55, alternative = \"greater\", conf.level = 0.95)## \n##  1-sample proportions test with continuity correction\n## \n## data:  larvas out of n\n## X-squared = 1.8677, df = 1, p-value = 0.08587\n## alternative hypothesis: true p is greater than 0.55\n## 95 percent confidence interval:\n##  0.545601 1.000000\n## sample estimates:\n##     p \n## 0.572"},{"path":"inferencia-básica.html","id":"una-media","chapter":"Unidad 4 Inferencia básica","heading":"4.3.4.4 Una media","text":"Tenemos una población Normal que viene especificada partir de la media (\\(\\mu\\)) y varianza (\\(\\sigma^2\\)). Por el momento estamos interesados en los procedimientos de contraste sobre la media cuando desconocemos el valor de la varianza poblacional. Tomamos los estimadores habituales \\(\\bar X\\), \\(S^2\\), y consideramos el error estándar:\n\\[es(\\bar X) = \\frac{s}{\\sqrt{n-1}}.\\]\nTanto para el contraste unilateral como el bilateral el estadístico de contraste viene dado por:\n\\[EC = \\frac{\\mu - \\bar X}{\\frac{s}{\\sqrt{n-1}}}\\]\ncuya distribución en el muestreo es una \\(t\\) de Student con n-1 grados de libertad.Ejemplo. La concentración media de dióxido de carbono en el aire en una cierta zona es habitualmente mayor que 335 ppmv (partes por millon en volumen). Se sospecha que esta concentración es mayor en la capa de aire más próxima la superficie. Los investigadores quieren comprobar si la concentración en la superficie es mayor o igual dicho valor con una significación de 0.05. Se plantea el contraste:\\[\\left\\{\\begin{array}{ll} H_0: & \\mu \\geq 335\\\\ H_a: & \\mu < 335 \\end{array}\\right.\\]\nPara tratar de ratificar su hipótesis Se ha analizado el aire en 20 puntos elegidos aleatoriamente una misma altura cerca del suelo, resultando los siguientes datos: 332, 320, 312, 270, 330, 354, 356, 310, 341, 313, 223, 224, 305, 321, 325, 333, 332, 345, 312, 331.Para resolver este contraste utilizamos la función t.test()El p-valor resultante es 0.01038 que resulta inferior al nivel de significación prefijado de 0.05. Por lo tanto hay evidencias para rechazar la hipótesis nula y concluir que la media del nivel de dióxido de carbono en la capa de aire más cercana la superficie es menor 335 ppmv.","code":"\ndatos <- c(332, 320, 312, 270, 330, 354, 356, 310, 341, 313, 223, 224, 305, 321, 325, 333, 332, 345, 312, 331)\n# Debemos fijar el valor del contraste, el tipo (two.sided), y el nivel de significación o su análogo como el nivel de confianza\nt.test(datos, mu = 335, alternative = \"less\", conf.level = 0.95)## \n##  One Sample t-test\n## \n## data:  datos\n## t = -2.5219, df = 19, p-value = 0.01038\n## alternative hypothesis: true mean is less than 335\n## 95 percent confidence interval:\n##      -Inf 328.5403\n## sample estimates:\n## mean of x \n##    314.45"},{"path":"inferencia-básica.html","id":"inferencia-para-dos-poblaciones","chapter":"Unidad 4 Inferencia básica","heading":"4.4 Inferencia para dos poblaciones","text":"En este sección completamos el estudio inferencial visto en los puntos anteriores. Más concretamente se muestra el análisis inferencial para la comparación de dos proporciones o dos medias. En la situación de la comparación de dos medias veremos también la influencia que tiene el estudio de las varianzas de ambas poblaciones. se presentan todas las formulaciones y distribuciones asociadas con estos análisis sino que se presenta directamente la forma de resolverlos. se pueden consultar los desarrollos estadísticos en cualquier libro de estadística básica.Poblaciones Binomiales Sean dos poblaciones sobre las que se desea estudiar una misma característica de interés de tipo discreto (1 = éxito; 0 = fracaso), que identificamos por \\(X_{P1}\\) y \\(X_{P2}\\) respectivamente. El parámetro de interés en cada población es la proporción de éxito, \\(\\theta_1\\) y \\(\\theta_2\\) respectivamente, pero el interés inferencial principal es la comparación de \\(\\theta_1\\) y \\(\\theta_2\\), es decir, comprobar si la proporción de éxito en la población 1 es comparable con la proporción de éxito en la población 2. Para realizar dicha comparación se utiliza el parámetro que viene dado por la diferencia de proporciones de éxito:\n\\[\\theta_1 - \\theta_2\\]Si las proporciones son iguales la diferencia debería estar próximo cero, mientras que si son distintas la diferencia sería estadísticamente diferente cero.Poblaciones Normales Sean dos poblaciones normales sobre las que se desea estudiar una misma característica de interés de tipo continuo que identificamos por \\(X_{P1}\\) y \\(X_{P2}\\) respectivamente. Cada población viene caracterizada por su media y varianza, es decir,\n\\[X_{P1} \\sim N(\\mu_1,\\sigma^2_1) \\text{    ;  }  X_{P2} \\sim N(\\mu_2,\\sigma^2_2)\\]\nEn este caso el proceso inferencial se centras en todos los parámetros, medias y varianzas, pero habitualmente el objetivo inferencial principal se centra en comprobar si las medias de ambas poblaciones pueden considerarse iguales o diferentes. Por tanto, el parámetro de interés es la diferencia de medias poblacionales:\n\\[\\mu_1 - \\mu_2\\]\nComo ocurre con las proporciones, se considera que las medias son iguales cuando la diferencia de las medias es estadísticamente cero. Sin embargo, para poder realizar dicho estudio es necesario conocer en primer lugar si las varianzas de ambas poblaciones pueden considerarse iguales o distintas. En función del resultado de dicha comparación se deberá utilizar un proceso inferencial diferente para la comparación de medias. Dado que las varianzas siempre son positivas el parámetro de interés para la comparación de varianzas viene dado por su cociente:\n\\[\\frac{\\sigma^2_1}{\\sigma^2_1}\\]","code":""},{"path":"inferencia-básica.html","id":"inferencia-para-dos-proporciones","chapter":"Unidad 4 Inferencia básica","heading":"4.5 Inferencia para dos proporciones","text":"Dada una muestra aleatoria en cada una de las poblaciones de interés de tamaños \\(n_1\\) y \\(n_2\\), utilizamos los estimadores habituales de la proporción poblacional dados por las proporciones muestrales \\(\\hat{\\theta}_1\\) y \\(\\hat{\\theta}_2\\). Como ya hemos dicho el parámetro objetivo en esta situación es la diferencia de proporciones poblacionales.Ejemplo. La angina de pecho es una afección cardíaca en la que el paciente sufre ataque períodicos de dolor. En un estudio para analizar la efectividad de una nueva droga para prevenir dichos ataques se han seleccionado dos grupos de sujetos. Al primero de ellos se les dará la nueva droga mientras que al otro se les dará el tratamiento estándar. Los resultados obtenidos después de un periodo de 28 semanas viene dados en la tabla siguiente:Se está interesado en conocer con una confianza del 90% (significación de 0.1) si la porporción de pacientes mejorados con la nueva droga es diferente con respecto la droga antigua.","code":"\n# carga de datos \nmuestra <- c(160,147) \nmejoras <- c(44,19) \n# Tabla \nres <- data.frame(mejoras, muestra)\ncolnames(res) <- c(\"mejoras\",\"muestra\")\nres##   mejoras muestra\n## 1      44     160\n## 2      19     147"},{"path":"inferencia-básica.html","id":"estimador-puntual","chapter":"Unidad 4 Inferencia básica","heading":"4.5.1 Estimador puntual","text":"El estimador puntual de la diferencia de proporciones poblacionales se consigue partir de los estimadores puntuales de cada una de las proporciones de éxito muestrales.Para los datos de nuestro ejemplo, si la población 1 identifica los usjetos que toman la nueva droga y la pobalción 2 los que toman la droga antigua, tendríamos:\n\\(\\widehat{\\theta_1 - \\theta_2} = \\widehat{\\theta_1} - \\widehat{\\theta_2} = \\frac{44}{160} - \\frac{19}{147} = 0.1457\\)Se observa una diferencia en la mejora de los sujetos del 14.57% de los que toman la droga nueva frente los que toman la droga estándar.","code":""},{"path":"inferencia-básica.html","id":"estimador-por-intervalos-de-confianza","chapter":"Unidad 4 Inferencia básica","heading":"4.5.2 Estimador por intervalos de confianza","text":"Para obtener el intervalo de confianza para la diferencia de proporciones utilizamos la función prop.test(). Esta función también nos permite realizar el correspondiente contarte pero por le momento solo pediremos los resultados referidos al intervalo de confianza.Para los datos de nuestro ejemplo el intervalo de confianza al 90% indica que la diferencia de proporciones de mejora entre los que usan la droga nueva frente os que usan la droga estándar se sitúa entre el 6.5% y el 22.6%.","code":"\nanalisis <- prop.test(mejoras,muestra,conf.level = 0.90)\n# Intervalo de confianza\nanalisis$conf.int## [1] 0.0654468 0.2260498\n## attr(,\"conf.level\")\n## [1] 0.9"},{"path":"inferencia-básica.html","id":"contraste-de-hipótesis-1","chapter":"Unidad 4 Inferencia básica","heading":"4.5.3 Contraste de hipótesis","text":"El contraste habitual en esta situación viene dado por:\\[\\left\\{\\begin{array}{ll} H_0: & \\theta_1 = \\theta_2\\\\ H_a: & \\theta_1 \\neq \\theta_2 \\end{array}\\right.\\]\ndonde estamos interesados en verificar si las proporciones de éxito poblacionales pueden considerarse iguales o distintas.Para los datos de nuestro ejemplo tenemos:Dado que el pvalor obtenido (0.00255) es inferior al nivel de siginificación prefijado (0.1) hay eviedencias estadísticas para rechazar la hipótesis nula, es decir, hay evidendaicas para concluir que las proporciones de mejora con mabsa drogas son distintas. Además el intervalo de confianza ya nos indicaba qye dicha mejoría era favor de la droga nueva con los valores obtenidos en el aprtado anterior","code":"\nanalisis <- prop.test(mejoras,muestra,conf.level = 0.90)\n# Resultados completos\nanalisis## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  mejoras out of muestra\n## X-squared = 9.1046, df = 1, p-value = 0.00255\n## alternative hypothesis: two.sided\n## 90 percent confidence interval:\n##  0.0654468 0.2260498\n## sample estimates:\n##    prop 1    prop 2 \n## 0.2750000 0.1292517"},{"path":"inferencia-básica.html","id":"inferencia-para-dos-medias","chapter":"Unidad 4 Inferencia básica","heading":"4.6 Inferencia para dos medias","text":"Los problemas de inferencia asociados con la comparación de dos medias poblacionales para variables Normales presentan diferentes situaciones:Estudio de dos poblaciones independientes con variabilidades igualesEstudio de dos poblaciones independientes con variabilidades distintasEstudio de la evolución de una población (medidas antes - después)continuación se detalla como realizar el análisis de cada uno de ellos, pero antes de pasar con ellos debemos estudiar el problema de como comparar las variabilidades en dos poblaciones independientes. Presentamos en primer lugar los diferentes ejemplos de trabajo.Ejemplo 1. Para realizar un estudio de la concentración de una hormona en una solución vamos utilizar dos métodos. Disponemos de 10 dosis preparadas en el laboratorio y medimos la concentración de cada una con los dos métodos. Se obtienen los siguientes resultados:Se desea realizar el estudio inferencial con una confianza del 95%.Ejemplo 2. Una compañía contrata 10 tubos con filamentos del tipo y 12 tubos con filamentos del tipo B. Las duraciones medias observadas se muestran en la siguiente tabla:Se desea realizar el estudio inferencial con una confianza del 90%.Ejemplo 3. En una unidad del sueño se está probando con un nuevo somnífero. Para comprobar su eficacia se toman 10 individuos al azar. Un día se les suministra el somnífero y se les anota el número de horas de sueño, al día siguiente se les suministra y se vuelve comprobar las horas de sueño. Los resultados entes y después del tratamiento han sido los siguientes:Se desea realizar el estudio inferencial con una confianza del 90%.","code":""},{"path":"inferencia-básica.html","id":"análisis-de-dos-varianzas-poblacionales","chapter":"Unidad 4 Inferencia básica","heading":"4.6.1 Análisis de dos varianzas poblacionales","text":"Supongamos que tenemos dos poblaciones Normales y que deseamos comprobar si la variabilidad en ambas poblaciones pueden considerarse estadísticamente iguales o distintas. Como ya vimos en la introducción este problema se reduce la comparación de ambas varianzas través del cociente de ambas. Para resolver este problema utilizamos la función var.test(). El contraste utilizado es:\\[\\left\\{\\begin{array}{ll} H_0: & \\frac{\\sigma^2_1}{\\sigma^2_2} = 1\\\\ H_a: & \\frac{\\sigma^2_1}{\\sigma^2_2} \\neq 1 \\end{array}\\right.\\]Para los datos del ejemplo 1Dado que el pvalor resultante es superior la significatividad prefijada, tenemos evidencias estadísticas para rechazar la hipótesis nula, y por tanto concluir que ambas varianzas pueden considerarse distintas.Para los datos del ejemplo 2Puesto que el pvalor es inferior la significatividad prefijada podemos concluir que hya evidencias estad´sitivas apra concluir que las varaibilidades en ambas poblaciones pueden considerarse distintas.En todos los análisis inferenciales asociados con la comparación de dos medias utilizamos la función t.test(), aunque con difrentes opciones en función de que las varianzas sean iguales o , o de que las muestras sean independientes o .*","code":"\n# Cargamos los datos\nMetodoA <- c(10.7, 11.2, 15.3, 14.9, 13.9, 15, 15.6, 15.7, 14.3, 10.8)\nMetodoB <- c(11.1, 11.4, 15, 15.1, 14.3, 15.4, 15.4, 16, 14.3, 11.2)\nvar.test(MetodoA, MetodoB, conf.level = 0.95)## \n##  F test to compare two variances\n## \n## data:  MetodoA and MetodoB\n## F = 1.1229, num df = 9, denom df = 9, p-value = 0.8657\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  0.2789187 4.5208902\n## sample estimates:\n## ratio of variances \n##           1.122925\n# Cargamos los datos\nTipoA <- c(1614,1094,1293,1643,1466,1270,1340,1380,1081,1497)\nTipoB <- c(1383,1138,920,1143,1017,961,1627,821,1711,865,1662,1698)\nvar.test(TipoA, TipoB, conf.level = 0.9)## \n##  F test to compare two variances\n## \n## data:  TipoA and TipoB\n## F = 0.3052, num df = 9, denom df = 11, p-value = 0.08543\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 90 percent confidence interval:\n##  0.1053789 0.9468807\n## sample estimates:\n## ratio of variances \n##          0.3052007"},{"path":"inferencia-básica.html","id":"dos-medias-para-poblaciones-independientes-con-varianzas-iguales","chapter":"Unidad 4 Inferencia básica","heading":"4.6.2 Dos medias para poblaciones independientes con varianzas iguales","text":"El contraste de hipótesis para esta situación viene dado por:\\[\\left\\{\\begin{array}{ll} H_0: & \\mu_1 = \\mu_2\\\\ H_a: & \\mu_1 \\neq \\mu_2 \\end{array}\\right.\\]Utilizamos los datos del ejemplo 1, ya que como hemos visto anteriormente las varianzas de ambas pobalciones puden considerarse igualesDado que el pavalor es superior la significatividad prefijada, hay evidencias estadísticas para concluir que las medias de concentración con ambos métodos pueden considerarse iguales.","code":"\n# Cargamos los datos\nMetodoA <- c(10.7, 11.2, 15.3, 14.9, 13.9, 15, 15.6, 15.7, 14.3, 10.8)\nMetodoB <- c(11.1, 11.4, 15, 15.1, 14.3, 15.4, 15.4, 16, 14.3, 11.2)\nt.test(MetodoA, MetodoB, alternative = \"two.sided\", var.equal = TRUE, conf.level = 0.95)## \n##  Two Sample t-test\n## \n## data:  MetodoA and MetodoB\n## t = -0.20323, df = 18, p-value = 0.8412\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -2.040763  1.680763\n## sample estimates:\n## mean of x mean of y \n##     13.74     13.92"},{"path":"inferencia-básica.html","id":"dos-medias-para-poblaciones-independientes-con-varianzas-distintas","chapter":"Unidad 4 Inferencia básica","heading":"4.6.3 Dos medias para poblaciones independientes con varianzas distintas","text":"El constaste de hipótesis en esta situación es el mismo que en el punto anterior.Utilizamos los datos del ejemplo 2, ya que como hemos visto anteriormente las varianzas de ambas pobalciones puden considerarse distintasDado que el pvalor es superior la significatividad prefijada, hay evidencias estadísticas para concluir que las medias de duración de los filamentos en ambos tipos pueden considerarse iguales.","code":"\n# Cargamos los datos\nTipoA <- c(1614,1094,1293,1643,1466,1270,1340,1380,1081,1497)\nTipoB <- c(1383,1138,920,1143,1017,961,1627,821,1711,865,1662,1698)\nt.test(TipoA, TipoB, alternative = \"two.sided\", var.equal = TRUE, conf.level = 0.9)## \n##  Two Sample t-test\n## \n## data:  TipoA and TipoB\n## t = 0.98508, df = 20, p-value = 0.3364\n## alternative hypothesis: true difference in means is not equal to 0\n## 90 percent confidence interval:\n##  -91.82747 336.42747\n## sample estimates:\n## mean of x mean of y \n##    1367.8    1245.5"},{"path":"inferencia-básica.html","id":"dos-medias-para-poblaciones-emparejadas","chapter":"Unidad 4 Inferencia básica","heading":"4.6.4 Dos medias para poblaciones emparejadas","text":"Es muy habitual que en ciertas situaciones experimentales nos encontramos que queremos estudiar la evolución (medidas antes -después) de un grupo de sujetos después de ser sometidos cierta prueba experimental. En esta caso tenemos dos poblaciones independientes sino sólo una que medimos en dos ocasiones. Por tanto, los procedimientos anteriores tienen que ser modificados para tener en cuenta esta situación. El constaste de hipótesis para esta situación viene dado por:\\[\\left\\{\\begin{array}{ll} H_0: & \\mu_{antes} = \\mu_{despues}\\\\ H_a: & \\mu_{antes} \\neq \\mu_{despues} \\end{array}\\right.\\]De nuevo podemos utilizar la función t.test() con el parámetro paired.Utilizamos los datos del ejemplo 3, donde tenemos una única muestra de sujetosDado que el pvalor es superior la significatividad prefijada, hay evidencias estadísticas para concluir que las horas medias de sueño antes y después de tomar el somnifero pueden considerarse estadísticamente distintas.","code":"\n# Cargamos los datos\nantes <- c(7.3,8.2,6.3,5.2,6.9,5.8,5.3,7.1,6.9,8.1)\ndespues <- c(8.2,7.9,6.4,5.1,7.1,6.3,5.9,8.2,7.1,7.7)\nt.test(antes, despues, alternative = \"two.sided\", paired = TRUE, var.equal = TRUE, conf.level = 0.9)## \n##  Paired t-test\n## \n## data:  antes and despues\n## t = -1.7925, df = 9, p-value = 0.1066\n## alternative hypothesis: true difference in means is not equal to 0\n## 90 percent confidence interval:\n##  -0.566341394  0.006341394\n## sample estimates:\n## mean of the differences \n##                   -0.28"},{"path":"inferencia-básica.html","id":"procedimientos-no-paramétricos","chapter":"Unidad 4 Inferencia básica","heading":"4.7 Procedimientos no paramétricos","text":"Todos los procedimientos de inferencia sobre dos medias se basan en la suposición de que las variables sobre las que estamos trabajando se puede considerar que se distribuyen normalmente. Sin embargo, cuando tenemos tamaños muestrales pequeños o simplemente por el tipo de variable que estamos midiendo, dicha suposición resulta creible y es necesario comporbarla antes de poder aplicar estos procedimientos. SI la distribución resulta Normal podemos utilizar los procedimientos denominados parámetricos para la comparación de dos medias. continuación presentamso el test de normalidad y los contrastes paramétricos.","code":""},{"path":"inferencia-básica.html","id":"normalidad","chapter":"Unidad 4 Inferencia básica","heading":"4.7.1 Normalidad","text":"Este requisito implica que la variable objetivo tiene que distribuirse según una normal para cualquiera de las pobalciones donde se pueda medir. El test utilizado para resolver este problema es el de Shapiro-Wilks. En R utilizamos la función shapiro.test() para concluir estadísticamente sobre este contraste. Siempre utilizamos significatividad de 0.05 en estas situaciones.Para los datos del ejemplo 1 comprobamos si los datos muestrales en cada población pueden considerarse que se distribuyen según una normal.En ambos casos las conclusión es que rechazamos que los datos se distribuyan según una normal, ya que el pvalor es inferior la significatividad prefijada.Para los datos del ejemplo 2:En ambos casos podemos rechzar que los datos sean normales, ya que el pvalor es superior la significatividad.","code":"\n# Cargamos los datos\nMetodoA <- c(10.7, 11.2, 15.3, 14.9, 13.9, 15, 15.6, 15.7, 14.3, 10.8)\nMetodoB <- c(11.1, 11.4, 15, 15.1, 14.3, 15.4, 15.4, 16, 14.3, 11.2)\nshapiro.test(MetodoA)## \n##  Shapiro-Wilk normality test\n## \n## data:  MetodoA\n## W = 0.8058, p-value = 0.01705\nshapiro.test(MetodoB)## \n##  Shapiro-Wilk normality test\n## \n## data:  MetodoB\n## W = 0.80577, p-value = 0.01704\n# Cargamos los datos\nTipoA <- c(1614,1094,1293,1643,1466,1270,1340,1380,1081,1497)\nTipoB <- c(1383,1138,920,1143,1017,961,1627,821,1711,865,1662,1698)\nshapiro.test(TipoA)## \n##  Shapiro-Wilk normality test\n## \n## data:  TipoA\n## W = 0.95009, p-value = 0.6696\nshapiro.test(TipoB)## \n##  Shapiro-Wilk normality test\n## \n## data:  TipoB\n## W = 0.86377, p-value = 0.05451"},{"path":"inferencia-básica.html","id":"dos-medianas-en-poblaciones-independientes","chapter":"Unidad 4 Inferencia básica","heading":"4.7.2 Dos medianas en poblaciones independientes","text":"El test paramétrico se centra en la comparación de medias sino en la comparación de las medianas. Esto es así porque uno de los incumplimientos más habituales de la normalidad es porque los datos son simétricos, es decir, la media coincide con la mediana. Para resolver este contraste utilizamos el test de Wilcoxon y su función en R wilcox.test().Dado que los datos del ejmplo 1 pueden considerarse normales, utilizamos el test paramétrico apra concluir si las medianas de ambas poblaciones pueden considerarse iguales o distintas. Fijamos la significatividad en 0.05.Dado que el pvalor es superior la significatividad, tenemos evidencias para concluir que podemos considerar que las medianas de ambas poblaciones sean distintas.","code":"\n# Cargamos los datos\nMetodoA <- c(10.7, 11.2, 15.3, 14.9, 13.9, 15, 15.6, 15.7, 14.3, 10.8)\nMetodoB <- c(11.1, 11.4, 15, 15.1, 14.3, 15.4, 15.4, 16, 14.3, 11.2)\nwilcox.test(MetodoA,MetodoB)## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  MetodoA and MetodoB\n## W = 44, p-value = 0.6768\n## alternative hypothesis: true location shift is not equal to 0"},{"path":"inferencia-básica.html","id":"dos-medianas-en-poblaciones-dependientes","chapter":"Unidad 4 Inferencia básica","heading":"4.7.3 Dos medianas en poblaciones dependientes","text":"También existe una versión del test de wilcoxon para la comparación en poblaciones dependientes. Este es muy habitual ya que en este tipo de situaciones lo nomral es tener pocos sujetos, y por tanto la hipótesis de normalidad es muy difícil de verificar.Para los datos del ejemplo 3 tenemos:Dado que el pvalor es superior la significatividad, podemos conluir que la mediana antes y después pueden considerarse distintas.","code":"\n# Cargamos los datos\nantes <- c(7.3,8.2,6.3,5.2,6.9,5.8,5.3,7.1,6.9,8.1)\ndespues <- c(8.2,7.9,6.4,5.1,7.1,6.3,5.9,8.2,7.1,7.7)\nwilcox.test(antes, despues,paired = TRUE)## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  antes and despues\n## V = 12.5, p-value = 0.1389\n## alternative hypothesis: true location shift is not equal to 0"},{"path":"modelstats.html","id":"modelstats","chapter":"Unidad 5 Modelos estadísticos","heading":"Unidad 5 Modelos estadísticos","text":"De forma habitual, cuando el investigador (o investigadores) se plantea un diseño experimental y comienza con la recogida de datos es porque persigue el estudio de o verificación de un objetivo planteado sobre la población bajo estudio. Estos objetivos se suelen establecer en base teorías o hipótesis que se desean verificar sobre le funcionamiento de la población bajo ciertas condiciones experimentales. Por ejemplo:Teorías que establezcan la posible relación entre dos características de la población.Teorías que plateen la idea de comportamientos distintas para una característica de la población\nen función de una variable que clasifica los sujetos bajo estudio en diferentes grupos.Es entonces cuando la modelización estadística interviene y el analista busca el mejor modelo que ajusta los datos disponibles y proporciona predicciones fiables. El objetivo de la modelización estadística es el planteamiento de una expresión matemática que representa el comportamiento general de la población bajo estudio, teniendo en cuenta el diseño experimental establecido y el objetivo u objetivos que se desean verificar","code":""},{"path":"modelstats.html","id":"componentes-del-modelo","chapter":"Unidad 5 Modelos estadísticos","heading":"5.1 Componentes del modelo","text":"Un primer paso en la modelización estadística es el planteamiento de una expresión matemática que represente el comportamiento general de la población bajo estudio teniendo en cuenta el diseño experimental establecido y el objetivo u objetivos que se desean verificar. Esto es lo que se conoce como componente sistemática del modelo y se basa únicamente en la parte controlada del diseño experimental. Por ejemplo, si nos planteamos como objetivo conocer la suma de dos números \\(\\) y \\(b\\), la función matemática (sistemática) que permite expresar la suma de forma única es \\(+b\\). Esta componente sistemática es una función determinista, pues siempre proporciona el mismo resultado si los valores de entrada son iguales. Al proponer la parte sistemática (o determinista) de un modelo será siempre necesario concretar la variable que se asocia al objetivo o hipótesis planteada sobre la población (representada por \\(Y\\)) y la variable o variables \\((X_1, X_2,…)\\) relacionadas o supuestamente relacionadas con ella través de la función matemática especificada.Supongamos un diseño experimental en el que tenemos una variable \\(Y\\) que está ligada directamente con el objetivo de la investigación, y un conjunto de variables \\(X_1, X_2,…,X_k\\), que se supone que pueden influir en el comportamiento de \\(Y\\). Habitualmente \\(Y\\) se la denomina variable respuesta o variable dependiente y las \\(X’s\\) variables predictoras, variables explicativas e incluso covariables cuando se trata de variables de tipo numérico continuo. Cuando las variables \\(X\\) son de tipo categórico se suelen denominar factores explicativos o de clasificación. las variables \\(X\\) se las suele denominar también variables independientes, asumiendo independencia entre ellas, aunque esta acepción puede estar algo alejada de la realidad como discutiremos más adelante; en adelante utilizaremos esta denominación y optaremos por cualquiera de las anteriormente presentadas. En la situación más sencilla donde la respuesta puede venir influenciada de forma directa por las posibles predictoras, la respuesta media (\\(\\hat{Y}\\)) se puede modelizar través de una función \\(f\\) que describe la componente sistemática del modelo:\\[\\begin{equation}\n  \\hat{Y} = f(X_1,X_2,...,X_k)\n  \\tag{5.1}\n\\end{equation}\\]Si nuestro modelo es adecuado, esta función debe reflejar el comportamiento medio esperado de la variable respuesta. Dado que sujetos distintos con los mismos valores de las \\(X’s\\) producirán generalmente valores distintos en la respuesta, se hace necesaria la introducción de una componente variable en el modelo. Esta componente se denomina componente aleatoria y está relacionada directamente con la variabilidad de los sujetos en la respuesta para una misma combinación de valores de las variables predictoras. La denotaremos habitualmente por:\\[\\begin{equation}\n  \\epsilon\n  \\tag{5.2}\n\\end{equation}\\]que es una variable aleatoria con distribución de probabilidad \\(F\\).\nAsumiendo que (5.1) y (5.2) tienen un efecto aditivo sobre la respuesta, nuestro modelo base de partida vendrá dado por la expresión:\\[\\begin{equation}\n  Y = \\hat{Y} + \\epsilon = f(X_1,X_2,...,X_k) + \\epsilon \n  \\tag{5.3}\n\\end{equation}\\]En función del tipo de variable respuesta, las predictoras, de la relación que se pueden establecer entre ellas través de \\(f\\), y del establecimiento de las estructuras aleatorias \\(F\\) para los errores tendremos diferentes tipos de modelos. lo largo de esta materia veremos las diferentes posibilidades de modelización.","code":""},{"path":"modelstats.html","id":"tipos-de-modelos","chapter":"Unidad 5 Modelos estadísticos","heading":"5.2 Tipos de modelos","text":"En función del tipo de variable respuesta, las predictoras, de la relación que se pueden establecer entre ellas través de \\(f\\), y del establecimiento de las estructuras aleatorias \\(F\\) para los errores tendremos diferentes tipos de modelos. lo largo de esta materia veremos las diferentes posibilidades de modelización. lo largo de las unidades siguientes iremos estudiando las características de los diferentes modelos, pero estos se pueden agrupar en dos grandes apartados:Modelos Lineales (LM), que engloban los modelos de regresión, los modelos ANOVA y los modelos ANCOVA.Modelos Lineales Generalizados (GLM), que engloba los modelos de respuesta binomial (modelos de regresión logística), modelos de respuesta poisson, modelos para tablas de contingencia (modelos log-lineales), y modelos de supervivencia.Introduciremos además los modelos de suavizado y una breve introducción los modelos de efectos aleatorios, que pueden ser utilizados en conjunción con los LM y los GLM.","code":""},{"path":"modelstats.html","id":"fases-en-la-construcción-de-un-modelo","chapter":"Unidad 5 Modelos estadísticos","heading":"5.3 Fases en la construcción de un modelo","text":"El proceso de modelización y análisis estadístico de un banco de datos se puede estructurar según las siguientes pautas de actuación:Contextualización del problema. Definición de objetivos y variables.Diseño del experimento y recogida de información.Registro y procesado previo de la información disponible.Inspección gráfica e identificación de tendencias.Consideración de hipótesis distribucionales y relacionales. Propuesta de modelización.Ajuste del modelo. Comparación y selección del mejor modelo.Diagnóstico y validación del modelo ajustado.Valoración de la capacidad predictiva del modelo y predicción.Interpretación y conclusiones.Si la revisión/validación del modelo nos lleva descartarlo (punto 7), será preciso una nueva propuesta, de modo que entraremos en un bucle entre los puntos (5)-(7) que culminará cuando quedemos satisfechos con el diagnóstico y la validación del modelo.la hora de representar gráficamente la información de cada banco de datos tendremos en cuenta esta serie de principios básicos:La información asociada con la variable respuesta que identifica el objetivo del estudio debe situarse siempre en el eje Y o eje de ordenadas.El tipo de las variables que pueden influir en nuestra variable objetivo condiciona el tipo de gráfico. Así si estas son de tipo numérico debemos realizar un gráfico de dispersión, situando cada una de las variables predictoras \\(X\\) en el eje de abcisas. Si las predictoras son de tipo categórico deberemos realizar un gráfico de cajas, visualizando las distintas categorías en el eje X (si bien siempre podremos invertir los ejes para mostrar las cajas en sentido horizontal y vertical).Si combinamos variables de tipo numérico y categórico debemos realizar gráficos múltiples de dispersión donde mostremos la relación \\(Y\\) versus \\(X\\) para las variables numéricas en cada uno de los niveles de las variables \\(X\\) categóricas.","code":""},{"path":"rls.html","id":"rls","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"Unidad 6 Regresión Lineal Simple (RLS)","text":"Nos preocupamos en este tema del Modelo de Regresión Lineal Simple (RLS), que podemos catalogar como el modelo lineal más sencillo, través del cual pretendemos explicar (predecir) una variable respuesta continua \\(Y\\) partir de una variable predictora también continua \\(X\\). Tal modelo vendrá justificado por unos buenos resultados previos en el análisis de correlación (lineal) entre las dos variables en cuestión.En el experimento o estudio del que obtenemos los datos, los valores de \\(Y\\) se han observado y los de \\(X\\), bien se han observado, bien se han prefijado por parte del investigador. En cualquier caso, asumimos que la aleatoriedad (incertidumbre) está contenida sólo en la variable \\(Y\\), mientras que la \\(X\\) carece de aleatoriedad y simplemente informa de lo que ocurre en los valores observados. La variable explicativa puede ser, tanto una causa de la respuesta, como un mero testigo que informa sobre cómo varía la respuesta.De ahora en adelante denotamos por \\((x_1, x_2,...,x_n)\\) e \\((y_1, y_2,...,y_n)\\) los valores observados de las variables \\(X\\) e \\(Y\\) en un experimento dado.","code":""},{"path":"rls.html","id":"bancos-de-datos","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.1 Bancos de datos","text":"Presentamos continuación los bancos de datos con los que trabajamos lo largo de esta unidad.Ejemplo 1. Datos de Corrosión. Treinta aleaciones del tipo 90/10 Cu-Ni, cada una con un contenido específico de hierro son estudiadas bajo un proceso de corrosión. Tras un período de 60 días se obtiene la pérdida de peso (en miligramos al cuadrado por decímetro y día) de cada una de las aleaciones debido al proceso de corrosión. El objetivo es estudiar el nivel de corrosión en función del contenido de hierro. continuación se presenta el banco de datos y se realiza la primera inspección gráfica.\nFigura 6.1: Gráfico de dispersión de pérdida de peso vs contenido en hierro.\nEn la figura 6.1 se observa cómo al ir aumentando el contenido en hierro de la aleación disminuye linealmente la pérdida de peso. El modelo estadístico que propongamos deberá ser capaz de explicar dicho comportamiento.Ejemplo 2. Datos de Papel Queremos estudiar la relación existente entre la concentración de madera contenida en la pulpa partir de la que se elabora papel (madera), y la resistencia (tension, en términos de tensión que soporta) del papel resultante. El objetivo del análisis es describir la tendencia observada. continuación se presenta el banco de datos y se realiza la primera inspección gráfica.\nFigura 6.2: Gráfico de dispersión de resistencia del papel vs concentración de madera.\nEn la figura 6.2 podemos ver cómo la resistencia del papel crece al aumentar la concentración de madera hasta llegar valores de 9 y disminuye partir de ese valor. En este caso la relación apreciada es de tipo parabólico (descrita por una parábola). Este hecho se debe tener en cuenta en la propuesta de un modelo preliminar.Ejemplo 3. Datos de Viscosidad. Se ha realizado un experimento para tratar de conocer la viscosidad de cierto compuesto en función de la cantidad de un tipo der aceite que se usa en su fabricación. Se asume una relación de tipo lineal entre la viscosidad y la cantidad de aceite utilizada.\nFigura 6.3: Gráfico de dispersión de viscosidad vs cantidad de aceite.\n","code":"\nhierro <- c(0.01, 0.48, 0.71, 0.95, 1.19, 0.01, 0.48, 1.44, 0.71, \n            1.96, 0.01, 1.44, 1.96)\npeso <- c(127.6, 124, 110.8, 103.9, 101.5, 130.1, 122, 92.3, 113.1, \n          83.7, 128, 91.4, 86.2)\ncorrosion <- data.frame(hierro,peso)\nggplot(corrosion, aes(x = hierro, y = peso)) +\n  geom_point() +\n  labs(x = \"Contenido en hierro\", y = \"Pérdida de peso\") \nmadera <- c(1, 1.5, 2, 3, 4, 4.5, 5, 5.5, 6, 6.5, 7, 8, 9, 10, 11, \n            12, 13, 14, 15)\ntension <- c(6.3, 11.1, 20.0, 24, 26.1, 30, 33.8, 34, 38.1, 39.9, 42,\n             46.1, 53.1, 52, 52.5, 48, 42.8, 27.8, 21.9)\npapel <- data.frame(madera, tension)\nggplot(papel, aes(x = madera, y = tension)) +\n  geom_point() +\n  labs(x = \"Concentración de madera\", y = \"Resistencia del papel\") \naceite <- c(0, 12, 24, 36, 48, 60, 0, 12, 24, 36, 48, 60, 0, 12, 24,\n            36, 48, 60, 12, 24, 36, 48, 60)\nviscosidad <- c(26, 38, 50, 76, 108, 157, 17, 26, 37, 53, 83, 124, 13,\n                20, 27, 37, 57, 87, 15, 22, 27, 41, 63)\naceites<-data.frame(aceite, viscosidad)\nggplot(aceites, aes(x = aceite, y = viscosidad)) +\n  geom_point() +\n  labs(x = \"Cantidad de aceite\", y = \"Viscosidad\") "},{"path":"rls.html","id":"modelorls","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.2 El modelo RLS","text":"El modelo de Regresión lineal Simple (RLS) de la variable respuesta (\\(Y\\)) sobre la variable predictora (\\(X\\)) se formula prediciendo la respuesta media para un valor observado de \\(X = x\\), con una recta de regresión:Es de esperar cierta desviación ‘aleatoria’ entre la respuesta observada y la respuesta media. Dicha desviación es denominada error aleatorio y denotada habitualmente por \\(\\epsilon\\). Así, el modelo completo de regresión simple se formula según:Los coeficientes de la regresión, esto es, los parámetros que hemos de estimar para ajustar el modelo RLS son:\\(\\beta_{0}\\).- la interceptación de la recta, esto es, la altura de la recta cuando \\(x = 0\\).\\(\\beta_{0}\\).- la interceptación de la recta, esto es, la altura de la recta cuando \\(x = 0\\).\\(\\beta_{1}\\).- la pendiente de la recta, que refleja cuánto varía la respuesta media \\(E\\)(y) cuando pasamos de observar x = \\(x\\) x = \\(x\\) + 1.\\(\\beta_{1}\\).- la pendiente de la recta, que refleja cuánto varía la respuesta media \\(E\\)(y) cuando pasamos de observar x = \\(x\\) x = \\(x\\) + 1.Dada una muestra de valores observados \\(\\{{(x_{},y_{})}_{=1}^{n}\\}\\), el modelo propuesto implica que todas las observaciones responden la ecuación (6.2), de forma que:donde \\(\\epsilon_{}\\) son errores aleatorios, que además se consideran incorrelados, con media cero y varianza constante \\(\\sigma^{2}\\). Estas características constituyen las hipótesis básicas del modelo RLS, que formulamos con más detalle continuación sobre los errores aleatorios \\(\\epsilon_{}\\):Incorrelación: \\(Corr(\\epsilon_{},\\epsilon_{j}) = 0\\). Significa que las observaciones de la respuesta y, \\(y_{1},y_{2},\\ldots,y_{n}\\) están incorreladas entre sí, esto es, los valores de unas afectan los de otras.Media cero: \\(E(\\epsilon_{}) = 0\\). Lo que implica que la respuesta esperada según el modelo RLS depende linealmente de los coeficientes de regresión \\(\\beta_{0}\\) y \\(\\beta_{1}\\).Varianza constante: \\(Var(\\epsilon_{} = \\sigma^{2})\\). Lo que significa que las observaciones \\(\\{y_{},=1,\\ldots,n\\}\\) provienen de una misma población cuya variabilidad respecto de su media, \\(\\{\\beta_{0} + \\beta_{1}x_{}, =1,\\ldots,n\\}\\), viene dada por \\(\\sigma^{2}\\).","code":""},{"path":"rls.html","id":"estimacionrls","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3 Estimación del modelo","text":"Estimar la recta de regresión consiste en estimar los coeficientes de la regresión \\(\\beta_{0}\\) y \\(\\beta_{1}\\) para obtener la recta:donde \\(\\hat{Y}\\) denota el valor de Y predicho por la recta para el valor observado de \\(X = x\\).Disponemos de dos criterios básicos de estimación, que proporcionan la misma solución. Utilizar uno u otro depende de nuestros intereses estadísticos. Si tan sólo queremos determinar la recta, basta con considerar el criterio de Mínimos Cuadrados. Si además pretendemos utilizarla con fines inferenciales o predictivos, hablaremos de que nuestra solución es la máximo-verosímil, pero su vez habremos de ser más exigentes con las hipótesis del modelo, como veremos continuación.","code":""},{"path":"rls.html","id":"mincuadrls","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.1 Estimación Mínimos Cuadrados","text":"El criterio de mínimos cuadrados o minimización del error cuadrático medio, consiste en minimizar las distancias entre los puntos observados y los predichos por la recta de ajuste. El error cuadrático medio de la recta se define como:La solución de mínimos cuadrados \\(\\hat{\\beta} = (\\hat{\\beta_{0}},\\hat{\\beta_{1}})\\) se obtiene minimizando \\(S(\\beta)\\). El mínimo se consigue derivando \\(S(\\beta)\\) respecto de \\(\\beta_{0}\\) y \\(\\beta_{1}\\) e igualando cero:\\[\n\\frac{\\partial S(\\beta)}{\\partial \\beta_{0}} \\mid_{\\hat{\\beta_{0}},\\hat{\\beta_{1}}} = -2 \\sum_{=1}^{n} (y_{} - \\hat{\\beta_{0}}-\\hat{\\beta_{1}}x_{}) = 0\n\\]\\[\n\\frac{\\partial S(\\beta)}{\\partial \\beta_{1}} \\mid_{\\hat{\\beta_{0}},\\hat{\\beta_{1}}} = -2 \\sum_{=1}^{n} (y_{} - \\hat{\\beta_{0}}-\\hat{\\beta_{1}}x_{})x_{} = 0.\n\\]De ahí se obtienen las ecuaciones normales:\\[\nn\\hat{\\beta_{0}}+\\hat{\\beta_{1}}\\sum_{=1}^{n} x_{}= \\sum_{=1}^{n} y_{}\n\\]\\[\n\\hat{\\beta_{0}}\\sum_{=1}^{n} x_{} + \\hat{\\beta_{1}} \\sum_{=1}^{n} x_{}^{2} = \\sum_{=1}^{n} y_{}x_{}\n\\]de donde las estimaciones para \\(\\beta_{0}\\) y \\(\\beta_{1}\\) resultan:\\[\n\\hat{\\beta_{0}}=\\bar{y}-\\hat{\\beta_{1}}\\bar{x}\n\\]\\[\n\\hat{\\beta_{1}}=\\frac{S_{xy}}{S_{xx}},\n\\]con:\\[\n\\bar{y} = \\frac{\\sum_{=1}^{n} y_{}}{n}\n\\]\\[\n\\bar{x} = \\frac{\\sum_{=1}^{n} x_{}}{n}\n\\]\\[\nS_{xx} = \\sum_{=1}^{n} (x_{}-\\bar{x})^2\n\\]\\[\nS_{xy} = \\sum_{=1}^{n} (x_{}-\\bar{x})(y_{}-\\bar{y}).\n\\]","code":""},{"path":"rls.html","id":"emvrls","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.2 Estimación Máximo Verosímil","text":"Habitualmente el objetivo de un análisis de regresión consiste únicamente en estimar la recta, sino en inferir con ella, esto es, asociar un error las estimaciones obtenidas, contrastar un determinado valor de los parámetros, y/o incluso predecir la respuesta, junto con una banda de confianza, para un \\(X = x\\) dado. En ese caso, precisamos de distribuciones de probabilidad para controlar la incertidumbre y el error. Añadimos pues, una hipótesis más sobre la distribución de la variable respuesta, o lo que es lo mismo, sobre el error aleatorio \\(\\epsilon\\). Dicha hipótesis es la de normalidad de los errores.Así, el total de hipótesis básicas del modelo de regresión con fines inferenciales, viene resumido en la siguiente expresión:esto es, hablamos de errores aleatorios independientes e idénticamente distribuidos (iid) según una distribución Normal con media cero y varianza \\(\\sigma^{2}\\), lo que implica directamente que la distribución para la variable respuesta será:Desde este momento, los datos proporcionan información sobre los parámetros del modelo, \\(\\beta = (\\beta_{0},\\beta_{1})\\), través de la verosimilitud conjunta:Por tanto, obtener la solución más factible la vista de los datos observados \\(\\{(x_i,y_i), =1,\\ldots, n\\}\\) equivale obtener la solución máximo-verosímil, esto es, la que maximiza la verosimilitud (6.8). Maximizar la verosimilitud es equivalente maximizar la log-verosimilitud \\(l(\\beta,y)\\), que tiene una expresión más sencilla sin exponenciales. La solución máximo-verosímil se obtiene derivando e igualando cero \\(l(\\beta,y)\\), lo que da lugar, de nuevo, las ecuaciones normales. Así pues, la solución máximo-verosímil coincide con la de mínimos cuadrados.","code":""},{"path":"rls.html","id":"emR","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.3 Estimación con R","text":"Para obtener el ajuste máximo verosímil con R utilizamos la función lm() que permite el ajuste de cualquier modelo lineal. Su expresión más básica viene dada por:\\[model <- lm(y \\sim x, data = ´´data´´)\\] donde \\(y\\) es la respuesta y \\(x\\) es la predictora.Para obtener las estimaciones del modelo podemos hacer uso de diferentes funciones:tidy(model) de la librería tidymodels que nos proporciona el modelo estimado, los errores en la estimación y la soluciones del contraste sobre cada parámetro del modelo que veremos en el apartado de inferencia sobre los coeficientes del modelo.glm_coef(model) de la librería pubh que nos proporciona las estimaciones del modelo, los intervalos de confianza al 95% de cada parámetro, y el p-valor asociado los contrastes sobre cada uno de los parámetros del modelo.summary(model) que proporciona un resumen completo del modelo (inferencia sobre los parámetros del modelo y bondad de ajuste).Por el momento utilizamos las dos primeras para mostrar los resultados del ajuste. Además utilizaremos la función plot_model para representar gráficamente el modelo obtenido como alternativa la función ggplot() que hemos utilizado en la figura 6.4.","code":""},{"path":"rls.html","id":"ejemplos","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.4 Ejemplos","text":"Para los datos de Corrosión se propone un modelo de regresión lineal simple para estudiar la relación entre la pérdida de peso debida la corrosión y el contenido de hierro de la forma siguiente:\\[\n\\text{peso} = \\beta_{0} + \\beta_{1}*\\text{hierro} + \\epsilon\n\\]de forma que el modelo estimado viene dado por: \\[\n\\widehat{\\text{peso}} = 129.79 - 24.02*\\text{hierro}\n\\]esto es, un aumento de una unidad del contenido de hierro reporta una pérdida de peso de 24.02 unidades. Representamos gráficamente la recta del ajuste obtenida:\nFigura 6.4: Ajuste de mínimos cuadrados para los datos de corrosión.\n","code":"\n# Ajuste del modelo\nfit <- lm(peso ~ hierro, data = corrosion)\n# Solución con tidy\ntidy(fit)## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    130.       1.40      92.5 2.93e-17\n## 2 hierro         -24.0      1.28     -18.8 1.06e- 9\n# Solución con glm_coef\nglm_coef(fit)##     Parameter            Coefficient Pr(>|t|)\n## 1 (Intercept) 129.79 (126.7, 132.87)  < 0.001\n## 2      hierro -24.02 (-26.84, -21.2)  < 0.001\n# Gráfico del ajuste\nplot_model(fit,\"pred\", terms = ~hierro, \n                ci.lvl = NA, \n                show.data = TRUE, \n                axis.title = c(\"Contenido en hierro\", \"Peso\"),\n                title = \" \")"},{"path":"rls.html","id":"propiedades-de-la-recta-de-regresión.","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.5 Propiedades de la recta de regresión.","text":"Las propiedades más relevantes y básicas del ajuste de la recta de regresión son las siguientes:La estimación de la respuesta para un valor de x=\\(x\\) concreto según el modelo de regresión lineal simple se obtiene de la recta de regresión ajustada:\\[\n\\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1} x.\n\\]La suma de los residuos de una recta de regresión con término de interceptación \\(\\beta_0\\) es cero,\\[\ne_i=y_i-\\hat{y} \\rightsquigarrow \\sum_i e_i=0.\n\\]La media de los valores observados \\(y_i\\) coincide con la media de los valores predichos \\(\\hat{y_i}\\),\\[\n\\frac{1}{n}\\,\\sum_i y_i=\\frac{1}{n} \\,\\sum_i \\hat{y}_i.\n\\]La recta de regresión pasa por el centroide de medias \\((\\bar{x},\\bar{y})\\).La recta de regresión pasa por el centroide de medias \\((\\bar{x},\\bar{y})\\).La suma de los residuos ponderados por el valor correspondiente de la variable predictora \\(x\\) es cero,La suma de los residuos ponderados por el valor correspondiente de la variable predictora \\(x\\) es cero,\\[\n\\sum_i x_i e_i=0.\n\\]La suma de los residuos ponderados por el valor ajustado por la recta \\(\\hat{y}\\) es cero,\\[\n\\sum_i \\hat{y}_i e_i=0.\n\\]","code":""},{"path":"rls.html","id":"rls_varmodel","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.6 Estimación varianza del modelo.","text":"La varianza \\(\\sigma^2\\) de los errores es una medida de la variabilidad (heterogeneidad) entre los individuos respecto la media cuando el modelo RLS describe adecuadamente la tendencia entre las variables \\(y\\) y \\(x\\), o lo que es lo mismo, de la dispersión de las observaciones respecto de la recta de regresión. Así pues, da una medida de bondad de ajuste del modelo de regresión los datos observados. Cuando el modelo de regresión ajustado es bueno para nuestros datos, es posible conseguir una estimación de la varianza \\(\\sigma^2\\) partir de la suma de cuadrados residual \\(SSE\\), también llamada suma de cuadrados debida al error:\\[\nSSE=\\sum_i (y_i-\\hat{y}_i)^2=S_{yy}-\\hat{\\beta}_1 S_{xy}.\n\\]\\(SSE\\) da una medida de la desviación entre las observaciones \\(y_i\\) y las estimaciones que proporciona la recta de regresión, \\(\\hat{y}_i\\). Puesto que en el modelo de regresión lineal simple se estiman \\(2\\) parámetros, los grados de libertad asociados \\(SSE\\) son \\(n-2\\). Se define pues el cuadrado medio residual, \\(MSE\\), como un estimador de \\(\\sigma^2\\), que además resulta ser insesgado (esto es, su valor esperado es \\(\\sigma^2\\)):\\[\ns^2=MSE=\\frac{SSE}{n-2}.\n\\]El error estándar residual viene dado por \\(s=\\sqrt{MSE}\\).","code":""},{"path":"rls.html","id":"inferencia-sobre-los-coeficientes-del-modelo","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.7 Inferencia sobre los coeficientes del modelo","text":"Los estimadores de mínimos cuadrados \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son insesgados y de mínima varianza de entre todos los estimadores insesgados. El hecho de especificar una distribución normal sobre los errores para la estimación máximo-verosímil, permite derivar de forma directa la distribución de dichos estimadores, que resulta también normal:\\[\n\\hat{\\beta}_0 \\sim N\\left( \\beta_0, \\frac{\\sum_{=1}^n x_{}^2}{nS_{xx}} \\sigma^2 \\right)\n\\]\\[\n\\hat{\\beta}_1 \\sim N\\left( \\beta_1, \\frac{\\sigma^2}{S_{xx}}\\right),\n\\]Cuando el modelo de regresión es adecuado, podemos estimar las varianzas de dichas distribuciones sustituyendo \\(\\sigma^2\\) por \\(s^2\\) . De ahí podemos construir los estadísticos \\(t\\) para inferir sobre los parámetros:\\[\nt_0 = \\frac{\\hat{\\beta}_0-\\beta_0}{s \\ \\sqrt{\\sum_i x_i^2/n S_{xx}}}\n\\]\\[\nt_1 = \\frac{\\hat{\\beta}_1-\\beta_1}{s/\\sqrt{S_{xx}}}\n\\]Ambos estadísticos se distribuyen según una distribución \\(t\\)-Student con \\(n-2\\) grados de libertad, que nos permite inferir (estimar y resolver contrastes de hipótesis) sobre los coeficientes del modelo, y en particular contestar preguntas sobre la relación entre las variables respuesta y explicativa.","code":""},{"path":"rls.html","id":"procedimientos-de-estimación","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.7.1 Procedimientos de estimación","text":"Las estimaciones puntuales de \\(\\beta_0\\) y \\(\\beta_1\\) las obtenemos directamente de las ecuaciones normales.Los intervalos de confianza al nivel de confianza \\((1-\\alpha)100\\%\\) para \\(\\beta_0\\) y \\(\\beta_1\\) se construyen partir de los estadísticos \\(t\\) y resultan:\\[\nIC( \\beta_0;1-\\alpha) = \\hat{\\beta}_0 \\pm t_{\\left(n-2,1-\\frac{\\alpha}{2}\\right)} \\sqrt{\\frac{\\sum_{=1}^n x_i^2}{n S_{xx}} \\ s^2}\n\\]\\[\nIC(\\beta_1;1-\\alpha) = \\hat{\\beta}_1 \\pm t_{\\left(n-2,1-\\frac{\\alpha}{2}\\right)} \\ \\sqrt{\\frac{s^2}{S_{xx}}},\n\\]donde \\(t_{\\left(n-2,1-\\frac{\\alpha}{2}\\right)}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución \\(t\\)-Student con \\(n-2\\) grados de libertad (los correspondientes \\(s^2\\)).","code":""},{"path":"rls.html","id":"procedimientos-de-contrastes-de-hipótesis","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.7.2 Procedimientos de Contrastes de Hipótesis","text":"Si queremos contrastar Hipótesis sobre los coeficientes de la regresión: \\[\\left\\{\\begin{array}{ll} \nH_{0}:& \\beta_{} = \\beta^{*} \\\\ \nH_{1}:& \\beta_{} \\neq \\beta^{*}, =0,1\\\\\n\\end{array}\n\\right.\\]basta con considerar los estadísticos \\(t\\) anteriores, y sustituir el valor \\(\\beta_i\\) por el que se pretende contrastar, \\(\\beta^*\\). Estos estadísticos, bajo \\(H_0\\), tienen una distribución \\(t\\)-Student con \\(n-2\\) grados de libertad. La resolución del contraste consiste en calcular el p-valor asociado al valor absoluto de la estimación, \\(|t_0|\\) o \\(|t_1|\\), según el caso, esto es, \\(p-valor=Pr[t_{n-2}>|t_i|]\\), donde \\(t_{n-2}\\) representa una variable t-Student con \\(n-2\\) grados de libertad, y \\(t_i\\) es el valor observado para el estadístico correspondiente. Dicho contraste se resuelve de la forma habitual:se rechaza \\(H_{0}\\) nivel de confianza \\(1-\\alpha\\) cuando \\(p-valor \\leq \\alpha\\),si \\(p-valor > \\alpha\\), se dice que los datos proporcionan suficientes evidencias en contra de la hipótesis nula y ésta se puede rechazar.Cuando el contraste propuesto sobre \\(\\beta_0\\) o \\(\\beta_1\\) tiene \\(\\beta^*=0\\), en realidad se está contrastando, respectivamente, si la recta de regresión tiene interceptación o pendiente nula. Contrastar \\(\\beta_1=0\\) es equivalente contrastar correlación nula entre las variables \\(X\\) e \\(Y\\), esto es, ausencia de relación lineal. Si conseguimos rechazar esta hipótesis con significatividad, concluiremos que la variable \\(X\\) está relacionada linealmente con \\(Y\\) y por lo tanto se puede utilizar para predecir \\(Y\\) través de la recta de regresión ajustada.","code":""},{"path":"rls.html","id":"ejemplo","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.3.8 Ejemplo","text":"Realizamos el proceso de inferencia para el modelo para los datos de corrosión e interpretamos los resultados obtenidos. Concretamente:Construir intervalos de confianza al 95% para \\(\\beta_0\\) y \\(\\beta_1\\). ¿Qué podemos decir de la relación entre dichas variables?Concluir sobre los contrastes \\(\\beta_0=0\\) y \\(\\beta_1=0\\). Comprobar también que el último contraste \\(\\beta_1=0\\) es equivalente al contraste de correlación nula entre las variables del modelo.Recapturamos el resumen del modelo obtenido con la función glm_coef()-Como se puede observar en los resultados ninguno de los intervalos de confianza incluye al cero, lo que habla positivamente de su significatividad estadística, esto es, tenemos evidencias para predecir la pérdida de peso con el contenido de hierro inicial través de una recta con interceptación y pendientes (significativamente) distintas de cero. De esta forma podemos ver que el efecto asociado con un incremento en una unidad de hierro produce una pérdida de peso de entre 22.01 y 26.03 unidades.Puestos resolver el contraste \\(H_0^:\\beta_i=0\\), para \\(=0,1\\), observamos los p-valores obtenidos en el proceso de estimación que resultan ambos significativos (<0.001 para \\(\\beta_0\\) y <0.001 para \\(\\beta_1\\)), lo que concluye contundentemente sobre la significatividad de ambos favor de que son distintos de cero (se rechazan \\(H_0^0\\) y \\(H_0^1\\)), como ya habíamos comentado partir de los intervalos de confianza. En particular, el contenido en hierro explica significativamente la pérdida de peso través del modelo lineal ajustado.","code":"\n# Ajuste del modelo\nfit <- lm(peso ~ hierro, data = corrosion)\nglm_coef(fit)##     Parameter            Coefficient Pr(>|t|)\n## 1 (Intercept) 129.79 (126.7, 132.87)  < 0.001\n## 2      hierro -24.02 (-26.84, -21.2)  < 0.001"},{"path":"rls.html","id":"bondad-del-ajuste","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.4 Bondad del Ajuste","text":"Cuando hemos realizado el ajuste de un modelo de regresión lineal, hemos de verificar que efectivamente dicho modelo proporciona un buen ajuste la hora de explicar (predecir) la variable respuesta. Básicamente la bondad del ajuste la cuantificamos con el tanto por ciento de variabilidad de la respuesta, que consigue ser explicada por el modelo ajustado. Para ello contamos con varios tipos de medidas que cuantifican esta variabilidad de diversos modos. Como medidas fundamentales de bondad de ajuste contamos con:el error residual estimado \\(s = \\hat{\\sigma}\\);el test \\(F\\) de bondad de ajuste que se obtiene de la Tabla de Anova;el coeficiente de determinación \\(R^2\\).Todas estas medidas las desglosamos continuación. Para obtenerlas con R utilizaremos las funciones glance(), anova() y summary().","code":""},{"path":"rls.html","id":"rls_errorresidual","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.4.1 Error residual","text":"Es una medida de bondad del ajuste relativa la escala de medida utilizada. En general, se prefieren modelos con menor error residual estimado \\(s\\), donde \\(s^2\\) denota la estimación de la varianza \\(\\sigma^2\\) del modelo, dada en el apartado @ref(rls_varmodel).","code":""},{"path":"rls.html","id":"rls_tablaanova","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.4.2 Tabla Anova","text":"Una medida de lo bueno que resulta un modelo para ajustar unos datos pasa por cuantificar cuánta de la variabilidad contenida en éstos ha conseguido ser explicada por dicho modelo. Un modelo es bueno si la variabilidad explicada es mucha, o lo que es lo mismo, si las diferencias entre los datos y las predicciones según el modelo son pequeñas.Construir la tabla de ANOVA o Análisis de la Varianza consiste en:descomponer la variabilidad de los datos en la parte que es explicada por el modelo y la parte que se deja sin explicar, es decir, la variabilidad de los residuos,compararlas y valorar estadísticamente si la variabilidad explicada por el modelo ajustado es suficientemente grande.Si partimos de la identidad:y el hecho de que \\(\\sum_{} (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) = 0\\), podemos escribir:Las abreviaturas \\(SST\\), \\(SSE\\) y \\(SSR\\) provienen del inglés para suma de cuadrados (Sum Squares): Total, debida al Error (o residual) y debida la Regresión, respectivamente. partir de ellas es posible calcular la variabilidad total, la variabilidad explicada por el modelo obtenido, y la variabilidad que queda por explicar o variabilidad residual, sin más que dividir las sumas de cuadrados por sus respectivos grados de libertad. Obtenemos así los cuadrados medios asociados, \\(MST=SST/(n-1)\\), \\(MSE=SSE/(n-2)\\) y \\(MSR=SSR/1\\).Contrastar la bondad del ajuste de la recta de regresión significa resolver el contraste:\\[\\begin{array}{cc}\nH_0:& \\mbox{ el modelo lineal explica bien la respuesta} \\\\\nH_1:& \\mbox{ el modelo lineal explica bien la respuesta},\n\\tag{6.11}\n\\end{array}\\]que, en el modelo RLS, resulta equivalente contrastar \\(H_0:\\beta_1=0, \\ vs. \\ H_1:\\beta_1 \\neq 0\\), esto es, si la variable predictora \\(X\\) explica suficientemente bien la variable respuesta \\(Y\\) través del modelo lineal propuesto. El estadístico de bondad de ajuste de la regresión está basado en comparar la variabilidad explicada por el modelo con la que queda sin explicar, esto es, en el cociente de las sumas de cuadrados medias \\(MSR\\) y \\(MSE\\), que resulta tener una distribución \\(F\\) con \\(1\\) y \\(n-2\\) grados de libertad cuando el modelo es correcto:En el modelo RLS, el estadístico \\(F\\) es igual al estadístico \\(t\\) asociado \\(\\beta_1\\), elevado al cuadrado. Ya hemos dicho antes que el contraste de bondad de ajuste es equivalente al de \\(\\beta_1=0\\).Concluiremos que la recta de regresión es significativa para predecir la respuesta \\(Y\\) al nivel de confianza \\((1-\\alpha)100\\%\\), cuando el valor que obtenemos para el estadístico \\(F\\) supera el valor crítico que se corresponde con el cuantil \\(1-\\alpha\\) de una distribución \\(F\\) con \\(1\\) y \\(n-2\\) grados de libertad. Esto es equivalente que el p-valor asociado al contraste resulte inferior \\(\\alpha\\). En otro caso, diremos que hemos obtenido evidencias suficientes para rechazar que el modelo lineal es útil para predecir la variable \\(Y\\) través de \\(X\\).Todas estas sumas de cuadrados y estadísticos se suelen presentar en una tabla de análisis de la variabilidad o tabla ANOVA, cuya apariencia es:En R la \\(SSR\\) se descompone su vez para cada uno de los efectos o variables predictoras en el modelo.","code":""},{"path":"rls.html","id":"coeficiente-de-determinación","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.4.3 Coeficiente de determinación","text":"Otro estadístico útil para chequear la bondad del ajuste de la recta de regresión es el coeficiente de determinación \\(R^2\\). Éste se define como la proporción de la varianza que es explicada por la recta de regresión y se obtiene partir de la descomposición (6.10) como:De hecho, en el modelo RLS, \\(R^2\\) es el cuadrado del coeficiente de regresión lineal entre la respuesta \\(Y\\) y el predictor \\(X\\).Puesto que \\(0\\leq R^2 \\leq 1\\) (al tratarse del coeficiente de correlación al cuadrado), un valor cercano \\(1\\) (entre 0.6 y 1) implicará que buena parte de la varianza es explicada por la recta de regresión, y \\(R^2\\approx 0\\) significará que prácticamente toda la variabilidad de los datos queda sin explicar por la recta. Sin embargo, \\(R^2\\) sirve para medir la idoneidad del modelo de regresión para describir los datos. De hecho, \\(R^2\\) puede resultar grande pesar de que la relación entre \\(X\\) e \\(Y\\) sea lineal (de hecho tiene la misma interpretación que un coeficiente de correlación, válido para cuantificar la relación lineal sólo cuando ésta existe). Siempre ha de ser utilizado con cautela. Así por ejemplo, la magnitud de \\(R^2\\) depende del rango de variabilidad de la variable explicativa. Cuando el modelo de regresión es adecuado, la magnitud de \\(R^2\\) aumenta (o disminuye) cuando lo hace la dispersión de \\(X\\). Por otro lado, podemos obtener un valor muy pequeño de \\(R^2\\) debido que el rango de variación de \\(X\\) es demasiado pequeño, y entonces impedirá que se detecte su relación con \\(Y\\).","code":""},{"path":"rls.html","id":"ejemplo-1","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.4.4 Ejemplo","text":"Analizamos la bondad del ajuste obtenido para los datos de corrosión.Esta función proporciona diferentes medidas de bondad de ajuste, algunas de ellas las utilizaremos en las unidades siguientes, pero en este caso nos centramos en las que hace referencia al modelo de RLS:r.squared: \\(R^2\\) del modelo ajustado,sigma: error residual,statistic: valor del estadístico de contraste (6.12) asociado la tabla ANOVA,p.value: p-valor del contraste (6.12),df: grados de libertad asociados con \\(MSR\\),df.residual: grados de libertad asociados con \\(MSE\\).Para este modelo el error residual tiene una magnitud de 3.05778, pero dado que podemos comparar con otro modelo resulta difícil interpretar este valor como una medida de bondad de ajuste al tener una escala de medida que nos indique si este valor es lo suficientemente pequeño.El valor del estadístico F (352.27) y su p-valor (1.055e-09) nos permiten concluir que podemos rechazar la hipótesis \\(H_0:\\beta_1=0\\), o lo que es lo mismo, \\(H_0\\): el modelo explica los datos, favor de que el contenido en hierro resulta útil para predecir el la pérdida de peso debido la corrosión través de un modelo de regresión lineal. Veamos la descomposición de la tabla ANOVA.Simplemente observando la Tabla de Anova, vemos que la variabilidad explicada por la recta (en términos de sumas de cuadrados), \\(SSR = 3293.8\\), es superior la que queda por explicar, \\(SSE = 102.9\\) (casi tres veces superior). En este caso, al tener una única variable en el modelo la \\(SSR\\) coincide con la correspondiente la variable hierro tal y como aparece en la tabla anterior.la vista de estos resultados podemos concluir que efectivamente el modelo obtenido resulta útil para explicar la mayor parte de la variabilidad existente en la respuesta partir de la variabilidad explicada por dicho modelo.Por último, el valor del coeficiente de determinación es \\(R^2= 0.9697198\\), lo que implica que alrededor del \\(97\\%\\) de la variabilidad de la pérdida de peso es explicada por la recta ajustada. Es un valor especialmente alto que refleja el gran poder predictivo del contenido de hierro para tratar de conocer la pérdida de peso final del compuesto.Una versión resumida de las características del modelo ajustado se puede obtener con la función summary(). Con ella podemos obtener la ecuación del modelo, contrastes individuales sobre cada coeficiente, la varianza residual y el test F asociado.Los criterios utilizados nos permiten concluir que el modelo obtenido es adecuado desde el punto de vista de su capacidad explicativa, es decir, la hora de medir la asociación entre la respuesta y la predictora. Sin embargo, es importante tener presente los pasos que hemos de dar la hora de aceptar finalmente un modelo como bueno. sólo es preciso superar la bondad del ajuste. Una vez superada esta prueba, hay que llevar cabo el diagnóstico y validación del modelo, o verificación de las hipótesis del modelo RLS y de la capacidad predictiva del mismo. De entre todos los modelos propuestos para predecir una respuesta \\(Y\\) que hayan superado la bondad del ajuste, el diagnóstico y la validación, podremos optar por el mejor según algún criterio preferido de comparación y selección de modelos. En los modelos de RLS esta tarea es sencilla ya que el modelo ajustado es único, pero se complicará cuando se añadan más variables predictoras como veremos en las unidades siguientes.","code":"\n# Medidas de bondad del ajuste\nglance(fit)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic      p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>        <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.970         0.967  3.06      352.      1.06e-9     1  -31.9  69.8  71.5     103.          11\n## # … with 1 more variable: nobs <int>\nanova(fit)## Analysis of Variance Table\n## \n## Response: peso\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## hierro     1 3293.8  3293.8  352.27 1.055e-09 ***\n## Residuals 11  102.9     9.4                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"rls.html","id":"rls_diag","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5 Diagnóstico del Modelo","text":"Una vez ajustado un modelo y habiendo superado las pruebas de bondad de ajuste pertinentes (fundamentalmente el test \\(F\\) de Anova), hemos de proceder con el diagnóstico del modelo, que consiste en verificar si éste satisface las hipótesis básicas del modelo de regresión, que son:linealidad entre las variables \\(X\\) e \\(Y\\)\\[\\begin{array}{ll}\nH_{0}: & Linealidad\\\\\nH_{1}: & \\ linealidad\n\\tag{6.14}\n\\end{array}\\]para los errores del modelo, \\(\\epsilon_{}\\):\nmedia cero\nvarianza constante u homocedasticidad\npara los errores del modelo, \\(\\epsilon_{}\\):media ceromedia cerovarianza constante u homocedasticidadvarianza constante u homocedasticidad\\[\\begin{array}{ll}\nH_{0}: & Varianza\\ constante\\\\\nH_{1}: & Varianza\\ \\ constante\n\\tag{6.15}\n\\end{array}\\]Si rechazamos la hipótesis nula estaremos concluyendo que nuestro modelo incumple la hipótesis de varianza constante.\nincorrelación\nSi rechazamos la hipótesis nula estaremos concluyendo que nuestro modelo incumple la hipótesis de varianza constante.incorrelación\\[\\begin{array}{ll}\nH_{0}: & Residuos\\ \\ correlados \\\\\nH_{1}: & Residuos\\ correlados\n\\tag{6.16}\n\\end{array}\\]normalidad\\[\\begin{array}{ll}\nH_{0}: & Residuos\\ normales\\\\\nH_{1}: & Residuos\\ \\ normales\n\\tag{6.17}\n\\end{array}\\]El análisis de los residuos del modelo nos permitirá detectar deficiencias en la verificación de estas hipótesis, así como descubrir observaciones anómalas o especialmente influyentes en el ajuste. Una vez encontradas las deficiencias, si existen, cabrá considerar el replanteamiento del modelo, bien empleando transformaciones de las variables, bien proponiendo modelos alternativos al de RLS, que trataremos con detalle en las unidades siguientes.El diagnóstico del modelo se lleva cabo fundamentalmente partir de la inspección de los residuos del modelo. Éstos sólo son buenos estimadores de los errores cuando el modelo ajustado es bueno. Aun así, es lo más aproximado con lo que contamos para indagar qué ocurre con los errores y si éstos satisfacen las hipótesis del modelo. El análisis de los residuos habitual es básicamente gráfico, si bien existen varios tests estadísticos útiles para detectar inadecuaciones del modelo, que presentaremos brevemente.Definimos los residuos de un modelo lineal como las desviaciones entre las observaciones y los valores ajustados:\\[\nr_i = y_i - \\hat{y}_i, \\qquad =1,\\ldots,n.\n\\]En ocasiones, es preferible trabajar con los residuos estandarizados, que tienen media cero y varianza aproximadamente unidad, y facilitan la visualización de las hipótesis:\\[\nd_i = \\frac{r_i}{\\sqrt{MSE}}, \\qquad =1,\\ldots,n.\n\\]Los residuos asociados un modelo ajustado se pueden obtener con la función fortify(). Esta función proporciona además las medidas de influencia para detectar observaciones anómalas, es decir, observaciones con residuos excesivamente grandes.","code":""},{"path":"rls.html","id":"linealidad-y-homocedasticidad.","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.1 Linealidad y homocedasticidad.","text":"Los gráficos de residuos estandarizados frente valores ajustados nos permiten detectar varios tipos de deficiencias del modelo ajustado. Si los residuos están distribuidos alrededor del cero y el gráfico presenta ninguna tendencia entonces el modelo se considera adecuado. Cuando aparece alguna tendencia como una forma de embudo o un abombamiento, etc., podemos tener algún problema con la hipótesis de varianza constante para los errores (heterocedasticidad). Cuando se aprecia alguna tendencia hablamos de violación de la hipótesis de linealidad: el modelo lineal ha sido incapaz de capturar una tendencia lineal apreciada en los residuos, posiblemente debido que existen otras variables explicativas adicionales consideradas en el modelo, o que la variable predictora explica la respuesta de un modo más complejo (quizás polinómico, etc.) al considerado en el modelo lineal.Para verificar la hipótesis de homocedasticidad podemos usar el test de Breusch-Pagan para variables predictoras de tipo numérico, y el de Bartlett para variables predictoras de tipo categórico (ver unidades siguientes). Para realizar el test de Breusch-Pagan utilizamos la función bptest() de la librería lmtest, mientras que para realizar el test de Bartlett utilizamos la función bartlett.test().","code":""},{"path":"rls.html","id":"ejemplo-2","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.2 Ejemplo","text":"Analizamos la hipótesis de homocedasticidad para el modelo obtenido para los datos de corrosión. En primer lugar obtenemos los residuos del modelo.\nFigura 6.5: Gráfico de residuos estandarizados vs valores ajustados.\nSe puede concluir que se verifica la hipótesis de linealidad (recta horizontal), ya que existen tendencias en los residuos. Además con los residuos se comportan de forma aleatoria sin agrupaciones ni tendencias podemos concluir que se cumple la hipótesis de homogeneidad de varianzas. Hay que tener mucho cuidad con la interpretación de estos gráficos, ya que cuando el tamaño de muestra es pequeño, resulta difícil apreciar tenencias o agrupaciones en los residuos. Por este motivo realizamos el test de diagnóstico.Dado que el p-valor del contraste es superior 0.05 podemos rechazar la hipótesis nula dada en (6.15), y por tanto podemos concluir que se verifica la hipótesis de homogeneidad o varianza constante.","code":"\n# Residuos y medidas de diagnóstico\ndiagnostico <- fortify(fit)\n# Gráfico de residuos estandarizados vs ajustados\nggplot(diagnostico, aes(x = .fitted, y = .stdresid)) + \n  geom_point() + \n  stat_smooth(method = \"lm\", se = FALSE) +\n  theme_bw()\n# Test de diagnóstico\nbptest(fit)## \n##  studentized Breusch-Pagan test\n## \n## data:  fit\n## BP = 0.024539, df = 1, p-value = 0.8755"},{"path":"rls.html","id":"normalidad-1","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.3 Normalidad","text":"Para verificar la normalidad de los errores, disponemos de gráficos qq-plot de normalidad, en los que se representan los residuos ordenados \\(r_{[]}\\) (cuantiles empíricos) versus los cuantiles correspondientes una normal estándar, \\(\\Phi^{-1}[(-1)/n]\\). Si es cierta la normalidad de los residuos, los puntos han de estar alineados con la diagonal. Desviaciones de la diagonal más o menos severas en las colas, e incluso en el centro de la distribución, dan indicios de desviaciones de normalidad. La hipótesis de normalidad se puede chequear también con histogramas de los residuos cuando el tamaño muestral es grande.Los residuos estandarizados también son útiles para detectar desviaciones de la normalidad. Si los errores se distribuyen según una normal, entonces aproximadamente el \\(68\\%\\) de los residuos estandarizados quedarán entre \\(-1\\) y \\(+1\\), y el \\(95\\%\\) entre \\(-2\\) y \\(+2\\).Para diagnosticar la hipótesis de normalidad se utiliza el test de Shapiro-Wilks, donde rechazar la hipótesis nula implica el rechazo de la hipótesis de normalidad. Para realizar dicho contraste utilizamos la función shapiro.test().\n(#fig:rls011, )Gráfico de normalidad de los residuos estandarizados.\nEl gráfico de normalidad muestra un comportamiento correcto ya que los punto se distribuyen lo largo de la recta de normalidad. se realiza el histograma ya que el tamaño muestral es demasiado pequeño. Pasamos al test de diagnóstico.Dado que el p-valor del contraste es superior 0.05 podemos rechazar la hipótesis nula dada en (6.17), y por tanto podemos considerar que los residuos se distribuyen normalmente.","code":"\n# grafico qq\nggplot(diagnostico, aes(sample = .stdresid)) + \n  stat_qq() + \n  geom_abline() +\n  theme_bw()\n# Test de diagnóstico\nshapiro.test(diagnostico$.stdresid)## \n##  Shapiro-Wilk normality test\n## \n## data:  diagnostico$.stdresid\n## W = 0.92905, p-value = 0.3312"},{"path":"rls.html","id":"independencia.","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.4 Independencia.","text":"La correlación entre los datos es un proceso intrínseco al muestreo; saber cómo se ha llevado cabo éste da información, generalmente suficiente, para poder hablar de correlación o incorrelación. En todo caso, los gráficos secuenciales de residuos sirven para detectar problemas de correlación de éstos (autocorrelación), o de inestabilidad de la varianza lo largo del tiempo. También son útiles para esto los gráficos en que se representa un residuo versus el anterior en la secuencia en que han sido observados; si hay correlación se apreciará tendencia. Detectar autocorrelación llevará considerar otro tipo de modelos distintos (autocorrelados: modelos de series temporales).Aparte de los métodos gráficos, para resolver dicho contraste se utiliza el test de Durbin-Watson, cuya función es dwtest(). Rechazar la hipótesis nula implica el rechazo de la hipótesis de incorrelación.\n(#fig:rls013, )Gráfico de autocorrelación de los residuos estandarizados.\nEl gráfico de la función de autocorrelación muestra la independencia de las observaciones. Todos los lags quedan dentro del rango de independencia.Dado que el p-valor del contraste es superior 0.05 podemos rechazar la hipótesis nula dada en (6.16), y por tanto podemos considerar que los residuos se distribuyen de froma independiente.","code":"\n# grafico función autocorrelación\nacf(diagnostico$.stdresid)\n# Test de diagnóstico\ndwtest(fit, alternative = \"two.sided\")## \n##  Durbin-Watson test\n## \n## data:  fit\n## DW = 2.5348, p-value = 0.2952\n## alternative hypothesis: true autocorrelation is not 0"},{"path":"rls.html","id":"otros-gráficos-de-diagnóstico","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.5 Otros gráficos de diagnóstico","text":"Los gráficos de residuos versus valores de la variable predictora son útiles para apreciar tendencias en los residuos que han quedado sin explicar por el modelo ajustado. Básicamente se interpretan como los gráficos de residuos versus valores ajustados \\(\\hat{y}_i\\). Es deseable que los residuos aparezcan representados en una banda horizontal sin tendencias alrededor del cero. Por ejemplo, si hay tendencias de tipo cuadrático, posiblemente hayamos de incorporar la variable \\(x^2\\) en el modelo, o bien abordar algún tipo de transformación que permita una relación de tipo lineal entre predictor y respuesta.","code":""},{"path":"rls.html","id":"incumplimiento-de-hipótesis","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.6 Incumplimiento de hipótesis","text":"Una vez identificado el incumplimiento de alguna de las hipótesis del modelo, hay que tratar de identificar porque se produce dicho incumplimiento. Se estudia si el incumplimiento es debido :Subconjunto de los datos que influye desproporcionadamente en el ajuste del modelo propuesto, con lo cual las estimaciones y predicciones dependen mucho de él. En primer lugar, el objetivo es identificar dichas observaciones. Una vez detectadas la forma de proceder es la siguiente:\nComprobar si la influencia se debe un error en la toma de observaciones, si es así se corrigen los defectos encontrados y se comienza de nuevo.\nSi los datos son correctos y el subconjunto de influyentes es pequeño se opta casi siempre por su eliminación del banco de datos. En otras ocasiones se puede optar por estudiar de forma separada dichas observaciones.\nSubconjunto de los datos que influye desproporcionadamente en el ajuste del modelo propuesto, con lo cual las estimaciones y predicciones dependen mucho de él. En primer lugar, el objetivo es identificar dichas observaciones. Una vez detectadas la forma de proceder es la siguiente:Comprobar si la influencia se debe un error en la toma de observaciones, si es así se corrigen los defectos encontrados y se comienza de nuevo.Si los datos son correctos y el subconjunto de influyentes es pequeño se opta casi siempre por su eliminación del banco de datos. En otras ocasiones se puede optar por estudiar de forma separada dichas observaciones.Comportamiento sistemático del modelo. Este caso es más complicado y requiere de procedimientos más sofisticados para corregir los defectos que aparezcan en el modelo.Comportamiento sistemático del modelo. Este caso es más complicado y requiere de procedimientos más sofisticados para corregir los defectos que aparezcan en el modelo.De la primera parte se encarga de analizarla los diagnósticos de influencia, mientras que en el segundo caso se trata principalmente de realizar transformaciones de las variables involucradas en el modelo.","code":""},{"path":"rls.html","id":"análisis-de-influencia","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.7 Análisis de influencia","text":"El análisis de influencia pretende detectar aquellas observaciones cuya inclusión/exclusión en el ajuste altera sustancialmente los resultados. Es interesante siempre, localizar este tipo de datos, si existen, y evaluar su impacto en el modelo. Si estos datos influyentes son “malos” (provienen de errores en la medición, o de condiciones de experimentación diferentes, etc.) habrían de ser excluidos del ajuste; si son “buenos”, contendrán información sobre ciertas características relevantes considerar en el ajuste.primera vista, observaciones que dan lugar un residuo grande, pueden influir notablemente en el ajuste. Las denominaremos OBSERVACIONES ALEJADAS. Su existencia puede indicar también la inadecuación del modelo asumido la realidad experimental. Si dicha observación tiene un residuo exageradamente grande la denominamos ANÓMALA (outlier en inglés). Por otra parte, observaciones que adoptan valores extremos de alguna o varias variables explicativas pueden tener más influencia que las usuales. Las denominaremos OBSERVACIONES ATÍPICAS.Sin embargo, las dos características siempre suponen que las observaciones que las manifiestan sean también influyentes. Generalmente se dice que una observación es alejada si el valor absoluto del residuo es mayor que 2. Se considera anómala si el valor absoluto del residuo es mayor que 3.Un criterio para valora la influencia de una observación sobre los coeficientes del modelo es el cálculo de la distancia de CooK. Se consideran como observaciones influyentes todas aquellas cuyo valor de la distancia sea mayor que 1. Dicha distancia se obtiene directamente con la función fortify(modelo) en al columna denominada .cooksd. Existen otro tipo de medidas de influencia (se pueden obtener con la función influence.measures(ajuste)) pero las estudiaremos en las unidades siguientes.Si el incumplimiento de las hipótesis es debido la presencia de observaciones influyentes, sino más bien un comportamiento sistemático del modelo, los remedios para corregir estas deficiencias pasan principalmente por:Propuesta de otros modelos adecuados la distribución de la respuesta y su relación con los predictores. Este punto o trataremos ampliamente más adelanteTransformar la variable respuesta (si es de tipo continuo), o las variables predictoras (si son de tipo continuo).","code":""},{"path":"rls.html","id":"transformaciones","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.8 Transformaciones","text":"El tipo de transformaciones que podemos realizar se pueden dividir en tres apartados:Transformaciones debidas al modelo teórico. Existen situaciones experimentales donde ya partimos de un tipo de modelo de carácter lineal pero que se podría convertir en lineal con una sencilla transformación de la respuesta o la predictora, o de ambas. Ejemplos de estos modelos teóricos que se pueden convertir en modelos de RLS son:Transformaciones sobre la predictora. Se utilizan principalmente ante la falta de linealidad, y se basan principalmente en la construcción de modelos de predicción polinómicos. Estos modelos los estudiaremos con más detalle en la unidad siguiente.Transformaciones sobre la predictora. Se utilizan principalmente ante la falta de linealidad, y se basan principalmente en la construcción de modelos de predicción polinómicos. Estos modelos los estudiaremos con más detalle en la unidad siguiente.Transformaciones sobre la respuesta. Esta suele ser la opción más habitual. Obtener una transformación adecuada de la respuesta sin alterar las variables predictoras. Se suelen utilizar ante el incumplimiento de las hipótesis de normalidad o varianza constante. Como buscar una transformación adecuada es un tema que puede resulta costoso, se utilizar un procedimiento automático que nos da una transformación rápida. Dicho procedimiento se conoce con el nombre de transformaciones de Box-Cox, y sde puede obtener en R con la función boxcox(). Dicho procedimiento consiste en obtener un intervalo de confianza para un parámetro (\\(\\lambda\\)) que refleja la transformación de la respuesta utilizar. Las transformaciones más habituales son:Transformaciones sobre la respuesta. Esta suele ser la opción más habitual. Obtener una transformación adecuada de la respuesta sin alterar las variables predictoras. Se suelen utilizar ante el incumplimiento de las hipótesis de normalidad o varianza constante. Como buscar una transformación adecuada es un tema que puede resulta costoso, se utilizar un procedimiento automático que nos da una transformación rápida. Dicho procedimiento se conoce con el nombre de transformaciones de Box-Cox, y sde puede obtener en R con la función boxcox(). Dicho procedimiento consiste en obtener un intervalo de confianza para un parámetro (\\(\\lambda\\)) que refleja la transformación de la respuesta utilizar. Las transformaciones más habituales son:Una vez realizado el estudio de influencia la forma de proceder consiste en eliminar las observaciones influyentes y obtener un nuevo modelo sin ellas, o bien realizar alguna de las transformaciones planteadas y ajustar el nuevo modelo. Una vez construido deberemos ajustar el nuevo modelo y realizar un nuevo diagnóstico para verificar que se cumple las hipótesis. Se trata pues de un proceso circular donde cada modificación debemos obtener un nuevo modelo y analizarlo completamente hasta llegar un modelo que cumpla con todas las especificaciones. Sin embargo, en ocasiones puede ocurrir que seamos capaces de encontrar un modelo que cumpla las hipótesis y deberemos buscar entre modelos más complejos de los planteados aquí.","code":""},{"path":"rls.html","id":"ejemplos-1","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.9 Ejemplos","text":"Procedemos con el análisis de los bancos de datos de Papel y Viscosidad presentados al inicio de esta unidad para estudiar los posibles problemas de diagnóstico que hemos venido trabajando.","code":""},{"path":"rls.html","id":"papel","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.9.1 Papel","text":"Planteamos y ajustamos el modelo correspondiente los datos de Papel.\nFigura 6.6: Ajuste para los datos de resitencia del papel\nParece obvio que el modelo planteado es adecuado, ya que captura de forma adecuada la tendencia de los datos observados. De hecho, el coeficiente asociado con madera resulta significativo, lo que daría entender que la concentración de madera es relevante para explicar la tensión del papel. Esta afirmación es claramente falsa ya que se aprecia claramente una tendencia de tipo cuadrático. Realizamos los gráficos de diagnóstico para corroborar este hecho.\nFigura 6.7: Gráfico de residuos vs ajustados para el modelo de papel\nSe observa claramente una tendencia de tipo cuadrática en los residuos lo que indica que un modelo más adecuado para estos datos sería: \\[tension \\sim madera + madera^2\\] Realizamos el análisis de influencia para completar el diagnóstico, pesar de que la introducción de la nueva pedictora proporcionará un modelo más adecuado.se observa ninguna observación influyente por lo que el problema de ajuste se debe la falta de tendencia del modelo propuesto.","code":"\n# Ajuste del modelo\nfit.papel <- lm(tension ~ madera, data = papel)\n# Solución con glm_coef\nglm_coef(fit.papel)##     Parameter         Coefficient Pr(>|t|)\n## 1 (Intercept) 21.32 (9.86, 32.78)    0.001\n## 2      madera    1.77 (0.4, 3.14)    0.014\n# Gráfico del ajuste\nplot_model(fit.papel, \"pred\", terms = ~madera, \n                ci.lvl = NA, \n                show.data = TRUE, \n                axis.title = c(\"Concentración de madera\", \"Tensión del papel\"),\n                title = \" \")\n# Obtenemos los residuos del modelo\ndiganostico.papel <- fortify(fit.papel)\n# Gráfico de residuos vs ajustados\nggplot(diganostico.papel, aes(x = .fitted, y = .stdresid)) + \n  geom_point() + \n  theme_bw()\n# Valoramos si hay alguna observación con distancia mayor que 1\nabs(diganostico.papel$.cooksd) > 1##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n## [17] FALSE FALSE FALSE"},{"path":"rls.html","id":"viscosidad","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.5.9.2 Viscosidad","text":"Planteamos y ajustamos el modelo correspondiente los datos de Viscosidad.\nFigura 6.8: Ajuste para los datos de viscosidad\nEl modelo ajustado indica que la cantidad de aceite puede explicar la viscosidad final (p-valor significativo), de forma que por cada unidad que aumentamos la cantidad de aceite la viscosidad aumenta en 1.47 unidades. El modelo obtenido viene dado por:\\[\n\\widehat{\\text{viscosidad}} = 6.32 + 1.47*\\text{aceite}\n\\]Estudiamos la capacidad explicativa del modelo:El \\(R^2\\) nos indica que el 62.4% de la variabilidad viene explicada por el modelo. Además el p-valor asociado la tabla ANOVA resulta significativo indicando que el modelo tiene capacidad explicativa, es decir, podemos utilizar la cantidad de aceite para conocer el grado de viscosidad. Pasamos al diagnóstico del modelo.En este cao utilizamos los tests estadísticos en lugar de los gráficos para concluir sobre el diagnóstico:Como se puede ver debemos rechazar las hipótesis de varianza constante y de independencia (p-valores inferiores 0.05). Planteamos la familia de transformaciones de Box-Cox para tratar de corregir los problemas con las hipótesis del modelo:El intervalo de confianza al 95% para \\(\\lambda\\) incluye el valor de \\(\\lambda = 0\\), de forma que podríamos utilizar la transformación logaritmo para tratar de corregir los defectos encontrados en el modelo propuesto inicialmente.El modelo resulta significativo con ecuación dada por: \\[\n\\widehat{\\text{lviscosidad}} = 2.81 + 0.03*\\text{aceite}\n\\]La bondad del ajustenos da una capacidad explicativa del 73.8%. Hemos mejorado nuestra capacidad explicativa al transformar la respuesta. Por último realizamos el diagnóstico del nuevo modelo:El modelo verifica las hipótesis de varianza constante y normalidad. la hipótesis de independencia resulta significativa debido la propia estructura de los datos, y más concretamente de la variable predictora, ya que como se puede ver solo se dan ciertos valores específicos (como si se tratara de una variable categórica más que una numérica). Podemos verificar este hecho con el gráfico de autocorrelación:\nFigura 6.9: Gráfico de autocorrelación de los residuos estandarizados.\nEn esta situación este incumplimiento resulta concluyente y podemos utilizar el modelo construido para establecer una relación entre la cantidad de aceite y el logaritmo de la viscosidad.","code":"\n# Ajuste del modelo\nfit.aceite <- lm(viscosidad ~ aceite, data = aceites)\n# Solución con glm_coef\nglm_coef(fit.aceite)##     Parameter         Coefficient Pr(>|t|)\n## 1 (Intercept) 6.32 (-12.9, 25.53)    0.502\n## 2      aceite   1.47 (0.95, 1.99)  < 0.001\n# Gráfico del ajuste\nplot_model(fit.aceite,\"pred\", terms = ~aceite, \n                ci.lvl = NA, \n                show.data = TRUE, \n                axis.title = c(\"Cantidad de aceite\", \"Viscosidad\"),\n                title = \" \")\nglance(fit.aceite)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>      <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.624         0.606  23.8      34.9 0.00000731     1  -104.  215.  218.   11898.          21\n## # … with 1 more variable: nobs <int>\ndiagnostico.aceite <- fortify(fit.aceite)\n# Varianza constante\nbptest(fit.aceite)  ## \n##  studentized Breusch-Pagan test\n## \n## data:  fit.aceite\n## BP = 6.0786, df = 1, p-value = 0.01368\n# Normalidad\nshapiro.test(diagnostico.aceite$.stdresid)## \n##  Shapiro-Wilk normality test\n## \n## data:  diagnostico.aceite$.stdresid\n## W = 0.95818, p-value = 0.4276\n# Independencia\ndwtest(fit.aceite)## \n##  Durbin-Watson test\n## \n## data:  fit.aceite\n## DW = 0.51202, p-value = 5.084e-06\n## alternative hypothesis: true autocorrelation is greater than 0\nboxcox(fit.aceite)\n# Calculamos la nueva variable\naceites <- aceites %>% mutate(lviscosidad = log(viscosidad))\n# Ajuste el nuevo modelo\nfit.aceite2 <- lm(lviscosidad ~ aceite, data = aceites)\n# Solución con glm_coef\nglm_coef(fit.aceite2)##     Parameter       Coefficient Pr(>|t|)\n## 1 (Intercept)  2.81 (2.52, 3.1)  < 0.001\n## 2      aceite 0.03 (0.02, 0.04)  < 0.001\n# Gráfico del ajuste\nplot_model(fit.aceite2,\"pred\", terms = ~aceite, \n                ci.lvl = NA, \n                show.data = TRUE, \n                axis.title = c(\"Cantidad de aceite\", \"log(Viscosidad)\"),\n                title = \" \")\nglance(fit.aceite2)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.738         0.726 0.363      59.3 0.000000152     1  -8.31  22.6  26.0     2.77          21\n## # … with 1 more variable: nobs <int>\ndiagnostico.aceite2 <- fortify(fit.aceite2)\n# Varianza constante\nbptest(fit.aceite2)  ## \n##  studentized Breusch-Pagan test\n## \n## data:  fit.aceite2\n## BP = 0.39756, df = 1, p-value = 0.5284\n# Normalidad\nshapiro.test(diagnostico.aceite2$.stdresid)## \n##  Shapiro-Wilk normality test\n## \n## data:  diagnostico.aceite2$.stdresid\n## W = 0.92282, p-value = 0.07659\n# Independencia\ndwtest(fit.aceite2)## \n##  Durbin-Watson test\n## \n## data:  fit.aceite2\n## DW = 0.24384, p-value = 6.044e-10\n## alternative hypothesis: true autocorrelation is greater than 0\n# gráfico función autocorrelación\nacf(diagnostico.aceite2$.stdresid)"},{"path":"rls.html","id":"rls_pred","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.6 Predicción del modelo","text":"Una vez obtenido un modelo definitivo, la última fase de la modelización consiste en la predicción de la respuesta partir de un nuevo conjunto de valores de la predictora o predictoras. Básicamente se trata de utilizar valores dentro del rango de la variable predictora para conocer el valor de la respuesta sin necesidad de realizar el diseño experimental. Una vez ajustado el modelo si consideramos una observación \\(X = x_0\\) dentro del rango de valores de \\(X\\) la predicción de la respuesta se puede obtener través del modelo ajustado mediante:\\[y_0 = \\widehat{\\beta_0} + \\widehat{\\beta_1} x_0.\\]Esto nos proporciona una estimación puntual del valor predicho, pero sin embargo es necesario proporcionar un intervalo de confianza para dicho valor para tener en cuenta la variabilidad observada en el modelo propuesto. Existen dos posibilidades de predicción en este sentido:Predicción del valor medio de la respuesta. Se trata de predecir el valor medio de la respuesta para un valor especifico de la variable predictora (\\(X = x_0\\)). Esta es la herramienta de predicción habitual ya que tiene una menor variabilidad. La idea es que para un mismo valor de \\(X = x_0\\) obtendremos diferentes valores predichos de la respuesta, y por tanto, más que interesarnos la predicción de la respuesta, nos centramos en predecir la media de todos esos posibles valores de la respuesta.Predicción del valor medio de la respuesta. Se trata de predecir el valor medio de la respuesta para un valor especifico de la variable predictora (\\(X = x_0\\)). Esta es la herramienta de predicción habitual ya que tiene una menor variabilidad. La idea es que para un mismo valor de \\(X = x_0\\) obtendremos diferentes valores predichos de la respuesta, y por tanto, más que interesarnos la predicción de la respuesta, nos centramos en predecir la media de todos esos posibles valores de la respuesta.Predicción del valor de la respuesta. Se trata de predecir el valor de la respuesta para un valor especifico de la variable predictora (\\(X = x_0\\)). Dado que estamos intentando predecir un único valor y la media de un conjunto de valores el intervalo de confianza de predicción es mayor que en el caso anterior. Tenemos más variabilidad cuando queremos predecir un valor que cuando queremos predecir la media de un conjunto de valores.Predicción del valor de la respuesta. Se trata de predecir el valor de la respuesta para un valor especifico de la variable predictora (\\(X = x_0\\)). Dado que estamos intentando predecir un único valor y la media de un conjunto de valores el intervalo de confianza de predicción es mayor que en el caso anterior. Tenemos más variabilidad cuando queremos predecir un valor que cuando queremos predecir la media de un conjunto de valores.Utilizaremos la función predict() para construir la predicción para un modelo dado. Veremos su aplicación en los diferentes ejemplos.","code":""},{"path":"rls.html","id":"respuesta-media","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.6.1 Respuesta media","text":"Como ya hemos indicado resulta posible obtener una estimación puntual del valor de la respuesta media través de: \\[\\begin{equation}\n\\hat{y}_{x_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0.\n (\\#eq:RLSpredmean)\n\\end{equation}\\] para un \\(X = x_0\\) dado.Un intervalo de confianza para la estimación del valor esperado de la respuesta para un \\(X = x_0\\) dado es:","code":""},{"path":"rls.html","id":"nueva-observación","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.6.2 Nueva observación","text":"Predeciremos una futura observación de la variable \\(Y\\) para cierto valor de \\(X = x_0\\), cony el intervalo de confianza vendrá dado por:Notar que tanto la estimación de la respuesta media como la predicción coinciden, aunque difieren en cuanto al grado de incertidumbre de la misma. Como es de esperar, predecir un hecho puntual en el futuro conlleva más incertidumbre que estimar en términos medios qué va ocurrir. Por último, comentar que cuando hemos utilizado alguna transformación (monótona) sobre la respuesta y queremos recuperar la estimación o predicción de ésta en su escala original, basta con utilizar la transformación recíproca sobre el valor predicho para obtener la predicción en la escala original.","code":""},{"path":"rls.html","id":"ejemplos-2","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.6.3 Ejemplos","text":"Dado que la estimación puntual de la predicción coincide con el modelo ajustado, ya hemos mostrado anteriormente como representar gráficamente la ecuación del modelo o predicción de la respuesta en diferentes ejemplos. En este punto nos limitamos mostrar como obtener y representar los intervalos de confianza asociados, y como obtener la predicción para un conjunto determinado de valores de la variable predictora.","code":""},{"path":"rls.html","id":"corrosión","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.6.3.1 Corrosión","text":"En primer lugar recuperamos los datos y modelo obtenido para los datos de corrosión.Estamos interesados en conocer la pérdida de peso medio (estimación de la media) y la pérdida de peso específica (predicción de una futura observación) para una barra en particular cuando el contenido en hierro es de 0.5, 1, y 1.5.Hemos obtenido la estimación (fit) e intervalo de confianza ((lwr,upr)) al 95% de la predicción de la pérdida de peso medio para valores específicos de contenido de hierro.Como ya habíamos comentado la estimación que obtenemos es la misma pero los intervalos de confianza son más amplios.continuación se muestra como representar gráficamente la predicción de la respuesta media y los intervalos de predicción al 95% de confianza para todo el rango de valores de la predictora.\nFigura 6.10: Predicción para los datos de corrosión (media e IC95%).\n","code":"\n# cargamos datos de predicción\nnewpred <- data.frame(hierro = c(0.5, 1, 0.5))\n# Predicción para la media de la respuesta\n# Opción interval = \"confidence\" \nnewdata <- data.frame(newpred, \n                      predict(fit, \n                              newpred, \n                              interval = \"confidence\")) \nround(newdata, 2)##   hierro    fit    lwr    upr\n## 1    0.5 117.78 115.63 119.92\n## 2    1.0 105.77 103.87 107.67\n## 3    0.5 117.78 115.63 119.92\n# cargamos datos de predicción\nnewpred <- data.frame(hierro = c(0.5, 1, 0.5))\n# Predicción de un único valor\n# Opción interval = \"prediction\" \nnewdata <- data.frame(newpred, \n                      predict(fit, \n                              newpred, \n                              interval = \"prediction\")) \nround(newdata, 2)##   hierro    fit    lwr    upr\n## 1    0.5 117.78 110.71 124.84\n## 2    1.0 105.77  98.77 112.76\n## 3    0.5 117.78 110.71 124.84\n# Gráfico del ajuste\nplot_model(fit, \"pred\", terms = ~hierro, \n                ci.lvl = 0.95, \n                show.data = TRUE, \n                axis.title = c(\"Contenido en hierro\", \"Peso\"),\n                title = \" \")"},{"path":"rls.html","id":"viscosidad-1","chapter":"Unidad 6 Regresión Lineal Simple (RLS)","heading":"6.6.3.2 Viscosidad","text":"Vamos realizar la predicción para el modelo ajustado los datos de viscosidad. Recordemos que en este caso hemos transformado la respuesta con la función logaritmo para asegurar que se cumple las hipótesis del modelo, y por tanto nuestra predicción inicial es sobre dicha variable y sobre la viscosidad. Resulta necesario deshacer la transformación logaritmo para poder obtener la predicción en la escala original de la viscosidad.Estamos interesados en conocer la viscosidad media (estimación de la media) del producto final cuando el contenido de aceite es de 10, 20, 30, 40, y 50.De esta forma obtenemos las predicciones en la escala original de la variable viscosidad. ¿Cómo interpretamos los valores de predicción obtenidos?Realizamos ahora los gráficos de predicción para log(viscosidad) y viscosidad. Para este último introducimos el código necesario para deshacer la transformación.\nFigura 6.11: Predicción para los datos de log(viscosidad) (media e IC95%).\n\nFigura 6.12: Predicción para los datos de viscosidad (media e IC95%).\nEn este segundo gráfico se puede ver el efecto de la transformación propuesta. De hecho, la predicción obtenida captura la tendencia observada en los datos originales.","code":"\n# cargamos datos de predicción\nnewpred <- data.frame(aceite = c(10, 20, 30, 40, 50))\n# Predicción para la media de la respuesta\n# Opción interval = \"confidence\" \nnewdata <- data.frame(newpred, \n                      predict(fit.aceite2, \n                              newpred, \n                              interval = \"confidence\")) \nround(newdata, 2)##   aceite  fit  lwr  upr\n## 1     10 3.10 2.87 3.33\n## 2     20 3.40 3.21 3.58\n## 3     30 3.69 3.53 3.85\n## 4     40 3.98 3.81 4.15\n## 5     50 4.27 4.06 4.49\n# Deshacemos la transformación para volver a la escala de viscosidad\nnewdata[,2:4] <- exp(newdata[,2:4])\nround(newdata,2)##   aceite   fit   lwr   upr\n## 1     10 22.27 17.68 28.05\n## 2     20 29.84 24.90 35.77\n## 3     30 39.99 34.15 46.83\n## 4     40 53.59 45.12 63.64\n## 5     50 71.80 57.85 89.12\n# Gráfico del ajuste\nplot_model(fit.aceite2, \"pred\", terms = ~aceite, \n                ci.lvl = 0.95, \n                show.data = TRUE, \n                axis.title = c(\"Contenido de aceite\", \"log(Viscosidad)\"),\n                title = \" \")\n# Construímos una secuencia de predicción\nnewdata <- data.frame(aceite = seq(min(aceites$aceite),\n                                   max(aceites$aceite), \n                                   length = 50))\n# Predicción para la media de la respuesta\nnewdata <- data.frame(newdata, predict(fit.aceite2, \n                                       newdata, \n                                       interval = \"confidence\"))\n# Deshacemos la transformación para volver a la escala de viscosidad\nnewdata[,2:4] <- exp(newdata[,2:4])\n# Gráfico del ajuste\nggplot(newdata, aes(x = aceite, y = fit)) +\n  geom_line() + \n  geom_ribbon(aes(ymax = upr, ymin = lwr), alpha = 1/5) +\n  geom_point(data = aceites, aes(x = aceite, y = viscosidad)) +\n  labs(x = \"Cantidad de aceite\", y = \"Viscosidad\") +\n  theme_bw()"},{"path":"rlm.html","id":"rlm","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"Unidad 7 Regresión Lineal Múltiple y Polinómica","text":"Como extensión los modelos de regresión lineal simple presentados en la Unidad 6 estudiamos los modelos de regresión lineal múltiple (RLM) y los modelos polinómicos (MP). La diferencia principal entre estos modelos y el de RLS es que estos involucran al menos dos variables predictoras de tipo numérico para tratar de explicar el comportamiento de la respuesta. Aunque la base de construcción del modelo es similar lo tratado en la unidad anterior veremos y estudiaremos con detalle las particularidades de estos modelos. De hecho, veremos que todos los modelos se pueden expresar matemáticamente de una forma única lo que facilita su estudio, y nos permite considerar tanto modelos simples (con pocas predictoras) como los más complejos (con muchas predictoras).Antes de pasar la presentación de estos modelos vamos ver los ejemplos que iremos trabajando lo largo de esta unidad. Al igual que en el modelo RLS el primer paso es la representación de los datos recogidos y realizar un pequeño estudio descriptivo sobre la posible asociación entre la respuesta y cada una de las predictoras consideradas, dado que resulta imposible realizar gráficos multivariantes de la respuesta vs todas las predictoras.Veamos los diferentes ejemplos con los que vamos trabajar lo largo de esta unidad.Ejemplo 1. Datos de Bosque. Para estimar la producción en madera de un bosque se suele realizar un muestreo previo en el que se realizan una serie de medidas destructivas. Disponemos de mediciones para 20 árboles, así como el volumen (VOL) de madera que producen una vez cortados. Las variables consideradas son: HT o altura en pies, DBH el diámetro del tronco 4 píes de altura (en pulgadas), D16 el diámetro del tronco 16 pies de altura (en pulgadas), y VOL el volumen de madera conseguida (en pies cúbicos). El objetivo del análisis es determinar cuál es la relación entre dichas medidas y el volumen de madera, con el fin de poder predecir este último en función de las primeras.\nFigura 7.1: Gráfico de dispersión de Volumen respecto de cada predictora.\nsimple vista todas las predictoras tienen un efecto positivo en el volumen de madera obtenido, lo cual es bastante obvio, ya que cuanto más grande sea el árbol se espera que su volumen sea más grande. Sin embargo, parece que el efecto de los diámetros es superior al de la altura del árbol (pendientes más pronunciadas) aunque resulta difícil distinguir que diámetro puede ser más relevante ya que ambos se comportan de forma similar. Podemos confirmar este hecho realizando un análisis de correlación para este banco de datos.Ejemplo 2. Datos de Concentración. Se ha llevado cabo un experimento para estudiar la concentración presente de un fármaco en el hígado después de sufrir un tratamiento. Se piensa que las variables que pueden influir en la concentración son el peso del cuerpo, el peso del hígado y la dosis de fármaco administrada.\nFigura 7.2: Gráfico de dispersión de la concentración del fármaco respecto de cada predictora.\nEn este caso ninguno de los gráficos parciales muestra una gran asociación entre la concentración del fármaco y cada una de las predictoras. En todos ellos se aprecia una observación un poco más alejada del resto (concentración > 0.6) que podría ser influyente en la obtención del modelo correspondiente.Ejemplo 3. Datos de Papel.  Banco de datos de Papel de la unidad anterior, donde ya pudimos ver que la tendencia observada se comportaba más como una parábola (polinomio de grado 2) que como una recta.\nFigura 7.3: Gráfico de dispersión de resistencia del papel vs concentración de madera.\n","code":"\ndbh <- c(10.2, 13.72, 15.43, 14.37, 15, 15.02, 15.12, 15.24, 15.24, 15.28, 13.78, \n     15.67, 15.67, 15.98, 16.5, 16.87, 17.26, 17.28, 17.87, 19.13)\nd16 <- c(9.3, 12.1, 13.3, 13.4, 14.2, 12.8, 14, 13.5, 14, 13.8, 13.6, 14, \n     13.7, 13.9, 14.9, 14.9, 14.3, 14.3, 16.9, 17.3)\nht <- c(89, 90.07, 95.08, 98.03, 99, 91.05, 105.6, 100.8, 94, 93.09, 89, 102, \n    99, 89.02, 95.09, 95.02, 91.02, 98.06, 96.01, 101)\nvol <- c(25.93, 45.87, 56.2, 58.6, 63.36, 46.35, 68.99, 62.91, 58.13, 59.79, \n     56.2, 66.16, 62.18, 57.01, 65.62, 65.03, 66.74, 73.38, 82.87, 95.71)\nbosque <- data.frame(vol, dbh, d16, ht)\n# Gráficos parciales\ndatacomp = melt(bosque, id.vars = 'vol')\nggplot(datacomp) +\n geom_jitter(aes(value, vol, colour = variable)) + \n facet_wrap(~variable, scales = \"free_x\") +\n labs(x = \"\", y = \"Volumen\") \np.cuerpo <- c(176, 176, 190, 176, 200, 167, 188, 195, 176, 165, 158, 148, 149, 163, \n              170, 186, 146, 181, 149)\np.higado <- c(6.5, 9.5, 9.0, 8.9, 7.2, 8.9, 8.0, 10.0, 8.0, 7.9, 6.9, 7.3, 5.2, 8.4, \n              7.2, 6.8, 7.3, 9.0, 6.4) \ndosis <- c(.88, .88, 1.0, .88, 1.0, .83, .94, .98, .88, .84, .80, .74, .75, .81, .85, \n           .94, .73, .90, .75)\nconcen <- c(.42, .25, .56, .23, .23, .32, .37, .41, .33, .38, .27, .36, .21, .28, .34, \n            .28, .30, .37, .46)\nconcentracion <- data.frame(p.cuerpo, p.higado, dosis, concen)\n# Gráficos parciales\ndatacomp = melt(concentracion, id.vars = 'concen')\nggplot(datacomp) +\n geom_jitter(aes(value, concen, colour = variable)) + \n facet_wrap(~variable, scales = \"free_x\") +\n labs(x = \"\", y = \"Concentración del fármaco\") \nmadera <- c(1, 1.5, 2, 3, 4, 4.5, 5, 5.5, 6, 6.5, 7, 8, 9, 10, 11, 12, 13, 14, 15)\ntension <- c(6.3, 11.1, 20.0, 24, 26.1, 30, 33.8, 34, 38.1, 39.9, 42, 46.1, 53.1, \n             52, 52.5, 48, 42.8, 27.8, 21.9)\npapel <- data.frame(madera, tension)\nggplot(papel, aes(x = madera, y = tension)) +\n geom_point() +\n labs(x = \"Concentración de madera\", y = \"Resistencia del papel\") "},{"path":"rlm.html","id":"tipos-de-modelos-1","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.1 Tipos de modelos","text":"Vemos las diferencias de expresión de cada uno de los modelos que trabajaremos en esta unidad.","code":""},{"path":"rlm.html","id":"modelos-de-rlm","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.1.1 Modelos de RLM","text":"Los modelos de regresión lineal múltiple surgen cuando tratamos de explicar el comportamiento de una variable predictora de tipo continuo través de un conjunto de variables predictoras de tipo continuo mediante una función lineal. De hecho, se trata de describir dicha relación través de una superficie, lineal en las variables explicativas, lo más próxima posible los valores observados de la respuesta. Si \\(X_1, X_2, ..., X_p\\) son las variables predictoras el modelo viene dado por:\\[\\begin{equation}\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon \n\\tag{7.1}\n\\end{equation}\\]Las hipótesis de este modelo es que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante \\(\\sigma^2\\).Los parámetros desconocidos de este modelo son \\((\\beta_0, \\beta_1, ... , \\beta_p, \\sigma^2)\\) donde:\\(\\beta_0\\) se conoce como interceptación y representa el valor de la respuesta cuando la variable predictora toma el valor cero, interpretándose como un efecto común en la relación entre la predictora y la respuesta.Los \\(\\beta_i\\) son las pendientes de la recta asociadas con cada predictora y representa el aumento o disminución del valor de la respuesta cuando aumentamos en una unidad el valor de la predictora. En este tipo de modelos dicho parámetro se conoce también como el efecto de la predictora sobre la respuesta.\\(\\sigma^2\\) es la varianza residual del modelo.Dada un muestra de \\(n\\) sujetos de la variable respuesta \\((y_1, ..., y_n)\\) y de las variables predictoras \\((x_{11}, ..., x_{n1}), (x_{12}, ..., x_{n2}), ..., (x_{1p}, ..., x_{np})\\), el modelo de regresión lineal múltiple se puede escribir como:\\[Y = \\left(\\begin{array}{c}\n  y_1 \\\\\n  y_2 \\\\\n  ...\\\\\n  y_n\\\\\n \\end{array} \\right) = \n \\left(\\begin{array}{cccc}\n  1 & x_{11} & ... & x_{1p}\\\\\n  1 & x_{21} & ...& x_{2p}\\\\\n  ...& ...  & ...& ...\\\\\n  1 &  x_{n1}& ...& x_{np}\\\\\n \\end{array} \\right)\n \\left(\\begin{array}{c}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  ....\\\\\n  \\beta_p\\\\\n \\end{array} \\right) + \n \\left(\\begin{array}{c}\n  e_1 \\\\\n  e_2 \\\\\n  ...\\\\\n  e_n\n \\end{array} \\right) = X \\beta + \\epsilon\\]donde \\(X\\) se denomina matriz del diseño, representando el efecto común (columna de 1’s) y el efecto de las predictoras (cada columna con los valores de la variable), y los \\(e_i\\) representan los errores aleatorias para cada uno de los sujetos de la muestra.Los bancos de datos de bosque y concentración quedarían englobados dentro de este conjunto de modelos con la siguiente propuesta:Datos de bosque\\[\n\\text{vol} = \\beta_{0} + \\beta_{1}\\text{dbh} + \\beta_{2}\\text{d16} + \\beta_{3}\\text{ht} + \\epsilon\n\\]Datos de concentración\\[\n\\text{concen} = \\beta_{0} + \\beta_{1}\\text{p.cuerpo} + \\beta_{2}\\text{p.higado} + \\beta_{3}\\text{dosis} + \\epsilon\n\\]","code":""},{"path":"rlm.html","id":"modelos-de-rp","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.1.2 Modelos de RP","text":"Los modelos de regresión lineal múltiple surgen cuando tratamos de explicar el comportamiento de una variable predictora de tipo continuo través de una variable predictora de tipo continuo mediante una función polinómica lineal. En general, los modelos polinómicos son útiles cuando se aprecia una tendencia curvilínea entre los predictores y la respuesta. Asimismo, veces constituyen una aproximación sencilla (por serie de Taylor) modelos complejos e incluso -lineales. Si \\(X\\) es la variable predictora y queremos un polinomio de grado \\(k\\) el modelo viene dado por:\\[\\begin{equation}\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + ... + \\beta_k X^k + \\epsilon\n\\tag{7.2}\n\\end{equation}\\]Las hipótesis de este modelo es que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante \\(\\sigma^2\\).Los parámetros desconocidos de este modelo son \\((\\beta_0, \\beta_1, ... , \\beta_k, \\sigma^2)\\) donde:\\(\\beta_0\\) se conoce como interceptación y representa el valor de la respuesta cuando la variable predictora toma el valor cero, interpretándose como un efecto común en la relación entre la predictora y la respuesta.Los \\(\\beta_i\\) son las pendientes de la recta asociadas con cada predictora y representa el aumento o disminución del valor de la respuesta cuando aumentamos en una unidad el valor de la predictora. En este tipo de modelos dicho parámetro se conoce también como el efecto de la potencia de la predictora sobre la respuesta.\\(\\sigma^2\\) es la varianza residual del modelo.Dada un muestra de \\(n\\) sujetos de la variable respuesta \\((y_1, ..., y_n)\\) y de la variable predictora \\((x_{11}, ..., x_{n1})\\), el modelo de regresión polinómico se puede escribir como:\\[Y = \\left(\\begin{array}{c}\n  y_1 \\\\\n  y_2 \\\\\n  ...\\\\\n  y_n\\\\\n \\end{array} \\right) = \n \\left(\\begin{array}{cccc}\n  1 & x_{11} & ... & x^k_{11}\\\\\n  1 & x_{21} & ...& x^k_{21}\\\\\n  ...& ...  & ...& ...\\\\\n  1 &  x_{n1}& ...& x^k_{n1}\\\\\n \\end{array} \\right)\n \\left(\\begin{array}{c}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  ....\\\\\n  \\beta_k\\\\\n \\end{array} \\right) + \n \\left(\\begin{array}{c}\n  e_1 \\\\\n  e_2 \\\\\n  ...\\\\\n  e_n\n \\end{array} \\right) = X \\beta + \\epsilon\\]donde \\(X\\) se denomina matriz del diseño, representando el efecto común (columna de 1’s) y el efecto del grado del polinomio (cada columna con los valores de la variable), y los \\(e_i\\) representan los errores aleatorias para cada uno de los sujetos de la muestra.El banco de datos de papel quedaría englobado dentro de este conjunto de modelos con la siguiente propuesta:\\[\n\\text{tension} = \\beta_{0} + \\beta_{1}\\text{madera} + \\beta_{2}\\text{madera}^2 + \\epsilon\n\\]Ambos tipos de modelos se pueden describir mediante una única formulación:\n\\[\\begin{equation}\nY = X \\beta + \\epsilon\n\\tag{7.3}\n\\end{equation}\\]","code":""},{"path":"rlm.html","id":"expresión-en-r-de-los-modelos","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.1.3 Expresión en R de los modelos","text":"Antes de ver como afecta la estimación del modelo la presencia de más de una predictora o posible efecto sobre la respuesta, vamos ver como podemos expresar los modelos RLM y MP en R.El modelo RLM dado en (7.1) se expresa como:\n\\[Y \\sim X_1 + X_2 + ... + X_p\\]El modelo RLM para una predictora \\(X\\) dado en (7.2) se expresa como:\n\\[Y \\sim X + (X^2) + ... + (X^k)\\]Estas expresiones son una generalización directa del modelo RLS presentado en la unidad anterior.","code":""},{"path":"rlm.html","id":"modelo-saturado-y-anidado","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.1.4 Modelo saturado y anidado","text":"En modelos donde hay más de un efecto sobre la predictora, es decir, tenemos diferentes predictoras o un modelo polinómico, debemos introducir dos conceptos que resultan muy relevantes, y que utilizaremos de forma muy habitual en la selección del mejor modelo.El modelo saturado es aquel que contiene todos los efectos asociados con las diferentes predictoras consideradas. Para los tres ejemplos considerados tendríamos:\\[\\left\\{\n \\begin{array}{lc}\n  \\text{Ejemplo 1}    & vol \\sim dbh + d16 + ht\\\\\n  \\text{Ejemplo 2}    & concen \\sim p.cuerpo + p.higado + dosis\\\\\n  \\text{Ejemplo 3}    & resistencia \\sim madera + madera^2\\\\\n \\end{array}\n \\right. \\]Los modelos anidados son todos los modelos que podemos considerar y que contienen todos los efectos asociados con las predictoras. Si tenemos un modelo con dos predictoras \\(X_1\\), y \\(X_2\\) lo modelos anidados del modelo saturado \\[Y \\sim X_1 +X_2\\] son:\\[\\left\\{\n \\begin{array}{lc}\n  \\text{con } X_1     & Y \\sim X_1\\\\\n  \\text{con } X_2     & Y \\sim X_2\\\\\n  \\text{Sin ninguna}    & Y \\sim 1\\\\\n \\end{array}\n \\right. \\]Todos ellos están “anidados” dentro del modelo saturado y reflejan diferente información. El primero refleja que la respuesta sólo está relacionada con \\(X_1\\), el segundo que la respuesta está relacionada con \\(X_2\\), y el último refleja que hay ninguna predictora relacionada con la respuesta.Debemos tener en cuenta que al incluir más de una predictora debemos decidir si todas ellas son relevantes para explicar el comportamiento de la respuesta, o bien si podemos prescindir de algunas de ellas.La consideración de los diferentes modelos anidados varía en función del modelo con el que trabajemos. En el caso de los de RLM el orden de los modelos anidados es relevante, pero sin embargo si lo es los modelos polinómicos. tiene sentido considerar un modelo en el que sólo se incluya el efecto del polinomio de grado 2 pero que se incluya el de grado 1. Por su propia construcción cuando consideramos un modelo polinómico de grado \\(k\\) se deben considerar obligatoriamente todos los grados desde \\(1\\) hasta \\(k-1\\). Si consideramos un modelo polinómico de grado 4, el orden de los modelos anidados viene dado por:\\[\\left\\{\n \\begin{array}{ll}\n  \\text{saturado }     & Y \\sim X + X^2 + X^3 + X^4\\\\\n  \\text{grado 3 }      & Y \\sim X + X^2 + X^3 \\\\\n  \\text{grado 2 }      & Y \\sim X + X^2 \\\\\n  \\text{grado 1 }      & Y \\sim X \\\\\n  \\text{sin efectos }      & Y \\sim 1 \\\\\n \\end{array}\n \\right. \\]la hora de ajustar un modelo polinómico, siempre serán preferibles modelos con órdenes pequeños antes que grandes (principio de parsimonia o simplicidad). Siempre trataremos de seleccionar le modelo con un orden más pequeño, es decir, con menos efectos pero con igual predictivo que el modelo saturado.","code":""},{"path":"rlm.html","id":"estimación-e-inferencia","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2 Estimación e inferencia","text":"Los procesos de estimación e inferencia del RLM y MP se basan en los mismos principios que los del modelo RLS estudiados en la unidad anterior. De hecho, las hipótesis sobre los errores de incorrelación, varianza constante y media cero son suficientes para obtener el ajuste por mínimos cuadrados del modelo propuesto. La normalidad es necesaria para obtener las inferencias y concluir sobre su fiabilidad.Sin embargo, este tipo de modelos de regresión que consideran más de una predictora adolecen de un problema que puede ser muy relevante en su análisis. Dado que todas las predictoras vendrán medidas en la misma escala de medida, el modelo obtenido (más concretamente los coeficientes del modelo) exhibe una dependencia de dicha escala que puede provocar que una variable con una variabilidad pequeña aparezca con un coeficiente grande en el modelo estimado. Para evitar esa dependencia se suele trabajar con las variables estandarizadas, es decir, corregidas por su media y desviación típica para eliminar los efectos de escala. Aunque en el apartado teórico mostraremos la solución para las variables en escala original, en la parte práctica mostraremos los coeficientes para las variables estandarizadas y veremos los cambios entre ambos modelos. Para denotar las variables transformadas añadiremos el prefijo Z al nombre de la predictora la hora de escribir los modelos obtenidos.","code":""},{"path":"rlm.html","id":"mínimos-cuadrados","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.1 Mínimos cuadrados","text":"Para estimar \\(\\beta\\) seguimos el criterio de minimizar la suma de cuadrados debida al error, esto es,\\[\nmin_{\\beta} \\quad \\epsilon'\\epsilon = min_{\\beta} \\quad (Y-X \\beta)'(Y-X \\beta) = min_{\\beta} \\quad Y'Y -2 \\beta'X'Y + \\beta'X'X\\beta.\n\\]Tras derivar la expresión anterior respecto de \\(\\beta\\) e igualarlo cero, se obtiene el estimador de mínimos cuadrados de \\(\\beta\\) para el modelo (7.3), \\(\\hat{\\beta}\\), resolviendo las \\(p\\) ecuaciones normales:\\[\\begin{equation}\nX'X \\beta=X'Y.\n\\tag{7.4}\n\\end{equation}\\]la hora de resolver (7.4), se pueden presentar dos situaciones:Las \\(p\\) ecuaciones normales que resultan de (7.4) son independientes y por lo tanto existe la inversa de \\(X'X\\). Esto ocurre cuando las variables explicativas son independientes entre sí. Entonces el modelo ha de expresarse en términos de menos parámetros (modificarse) o han de incorporarse restricciones adicionales sobre los parámetros para dar una matriz singular.Cuando \\((X'X)\\) es singular, el estimador de \\(\\beta\\) se obtiene partir de una matriz inversa generalizada \\(X'X\\), \\((X'X)^{-}\\), como:\\[\\begin{equation}\n\\hat{\\beta}=(X'X)^{-} X'Y.\n\\tag{7.5}\n\\end{equation}\\]Así, diferentes elecciones de la inversa generalizada \\((X'X)^{-}\\) producen diferentes estimaciones de \\(\\beta\\). Sin embargo, el modelo ajustado es el mismo, esto es, \\(\\hat{y}=X \\hat{\\beta}\\) es invariante la inversa generalizada elegida.Las \\(p\\) ecuaciones normales son independientes, con lo que \\(X'X\\) es singular y existe su inversa. El estimador de mínimos cuadrados resulta:\\[\\begin{equation}\n\\hat{\\beta}=(X'X)^{-1} (X'Y).\n\\tag{7.6}\n\\end{equation}\\]","code":""},{"path":"rlm.html","id":"propiedades","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.2 Propiedades","text":"Cuando prescindimos de la hipótesis de normalidad de los errores, obtenemos la estimación por mínimos cuadrados, que tiene las siguientes propiedades:El estimador de mínimos cuadrados \\(\\hat{\\beta}\\) minimiza \\(\\epsilon'\\epsilon\\), independientemente de la distribución de los errores. La hipótesis de normalidad se añade para justificar las inferencias basadas en estadísticos \\(t\\) o \\(F\\).El estimador de mínimos cuadrados \\(\\hat{\\beta}\\) minimiza \\(\\epsilon'\\epsilon\\), independientemente de la distribución de los errores. La hipótesis de normalidad se añade para justificar las inferencias basadas en estadísticos \\(t\\) o \\(F\\).Los elementos de \\(\\hat{\\beta}\\) son funciones lineales de las observaciones \\(y_1, \\ldots, y_n\\) y son estimadores insesgados de mínima varianza, sea cual sea la distribución de los errores. Así tenemos:Los elementos de \\(\\hat{\\beta}\\) son funciones lineales de las observaciones \\(y_1, \\ldots, y_n\\) y son estimadores insesgados de mínima varianza, sea cual sea la distribución de los errores. Así tenemos:\\[\nE(\\hat{\\beta})=\\beta \\ \\quad \\mbox{ y }\\ \\quad Var(\\hat{\\beta})=\\sigma^2 (X'X)^{-1} .\n\\]Las estimaciones/predicciones de la variable respuesta \\(y\\) se obtienen con el modelo lineal ajustado:\\[\n\\hat{y}=X\\hat{\\beta}.\n\\]Los residuos \\(e=y-X\\hat{\\beta}\\) verifican:\\(\\sum_{=1}^n e_i \\hat{y}_i = 0 \\ \\Leftrightarrow \\ e'\\hat{y}=\\hat{y}' e = 0\\)\\(\\sum_{=1}^n e_i \\hat{y}_i = 0 \\ \\Leftrightarrow \\ e'\\hat{y}=\\hat{y}' e = 0\\)La ortogonalidad entre los vectores de estimaciones y de residuos, \\(\\hat{y}\\) y \\(e\\) respectivamente, implica el teorema de Pitágoras:La ortogonalidad entre los vectores de estimaciones y de residuos, \\(\\hat{y}\\) y \\(e\\) respectivamente, implica el teorema de Pitágoras:\\[\n  |y|^2=|\\hat{y}|^2+|e|^2 \\ \\Leftrightarrow \\ \\sum_{=1}^n y_i^2= \\sum_{=1}^n \\hat{y}_i^2 + \\sum_{=1}^n e_i^2.\n  \\]\\(\\sum_{=1}^n e_i = 0 \\ \\Leftrightarrow \\ e'\\mathbf{1}=\\mathbf{1}' e = 0\\)","code":""},{"path":"rlm.html","id":"máxima-verosimilitud","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.3 Máxima verosimilitud","text":"Como ocurría en el modelo RLS el estimador de mínimos cuadrados coincide con el máximo verosímil, ya que bajo la hipótesis de normalidad de los errores aleatorios, la verosimilitud conjunta tiene la forma:\\[\nL(\\beta;y) \\propto f(y;\\beta) \\propto \\left(\\frac{1}{\\sigma^2}\\right)^{n/2} \\quad exp\\left\\{-\\frac{(y-X\\beta)'(y-X\\beta)}{2 \\sigma^2}\\right\\}, \n\\]y maximizar la verosimilitud es equivalente minimizar la log-verosimilitud cambiada de signo, que coincide con la suma de cuadrados del error para un valor fijo de \\(\\sigma^2\\).De nuevo utilizaremos la hipótesis de normalidad para proceder con el proceso de inferencia sobre el modelo (7.3).","code":""},{"path":"rlm.html","id":"inferencia","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.4 Inferencia","text":"Para establecer los procedimientos de inferencia asociados con el modelo (7.3) es preciso incorporar la hipótesis de normalidad de los errores. partir de ella podemos obtener la distribución de los estadísticos y estimadores involucrados en el proceso de inferencia con el modelo lineal ajustado.","code":""},{"path":"rlm.html","id":"varianza-del-modelo","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.4.1 Varianza del modelo","text":"Podemos obtener un estimador de \\(\\sigma^2\\) basado en la variabilidad que ha quedado sin explicar por el modelo, cuantificada por lo que llamamos suma de cuadrados residual SSE:\\[\n\\begin{array}{ll}\nSSE=\\sum_{=1}^n (y_i-\\hat{y}_i)^2 &= e'e \\\\\n&= y'y - 2 \\hat{\\beta}' X'y + \\hat{\\beta}'X'X \\hat{\\beta} \\\\\n&= y'y - \\hat{\\beta}' X'y.\n\\end{array}\n\\]Puesto que en el modelo lineal propuesto se estiman \\(p\\) parámetros, la suma de cuadrados residual \\(SSE\\) tiene asociados \\(n-p\\) grados de libertad (el número de datos menos el de coeficientes del modelo). El cociente entre \\(SSE\\) y sus grados de libertad, \\(n-p\\), es el estimador de mínimos cuadrados de \\(\\sigma^2\\), y es además, un estimador insesgado:\\[\n\\hat{\\sigma}^2=s^2 = MSE=\\frac{SSE}{n-p}.\n\\]Asumiendo que el modelo es cierto, la distribución de probabilidad de la varianza del modelo es proporcional una \\(\\chi^2\\) con \\(n-p\\) grados de libertad,\\[\n\\frac{(n-p)s^2}{\\sigma^2} \\sim \\chi^2_{n-p}.\n\\]","code":""},{"path":"rlm.html","id":"coeficientes-del-modelo","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.4.2 Coeficientes del modelo","text":"Bajo la hipótesis de normalidad de los errores, tenemos que el estimador máximo-verosímil \\(\\hat{\\beta}\\) tiene una distribución normal:\\[\n\\hat{\\beta} \\sim N(\\beta, \\sigma^2 (X'X)^{-1}).\n\\]Esto implica que la distribución marginal de cada uno de los coeficientes de la regresión, \\(\\hat{\\beta}_i\\), también es normal,\\[\n\\hat{\\beta}_i \\sim N(\\beta_i, \\sigma^2 C^{X}_{ii}), \\ \\ =0, \\ldots, p-1, \n\\]con \\(C^{X}_{ii}\\) el \\(\\)-ésimo elemento de la diagonal de la matriz \\((X'X)^{-1}\\).En consecuencia, para construir intervalos de confianza o resolver contrastes sobre cada uno de los coeficientes del modelo, individualmente, podemos utilizar estadísticos \\(t\\) que se distribuyen con una distribución t de Student con \\(n-p\\) grados de libertad:\\[\n\\frac{\\hat{\\beta}_i-\\beta_i}{\\sqrt{s^2 C^X_{ii}}} \\ \\sim \\ t_{n-p}, \\ \\ =1, \\ldots, n, \n\\]construidos partir del estimador de \\(\\sigma^2\\), \\(s^2\\).Así, un intervalo de confianza para un coeficiente de interés \\(\\beta_i\\) al nivel \\((1-\\alpha)100\\%\\) viene dado por:\\[\n\\hat{\\beta}_i \\pm t_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 \\ C^X_{ii}}, \n\\]donde \\(t_{(n-p, 1-\\alpha/2)}\\) es el cuantil \\(1-\\alpha/2\\) de una distribución \\(t\\) con \\(n-p\\) grados de libertad.El contraste \\(H_0:\\beta_i=0\\) se resolverá con el rechazo de \\(H_0\\) nivel \\(1-\\alpha\\) si\\[\n|\\hat{\\beta}_i| > t_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 \\ C^X_{ii}}.\n\\]Cuando se pretende obtener intervalos de confianza para varios coeficientes del modelo la vez, es recomendable ser más conservador. Hay diversas soluciones propuestas para realizar “comparaciones múltiples”, esto es, testar todos los coeficientes la vez, y obtener regiones de confianza conjuntas. Quizá el más conocido es el ajuste de Bonferroni, basado en sustituir el cuantil \\(t_{(n-p, 1-\\alpha/2)}\\) en la expresión anterior, por \\(t_{(n-p, 1-\\alpha/2q)}\\), si \\(q\\) es el número de coeficientes para los que se desea una estimación en intervalo. Se obtendrán entonces unos intervalos de confianza ‘ensanchados’ respecto los intervalos de confianza individuales. Si tenemos ninguna prioridad particular sobre determinados coeficientes, lo lógico será obtener conjuntamente los intervalos de confianza para todos los coeficientes del modelo, esto es, \\(q=p\\).Otra opción para la estimación en intervalo es construir una región de confianza conjunta para todos los parámetros \\(\\beta\\) del modelo, determinando los puntos \\(\\beta\\) de la elipse definida por:\\[\n(\\beta-\\hat{\\beta})'X'X (\\beta-\\hat{\\beta})= (p+1) \\ s^2 \\ F_{(p, n-p, 1-\\alpha)}, \n\\]donde \\(F_{(p, n-p, 1-\\alpha)}\\) es el cuantil \\(1-\\alpha\\) de una distribución \\(F\\) con \\(p\\) y \\(n-p\\) grados de libertad.Es posible construir regiones de confianza conjuntas de este tipo para cualquier subconjunto de coeficientes del modelo. Bastará variar adecuadamente los grados de libertad \\(p\\) y \\(n-p\\). Estas regiones acaban siendo complicadas de interpretar, especialmente cuando la dimensión de \\(\\beta\\) es grande. Sin embargo, en la práctica se suele hacer cuando el número de predictoras es elevado.","code":""},{"path":"rlm.html","id":"ejemplos-3","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.5 Ejemplos","text":"Realizamos el proceso de estimación e inferencia para los modelos saturados correspondientes los ejemplos presentados al inicio de esta unidad. Obtendremos el modelo para las predictoras en escala original y estandarizadas, representaremos los intervalos de confianza de los coeficientes del modelo, y obtendremos el ajuste final del modelo. Utilizaremos la función tab_model de la libreria sjplot para el análisis de los coeficientes del modelo, ya que nos proporciona más información que la función glm_coef.","code":""},{"path":"rlm.html","id":"datos-de-bosque","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.5.1 Datos de Bosque","text":"Ajustamos un modelo RLM para el conjunto de datos bosque.de forma que el ajuste obtenido viene dado por:\n\\[\n\\widehat{\\text{vol}} = -108.58 + 1.63*\\text{dbh} + 5.67*\\text{d16} + 0.69*\\text{ht} \n\\]\nLa interpretación de los coeficientes nos indica que el valor predicho de volumen aumenta en 1.63 unidades por el aumenta de una unidad de DBH, en 5.67 unidades por cada unidad de D16, y 0.69 unidades por cada unidad de HT. la vista de los contrastes individuales (p-valores) podemos concluir que los coeficientes asociados con D16 y HT son significativos, es decir, sus coeficientes si sólo esa variable estuviera presente en el modelo serían distintos de cero. Esta información se ve reforzada por los intervalos de confianza individuales, que además muestran que dichos coeficientes son positivos indicando que el VOL aumenta directamente al aumentar los valores de D16 y HT. Por tanto, el modelo anidado dado por:\n\\[vol \\sim d16 + ht\\]\npodría ser igualmente válido que el que contiene todas las predictoras. continuación, se presenta el ajuste obtenido para cada variable de forma marginal.Representamos gráficamente la estimación e intervalo de confianza de los coeficientes del modelo para apreciar los efectos descritos:Comparamos los resultados con los del modelo estandarizado. La tabla proporciona las estimaciones e intervalo de confianza al 95% de los parámetros en la escala original (Estimates y CI), las estimaciones y CI de los coeficientes del modelo estandarizado (std.Beta y standarized CI), y el p-valor asociado cada coeficienteSe aprecia como la variable más relevante para explicar el comportamiento del volumen de madera es el diámetro del tronco 16 pies de altura con un coeficiente estandarizado de 0.65, que es tres veces superior los coeficientes de las otras predictoras. Podemos ver el gráfico de las estimaciones para el modelo estandarizado:Por último, obtenemos los gráficos del modelo ajustado. Para obtener estos gráficos se asume como valor para la predictoras que están en el gráfico igual su media muestral Por ejemplo, para el gráfico de vol con respecto dbh utilizamos el modelo:\n\\[\n\\widehat{\\text{vol}} = -108.58 + 1.63*\\text{dbh} + 5.67*\\overline{\\text{d16}} + 0.69*\\overline{\\text{ht}}\n\\]\ndonde \\(\\overline{\\text{d16}}\\) y \\(\\overline{\\text{ht}}\\) son respectivamente las medias muestrales de d16 y ht. Los gráficos para cada predictora son:","code":"\n# Ajuste del modelo\nfit.bosque <- lm(vol ~ dbh + d16 + ht, data = bosque)\n# Inferencia sobre los parámetros del modelo\nglm_coef(fit.bosque)##     Parameter              Coefficient Pr(>|t|)\n## 1 (Intercept) -108.58 (-138.56, -78.6)  < 0.001\n## 2         dbh        1.63 (-0.55, 3.8)    0.133\n## 3         d16        5.67 (3.12, 8.22)  < 0.001\n## 4          ht        0.69 (0.35, 1.04)  < 0.001\n# Gráfico del ajuste\nplot_model(fit.bosque, \n        show.values = TRUE, \n        vline.color = \"yellow\")\n# Inferencia sobre los parámetros del modelo\ntab_model(fit.bosque, \n     show.std = TRUE, \n     show.r2 = FALSE)\n# Gráfico del ajuste\nplot_model(fit.bosque, \n        show.values = TRUE, \n        vline.color = \"yellow\", \n        type = \"std\")\n# Gráfico del ajuste\nplot_model(fit.bosque, \"pred\",  \n        ci.lvl = NA, \n        show.data = TRUE, \n        title = \"Modelo ajustado\")## $dbh## \n## $d16## \n## $ht"},{"path":"rlm.html","id":"datos-de-concentración","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.5.2 Datos de concentración","text":"Ajustamos un modelo RLM para el conjunto de datos de concentración.de forma que el ajuste obtenido viene dado por:\n\\[\n\\widehat{\\text{concen}} = 0.27 - 0.02*\\text{p.cuerpo} + 0.01*\\text{p.higado} + 4.18*\\text{dosis} \n\\]\nLa interpretación de los coeficientes nos indica que el valor predicho de la concentración aumenta con el peso del hígado y dosis suministrada, pero disminuye con el peso del cuerpo. Los valores tan pequeños de los coeficientes asociados los pesos podrían indicar que dichas variables tienen gran capacidad predictiva, pero hay que tener en cuenta que dichas variables están medidas en un escala distinta de la dosis, y que por tanto la estimación de los coeficientes se ve influenciada por dicha escala. Si nos fijamos en los coeficientes estandarizados apreciamos la misma tendencia en todas la preditoras (signo del coeficiente) pero vemos como tanto el p.cuerpo como la dosis tienen un peso similar para explicar el comportamiento de la concentración.Del análisis de los contrastes individuales podríamos descartar la variable peso del hígado (p-valor >0.05) para explicar el comportamiento de la concentración del compuesto (algo que deberemos comprobar posteriomente), y considerar el resto de predictoras en el modelo anidado:\n\\[concen \\sim p.cuerpo + dosis\\]\nLos gráficos de los coeficientes del modelo (estandarizados y estandarizados) nos permite ver gráficamente estas conclusiones:¿qué ocurre cuando realizamos el gráfico del ajuste para este conjunto de datos?","code":"\n# Ajuste del modelo\nfit.concen <- lm(concen ~ p.cuerpo + p.higado + dosis, data = concentracion)\n# Inferencia\ntab_model(fit.concen, \n     show.std = TRUE, \n     show.r2 = FALSE)\n# Gráfico del ajuste sin estandarizar\nplot_model(fit.concen, \n        show.values = TRUE, \n        vline.color = \"yellow\")\n# Gráfico del ajuste estandarizados\nplot_model(fit.concen, \n        show.values = TRUE, \n        vline.color = \"yellow\", \n        type = \"std\")"},{"path":"rlm.html","id":"datos-de-papel","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.2.5.3 Datos de papel","text":"Ajustamos un modelo MP de grado 2 para el conjunto de datos de papel presentados en la unidad anterior.de forma que el ajuste obtenido viene dado por:\n\\[\n\\widehat{\\text{tension}} = -6.67 + 11.76*\\text{madera} - 0.63*\\text{madera}^2\n\\]\nEl ajuste obtenido es una parábola invertida (coeficiente negativo en la potencia 2) tal y como se observaba en el gráfico de los datos (Figura 6.2). En este tipo de modelos el análisis inferencial se debe centrar en el estudio del orden más alto, para determinar si es adecuado o si podríamos construir un modelo de un orden más simple. La significatividad del coeficiente (p-valor < 0.05) indica que dicho grado es necesario en el modelo. De hecho, los coeficientes estandarizados muestran un efecto similar tanto en el grado 1 como en grado 2. Lo vemos gráficamente:Vemos el gráfico del ajuste obtenido:La tendencia ajustada se corresponde con la observada en los datos del experimento.","code":"\n# Ajuste del modelo\nfit.papel <- lm(tension ~ madera + I(madera^2), data = papel)\n# Inferencia\ntab_model(fit.papel, \n     show.std = TRUE, \n     show.r2 = FALSE)\n# Gráfico del ajuste sin estandarizar\nplot_model(fit.papel, \n        show.values = TRUE, \n        vline.color = \"yellow\")\n# Gráfico del ajuste estandarizados\nplot_model(fit.papel, \n        show.values = TRUE, \n        vline.color = \"yellow\", \n        type = \"std\")\n# Gráfico del ajuste\nplot_model(fit.papel, \"pred\",  \n        ci.lvl = NA, \n        show.data = TRUE, \n        title = \"Modelo ajustado\")## $madera"},{"path":"rlm.html","id":"bondad-del-ajuste-1","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.3 Bondad del ajuste","text":"En este punto presentamos los procedimientos de bondad de ajuste habituales en los modelos de regresión: Análisis de la tabla Anova, el coeficiente de determinación, y el coeficiente de determinación ajustado. Por el momento nos centraremos en el estudio de bondad de ajuste del modelo saturado. En las secciones siguientes veremos como determinar el conjunto de predictoras más relevantes para explicar el comportamiento de la respuesta, y utilizaremos de nuevo estos criterios para valorar el ajuste obtenido.","code":""},{"path":"rlm.html","id":"tabla-anova","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.3.1 Tabla ANOVA","text":"Habitualmente, la primera forma de juzgar la calidad del ajuste obtenido consiste en valorar la variabilidad de la respuesta que se ha podido explicar con el modelo propuesto. En lo que sigue, asumiremos que el modelo ajustado es \\(\\hat{y}=X\\hat{\\beta}\\), donde la matriz de diseño \\(X\\) tiene por columnas todas las variables explicativas consideradas, sean continuas o dummies definidas para representar algún factor. En todo caso, suponemos que hemos estimado \\(p\\) coeficientes, esto es, \\(\\hat{\\beta} \\\\mathbb{R}^p\\).Descomponemos pues la variabilidad de las observaciones \\(y\\), en la parte explicada por el modelo ajustado y corregida por la media de los datos (suma de cuadrados de la regresión), \\(SSR\\), y la parte residual (suma de cuadrados debida al error) que ha quedado sin explicar, \\(SSE\\):\\[\n\\underbrace{(y-\\bar{y}1)'(y-\\bar{y}1)}_{S_{yy}}= \\underbrace{(\\hat{y}-\\bar{y}1)'(\\hat{y}-\\bar{y}1)}_{SSR} +\\underbrace{e'e}_{SSE}, \n\\]donde \\(\\bar{y}=\\sum_i y_i/n\\).Los grados de libertad asociados \\(SSR\\) son \\(p-1\\), pues se pierde un parámetro al corregir la estimación \\(\\hat{y}\\) (obtenida partir de \\(p\\) parámetros) por la media \\(\\bar{y}\\). La suma de cuadrados del error \\(SSE\\) tiene asociados \\(n-p\\) grados de libertad, esto es, el número de datos menos el número de parámetros estimados en el modelo. Al dividir las sumas de cuadrados por sus grados de libertad respectivos, obtenemos los cuadrados medios correspondientes, \\(MSR=SSR/(p-1)\\) y \\(MSE=SSE/(n-p)\\), que nos resultan útiles para valorar la bondad del ajuste. El test de bondad de ajuste propone el contraste:\\[\\begin{equation}\nH_0: \\beta_1=\\beta_2=\\ldots=\\beta_{p-1}=0, \\qquad H_1: \\mbox{ algún } \\beta_i \\neq 0.\n\\tag{7.7}\n\\end{equation}\\]Cuando el modelo es bueno, \\(MSR\\) y \\(MSE\\) siguen sendas distribuciones proporcionales chi-cuadrados independientes (con la misma constante de proporcionalidad \\(\\sigma^2\\)), con \\(p-1\\) y \\(n-p\\) grados de libertad respectivamente; de ahí que su cociente (libre ya de la constante desconocida \\(\\sigma^2\\)) resulta tener una distribución \\(F\\) con \\(p-1\\) y \\(n-p\\) grados de libertad:\\[\\begin{equation}\nF=\\frac{SSR/(p-1)}{SSE/(n-p)}=\\frac{MSR}{MSE} \\ \\sim \\ F_{(p-1, n-p)}.\n\\tag{7.8}\n\\end{equation}\\]Así, con dicho estadístico \\(F\\) contrastamos si la variabilidad explicada por el modelo ajustado es suficientemente grande comparada con la que queda sin explicar (la de los residuos); en otras palabras, si el modelo ajustado es significativo para explicar la variabilidad de los datos. Si el p-valor asociado al estadístico F es inferior la significatividad considerada (generalmente 0.05), rechazamos que el modelo propuesto explique conjuntamente la respuesta, y concluimos favor de que algunas de las covariables contienen información significativa para predecir la respuesta, esto es, favor de la bondad del ajuste. En otro caso, podemos garantizar significativamente la bondad del modelo propuesto.La Tabla de Anova es la forma habitual de presentar toda la información de las sumas, medias de cuadrados, estadísticos \\(F\\) y p-valores asociados al contraste de bondad de ajuste del modelo. La salida de la Tabla Anova que proporciona R es exactamente la habitual presentada en todos los libros. En dicha tabla, en lugar de contrastar globalmente el ajuste través de la suma de cuadrados asociada la regresión, se contrasta secuencialmente la significatividad de cada una de las covariables la hora de explicar la variable respuesta en presencia de las variables que ya han sido incorporadas al modelo (las que quedan por encima en la salida). Sin embargo, con dicha salida es posible calcular el test F de bondad de ajuste.Utilizaremos diferentes funciones para obtener el estadístico F y el p-valor asociado, y las funciones anova() para obtener la descomposición de la tabla ANOVA.","code":""},{"path":"rlm.html","id":"coeficiente-determinación","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.3.2 Coeficiente determinación","text":"El coeficiente de determinación, \\(R^2\\), se define como la parte proporcional de la variabilidad de los datos que es explicada por el modelo ajustado:\\[\\begin{equation}\nR^2 = \\frac{SSR}{S_{yy}} = 1 - \\frac{SSE}{S_{yy}}\n\\tag{7.9}\n\\end{equation}\\]Por definición tenemos que \\(0 \\leq R^2 \\leq 1\\). Un ajuste perfecto de los datos produciría \\(R^2=1\\). Si ninguna de las variables predictoras \\(X_1, \\ldots, X_{p-1}\\) es útil para explicar la respuesta \\(Y\\), entonces \\(R^2=0\\).Siempre es posible conseguir \\(R^2\\) suficientemente grande, simplemente añadiendo más términos en el modelo. Por ejemplo, si hay más de un valor de \\(y\\) para un mismo \\(x\\) observado, un polinomio de grado \\(n-1\\) proporcionará un ajuste “perfecto” (\\(R^2=1\\)) para \\(n\\) datos. Cuando esto ocurre y hay únicamente un valor de \\(y\\) por cada \\(x\\), \\(R^2\\) nunca puede ser igual 1 porque el modelo puede explicar la variabilidad debida al error puro.Aunque \\(R^2\\) siempre aumenta cuando añadimos una variable explicativa al modelo, esto significa necesariamente que el nuevo modelo sea superior al antiguo, es decir, que dicha variable sea útil para explicar mejor los datos. pesar de que la suma de cuadrados residual \\(SSE\\) del nuevo modelo se reduce por una cantidad igual al anterior \\(MSE\\), el nuevo modelo tendrá un \\(MSE\\) mayor debido que pierde un grado de libertad. Por lo tanto, el nuevo modelo será de hecho, peor que el antiguo.En consecuencia, algunos analistas prefieren utilizar una versión ajustada del estadístico \\(R^2\\). El \\(R^2\\) ajustado penaliza los modelos que incorporan variables innecesarias dividiendo las sumas de cuadrados por sus grados de libertad, esto es,\\[\\begin{equation}\nR^2_a=1-\\frac{SSE/(n-p)}{S_{yy}/(n-1)}=1-(1-R^2)\\left(\\frac{n-1}{n-p}\\right).\n\\tag{7.10}\n\\end{equation}\\]\\(R^2_a\\) es preferible \\(R^2\\) cuando sus valores difieren mucho. Su interpretación tiene algún problema debido que puede tomar valores negativos; esto ocurre cuando el estadístico \\(F\\) toma valores inferiores 1 (o produce p-valores mayores que 0.05).","code":""},{"path":"rlm.html","id":"ejemplos-4","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.3.3 Ejemplos","text":"Analizamos los diferentes ejemplos con los que venimos trabajando lo largo de la unidad.","code":""},{"path":"rlm.html","id":"datos-de-bosque-1","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.3.3.1 Datos de Bosque","text":"Bondad de ajuste para los datos bosque.Tanto el \\(R^2\\) como el \\(R^2\\) ajustado muestran porcentajes del 95% indicando que el modelo ajustado tiene buena capacidad explicativa. Además, el test \\(F\\) de la regresión resulta significativo (p-valor < 0.05) indicando que las predictoras consideradas pueden ser utilizadas para describir el comportamiento del volumen. Los tests individuales de la tabla ANOVA","code":"\n# Bondad del ajuste\nglance(fit.bosque)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.959         0.951  3.10      125. 2.59e-11     3  -48.7  107.  112.     153.          16\n## # … with 1 more variable: nobs <int>\n# Tabla ANOVA\nanova(fit.bosque)## Analysis of Variance Table\n## \n## Response: vol\n##           Df  Sum Sq Mean Sq F value    Pr(>F)    \n## dbh        1 3085.79 3085.79 322.064 5.051e-12 ***\n## d16        1  331.85  331.85  34.635 2.303e-05 ***\n## ht         1  173.42  173.42  18.100 0.0006056 ***\n## Residuals 16  153.30    9.58                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"rlm.html","id":"datos-de-concentración-1","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.3.3.2 Datos de Concentración","text":"Bondad de ajuste para los datos de concentraciónEl \\(R^2\\) y el \\(R^2\\) ajustado muestran valores bastante bajos indicando poco poder explicativo. Además, el p-valor del test \\(F\\) resulta significativo indicando que todos los coeficientes del modelo podrían ser considerados iguales cero. Esto contradice lo visto durante el proceso de estimación de los parámetros del modelo y la tabla ANOVA obtenida donde se aprecia que el efecto asociado con dosis resulta significativo. Este comportamiento puede ser debido al considerar más predictoras de las necesarias o simplemente que las predictoras son adecuadas para explicar el comportamiento de la concentración. En el punto siguiente, donde se tratará la selección del mejor modelo, analizaremos este modelo con más detalle y podremos concluir sobre la validez de las predcitoras.","code":"\n# Bondad del ajuste\nglance(fit.concen)## # A tibble: 1 × 12\n##   r.squared adj.r.squared  sigma statistic p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl>  <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.364         0.237 0.0773      2.86  0.0720     3   23.9 -37.9 -33.1   0.0896          15\n## # … with 1 more variable: nobs <int>\n# Tabla ANOVA\nanova(fit.concen)## Analysis of Variance Table\n## \n## Response: concen\n##           Df   Sum Sq  Mean Sq F value  Pr(>F)  \n## p.cuerpo   1 0.003216 0.003216  0.5383 0.47446  \n## p.higado   1 0.003067 0.003067  0.5134 0.48467  \n## dosis      1 0.044982 0.044982  7.5296 0.01507 *\n## Residuals 15 0.089609 0.005974                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"rlm.html","id":"datos-de-papel-1","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.3.3.3 Datos de Papel","text":"Bondad de ajuste para los datos de papelEn términos de \\(R^2\\) y \\(R^2\\) ajustado el modelo tiene buena capacidad explicativa (porcentajes del 90%) y el test \\(F\\) resulta significativo, indicando que alguno de los coeficientes del modelo debe ser considerado distinto de cero. Dado que se trata de un modelo polinómico nos debemos fijar de la significatividad del término de mayor orden en la tabla ANOVA. En este caso dicho efecto resulta significativo indicando que el modelo propuesto de grado 2 es necesario para explicar el comportamiento de la tensión del papel.","code":"\n# Bondad del ajuste\nglance(fit.papel)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic      p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>        <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.909         0.897  4.42      79.4      4.91e-9     2  -53.6  115.  119.     313.          16\n## # … with 1 more variable: nobs <int>\n# Tabla ANOVA\nanova(fit.papel)## Analysis of Variance Table\n## \n## Response: tension\n##             Df  Sum Sq Mean Sq F value    Pr(>F)    \n## madera       1 1043.43 1043.43   53.40 1.758e-06 ***\n## I(madera^2)  1 2060.82 2060.82  105.47 1.894e-08 ***\n## Residuals   16  312.64   19.54                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"rlm.html","id":"comparación-y-selección-de-modelos","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4 Comparación y selección de modelos","text":"La modelización de datos es siempre una faena tediosa debido la innumerable cantidad de alternativas posibles. Está por determinar el tipo de modelo, las transformaciones más adecuadas, identificar las variables más relevantes, descartar las innecesarias, y posteriormente abordar la diagnosis y validación del modelo, que trataremos en las secciones siguientes. Si el modelo está mal especificado, las estimaciones de los coeficientes pueden resultar considerablemente sesgadas. Una buena especificación del modelo es un trabajo, en general, complicado de obtener.Si se ha optado por la modelización lineal de una respuesta en función de una serie de posibles variables predictoras, y el objetivo es seleccionar el mejor subconjunto de predictores para explicar la respuesta, el planteamiento es siempre el de obtener “buenas” predicciones. Sin embargo, sabemos que cuantos más predictores incluyamos en el modelo, mejores predicciones tendremos (menos sesgo), pero la vez menos precisión sobre ellas (ya que la varianza es proporcional al número de variables predictoras en el modelo). Para la selección del “mejor” modelo habremos de llegar un compromiso entre estos dos propósitos. Tratamos pues la selección de variables como un problema de comparación y selección de modelos.Vamos presentar diversos criterios para comparar modelos y seleccionar el mejor modelo de entre dos alternativas. En ocasiones todos darían los mismos resultados, pero generalmente , por lo que habrá de ser el analista el que decida qué criterio utilizar en función de sus intereses prioritarios. La selección del modelo se puede realizar con múltiples criterios pero aquí presentamos los más habituales basados en:la significatividad de los predictores que están presentes en el modelo y los que ;Los estadísticos \\(AIC\\) (Akaike Information Criteria) y \\(BIC\\) (Bayesian Information Criteria);Una vez seleccionado el “mejor” modelo según el criterio elegido, habremos de proseguir la confirmación del mismo realizando la diagnosis y la validación del modelo, que puede fallar en algún paso, lo que nos conduciría de nuevo la reformulación del modelo (y todos los pasos que le siguen), optando por aquellas correcciones y/o transformaciones de variables sugeridas en el diagnóstico. La consecución del mejor modelo será pues, un procedimiento iterativo, basado en selección y valoración de la calidad del ajuste, diagnóstico y validación. En muchas situaciones prácticas nos conformaremos con encontrar el modelo que tenga un funcionamiento más adecuado aunque sea prefecto.","code":""},{"path":"rlm.html","id":"significatividad-de-los-predictores","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.1 Significatividad de los predictores","text":"Este procedimiento se basa en la comparación de modelos basada en las sumas de cuadrados y el test \\(F\\) resultante de compararlas.Supongamos que tenemos ajustado un modelo definido por \\(p\\) coeficientes. Si queremos valorar la contribución que hacen al ajuste de los datos un subconjunto de \\(q\\) variables predictoras adicionales, debemos plantear el contraste\\[\\begin{equation}\nH_0: y=X_p \\beta_p + \\epsilon, \\ \\ vs. \\ \\ H_1:y=X_{p+q} \\beta_{p+q} + \\epsilon.\n\\tag{7.11}\n\\end{equation}\\]Para resolver el contraste (7.11) se utiliza una versión del test \\(F\\) de regresión de la tabla ANOVA. Para resolverlo basta con ajustar los modelos con \\(p\\) y \\(p+q\\) predictoras, para obtener las sumas de cuadrados del error respectivas, \\(SSE(p)\\) y \\(SSE(p+q)\\). Su diferencia representa la reducción del error debida la inclusión de los \\(q\\) regresores adicionales, y bajo \\(H_0\\) tienen una distribución chi-cuadrado, independiente de \\(SSE(p)\\). Se puede definir entonces un estadístico \\(F\\) para realizar la comparación de modelos y resolver el contraste (7.11), dado por:\\[\\begin{equation}\nF_q=\\frac{(SSE(p)-SSE(p+q))/q}{SSE(p)/(n-p)} \\sim F_{q, n-p}.\n\\tag{7.12}\n\\end{equation}\\]Las \\(q\\) variables adicionales se consideran relevantes (significativas) en la explicación de la respuesta, si \\(F_q\\) tiene asociado un p-valor significativo. Un criterio para seleccionar el mejor modelo es quedarse con aquel menos complejo (en términos de predictoras presentes en el modelo) y que pueda considerarse con la misma capacidad predictiva (test \\(F\\) parcial significativo) que cualquier otro más complejo.","code":""},{"path":"rlm.html","id":"estadísticos-aic-y-bic","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.2 Estadísticos AIC y BIC","text":"El criterio de información de Akaike (Akaike, 1973) está basado en la función de verosimilitud e incluye una penalización que aumenta con el número de parámetros estimados en el modelo. Premia pues, los modelos que dan un buen ajuste en términos de verosimilitud y la vez son parsimoniosos (tienen pocos parámetros).Si \\(\\hat{\\beta}\\) es el estimador máximo-verosímil del modelo de dimensión \\(p\\), y \\(l(\\theta)\\) denota el logaritmo (neperiano) de la verosimilitud asociada con dicho modelo, el estadístico \\(AIC\\) se define por:\\[\\begin{equation}\nAIC=-2\\, l(\\hat{\\beta})+2p.\n\\tag{7.13}\n\\end{equation}\\]Una versión del \\(AIC\\) que tiene en cuenta también el número de datos utilizados en el ajuste, es el Schwarz’s Bayesian criterion (Schwarz, 1978), conocido como \\(BIC\\), y definido por:\\[\\begin{equation}\nBIC=-2\\, l(\\hat{\\beta})+log(n)\\, p.\n\\tag{7.14}\n\\end{equation}\\]Si queremos comparar dos modelos con estos criterios, se debe seleccionar el modelo con un menor valor en estos estadísticos.","code":""},{"path":"rlm.html","id":"selección-automática","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.3 Selección automática","text":"Los criterios anteriores resultan de utilidad cunado queremos comparar dos modelos diferentes (“modelos en competencia”), pero pueden resultar poco prácticos si el número de modelos en competencia es muy elevado, es decir, tenemos muchas posibles variables predictoras. Por ese motivo se introducen los conocidos como procedimientos secuenciales que permiten la evaluación de muchos modelos en competencia en muy poco tiempo, utilizando cualquiera de los criterios anteriores.La idea básica es partir de un modelo con cierto número de regresores, y secuencialmente moverse hacia modelos mejores (según el criterio elegido) con más o menos regresores de entre todos los observados. Una vez elegido el criterio para la selección, distinguimos básicamente entre los siguientes procedimientos secuenciales, en función de cuál es el punto (modelo) de partida y la forma de ir considerando modelos alternativos:hacia adelante,  se parte del modelo más simple y se van incluyendo una una las variables que satisfacen el criterio de inclusión;hacia atrás,  se parte del modelo más complejo y se van excluyendo una una las variables que satisfacen el criterio de exclusión;paso paso,  se suele partir de un modelo y en cada paso se incluye o excluye la variable que satisface el criterio de inclusión/exclusión.Hay que tener en cuenta que dependiendo del tipo de modelo deberemos utilizar un tipo de procedimiento u otro. En el caso de los MRP podemos utilizar el procedimiento hacia adelante, ya que se parte siempre del modelo con un mayor grado y se trata de identificar si dicho grado puede ser eliminado, dado que siempre tratamos de obtener el modelo más parsimonioso. En el caso de los modelos RLM hay una preferencia con respecto al procedimiento secuencial.Los procedimientos hacia adelante y hacia atrás los hemos de llevar cabo en R de forma manual y generalmente se utiliza el test F asociado cada paso para resolver si una variable o efecto debe entrar o salir del modelo. Para ello utilizaremos las funciones drop1() y add1(). El procedimiento paso paso es automático y se realiza con la función step().","code":""},{"path":"rlm.html","id":"funciones-en-r","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.4 Funciones en R","text":"En la librería olsrr dedicada exclusivamente al análisis de modelos de regresión (simple, múltiple y polinómica) se presentan diferentes funciones para los procesos de selección automática de variables utilizando el test \\(F\\) parcial y el criterio AIC. Presentamos sólo aquellas funciones que utilizan como punto de partida el modelo saturado. Dichas funciones son:ols_step_backward_p(model): selección desde el modelo saturado mediante el test \\(F\\). Fijamos el parámetro prem igual 0.05 para marcar el nivel de significatividad del contraste.ols_step_backward_aic(model): selección desde el modelo saturado mediante AIC.Aunque estas funciones pueden mostrar todo el desarrollo de selección (al igual que la función step()), la ventaja principal es que puede mostrar un resumen del proceso final para estudiar el modelo final obtenido. En los ejemplos mostraremos el uso de estas funciones.","code":""},{"path":"rlm.html","id":"ejemplos-5","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.5 Ejemplos","text":"continuación, se muestra como utilizar los criterios de selección de variables y los procedimientos secuenciales de selección en los bancos de datos que venimos trabajando en esta unidad.","code":""},{"path":"rlm.html","id":"datos-de-bosque-2","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.5.1 Datos de Bosque","text":"Veamos como seleccionar el mejor modelo para los datos de bosque. En puntos anteriores ya hemos obtenido el modelo saturado y pudimos ver como la variable dbh parecía resultar relevante para explicar el comportamiento del volumen obtenido. Proponemos un nuevo modelo sin dicha variable y comparamos ambos modelos utilizando los criterios de comparación.Para la comparación de modelos anidados siempre deberemos empezar desde el modelo más sencillo al más complejo.Los modelos que deseamos comparar son:\n\\[\\begin{array}{ll}\nM_2: & vol \\sim d16 + ht\\\\\nM_1: & vol \\sim dbh + d16 + ht\n\\end{array}\\]El test \\(F\\) parcial resulta significativo (p-valor = 0.1326) al comparar los modelos \\(M_1\\) y \\(M_2\\), lo que implica que ambos modelos pueden ser considerados iguales. Comparamos el proceso inferencial en cada modelo, dado que al considerar el modelo más simple estamos admitiendo que dbh es relevante para explicar el comportamiento del volumen.Se puede ver que el \\(R^2\\) ajustado para ambos modelos es prácticamente idéntico reflejando que poseen la misma capacidad explicativa. Los modelos obtenidos muestran estimaciones de los coeficientes muy parecidos para ambos modelos. Eliminar la variable dbh afecta la capacidad explicativa del modelo, y altera la contribución de cada predictora la explicación de la respuesta. El modelo resultante viene dado por:\n\\[\n\\widehat{\\text{vol}} = -105.90 + 7.41*\\text{d16} + 0.68*\\text{ht} \n\\]Utilzamos ahora los criterios \\(AIC\\) y \\(BIC\\) para comparar ambos modelos:Si utilizamos el \\(AIC\\) podemos concluir que el modelo preferido es \\(M_2\\) dado que obtenemos un valor más pequeño, mientras que si usamos el \\(BIC\\) el preferido es \\(M1\\). Sin embargo, dado que en ambos casos las diferencias entre ambos modelos son excesivamente pequeñas concluir que uno es mejor que otro resulta complicado y utilizamos el criterio de simplicidad. Ante modelos parecidos elegimos el menos complejo que en este caso sería el que tiene menos predictoras (modelo \\(M2\\)).Por último, veremos como utilizar el procedimiento secuencial automático por pasos para obtener el mejor modelo para este conjunto de datos. En este caso partimos del modelo saturado.El proceso de selección comienza partir del moldeo saturado y determina para cada predictora cual sería el cambio en el \\(AIC\\) (columna AIC) si dicha variable fuera eliminada del modelo. El modelo saturado tiene un \\(AIC\\) de 48.733 (fila <none>), mientras que el modelo donde se elimina la variable dbh (fila - dbh) tiene un \\(AIC\\) de 49.649. Atendiendo al criterio establecido de quedarnos con el modelo con un menor \\(AIC\\) el modelo preferido sería el saturado. Esto contradice los resultados obtenidos con el test \\(F\\) parcial pero es posible cuando tenemos pocos datos o los valores de \\(AIC\\) están muy próximos.Repetimos el análisis de selección automática con las funciones de la libreria olsrr.Como se puede ver la solución es la misma que la obtenida con la función step()pero la forma de mostrar los resultados es mucho más simple.En casos con muchas posibles predictoras puede resultar más útil compara solo los mejores modelos que podríamos obtener con todas las posibles combinaciones de predictoras. Para realizar esta tarea podemos utilizar la función ols_step_best_subset(). Veamos su funcionamiento con este ejemplo pesar de que el número de predictoras es pequeño, y el número de posibles modelos es reducido.Se presentan diferentes criterios para valorar el mejor modelo de entre todas las combinaciones posibles. En este caso el analista debe decidir cual de los propuestos es más adecuado. El único criterio que se encuentra disponible es el test \\(F\\) parcial.Dado que la capacidad explicativa los dos modelos propuestos es muy similar será preferible el menos complejo. En la fase de diagnóstico ya comprobaremos si ese modelo más simple debe ser modificado o si por el contrario es adecuado para proceder con la fase de predicción.Almacenamos el nuevo modelo:","code":"\n# Modelo saturado\nM1 <- lm(vol ~ dbh + d16 + ht, data = bosque)\n# Construimos modelo sin dbh\nM2 <- lm(vol ~ d16 + ht, data = bosque)\n# Comparación mediante test F\nanova(M2, M1)## Analysis of Variance Table\n## \n## Model 1: vol ~ d16 + ht\n## Model 2: vol ~ dbh + d16 + ht\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     17 177.36                           \n## 2     16 153.30  1     24.06 2.5111 0.1326\n# Comparativa de modelos\ntab_model(M1, M2, show.ci = FALSE)\ng2 <- glance(M2)\ng1 <- glance(M1)\nkable(rbind(g1, g2), digits = 2)\nstats::step(fit.bosque)## Start:  AIC=48.73\n## vol ~ dbh + d16 + ht\n## \n##        Df Sum of Sq    RSS    AIC\n## <none>              153.30 48.733\n## - dbh   1     24.06 177.36 49.649\n## - ht    1    173.42 326.72 61.867\n## - d16   1    213.21 366.51 64.166## \n## Call:\n## lm(formula = vol ~ dbh + d16 + ht, data = bosque)\n## \n## Coefficients:\n## (Intercept)          dbh          d16           ht  \n##   -108.5758       1.6258       5.6714       0.6938\nols_step_backward_p(fit.bosque, prem = 0.05)## \n## \n##                           Elimination Summary                            \n## ------------------------------------------------------------------------\n##         Variable                  Adj.                                      \n## Step    Removed     R-Square    R-Square     C(p)       AIC        RMSE     \n## ------------------------------------------------------------------------\n##    1    dbh           0.9526      0.9471    4.5111    108.4066    3.2300    \n## ------------------------------------------------------------------------\nols_step_backward_aic(fit.bosque)## [1] \"No variables have been removed from the model.\"\nols_step_best_subset(fit.bosque)##  Best Subsets Regression \n## -------------------------\n## Model Index    Predictors\n## -------------------------\n##      1         d16        \n##      2         d16 ht     \n##      3         dbh d16 ht \n## -------------------------\n## \n##                                                     Subsets Regression Summary                                                    \n## ----------------------------------------------------------------------------------------------------------------------------------\n##                        Adj.        Pred                                                                                            \n## Model    R-Square    R-Square    R-Square     C(p)        AIC        SBIC        SBC         MSEP        FPE       HSP       APC  \n## ----------------------------------------------------------------------------------------------------------------------------------\n##   1        0.9084      0.9033      0.8839    19.8001    119.5982    60.6857    122.5854    381.3477    20.9618    1.1210    0.1120 \n##   2        0.9526      0.9471       0.933     4.5111    108.4066    52.1187    112.3895    209.5068    11.9979    0.6521    0.0641 \n##   3        0.9591      0.9514      0.9242     4.0000    107.4909    52.6084    112.4696    193.1589    11.4976    0.6388    0.0614 \n## ----------------------------------------------------------------------------------------------------------------------------------\n## AIC: Akaike Information Criteria \n##  SBIC: Sawa's Bayesian Information Criteria \n##  SBC: Schwarz Bayesian Criteria \n##  MSEP: Estimated error of prediction, assuming multivariate normality \n##  FPE: Final Prediction Error \n##  HSP: Hocking's Sp \n##  APC: Amemiya Prediction Criteria\n# Modelo seleccionado\nfit.bosque <- lm(vol ~ d16 + ht, data = bosque)"},{"path":"rlm.html","id":"datos-de-concentración-2","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.5.2 Datos de Concentración","text":"Veamos como seleccionar el mejor modelo para los datos de concentración. En puntos anteriores ya hemos podido ver que el peso del hígado resultaba poco relevante, con lo que podríamos plantear un contraste para saber si podemos prescindir de dicha variable. Sin embargo, la forma habitual de proceder sería utilizar en primer lugar un procedimiento automático para seleccionar las predictoras y chequear posteriormente mediante un test \\(F\\) parcial si el modelo obtenido posee la misma capacidad explicativa que el modelo saturado.Planteamos el proceso secuencial:En la primera iteración el \\(AIC\\) del modelo saturado es igual -93.78 mientras que el del modelo que prescinde de p.higado es de -94.92. Por tanto, dicha variable se elimina del modelo que pasa tener un \\(AIC\\) de -94.92. En la segunda iteración la variable candidata salir es p.cuerpo, pero su \\(AIC\\) asociado es superior al del modelo actual y se descarta.Verificamos mediante el test \\(F\\) parcial:EL test \\(F\\) resulta significativo indicando que el modelo más simple tiene la misma capacidad explicativa que el más complejo. Estudiamos dicho modelo comparándolo con el saturado.Podemos ver coo el \\(R^2\\) ajustado mejora al eliminar p.higado y los coeficientes son prácticamente idénticos:\\[\n\\widehat{\\text{concen}} = 0.29 - 0.02*\\text{p.cuerpo} + 4.13*\\text{dosis} \n\\]Utilizamos ahora las funciones específicas:Almacenamos el modelo resultante para la fase de diagnóstico:","code":"\nstats::step(fit.concen)## Start:  AIC=-93.78\n## concen ~ p.cuerpo + p.higado + dosis\n## \n##            Df Sum of Sq      RSS     AIC\n## - p.higado  1  0.004120 0.093729 -94.924\n## <none>                  0.089609 -93.778\n## - p.cuerpo  1  0.042408 0.132017 -88.416\n## - dosis     1  0.044982 0.134591 -88.049\n## \n## Step:  AIC=-94.92\n## concen ~ p.cuerpo + dosis\n## \n##            Df Sum of Sq      RSS     AIC\n## <none>                  0.093729 -94.924\n## - p.cuerpo  1  0.039851 0.133580 -90.192\n## - dosis     1  0.043929 0.137658 -89.621## \n## Call:\n## lm(formula = concen ~ p.cuerpo + dosis, data = concentracion)\n## \n## Coefficients:\n## (Intercept)     p.cuerpo        dosis  \n##     0.28552     -0.02044      4.12533\n# Modelo saturado\nM1 <- lm(concen ~ p.higado + p.cuerpo + dosis, data = concentracion)\n# Construimos modelo sin dbh\nM2 <- lm(concen ~ p.cuerpo + dosis, data = concentracion)\n# Comparación mediante test F\nanova(M2, M1)## Analysis of Variance Table\n## \n## Model 1: concen ~ p.cuerpo + dosis\n## Model 2: concen ~ p.higado + p.cuerpo + dosis\n##   Res.Df      RSS Df Sum of Sq      F Pr(>F)\n## 1     16 0.093729                           \n## 2     15 0.089609  1   0.00412 0.6897 0.4193\n# Comparativa de modelos\ntab_model(M1, M2, show.ci = FALSE)\nols_step_backward_p(fit.concen, prem = 0.05)## \n## \n##                           Elimination Summary                            \n## ------------------------------------------------------------------------\n##         Variable                  Adj.                                      \n## Step    Removed     R-Square    R-Square     C(p)       AIC        RMSE     \n## ------------------------------------------------------------------------\n##    1    p.higado      0.3347      0.2515    2.6897    -39.0043    0.0765    \n## ------------------------------------------------------------------------\nols_step_backward_aic(fit.concen)## \n## \n##                    Backward Elimination Summary                   \n## ----------------------------------------------------------------\n## Variable        AIC       RSS     Sum Sq     R-Sq      Adj. R-Sq \n## ----------------------------------------------------------------\n## Full Model    -37.858    0.090     0.051    0.36390      0.23668 \n## p.higado      -39.004    0.094     0.047    0.33466      0.25149 \n## ----------------------------------------------------------------\nfit.concen <- lm(concen ~ p.cuerpo + dosis, data = concentracion)"},{"path":"rlm.html","id":"datos-de-papel-2","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.4.5.3 Datos de Papel","text":"Para el bando de datos de Papel se ha propuesto como modelo uno del tipo polinómico de grado 2. El proceso de selección en este caso se basa en comparar el modelo cuadrático frente al lineal para saber si es posible prescindir del grado 2, o si por el contrario es necesario para explicar la tensión del papel.Los modelos que deseamos comparar son:\n\\[\\begin{array}{ll}\nM_2: & tension \\sim madera\\\\\nM_1: & tension \\sim madera + (madera^2)\n\\end{array}\\]El test \\(F\\) parcial resulta significativo indicando que los modelos considerados tienen capacidades explicativas estadísticamente distintas. podemos rechazar el modelo cuadrático frente al modelo lineal. Veamos la tabla de estimación de ambos modelos:El \\(R^2\\) ajustado pasa del 26.5% en el modelo lineal al 89.7% en el modelo cuadrático indicando una gran mejora en la capacidad explicativa, y por tanto eligiendo este último como el modelo que debe pasar la fase de diagnóstico.Utilizamos las funciones resumen\nUtilizamos ahora las funciones específicas:","code":"\n# Modelo saturado\nM1 <- lm(tension ~ madera + I(madera^2), data = papel)\n# Construimos modelo sin dbh\nM2 <- lm(tension ~ madera, data = papel)\n# Comparación mediante test F\nanova(M2, M1)## Analysis of Variance Table\n## \n## Model 1: tension ~ madera\n## Model 2: tension ~ madera + I(madera^2)\n##   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n## 1     17 2373.46                                  \n## 2     16  312.64  1    2060.8 105.47 1.894e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Comparativa de modelos\ntab_model(M1, M2, show.ci = FALSE)\nols_step_backward_p(fit.papel, prem = 0.05)## [1] \"No variables have been removed from the model.\"\nols_step_backward_aic(fit.papel)## [1] \"No variables have been removed from the model.\""},{"path":"rlm.html","id":"multicolinealidad","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5 Multicolinealidad","text":"La multicolinealidad es un problema relativamente frecuente en regresión lineal múltiple, y en general en análisis con varias variables explicativas, entre cuyas soluciones se halla la selección de variables. Cuando los regresores están relacionados linealmente entre sí, se dice que son ortogonales. Que exista multicolinealidad significa que las columnas de \\(X\\) son linealmente independientes. Si existiera una dependencia lineal total entre algunas de las columnas, tendríamos que el rango de la matriz \\(X'X\\) sería menor \\(p\\) y \\((X'X)^{-1}\\) existiría. El hecho de que haya multicolinealidad, esto es, una relación casi lineal entre algunos regresores, afecta la estimación e interpretación de los coeficientes del modelo.La multicolinealidad es un problema de violación de hipótesis; simplemente es una situación que puede ocasionar problemas en las inferencias con el modelo de regresión. Nos ocupamos continuación de examinar las causas de la multicolinealidad, algunos de los efectos que tiene en las inferencias, los métodos básicos para detectar el problema y algunas formas de tratarlo.","code":""},{"path":"rlm.html","id":"causas","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.1 Causas","text":"Montgomery y Peck (1992) comentan que la colinealidad puede surgir por el método de recogida de datos, restricciones en el modelo o en la población, especificación y sobreformulación del modelo (consideración de más variables de las necesarias); en modelos polinómicos, por ejemplo, se pueden presentar problemas serios de multicolinealidad en la matriz de diseño \\(X\\) cuando el rango de variación de los predictores es muy pequeño. Obviamente, modelos con más covariables son más propicios padecer problemas de multicolinealidad.","code":""},{"path":"rlm.html","id":"efectos","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.2 Efectos","text":"Los principales efectos de la multicolinealidad son los siguientes:Una multicolinealidad fuerte produce varianzas y covarianzas grandes para los estimadores de mínimos cuadrados. Así, muestras con pequeñas diferencias podrían dar lugar estimaciones muy diferentes de los coeficientes del modelo. Es decir, las estimaciones de los coeficientes resultan poco fiables cuando hay un problema de multicolinealidad. De hecho, dichos coeficientes vienen explicar cómo varía la respuesta cuando varía la variable independiente en cuestión y todas las demás quedan fijas; si las variables predictoras están relacionadas entre sí, es inviable que al variar una lo vayan hacer las demás y en consecuencia puedan quedar fijas. La multicolinealidad reduce la efectividad del ajuste lineal si su propósito es determinar los efectos de las variables independientes.consecuencia de la gran magnitud de los errores estándar de las estimaciones, muchas de éstas resultarían significativamente distintas de cero: los intervalos de confianza serán ‘grandes’ y por tanto, con frecuencia contendrán al cero.La multicolinealidad tiende producir estimaciones de mínimos cuadrados \\(\\hat{\\beta}_j\\) muy grandes en valor absoluto.Los coeficientes del ajuste con todos los predictores difieren bastante de los que se obtendrían con una regresión simple entre la respuesta y cada variable explicativa.La multicolinealidad afecta al ajuste global del modelo (medidas como la \\(R^2\\), etc.) y por lo tanto afecta la habilidad del modelo para estimar puntualmente la respuesta o la varianza residual. Sin embargo, al aumentar los errores estándar de las estimaciones de los coeficientes del modelo, también lo hacen los errores estándar de las estimaciones de la respuesta media y de la predicción de nuevas observaciones, lo que afecta la estimación en intervalo.","code":""},{"path":"rlm.html","id":"diagnósticos","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.3 Diagnósticos","text":"Existen diversos diagnósticos propuestos para detectar problemas de multicolinealidad. Consideramos los más relevantes, que son:Los gráficos entre variables explicativas son útiles para estudiar la relación entre las variables explicativas y su disposición en el espacio, y con ello detectar correlaciones o identificar observaciones muy alejadas del resto de datos y que pueden influenciar notablemente la estimación. Consisten en gráficos de dispersión entre un par de covariables continuas o un par de factores (través de sus códigos), y gráficos de cajas cuando se trata de investigar la relación entre un factor y una covariable.Una medida simple de multicolinealidad consiste en la inspección de los elementos fuera de la diagonal de la matriz \\(X'X\\), es decir, las correlaciones simples \\(r_{ij}\\) entre todos los regresores. Si dos regresores \\(x_i\\) y \\(x_j\\) son casi linealmente dependientes, entonces \\(|r_{ij}| \\approx 1\\). Sin embargo, cuando la multicolinealidad involucra varias variables, hay garantías de detectarla través de las correlaciones bivariadas.Puesto que uno de los efectos principales de la multicolinealidad es la inflación de la varianza y covarianza de las estimaciones, es posible calcular unos factores de inflación de la varianza, FIV, que permiten apreciar tal efecto. En concreto, la varianza de \\(\\hat{\\beta}_j\\) viene estimada por \\(Var(\\hat{\\beta}_j)=s^2 \\, C_{jj}\\), donde \\(C_{jj}^X\\) son los elementos de la diagonal de la matriz \\((X'X)^{-1}\\), es decir,\\[\nC_{jj}^X=\\frac{1}{(1-R_j^2) \\, S_{x_j x_j}}, \\ \\ j=1, 2, \\ldots, p, \n\\]con \\(R_j^2\\) el coeficiente de determinación múltiple para la regresión de \\(x_j\\) sobre las restantes \\(p-1\\) covariables. Si hay una correlación muy alta entre \\(x_j\\) y los restantes regresores, entonces \\(R_j^2 \\approx 1\\). En particular, puesto que \\(s^2\\) varía ante un problema de multicolinealidad, si ésta existe, la varianza de \\(\\hat{\\beta}_j\\) aumenta por un factor igual \\(1/(1-R_j^2)\\), que se define como el FIV para \\(x_j\\):\\[\nFIV_j=1/(1-R_j^2).\n\\]Generalmente, valores de un FIV superiores 10 dan indicios de un problema de multicolinealidad, si bien su magnitud depende del modelo ajustado. Lo ideal es compararlo con su equivalente en el modelo ajustado, esto es, \\(1/(1-R^2)\\), donde \\(R^2\\) es el coeficiente de determinación del modelo. Los valores FIV mayores que esta cantidad implican que la relación entre las variables independientes es mayor que la que existe entre la respuesta y los predictores, y por tanto dan indicios de multicolinealidad.Dado que la multicolinealidad afecta la singularidad (rango menor que \\(p\\)) de la matriz \\(X'X\\), sus valores propios \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p\\) pueden revelar multicolinealidad en los datos. De hecho, si hay una o más dependencias casi lineales en los datos, entonces uno o más de los valores propios será pequeño.Dado que la multicolinealidad afecta la singularidad (rango menor que \\(p\\)) de la matriz \\(X'X\\), sus valores propios \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_p\\) pueden revelar multicolinealidad en los datos. De hecho, si hay una o más dependencias casi lineales en los datos, entonces uno o más de los valores propios será pequeño.En lugar de buscar valores propios pequeños, se puede optar por calcular el número de condición de \\(X'X\\), definido por:En lugar de buscar valores propios pequeños, se puede optar por calcular el número de condición de \\(X'X\\), definido por:\\[\n\\kappa = \\lambda_{max}/\\lambda_{min}, \n\\]que es una medida de dispersión en el espectro de valores propios de \\(X'X\\). Generalmente, si el número de condición es menor que 100, hay problemas de multicolinealidad. Números de condición entre 100 y 1000 implican multicolinealidad moderada, y mayores que 1000 implican multicolinealidad severa.Los índices de condición de la matriz \\(X'X\\) también son útiles para el diagnóstico de multicolinealidad y se definen por:\\[\n\\kappa_j = \\lambda_{max}/\\lambda_j, \\ \\ \\ j=1, \\ldots, p.\n\\]El número de índices de condición que son grandes (por ejemplo, \\(\\geq 1000\\)) es una medida útil del número de dependencias casi lineales en \\(X'X\\).Otra posibilidad de diagnóstico es través de un análisis de componentes principales. Este tipo de análisis multivariante se plantea sobre conjuntos de variables relacionadas linealmente entre sí y tiene como finalidad la de definir un conjunto menor de nuevas variables obtenidas como combinación lineal de las originales, y que la vez resultan ortogonales entre sí. Si el análisis de componentes principales resulta significativo, estamos reconociendo multicolinealidad.","code":""},{"path":"rlm.html","id":"soluciones","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.4 Soluciones","text":"Una vez detectado un problema de multicolinealidad, es recomendable intentar aliviarlo (por sus efectos). Para ello disponemos de diversos recursos, y en función del objetivo del análisis, será más aconsejable uno u otro. Básicamente podemos distinguir como objetivos esenciales:Estimar bien la respuesta media en función de un conjunto de variables explicativas, sin importar demasiado la contribución individual de cada una de esas variables.Hacer un análisis de estructura, esto es, describir el efecto de las variables explicativas en la predicción de la respuesta. Las magnitudes y significatividades de los coeficientes son entonces de interés. Así, en un análisis de estructura es importante conseguir un buen modelo de ajuste para cuantificar bien la información que aportan las variables explicativas sobre la respuesta.Hay tres aproximaciones básicas como remedio la multicolinealidad:Selección de variables (ver Sección XX). Respecto la selección de variables, lo ideal ante un problema de multicolinealidad es seleccionar aquellas variables predictoras que son más significativas y contienen la mayor parte de la información sobre la respuesta. Sin embargo, hay que actuar con precaución, pues los métodos automáticos de selección de variables son bastante sensibles cuando existe relación entre los regresores y está garantizado que el modelo resultante tenga menor multicolinealidad. Por otro lado, la capacidad predictiva del modelo puede verse seriamente menguada al reducir el número de covariables consideradas, de modo que este remedio iría más indicado cuando el objetivo del análisis es el 2.Selección de variables (ver Sección XX). Respecto la selección de variables, lo ideal ante un problema de multicolinealidad es seleccionar aquellas variables predictoras que son más significativas y contienen la mayor parte de la información sobre la respuesta. Sin embargo, hay que actuar con precaución, pues los métodos automáticos de selección de variables son bastante sensibles cuando existe relación entre los regresores y está garantizado que el modelo resultante tenga menor multicolinealidad. Por otro lado, la capacidad predictiva del modelo puede verse seriamente menguada al reducir el número de covariables consideradas, de modo que este remedio iría más indicado cuando el objetivo del análisis es el 2.Redefinición de variables. Otra alternativa es transformar las covariables. Para ello es importante identificar entre qué covariables hay relación, con el fin de utilizar transformaciones apropiadas. Si varias variables están relacionadas linealmente, veces funciona considerar la más completa de ellas tal y como es, y transformaciones de las otras con cocientes o diferencias respecto de la más completa. Es decir, si \\(x_i\\) y \\(x_j\\) están relacionadas y \\(x_i\\) da una información más completa que \\(x_j\\), se puede considerar un nuevo ajuste que involucre las variables \\(x_i\\) y \\(x_j/x_i\\), o bien \\(x_i\\) y \\(x_j-x_i\\).\nCuando la intuición o el conocimiento de las variables sugiere ninguna transformación concreta, una opción es llevar cabo un análisis de componentes principales con el fin de obtener nuevas variables, expresables como combinación lineal de las originales, ortogonales entre sí y que contengan toda la información disponible en las primeras. En ocasiones, las componentes que resultan tienen un significado intuitivo por la forma de asimilar la información de las variables originales, y en ocasiones , en cuyo caso se puede proceder la realización de un análisis factorial y la búsqueda de alguna rotación geométrica que permita llegar variables “interpretables”.\nUna vez obtenidas las componentes \\(Z\\), se pueden seguir dos alternativas: ) plantear una regresión de la respuesta explicada por todas las componentes principales obtenidas, o ii) ajustar un modelo de regresión sólo con las componentes más relevantes como variables predictoras (componentes principales incompletas). En el primer caso, partir del modelo ajustado \\(y=Z\\gamma+\\epsilon\\), es posible recuperar el efecto de las variables originales sobre la respuesta sin más que deshacer el cambio. Esto es posible para la segunda alternativa, pues las estimaciones que se consiguen están sesgadas; sin embargo, esta opción reduce la varianza de las estimaciones respecto del modelo original.Redefinición de variables. Otra alternativa es transformar las covariables. Para ello es importante identificar entre qué covariables hay relación, con el fin de utilizar transformaciones apropiadas. Si varias variables están relacionadas linealmente, veces funciona considerar la más completa de ellas tal y como es, y transformaciones de las otras con cocientes o diferencias respecto de la más completa. Es decir, si \\(x_i\\) y \\(x_j\\) están relacionadas y \\(x_i\\) da una información más completa que \\(x_j\\), se puede considerar un nuevo ajuste que involucre las variables \\(x_i\\) y \\(x_j/x_i\\), o bien \\(x_i\\) y \\(x_j-x_i\\).\nCuando la intuición o el conocimiento de las variables sugiere ninguna transformación concreta, una opción es llevar cabo un análisis de componentes principales con el fin de obtener nuevas variables, expresables como combinación lineal de las originales, ortogonales entre sí y que contengan toda la información disponible en las primeras. En ocasiones, las componentes que resultan tienen un significado intuitivo por la forma de asimilar la información de las variables originales, y en ocasiones , en cuyo caso se puede proceder la realización de un análisis factorial y la búsqueda de alguna rotación geométrica que permita llegar variables “interpretables”.\nUna vez obtenidas las componentes \\(Z\\), se pueden seguir dos alternativas: ) plantear una regresión de la respuesta explicada por todas las componentes principales obtenidas, o ii) ajustar un modelo de regresión sólo con las componentes más relevantes como variables predictoras (componentes principales incompletas). En el primer caso, partir del modelo ajustado \\(y=Z\\gamma+\\epsilon\\), es posible recuperar el efecto de las variables originales sobre la respuesta sin más que deshacer el cambio. Esto es posible para la segunda alternativa, pues las estimaciones que se consiguen están sesgadas; sin embargo, esta opción reduce la varianza de las estimaciones respecto del modelo original.Estimación sesgada. Si uno de los efectos de la multicolinealidad es que aumenta el error estándar de las estimaciones por mínimos cuadrados de los coeficientes del modelo, cabe la posibilidad de utilizar estimadores que, aun sesgados, produzcan estimaciones con menor error estándar y un error cuadrático medio inferior al de los estimadores de mínimos cuadrados (que son, de los insesgados, los de mínima varianza).Estimación sesgada. Si uno de los efectos de la multicolinealidad es que aumenta el error estándar de las estimaciones por mínimos cuadrados de los coeficientes del modelo, cabe la posibilidad de utilizar estimadores que, aun sesgados, produzcan estimaciones con menor error estándar y un error cuadrático medio inferior al de los estimadores de mínimos cuadrados (que son, de los insesgados, los de mínima varianza).Hay varios procedimientos de estimación sesgada. Las componentes principales incompletas es uno de ellos. La regresión Ridge es otro método interesante. La regresión Ridge consiste en utilizar como estimador de \\(\\beta\\), el siguiente:\\[\n\\hat{\\beta}_k=(X'X+kI)^{-1} X'y, \n\\]donde \\(k\\) es una constante pequeña arbitraria.Cuando todos los predictores están estandarizados, tenemos que \\(X'X\\) es la matriz de correlaciones, con unos en la diagonal. Así, la correlación “efectiva” que se consigue ahora entre \\(x_i\\) y \\(x_j\\) es \\(r_{ij}/(1+k)\\). Es decir, todas las correlaciones se reducen artificialmente en un factor \\(1/(1+k)\\), reduciendo entonces la multicolinealidad. Valores grandes de \\(k\\) reducen la multicolinealidad pero, como contraprestación, aumentan el sesgo de las estimaciones. Para determinar el valor de \\(k\\) utilizar, se suelen considerar gráficos en los que se representa \\(k\\) versus las estimaciones del modelo (ridge plots). Para valores pequeños de \\(k\\), las estimaciones de los coeficientes cambian mucho, mientras que medida que \\(k\\) aumenta, las estimaciones parecen estabilizarse. Se dice que se consigue un valor óptimo para \\(k\\) cuando se da dicha estabilización en las estimaciones. Este procedimiento resulta pues, algo subjetivo, pero sin embargo ha resultado efectivo en la práctica.Hay otros procedimientos sesgados de estimación propuestos en la literatura que alivian el problema de la multicolinealidad.","code":""},{"path":"rlm.html","id":"ejemplos-6","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.5 Ejemplos","text":"continuación, realizamos el estudio de multicolinealidad para los diferentes bancos de datos que hemos venido trabajando. Para el calculo de los factores de inflacción de la varianza y los números de condición utilizamos la función ols_coll_diag() de la librería olsrr. Con ella obtenemos el VIF asociado con cada variable, el índice de condición asociado con cada valor propio, y la matriz de correlaciones asociada al modelo ajustado.En caso de detectar multicolinealidad trataremos de corregirla con los procedimientos presentados.","code":""},{"path":"rlm.html","id":"datos-de-bosque-3","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.5.1 Datos de bosque","text":"Para el análisis de multicolinealidad tomamos el modelo obtenido después del proceso de selección de variables de la sección anterior.Del análisis realizado parece detectarse multicolinealidad través de \\(VIF\\), ni través de los índices de condición. El valor de \\(1/(1-R^2)\\) para dicho modelo es 21.28 que es superior los valore de \\(VIF\\) observados. Por tanto, parece haber un problema de multicolinealidad con el modelo obtenido.","code":"\n# Modelos\nfit.bosque<- lm(vol ~ d16 + ht, data = bosque)\n# Análisis de multicolinealidad\nols_coll_diag(fit.bosque)## Tolerance and Variance Inflation Factor\n## ---------------------------------------\n##   Variables Tolerance      VIF\n## 1       d16  0.813726 1.228915\n## 2        ht  0.813726 1.228915\n## \n## \n## Eigenvalue and Condition Index\n## ------------------------------\n##    Eigenvalue Condition Index   intercept         d16           ht\n## 1 2.991530311         1.00000 0.000270918 0.001143249 0.0002206333\n## 2 0.007330472        20.20137 0.077611612 0.924370594 0.0288009800\n## 3 0.001139217        51.24405 0.922117470 0.074486157 0.9709783867"},{"path":"rlm.html","id":"datos-de-concentración-3","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.5.2 Datos de concentración","text":"En este caso analizamos el modelo saturado en primer lugar.Hay dos \\(VIF\\) que indican multicolinealidad y un número de condición por encima de 100. Probamos con el modelo obtenido en el proceso de selección de variables:Se siguen presentando problemas de multicolinealidad. Sin embargo, aunque esto puede parecer un problema muy grave lo es dada la situación experimental dada. Es de esperar que la dosis suministrada este claramente asociada con el peso del sujeto, y por tanto dichas variables tienen que estar relacionadas. Aunque la multicolinealidad afecta la precisión del modelo (en este caso es poco relevante porque nuestro ajuste es bastante malo) es un problema con el diagnóstico del modelo. En la sección siguiente determinaremos si el modelo debe ser modificado o si por el contrario nos quedamos con el modelo obtenido tras la selección de variables.","code":"\n# Modelos\nfit.concen<- lm(concen ~ p.cuerpo + p.higado + dosis, data = concentracion)\n# Análisis de multicolinealidad\nols_coll_diag(fit.concen)## Tolerance and Variance Inflation Factor\n## ---------------------------------------\n##   Variables  Tolerance       VIF\n## 1  p.cuerpo 0.01919315 52.101917\n## 2  p.higado 0.74868308  1.335679\n## 3     dosis 0.01944498 51.427154\n## \n## \n## Eigenvalue and Condition Index\n## ------------------------------\n##     Eigenvalue Condition Index    intercept     p.cuerpo    p.higado        dosis\n## 1 3.980955e+00         1.00000 0.0005211255 1.049346e-05 0.001061756 1.138353e-05\n## 2 1.307262e-02        17.45068 0.0912614776 7.180392e-04 0.963765107 8.095775e-04\n## 3 5.885352e-03        26.00803 0.8549633200 4.879743e-03 0.028213823 6.508870e-03\n## 4 8.720917e-05       213.65475 0.0532540769 9.943917e-01 0.006959314 9.926702e-01\n# Modelos\nfit.concen<- lm(concen ~ p.cuerpo + dosis, data = concentracion)\n# Análisis de multicolinealidad\nols_coll_diag(fit.concen)## Tolerance and Variance Inflation Factor\n## ---------------------------------------\n##   Variables  Tolerance      VIF\n## 1  p.cuerpo 0.01947892 51.33755\n## 2     dosis 0.01947892 51.33755\n## \n## \n## Eigenvalue and Condition Index\n## ------------------------------\n##     Eigenvalue Condition Index    intercept     p.cuerpo        dosis\n## 1 2.993933e+00         1.00000 0.0009364888 1.884523e-05 2.017998e-05\n## 2 5.978794e-03        22.37764 0.9398136118 4.217372e-03 5.578232e-03\n## 3 8.781609e-05       184.64349 0.0592498994 9.957638e-01 9.944016e-01"},{"path":"rlm.html","id":"datos-de-papel-3","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.5.5.3 Datos de papel","text":"Para este conjunto de datos es de esperar que los indicadores de multicolinealidad proporcionen resultados altos, ya que al tratarse de un MP la variable predictora es la misma.El resultado del \\(VIF\\) muestra multicolinealidad pero como es el comportamiento natural para este tipo de modelos se decide actuar.","code":"\n# Análisis de multicolinealidad\nols_coll_diag(fit.papel)## Tolerance and Variance Inflation Factor\n## ---------------------------------------\n##     Variables  Tolerance      VIF\n## 1      madera 0.05840859 17.12077\n## 2 I(madera^2) 0.05840859 17.12077\n## \n## \n## Eigenvalue and Condition Index\n## ------------------------------\n##   Eigenvalue Condition Index  intercept      madera I(madera^2)\n## 1  2.7005883        1.000000 0.01001057 0.001973208 0.003447858\n## 2  0.2904492        3.049257 0.18853949 0.001001403 0.035409847\n## 3  0.0089625       17.358596 0.80144993 0.997025390 0.961142295"},{"path":"rlm.html","id":"diagnóstico","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6 Diagnóstico","text":"Estudiamos en este punto el proceso de diagnóstico de un modelo RLM o MP de los que hemos venido estudiando hasta ahora. El diagnóstico del modelo es realmente valioso por cuanto nos permite corroborar que se cumplen (o ) cada una de las hipótesis asumidas para el ajuste del modelo y que dan credibilidad las conclusiones que obtenemos. Este diagnóstico suele sugerir con frecuencia alguna modificación correctora del modelo propuesto y nos obliga repetir la dinámica de análisis (modelo alternativo y selección de variables) hasta dar con una solución satisfactoria.La herramienta básica para el diagnóstico del modelo es el análisis de los residuos, tanto través de gráficos, como de tests que verifican la validez de las hipótesis asumidas en el ajuste del modelo lineal:\\(E(\\epsilon_i)=0 , \\ \\forall =1, \\ldots, n \\ \\rightsquigarrow\\) bondad del ajuste o linealidad.\\(Var(\\epsilon_i)=\\sigma^2, \\ \\forall \\ \\rightsquigarrow\\) Varianza constante (homocedasticidad).\\(\\epsilon \\sim N(0, \\sigma^2I) \\ \\rightsquigarrow\\) Normalidad de los errores.\\(Cov(\\epsilon_i, \\epsilon_j)=0, \\forall \\ \\neq j \\ \\rightsquigarrow\\) Independencia de los errores.Si encontramos indicios de violación de alguna de ellas, en ocasiones podremos resolverlas través de las soluciones que proponemos continuación. Tanto las herramientas de diagnóstico como las soluciones propuestas para cuando encontramos problemas, son una ampliación del análisis de residuos que ya estudiamos para el modelo de regresión lineal simple. Aunque se pueden definir diferentes tipos de residuos, aquí nos concentramos en los que son de uso habitual en el diagnóstico de modelos lineales.","code":""},{"path":"rlm.html","id":"tipos-de-residuos","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.1 Tipos de Residuos","text":"Presentamos diversos tipos de residuos, útiles tanto para la diagnosis del modelo como para el análisis de influencia (detección de observaciones influyentes y/o raras o anómalas). Generalmente, los procedimientos de diagnóstico del modelo basados en residuos son gráficos, si bien en ocasiones disponemos de algunos tests basados en ellos.","code":""},{"path":"rlm.html","id":"residuos-comunes","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.1.1 Residuos comunes","text":"Los residuos comunes del modelo lineal \\(y=X\\beta+\\epsilon\\) consisten simplemente en las desviaciones entre los datos observados \\(y_i\\) y los predichos \\(\\hat{y}_i\\), esto es, los obtenidos de:\\[\n\\textbf{e}=\\textbf{y}-\\hat{\\textbf{y}}=y-X\\hat{\\beta}=\\textbf{y}-X\\hat{\\beta}=(-X(X'X)^{-1}X')\\textbf{y}\n\\]cuando \\(X'X\\) es singular.Surge así una matriz básica en la definición de los residuos, denominada matriz gorro y definida por:\\[\nH=X(X'X)^{-1}X', \n\\]que tiene su importancia en la interpretación y redefinición de nuevos tipos de residuos, como veremos. sus elementos nos referiremos como \\(h_{ij}\\). Esta matriz \\(H\\) es simétrica (\\(H'=H\\)) e idempotente (\\(HH=H\\)), de dimensión \\(n \\times n\\) y de rango \\(p=rang(X)\\).En términos de \\(H\\), los residuos \\(\\textbf{e}\\) se pueden escribir como:\\[\n\\textbf{e} = \\textbf{y}-\\hat{\\textbf{y}} = (-H) \\textbf{y}, \n\\]esto es,\\[\ne_i=(1-\\sum_{j=1}^n h_{ij}) \\, y_i=y_i-\\hat{y}_i, \\ \\ \\ =1, \\ldots, n.\n\\]De esta forma se puede demostrar que la varianza de cada residuo viene dada por:\n\\[\nVar(e_i)=(1-h_{ii})\\sigma^2, \\ \\ =1, \\ldots, n, \n\\]y la correlación entre los residuos \\(e_i\\) y \\(e_j\\):\\[\nCor(e_i, e_j)=\\frac{-h_{ij}}{\\sqrt{(1-h_{ii})(1-h_{jj})}}.\n\\]","code":""},{"path":"rlm.html","id":"residuos-estandarizados.","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.1.2 Residuos estandarizados.","text":"Son residuos de media cero y varianza aproximadamente unidad, definidos por:\\[\nr_i=\\frac{e_i}{\\sqrt{s^2}}, \\qquad =1, \\ldots, n, \n\\]donde \\(s^2\\) es la estimación habitual de \\(\\sigma^2\\) que da el cuadrado medio residual.\nUna modificación de estos ´residuos son los denominados residuos estudentizados que se interpretan de forma similar estos. En los ejemplos introduciremos los procedimientos gráficos que podemos utilizar con este tipo de residuos para el estudio de la linealidad y homocedasticidad.Dado que la verificación de hipótesis se basa en los residuos del modelo, los procedimientos que utilizamos para verificarlas son los mismos los descritos para el modelo RLS en la unidad anterior. La única diferencia es la existencia de más de una predictora.","code":""},{"path":"rlm.html","id":"linealidad","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.2 Linealidad","text":"Si hay alguna variable explicativa que ha sido incluida en el ajuste del modelo, representarla versus los residuos ayuda identificar algún tipo de tendencia que dicha variable pueda explicar. Si se detecta ninguna tendencia en el gráfico de dispersión en principio tenemos ninguna evidencia que nos sugiera incorporar dicha variable al modelo para predecir mejor la respuesta. Estos gráficos son útiles también para detectar outliers y heterocedasticidad.","code":""},{"path":"rlm.html","id":"homocedasticidad","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.3 Homocedasticidad","text":"La heterocedasticidad, que es como se denomina el problema de varianza constante, aparece generalmente cuando el modelo está mal especificado, bien en la relación de la respuesta con los predictores, bien en la distribución de la respuesta, bien en ambas cuestiones. La violación de la hipótesis de varianza constante, \\(Var(\\epsilon)=\\sigma^2 \\), se detecta usualmente través del análisis gráfico de los residuos:Gráficos de residuos versus valores ajustados \\(\\hat{y}_i\\).- Cuando aparece alguna tendencia como una forma de embudo o un abombamiento, etc., entonces decimos que podemos tener algún problema con la violación de la hipótesis de varianza constante para los errores.Gráficos de residuos versus predictores \\(\\textbf{x}_j\\).- Básicamente se interpretan como los gráficos de residuos versus valores ajustados \\(\\hat{y}_i\\). Es deseable que los residuos aparezcan representados en una banda horizontal sin tendencias alrededor del cero.Hay numerosos tests en la literatura para reconocer heterocedasticidad. Unos están basados en considerar la variabilidad de los residuos que consiguen explicar las variables explicativas sospechosas de inducir heterocedasticidad. Otros tests están basados en diferenciar las observaciones en grupos de varianza constante y comparar los ajustes obtenidos respecto la hipótesis de tener una misma varianza común:El test de Breusch-Pagan.El test de Bartlett o el test de Levene.","code":""},{"path":"rlm.html","id":"normalidad-2","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.4 Normalidad","text":"La hipótesis de normalidad de los errores \\(\\epsilon_i\\) en el modelo lineal justifica la utilización de los tests \\(F\\) y \\(t\\) para realizar los contrastes habituales y obtener conclusiones confiables cierto nivel de confianza \\(1-\\alpha\\) dado. En muestras pequeñas, la normalidad de los errores es muy difícil de diagnosticar través del análisis de los residuos, pues éstos pueden diferir notablemente de los errores aleatorios \\(\\epsilon_i\\).En muestras grandes se esperan demasiadas diferencias entre residuos y errores, y por lo tanto hacer un diagnóstico de normalidad sobre los residuos equivale prácticamente hacerlo sobre los errores mismos.La forma habitual de diagnosticar normalidad es través de los gráficos qq de normalidad y de tests como el de Shapiro-Wilks, específico para normalidad, o el de bondad de ajuste de Kolmogorov-Smirnov.","code":""},{"path":"rlm.html","id":"incorrelación","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.5 Incorrelación","text":"Para los modelos RLM y MP asumimos que los errores observacionales están incorrelados dos dos. Si esta hipótesis es cierta, cabe esperar que un gráfico secuencial de los residuos manifieste alguna tendencia. Sin embargo, hay muchas formas en que los errores pueden estar correlados. De hecho, la independencia entre observaciones es una cuestión justificada básicamente por el muestreo realizado.Un gráfico de los residuos en función de la secuencia temporal en que se observaron los datos puede ayudar apreciar un problema de correlación de los residuos.Un gráfico de los residuos en función de la secuencia temporal en que se observaron los datos puede ayudar apreciar un problema de correlación de los residuos.Los gráficos de autocorrelación ayudan detectar correlación serial, es decir, que un residuo de pende de los residuos anteriores. Dichos gráficos consisten en representar cada residuo (excepto el primero) versus el residuo anterior en la secuencia temporal sospechosa de inducir la correlación.Los gráficos de autocorrelación ayudan detectar correlación serial, es decir, que un residuo de pende de los residuos anteriores. Dichos gráficos consisten en representar cada residuo (excepto el primero) versus el residuo anterior en la secuencia temporal sospechosa de inducir la correlación.Un test habitual para detectar cierto tipo de correlación serial es el test de Durbin-Watson.","code":""},{"path":"rlm.html","id":"soluciones-a-problemas-detectados-en-el-diagnóstico-del-modelo","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.6 Soluciones a problemas detectados en el diagnóstico del modelo","text":"Las soluciones los posibles problemas detectados en el diagnóstico son similares las utilizadas para los modelos RLS:Propuesta de otros modelos adecuados la distribución de la respuesta y su relación con los predictores (Modelos Lineales Generalizados que trataremos más adelante).Transformar la variable respuesta (Transformaciones de Box-Cox).Transformar las predictoras (Modelos de suavizado).Algunas de las soluciones, como las de transformar las predictoras mediante modelos de suavizado, tendrán una unidad especial de tratamiento ya que se tratan de modelos más generalistas que permiten ajustar muchos tipos de tendencias entre respuesta y predictoras.","code":""},{"path":"rlm.html","id":"análisis-de-influencia-1","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.7 Análisis de influencia","text":"En ocasiones hay algún subconjunto de los datos que influencia desproporcionadamente el ajuste del modelo propuesto, con lo cual las estimaciones y predicciones dependen mucho de él. Es interesante siempre, localizar este tipo de datos, si existen, y evaluar su impacto en el modelo. Si estos datos influyentes son “malos” (provienen de errores en la medición, o de condiciones de experimentación diferentes, etc.) habrían de ser excluidos del ajuste; si son “buenos”, esto es, efectivamente proceden de buenas mediciones aunque raras, contendrán información sobre ciertas características relevantes considerar en el ajuste. En todo caso, es importante localizarlos, y para ello existen una serie de procedimientos basados en diversos estadísticos que presentamos continuación.Hay diversos criterios para valorar la influencia de las observaciones en el ajuste, y en base los cuales se proponen diversos estadísticos. Vamos considerar tres de ellos: ) contribución la estimación de los coeficientes; ii) influencia en la predicción y iii) influencia sobre la precisión de las estimaciones.","code":""},{"path":"rlm.html","id":"sobre-los-coeficientes-del-modelo","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.7.1 Sobre los coeficientes del modelo","text":"Se han construido diversas medidas para valorar la influencia de las observaciones en la estimación de los coeficientes del modelo. Entre ellas, las más habituales son:Distancia de Cook. Medida de influencia para una observación \\(y_i\\), basada en la distancia entre la estimación de mínimos cuadrados obtenida con las \\(n\\) observaciones, \\(\\hat{\\textbf{y}}=X \\hat{\\beta}\\), y la obtenida eliminando dicha observación, \\(\\hat{\\textbf{y}}^{()}\\). Una formulación habitual del estadístico de Cook es:\\[\nD_i=\\frac{(\\hat{\\textbf{y}}-\\hat{\\textbf{y}}^{()})'(\\hat{\\textbf{y}}-\\hat{\\textbf{y}}^{()})}{p s^2}=\\frac{(\\hat{\\beta}^{()}-\\hat{\\beta})' X'X (\\hat{\\beta}^{()}-\\hat{\\beta})}{p s^2}, \\ \\ =1, \\ldots, n, \n\\]donde \\(\\hat{\\beta}^{()}\\) es el vector de parámetros estimados en la regresión \\(\\hat{\\textbf{y}}^{()}\\).Los puntos con un valor grande del estadístico \\(D_i\\) identifican observaciones tales que el hecho de incluirlas o en el ajuste dan lugar diferencias considerables en las estimaciones de los coeficientes. Generalmente se consideran como influyentes aquellas observaciones con un valor del estadístico \\(D_i>1\\), pero se identifican como potencialemnte influyentes todas aquellas con \\(D_i>4/n\\), con \\(n\\) el tamaño de la muestra.DFBETAS. Estadístico que indica cuánto cambia el coeficiente estimado \\(\\hat{\\beta}_j\\) en desviaciones estándar para un modelo dado cuando se excluye la \\(\\)-ésima observación:\\[\nDFBETAS_{j, }=\\frac{\\hat{\\beta}_j-\\hat{\\beta}^{()}_j}{s^2_{()} C_{jj}^X}, \\quad j=0, 1, \\ldots, p; \\ =1, \\ldots, n\n\\]donde \\(\\hat{\\beta}^{()}_j\\) es la j-ésima componente del vector \\(\\hat{\\beta}_{()}\\), y \\(C_{jj}^X\\) es el elemento \\(j\\) de la diagonal de \\((X'X)^{-1}\\).De forma habitual se considera como potencialmente influyente una observación si \\(|DFBETAS_{j, }|>2/\\sqrt{n}\\), con \\(n\\) el tamaño muestral.","code":""},{"path":"rlm.html","id":"influencia-sobre-las-predicciones","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.7.2 Influencia sobre las predicciones","text":"Para investigar la influencia de la \\(\\)-ésima observación sobre los valores predichos por el modelo utilizamos el estadístico DFFITS. Se define el estadístico DFFITS para la observación -ésima como:\\[\nDFFITS_i = \\frac{\\hat{y}_i-\\hat{y}^{()}_i}{\\sqrt{s^2_{()} h_{ii}}}, \\ \\ =1, \\ldots, n, \n\\]donde \\(\\hat{y}^{()}_i\\) es el valor predicho para \\(y_i\\) por el modelo sin utilizar en la estimación la observación \\(\\). Así, \\(DFFITS_i\\) se puede interpretar como el número de desviaciones estándar que cambia la predicción de la \\(\\)-ésima respuesta cuando dicha observación es excluida del ajuste.Generalmente, una observación para la que \\(|DFFITS_i|>2 \\sqrt{p/n}\\) merece ser tratada con atención.","code":""},{"path":"rlm.html","id":"influencia-sobre-la-precisión-de-las-estimaciones","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.7.3 Influencia sobre la precisión de las estimaciones","text":"Los diagnósticos vistos hasta ahora cuantifican de algún modo el efecto de las observaciones en las estimaciones. Sin embargo, proporcionan información alguna sobre la precisión conjunta del ajuste. La precisión de la estimación de \\(\\hat{\\beta}\\) se puede medir en función del estadístico \\(\\textsf{COVRATIO}\\). Si \\(COVRATIO_i<1\\), excluir la \\(\\)-ésima observación proporciona un ajuste más preciso; si \\(COVRATIO_i>1\\), la \\(\\)-ésima observación mejora la precisión de la estimación. En este manual utilizaremos este criterio y nos centraremos en los puntos anteriores","code":""},{"path":"rlm.html","id":"funciones-para-diagnóstico-e-influencia","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.8 Funciones para diagnóstico e influencia","text":"EN la unidad anterior mostramos como realizar los gráficos y test de diagnóstico para un modelo RLS. Esos gráficos se pueden utilizar también en los modelos tratados en esta unidad, pero además se muestran las funciones de la librería olsrr que pueden ser utilizadas para esta tarea. Concretamente:ols_plot_resid_stand(): gráfico secuencial de residuos estandarizados.ols_plot_resid_stud(): gráfico secuencial de residuos estudentizados.ols_plot_resid_stud_fit(): residuos estudentizados vs valores ajustados.ols_plot_resid_qq(): gráfico qq de normalidad.ols_test_normality(): tests de normalidad.ols_test_breusch_pagan(): Test de Bresuch-Pagan.Aunque la función por excelencia para obtener las medidas de influencia es influence.measures(), la librería olsrr presenta diversas funciones para este análisis con la ventaja de proporcionar herramientas gráficas y valores de detección, que permiten visualizar de forma rápida las posibles observaciones influyentes. Dichas funciones son:ols_plot_cooksd_chart(): proporciona la distancia de Cook.ols_plot_dfbetas(): proporciona los \\(DFBETAS\\).ols_plot_dffits(): proporciona \\(DFFITS\\).","code":""},{"path":"rlm.html","id":"ejemplos-7","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.9 Ejemplos","text":"Realizamos el diagnóstico y análisis de influencia de los modelos ajustados, tras el proceso de selección de variables.","code":""},{"path":"rlm.html","id":"datos-de-bosque-4","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.9.1 Datos de Bosque","text":"En primer lugar, ajustamos el modelo correspondiente y obtenemos los valores de diagnóstico.Estudiamos los gráficos de diagnóstico para detectar residuos grandes (gráfico secuencial de residuos), linealidad y homocedasticidad (gráfico de residuos estandarizados versus valores ajustados), y normalidad (gráfico qq).El gráfico de secuencia de los residuos indica que la observación 18 es potencialemente una observación anómala. Cuando realizamos el análisis de influencia deberemos verificar esta situación para considerar la posible eliminación de esta observación.\nEl gráfico de residuos versus valores ajustados muestra ningún tipo de tendencia (linealidad) ni comportamientos extraños que permitan pensar que se incumple la hipótesis de varianza constante.\nEl gráfico de normalidad también muestra un comportamiento adecuado teniendo en cuenta que el tamaño muestral es muy pequeño.\nEl gráfico de autocorelación muestra dependencia entre los residuos, indicando que se cumple la hipótesis de independencia.Dado que se ha detectado falta de linealidad entre residuos y valores ajustados es necesario realizar el gráfico de residuos versus predictoras. Sin embargo, los mostramos aquí para ver el código necesario.Como era de esperar los gráficos muestran ningún tipo de tendencia.Realizamos ahora los tests necesarios para verificar las hipótesis de normalidad, homocedasticidad, e independencia.Todos los tests resultan significativos indicando que se cumplen las hipótesis del modelo. Para la hipótesis de normalidad nos debemos fijar en los resultados de Kolmogorov-Smirnov que tiene un mejor comportamiento, desde el punto de vista estadístico, que Shapiro-Wilk. En caso de discrepancias entre ellos nos debemos quedar con Kolmmogorov-Smirnov.pesar de que se cumplen las hipótesis del modelo, obtenemos las medidas de influencia asociadas con el modelo ajustado:La distancia de Cook muestra dos observaciones (1 y 20) como potencialmente influyentes utilizando el punto de corte más restrictivo. Si utilizamos el punto de corte estándar que determina como influyente los que tienen una distancia de Cook mayor que 1, ninguna de ellas sería clasificada como influyente.EL resto de medidas de influencia siguen mostrando las observaciones 1 y 20 como potencialmente influyentes, pero dado que se cumplen las hipótesis del modelo nos planteamos la eliminación de dichas observaciones. Además, con tamaños de muestras tan pequeños sólo nos planteamos su eliminación si es la única solución para que se cumplan las hipótesis del modelo.","code":"\n# Modelos\nfit.bosque <- lm(vol ~ d16 + ht, data = bosque)\n# Valores de diagnóstico\ndiag.bosque <- fortify(fit.bosque)\nols_plot_resid_stand(fit.bosque)\nggplot(diag.bosque, aes(x = .fitted, y = .stdresid)) + \n geom_point() + \n geom_smooth(method = \"lm\", se = FALSE)\nols_plot_resid_qq(fit.bosque)\nacf(diag.bosque$.stdresid)\nggplot(diag.bosque, aes(x = ht, y = .stdresid)) + \n geom_point() + \n geom_smooth(method = \"lm\", se = FALSE)\nggplot(diag.bosque, aes(x = d16, y = .stdresid)) + \n geom_point()+ \n geom_smooth(method = \"lm\", se = FALSE)\nols_test_normality(fit.bosque)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9281         0.1420 \n## Kolmogorov-Smirnov        0.1544         0.6712 \n## Cramer-von Mises          1.6812         0.0000 \n## Anderson-Darling          0.5362         0.1485 \n## -----------------------------------------------\nols_test_breusch_pagan(fit.bosque)## \n##  Breusch Pagan Test for Heteroskedasticity\n##  -----------------------------------------\n##  Ho: the variance is constant            \n##  Ha: the variance is not constant        \n## \n##              Data               \n##  -------------------------------\n##  Response : vol \n##  Variables: fitted values of vol \n## \n##         Test Summary         \n##  ----------------------------\n##  DF            =    1 \n##  Chi2          =    0.8615473 \n##  Prob > Chi2   =    0.353306\ncar::durbinWatsonTest(fit.bosque)##  lag Autocorrelation D-W Statistic p-value\n##    1       0.1645488      1.487166   0.158\n##  Alternative hypothesis: rho != 0\nols_plot_cooksd_chart(fit.bosque)\nols_plot_dfbetas(fit.bosque)\nols_plot_dffits(fit.bosque)"},{"path":"rlm.html","id":"datos-de-concentración-4","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.9.2 Datos de Concentración","text":"Ajustamos el modelo y obtenemos los valores de diagnóstico.Estudiamos los gráficos de diagnóstico para detectar residuos grandes (gráfico secuencial de residuos), linealidad y homocedasticidad (gráfico de residuos estandarizados versus valores ajustados), y normalidad (gráfico qq).Aunque todos los gráficos parecen mostrar comportamientos adecuados, el gráfico de residuos versus ajustados muestra un punto alejado (valor ajustado alto) del resto lo que podría indicar que debemos tratar ese valor como anómalo y considerar su eliminación del banco de datos. Antes de tomar una decisión revisamos toda la batería de gráficos y tests de diagnóstico e influencia.En los gráficos de residuos versus predictoras se observan comportamientos anómalos.Realizamos ahora los tests necesarios para verificar las hipótesis de normalidad, homocedasticidad, e independencia.Todos los tests resultan significativos indicando que se cumplen las hipótesis del modelo.En último lugar realizamos el análisis de influencia:La distancia de Cook muestra que la observación en la posición 3 es claramente influyente. De hecho, también es influyente en los coeficientes del modelo, y en el valor ajustado. Pasamos eliminar dicha observación y ajustar un nuevo modelo. Comenzaremos con el modelo saturado e iremos completando todo el análisis.Creamos el nuevo banco de datos y ajustamos el nuevo modelo:EL test \\(F\\) de la regresión nos indica que la variables predictoras están relacionadas con la respuesta (p-valor > 0.05), con lo que tendría sentido seguir trabajando con este modelo y la conclusión obtenida es que hemos podido obtener una relación entre concentración y peso cuerpo, peso del hígado, y dosis.El efecto de eliminar una observación (pesar de que se cumplen las hipótesis del modelo), es que la débil relación que habíamos establecido entre concentración frente peso del cuerpo y dosis resulta inexistente. En este caso debe ser el investigador el que decida entre las dos opciones:Quedarse con un modelo malo (sin quitar la observación influyente) que verifica las hipótesis.Concluir que existe relación entre respuesta y predictoras, desechando el experimento realizado.","code":"\n# Modelos\nfit.concen <- lm(concen ~ p.cuerpo + dosis, data = concentracion)\n# Valores de diagnóstico\ndiag.concen <- fortify(fit.concen)\nols_plot_resid_stand(fit.concen)\nggplot(diag.concen, aes(x = .fitted, y = .stdresid)) + \n geom_point() + \n geom_smooth(method = \"lm\", se = FALSE)\nols_plot_resid_qq(fit.concen)\nacf(diag.concen$.stdresid)\nggplot(diag.concen, aes(x = p.cuerpo, y = .stdresid)) + \n geom_point() + \n geom_smooth(method = \"lm\", se = FALSE)\nggplot(diag.concen, aes(x = dosis, y = .stdresid)) + \n geom_point()+ \n geom_smooth(method = \"lm\", se = FALSE)\nols_test_normality(fit.concen)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9544         0.4672 \n## Kolmogorov-Smirnov        0.1414         0.7919 \n## Cramer-von Mises          5.4464         0.0000 \n## Anderson-Darling          0.3769         0.3742 \n## -----------------------------------------------\nols_test_breusch_pagan(fit.concen)## \n##  Breusch Pagan Test for Heteroskedasticity\n##  -----------------------------------------\n##  Ho: the variance is constant            \n##  Ha: the variance is not constant        \n## \n##                Data                \n##  ----------------------------------\n##  Response : concen \n##  Variables: fitted values of concen \n## \n##         Test Summary          \n##  -----------------------------\n##  DF            =    1 \n##  Chi2          =    0.05255342 \n##  Prob > Chi2   =    0.8186782\ncar::durbinWatsonTest(fit.concen)##  lag Autocorrelation D-W Statistic p-value\n##    1      -0.0227244      1.762427   0.558\n##  Alternative hypothesis: rho != 0\nols_plot_cooksd_chart(fit.concen)\nols_plot_dfbetas(fit.concen)\nols_plot_dffits(fit.concen)\n# Datos sin observación 3\nconcentracion <- concentracion[-3, ]\n# Ajuste del modelo\nfit.concen <- lm(concen ~ p.cuerpo + dosis, data = concentracion)\n# Modelo ajustado\ntab_model(fit.concen)\n# Bondad del ajuste\nglance(fit.concen)## # A tibble: 1 × 12\n##   r.squared adj.r.squared  sigma statistic p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl>  <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1   0.00483        -0.128 0.0762    0.0364   0.964     2   22.4 -36.9 -33.3   0.0871          15\n## # … with 1 more variable: nobs <int>"},{"path":"rlm.html","id":"datos-de-papel-4","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.6.9.3 Datos de Papel","text":"Ajustamos el modelo y obtenemos los valores de diagnóstico.Estudiamos los gráficos de diagnóstico para detectar residuos grandes (gráfico secuencial de residuos), linealidad y homocedasticidad (gráfico de residuos estandarizados versus valores ajustados), y normalidad (gráfico qq).se observan residuos excesivamente grandes, ni comportamientos anómalos pero si cierta autocorrelación en los residuos debido la propia estructura del modelo polinómico considerado. Realizamos los tests de diagnóstico:Se verifican las hipótesis de homocedasticidad y normalidad, y como era de espera se cumple la hipótesis de incorrrelación. Dado que este incumplimiento se debe la propia estructura del modelo tendremos en cuenta este resultado par concluir sobre este modelo.En último lugar realizamos el análisis de influencia:Tenemos dos observaciones (18 y 19) que se detectan como potencialmente influyentes (distancia de Cook) y que podrían ser consideradas para su eliminación. Como el modelo tiene un buen ajuste y cumple con las hipótesis consideramos el modelo como válido.","code":"\n# Modelos\nfit.papel <- lm(tension ~ madera + I(madera^2), data = papel)\n# Valores de diagnóstico\ndiag.papel <- fortify(fit.papel)\nols_plot_resid_stand(fit.papel)\nggplot(diag.papel, aes(x = .fitted, y = .stdresid)) + \n geom_point() + \n geom_smooth(method = \"lm\", se = FALSE)\nols_plot_resid_qq(fit.papel)\nacf(diag.papel$.stdresid)\nols_test_normality(fit.papel)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9113         0.0783 \n## Kolmogorov-Smirnov        0.198          0.3942 \n## Cramer-von Mises          1.5965          1e-04 \n## Anderson-Darling          0.6399         0.0806 \n## -----------------------------------------------\nols_test_breusch_pagan(fit.papel)## \n##  Breusch Pagan Test for Heteroskedasticity\n##  -----------------------------------------\n##  Ho: the variance is constant            \n##  Ha: the variance is not constant        \n## \n##                Data                 \n##  -----------------------------------\n##  Response : tension \n##  Variables: fitted values of tension \n## \n##         Test Summary         \n##  ----------------------------\n##  DF            =    1 \n##  Chi2          =    0.2755593 \n##  Prob > Chi2   =    0.5996267\ncar::durbinWatsonTest(fit.papel)##  lag Autocorrelation D-W Statistic p-value\n##    1       0.6040252     0.6974667       0\n##  Alternative hypothesis: rho != 0\nols_plot_cooksd_chart(fit.papel)\nols_plot_dfbetas(fit.papel)\nols_plot_dffits(fit.papel)"},{"path":"rlm.html","id":"predicción","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.7 Predicción","text":"El proceso de predicción en este tipo de modelos es similar al de los modelos de regresión lineal simple. Si \\(X_0 \\\\mathbb{R}^p\\) representa un vector fijo de valores de las variables explicativas contenidas en la matriz de diseño \\(X\\), podemos predecir la respuesta \\(y\\) en \\(X_0\\) través del modelo ajustado con\\[\n\\hat{y}=X_0 \\hat{\\beta}, \n\\]\npero el error asociado la estimación depende de la situación que estemos prediciendo:","code":""},{"path":"rlm.html","id":"estimación-de-la-respuesta-media.","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.7.1 Estimación de la respuesta media.","text":"La varianza asociada dicha estimación viene dada por:\n\\[\nVar[\\hat{E}(y|X_0)]=\\sigma^2 X_0(X'X)^{-1}X_0'.\n\\]Un intervalo de confianza nivel \\(1-\\alpha\\) está basado en la distribución \\(t-Student\\):\\[\n\\hat{y}_{X_0} \\pm\nt_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 X_0(X'X)^{-1}X_0'}, \n\\]siendo \\(t_{(n-p, 1-\\alpha/2)}\\) el cuantil \\(1-\\alpha/2\\) de una distribución \\(t-Student\\) con \\(n-p\\) grados de libertad, con \\(p\\) el número de coeficientes en el modelo y \\(n\\) el número de datos.","code":""},{"path":"rlm.html","id":"predicción-de-nuevas-observaciones.","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.7.2 Predicción de nuevas observaciones.","text":"La predicción de la respuesta \\(y\\) para un determinado valor \\(X_0\\) de las variables explicativas involucra más incertidumbre que la estimación de un promedio. En este caso, la varianza asociada la predicción es:\\[\nVar(\\hat{y}|X_0)=\\sigma^2 (1+ X_0(X'X)^{-1}X_0').\n\\]Un intervalo de confianza nivel \\(1-\\alpha\\) para dicha predicción viene dado por:\\[\n\\hat{y}_{X_0} \\pm t_{(n-p, 1-\\alpha/2)} \\ \\sqrt{s^2 \\ (1+X_0'(X'X)^{-1}X_0)}.\n\\]","code":""},{"path":"rlm.html","id":"ejemplos-8","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.7.3 Ejemplos","text":"Obtenemos únicamente la predicción de la respuesta media para ciertos valores de las predictoras y los gráficos de predicción marginales.","code":""},{"path":"rlm.html","id":"datos-de-bosque-5","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.7.3.1 Datos de bosque","text":"Estamos interesados en conocer el volumen de madera que podemos obtener para las combinaciones de d16 y ht dadas por (10, 90), (12, 95), (14, 100) y (16, 105).Veamos ahora las predicciones marginales de cada variable:","code":"\n# cargamos datos de predicción\nnewpred <- data.frame(d16 = c(10, 12, 14, 16), \n                      ht = c(90, 95, 100, 105))\n# Predicción para la media de la respuesta\n# Opción interval = \"confidence\" \nnewdata <- data.frame(newpred, \n           predict(fit.bosque, newpred, interval = \"confidence\")) \nround(newdata, 2)##   d16  ht   fit   lwr   upr\n## 1  10  90 29.11 25.03 33.20\n## 2  12  95 47.32 44.83 49.82\n## 3  14 100 65.53 63.35 67.71\n## 4  16 105 83.74 80.24 87.24\n# Gráfico del ajuste\nplot_model(fit.bosque, \"pred\", \n        ci.lvl = 0.95, \n        show.data = TRUE, \n        title = \" \")## $d16## \n## $ht"},{"path":"rlm.html","id":"datos-de-papel-5","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.7.3.2 Datos de papel","text":"Estamos interesados en conocer la tensión del papel para los contenidos de madera dados por 4, 8, y 12.Veamos ahora las predicciones marginales de cada variable:","code":"\n# cargamos datos de predicción\nnewpred <- data.frame(madera = c(4, 8, 12))\n# Predicción para la media de la respuesta\n# Opción interval = \"confidence\" \nnewdata <- data.frame(newpred, \n           predict(fit.papel, newpred, interval = \"confidence\")) \nround(newdata, 2)##   madera   fit   lwr   upr\n## 1      4 30.23 27.48 32.98\n## 2      8 46.83 43.63 50.03\n## 3     12 43.12 39.87 46.37\n# Gráfico del ajuste\nplot_model(fit.papel, \"pred\", \n        ci.lvl = 0.95, \n        show.data = TRUE, \n        axis.title = c(\"Contenido madera\", \"Tensión\"), \n        title = \" \")## $madera"},{"path":"rlm.html","id":"ejercicios","chapter":"Unidad 7 Regresión Lineal Múltiple y Polinómica","heading":"7.8 Ejercicios","text":"continuación, se presenta una colección de ejercicios referidos los modelos de regresión. Los pasos seguir para la obtención del modelo son los que hemos ido desarrollando: representación gráfica y propuesta de modelo, ajuste, bondad de ajuste, diagnóstico y predicción. En caso de encontrar problemas con el diagnóstico se deberá proponer un nuevo modelo alternativo.Ejercicio 1. Se propone una empresa que fabrica vasos de cristal un nuevo proceso de control de calidad. Hasta ahora la empresa seleccionaba una caja de vasos al final de la fabricación y observaba si había alguno roto. Esto provocaba un gran gasto ya que en caso de encontrar algún defecto la caja se desembala y los vasos vuelven la cadena de embalaje. Ahora se propone seleccionar vasos antes de embalar y determinar así el porcentaje de defectos. Se desea saber si ambos porcentajes están relacionados. Los datos aparecen continuación:Ejercicio 2. En 1929 Edwin Hubble investiga la relación entre la distancia (en megaparsecs. 1 parsec=3.26 años luz) de una galaxia la tierra y la velocidad (en Km/sg) con la cual parece retroceder. Observo que las galaxias aparecen alejarse de nosotros importa cual sea la dirección en que miremos. Se piensa de hecho que esto es el resultado del ”Big Bang”. Hubble esperaba aportar conocimiento sobre la formación del universo y sobre cual podría ser su evolución en el futuro. Los datos que recogió incluyen las distancias de la tierra 24 galaxias, así como sus correspondientes velocidades de retroceso. El objetivo principal del estudio que Hubble llevo cabo era determinar la edad del universo, y para ello establece lo que se conoce hasta ahora como la ley de Hubble:\n\\[V = H_0*D\\]\ndonde \\(V\\) es la velocidad, \\(D\\) es la distancia, y \\(H_0\\) es la constante de Hubble.(HINT: En primer lugar debes transformar el modelo teórico propuesto para expresarlo en términos de un modelo RLS. Recuerda almacenar las transformaciones en tu banco de datos para proceder con el ajuste del modelo.)Ejercicio 3. Un grupo de ingenieros agrónomos esta estudiando la evolución de un proceso infeccioso sobre un conjunto de plantas. En concreto durante un período de días se calcula de forma aproximada la fracción de infección de dichas plantas. Por este motivo, dos días consecutivos pueden tener fracciones de infección inferiores. Para evitar esto se proponen dos modelos teóricos:\n\\[() \\text{  } y = \\beta_0 x^{\\beta_1}\\]\n\\[(II) \\text{  } y = \\beta_0 + \\beta_1 log(x)\\]donde y es el grado de infección y x el número de días transcurridosLos modelos teóricos anteriores se pueden transformar modelos de regresión lineal simple sin muchos problemas. Para el modelo () consideramos la transformación logarítmica:\\[log(y) = log(\\beta_0) + \\beta_1 log(x) \\longrightarrow y' = \\beta_0 + \\beta_1 x'\\]donde el \\('\\) indica la nueva variable o parámetro. Podemos estudiar el comportamiento lineal obteniendo dichas variables, y volver al modelo original utilizando las transformaciones.Para el modelo (II) ya viene expresado como un modelo lineal donde únicamente tenemos que obtener la variable predictora transformada mediante la función logaritmo.Ejercicio 4. Se trata de determinar la pérdida de color sufrida por cierto compuesto cuando es sometido altas temperaturas. Los datos recogidos son los siguientes:Ejercicio 5. Karl Pearson recogió información sobre 1100 familias en Inglaterra en el periodo de 1893 1898. En particular este banco de datos contiene las alturas de las madres y las hijas de dichas familias. En concreto se registran las alturas de las hijas mayores de 18 años y las madres de menos de 65 años. Originalmente se estaba interesado en estudiar una posible asociación entre la altura de las madres y las hijas ¿Que podríamos concluir la vista de los datos? las variables recogidas son: “Mheight” (altura de la madre) y “Dheight” (altura de la hija)Ejercicio 6. Se realiza un estudio de campo para conocer el desarrollo de cierta especie de pez del lago lakemary en EEUU. Para medir el desarrollo se establece la edad de cada pez capturado mediante un procedimiento proporcionado por los biólogos. Además se mide la longitud del pez para tratar de establecer el estado de maduración de cada ejemplar. La investigación trata de relacionar la longitud del pez con su edad para determinar el número de capturas permitidas. Las variables recogidas son: “Age” (edad del pez), y “Length” (longitud del pez en mm).Ejercicio 7. Se realiza un estudio para determinar el tiempo de erupción del geyser (Old Faithful Geyser, dentro del parque nacional de Yellowstone) partir del tiempo de espera entre dos erupciones consecutivas. Se trata de analizar la información para determinar el periodo de visitas. Para ellos se recogieron datos durante un periodo determinado que se considera estándar para establecer la distribución de erupciones. Las variables consideradas son: “Duration” (duración de la erupción en segundos) y “Interval” (tiempo de espera entres dos erupciones consecutivas).Ejercicio 8. Los datos muestran el porcentaje de calorías totales obtenidas de carbohidratos complejos, para veinte diabéticos dependientes de insulina que habían seguido una dieta alta en carbohidratos durante seis meses. Se consideró que el cumplimiento del régimen estaba relacionado con la edad (en años), age, el peso corporal (relativo al peso “ideal” para la altura), weight, y otros componentes de la dieta como el porcentaje de proteínas ingeridas. Los datos corresponden con la tabla 6.3 de Dobson (2002).Ejercicio 9. Es bien sabido que la concentración de colesterol en el suero sanguíneo aumenta con la edad, pero es menos claro si el nivel de colesterol también está asociado con el peso corporal. Los datos muestran para una treinta de mujeres el colesterol sérico (milimoles por litro), la edad (años) y el índice de masa corporal (peso dividido por la altura al cuadrado, donde el peso se midió en kilogramos y la altura en metros). Los datos corresponden con la tabla 6.17 de Dobson (2002).Ejercicio 10. En un estudio sobre las diferentes clases de queso cheddar que se fabrican en LaTrobe Valley de Victoria, Australia, se analizaron muestras de queso por su composición química: concentración de ácido acético (escala logarítmica); concentración de sulfuro de hidrógeno (escala logarítmica); y la concentración de ácido láctico. Por otro lado, se paso una muestra de cada uno de ellos un conjunto de catadores y se registro la puntuación obtenida por cada uno de ellos. Estamos interesados en relacionar la puntuación final de los catadores con los resultados del análisis químico.Ejercicio 11. Los datos correspondientes esta base de datos muestran la producción, Q, en toneladas, la mano de obra, L, en horas, y el capital, K, en horas máquina, de 14 empresas de un sector industrial. Se desea ajustar los datos la función de producción Cobb-Douglas dada por:\n\\[Q = L^l K^k e^{\\epsilon}\\]Identifica el objetivo u objetivos del problema propuesto. Describe el tipo de variables involucradas para el modelo propuesto. Escribe los modelos, identifica el tipo, e indica el número de parámetros involucrados.Ajusta los modelos mediante el proceso de selección de variables que consideres más adecuado. Describe los modelos obtenidos (ecuación del modelo) interpretando en términos económicos los coeficientes estimados de dicho modelo, y las medidas de bondad de ajuste de cada uno de ellos.Realiza el diagnóstico del modelo ajustado indicando los gráficos utilizados y las conclusiones obtenidas. Si el modelo debe ser mejorados indica como lo haces, y describe los resultados del nuevo modelo obtenido.¿Cuál es la predicción de la producción media para una empresa con 1900 horas de trabajo y 475 de capital? ¿Cuál consideras que es la combinación óptima para determinar las empresas más eficientes en términos de productividad?.Ejercicio 12. La producción de cereales viene determinada principalmente por las condiciones climáticas previas la recolección. En concreto se recogen las condiciones climáticas en diferentes años. Las características medidas son:anyo = año de la medición.preinv = precipitación de invierno.tempmay = temperatura de mayo.prejun = precipitación de junio.tempjun = temperatura de junio.prejul = precipitación de julio.tempjul = temperatura de julio.preago = precipitación de agosto.tempago = temperatura de agosto.produccion = producción de cereales.Para el banco de datos correspondientePropón el tipo de modelo utilizar. Selecciona el mejor modelo basado en el AIC. Expresa el modelo obtenido y extrae todas las conclusiones que se deriven de él. ¿Crees que un procedimiento de selección basado en el test F proporcionaría el mismo modelo?Las variables incluidas en dicho modelo puede considerarse que contribuyen explicar la producción de cereales. ¿Qué haces para comprobar esto? ¿Las variables incluidas contribuyen de forma significativa explicar la producción de cereales?Realiza un análisis de influencia y comenta los resultados obtenidos. ¿Crees necesario plantear un nuevo modelo?Comprueba las hipótesis sobre el modelo ajustado y comenta los resultados. Si resulta necesario propón un modelo alternativo y analízalo de forma completa.Ejercicio 13. En un estudio medio ambiental sobre la diversidad de especies de tortuga en las islas Galápagos se recogió información sobre el número de especies endémicas (Endemics), así como el área de la isla (área), la altura del pico más alto de isla (Elevation), la distancia la isla más cercana (Nearest), la distancia la isla de Santa Cruz (Scruz), y el área de la isla más próxima (Adjacent).El estudio está interesado las condiciones que pueden afectar un mayor número de especies\nendémicas de tortuga sin tener en cuenta el número total de especies presentes.Identifica la variable respuesta, las predictoras, y el tipo de cada una de ellas.Realiza e interpreta los gráficos individuales descriptivos entre la respuesta y cada predictora de forma individual.Para tratar de linealizar las relaciones entre respuesta y predictora se transforman tomas las variables con la función logaritmo neperiano. ¿Cómo se interpretan los gráficos individuales descriptivos entre las variables transformadas?¿Qué tipo de modelo parece el más adecuado para esta situación experimental? Escribe la forma reducida de dicho modelo.Partiendo del modelo más complejo y utilizando el AIC como criterio de selección, escribe la ecuación del modelo resultante del proceso de selección. ¿Qué podemos decir de la bondad del ajuste del modelo obtenido?¿Cómo interpretamos los gráficos y test de diagnóstico asociados con el modelo obtenido?Realiza un análisis de influencia y si lo consideras necesario ajusta un nuevo modelo y analízalo.En función del modelo obtenido, construye diferentes escenarios para predecir el número de especies en una isla en función de las variables predictoras presentes en él. ¿Cuáles son las condiciones óptimas para la determinación de especies endémicas?","code":"\n# carga de datos\ncajas <- c(3.0, 3.1, 3.0, 3.6, 3.8, 2.7, 3.1, 2.7, 2.7, 3.3, 3.2, 2.1, 3.0, 2.6)\nvasos <- c(3.1, 3.9, 3.4, 4.0, 3.6, 3.6, 3.1, 3.6, 2.9, 3.6, 4.1, 2.6, 3.1, 2.8)\nejer01 <- data.frame(cajas, vasos)\ndistancia <- c(.032, .034, .214, .263, .275, .275, .45, .5, .5, .63, .8, .9, .9, \n        .9, .9, 1.0, 1.1, 1.1, 1.4, 1.7, 2.0, 2.0, 2.0, 2.0)\nvelocidad <- c(170, 290, -130, -70, -185, -220, 200, 290, 270, 200, 300, -30, 650, \n        150, 500, 920, 450, 500, 500, 960, 500, 850, 800, 1090) \nejer02 <- data.frame(distancia, velocidad)\ndia <- 1:200\ninfecc <- c(0.02063762, 0.05637206, 0.11889346, 0.11376972, 0.13772089, 0.19055130, \n      0.18509772, 0.16887353, 0.19753055, 0.21835949, 0.26677238, 0.26360653, \n      0.27772497, 0.28447172, 0.28300082, 0.34108177, 0.32594214, 0.28675461, \n      0.34971616, 0.33537176, 0.33217888, 0.35748261, 0.34925483, 0.36278067, \n      0.37211460, 0.35783240, 0.41498583, 0.40769174, 0.38800213, 0.44174297, \n      0.43087261, 0.42190606, 0.45097338, 0.45570700, 0.45946960, 0.46153400, \n      0.46340109, 0.45549254, 0.45487365, 0.45750686, 0.45521341, 0.46881463, \n      0.45141048, 0.52372846, 0.50803021, 0.46482596, 0.48254773, 0.48449405, \n      0.51255671, 0.49833262, 0.50802495, 0.50526564, 0.50777983, 0.53873568, \n      0.50950327, 0.54693458, 0.48815063, 0.53327501, 0.52645577, 0.53063462, \n      0.53618898, 0.52077545, 0.52633078, 0.51474555, 0.51575426, 0.54528626, \n      0.55015967, 0.54419107, 0.56346905, 0.58787669, 0.53886562, 0.50427534, \n      0.57230842, 0.53970820, 0.54179538, 0.57769618, 0.55308538, 0.53593047, \n      0.56550374, 0.56060245, 0.56496884, 0.57400395, 0.56030226, 0.58199322, \n      0.56606007, 0.57844415, 0.59505931, 0.58311616, 0.56916054, 0.59989923, \n      0.59801493, 0.59031303, 0.58529898, 0.56912505, 0.61003513, 0.57193641, \n      0.62878888, 0.61677661, 0.58247460, 0.56770688, 0.57505675, 0.59541545, \n      0.58634056, 0.58530427, 0.57418797, 0.59326985, 0.57940758, 0.56266765, \n      0.58932866, 0.61620602, 0.58719856, 0.61173102, 0.56806743, 0.60015458, \n      0.61248238, 0.60893367, 0.60582869, 0.59169408, 0.58829584, 0.58557803, \n      0.60917339, 0.58862023, 0.59849746, 0.60391548, 0.64663334, 0.59742612, \n      0.61587231, 0.61341390, 0.59329848, 0.61178139, 0.64276168, 0.62355522, \n      0.61599580, 0.60735889, 0.57537341, 0.63968664, 0.58846078, 0.63307852, \n      0.65706008, 0.59059116, 0.63408846, 0.61538542, 0.58975607, 0.59146830, \n      0.59028687, 0.61224876, 0.59417456, 0.63770437, 0.66647829, 0.59925939, \n      0.64127259, 0.64141050, 0.63317968, 0.60686830, 0.62514131, 0.62241142, \n      0.63976259, 0.62153212, 0.64899315, 0.62242964, 0.65143794, 0.60985758, \n      0.60609047, 0.69656194, 0.62384676, 0.63858650, 0.64578674, 0.62380855, \n      0.64424572, 0.64170765, 0.63043627, 0.63646096, 0.63488074, 0.67853395, \n      0.62153691, 0.61483840, 0.63790480, 0.64374543, 0.64664922, 0.62913057, \n      0.61740673, 0.66430865, 0.63241999, 0.62246721, 0.63541282, 0.63655235, \n      0.66304830, 0.64289529, 0.65662894, 0.63190605, 0.64652159, 0.63607656, \n      0.64479640, 0.62532881, 0.61734833, 0.68383389, 0.65622608, 0.61950582, \n      0.63262438, 0.62145169) \nejer03 <- data.frame(dia, infecc)\ntemperatura <- c(460, 450, 440, 430, 420, 410, 450, 440, 430, 420, \n         410, 400, 420, 410, 400)\nperdida <- c(0.3, 0.3, 0.4, 0.4, 0.6, 0.5, 0.5, 0.6, 0.6, 0.6, 0.7, \n       0.6, 0.6, 0.6, 0.6)\nejer04 <- data.frame(temperatura, perdida)\ndata(\"heights\")\nejer05 <-heights\ndata(\"lakemary\")\nejer06 <-lakemary\ndata(\"geyser\")\nejer07 <-geyser\nejer08 <- read_csv(\"https://goo.gl/Grm8xM\", col_types = \"dddd\")\nejer09 <- read_csv(\"https://goo.gl/EKXWRc\", col_types = \"ddd\")\nejer10 <- read_csv(\"https://goo.gl/V4lDNs\", col_types = \"dddd\")\nejer11 <- read_csv(\"https://bit.ly/2UIq9F9\", col_types = \"dddd\")\nejer11 <- ejer11 %>%\n mutate_if(sapply(ejer11, is.character), as.factor)\nlectura <- read.table(\"https://goo.gl/Yi5g6S\", header = TRUE)\nejer12 <- as_tibble(lectura)\nEndemics <- c(23, 21, 3, 9, 1, 11, 0, 7, 4, 2, 26, 35, 17, 4, 19, 89, 23, \n2, 37, 33, 9, 30, 65, 81, 95, 28, 73, 16, 8, 12)\nArea <- c(25.09, 1.24, 0.21, 0.1, 0.05, 0.34, 0.08, 2.33, 0.03, 0.18, \n58.27, 634.49, 0.57, 0.78, 17.35, 4669.32, 129.49, 0.01, 59.56, 17.95, \n0.23, 4.89, 551.62, 572.33, 903.82, 24.08, 170.92, 1.84, 1.24, 2.85)\nElevation <- c(346, 109, 114, 46, 77, 119, 93, 168, 71, 112, 198, 1494, \n49, 227, 76, 1707, 343, 25, 777, 458, 94, 367, 716, 906, 864, 259, 640, \n147, 186, 253)\nNearest <- c(0.6, 0.6, 2.8, 1.9, 1.9, 8, 6, 34.1, 0.4, 2.6, 1.1, 4.3, \n1.1, 4.6, 47.4, 0.7, 29.1, 3.3, 29.1, 10.7, 0.5, 4.4, 45.2, 0.2, 0.6, \n16.5, 2.6, 0.6, 6.8, 34.1)\nScruz <- c(0.6, 26.3, 58.7, 47.4, 1.9, 8, 12, 290.2, 0.4, 50.2, 88.3, \n95.3, 93.1, 62.2, 92.2, 28.1, 85.9, 45.9, 119.6, 10.7, 0.6, 24.4, 66.6, \n19.8, 0, 16.5, 49.2, 9.6, 50.9, 254.7)\nAdjacent <- c(1.84, 572.33, 0.78, 0.18, 903.82, 1.84, 0.34, 2.85, 17.95, \n0.1, 0.57, 4669.32, 58.27, 0.21, 129.49, 634.49, 59.56, 0.1, 129.49, \n0.03, 25.09, 572.33, 0.57, 4.89, 0.52, 0.52, 0.1, 25.09, 17.95, 2.33)\nejer13 <- data.frame(Endemics, Area, Elevation, Nearest, Scruz, Adjacent)"},{"path":"anova.html","id":"anova","chapter":"Unidad 8 Modelos ANOVA","heading":"Unidad 8 Modelos ANOVA","text":"Los modelos ANOVA surgen cuando la variable o variables predictoras son de tipo categórico, es decir, factores con diferentes niveles de clasificación, de forma que cada sujeto es medido (respecto de la respuesta) para una combinación específica de los factores considerados. La variable respuesta sigue siendo de tipo numérico y el objetivo principal es el estudio de la media para los diferentes niveles del factor o combinaciones de los factores. solo se está interesado en comparar los diferentes grupos, sino además en cuantificar numéricamente esas diferencias.La principal diferencia con los modelos de regresión es que en este caso modelizamos toda la respuesta observada sino la media observada del conjunto de observaciones para cada nivel del factor. Además, veremos como estos modelos pueden ser descritos en términos de un modelo lineal de regresión lo que nos permite utilizar parte de los procesos de descripción, análisis y diagnóstico utilizamos en los capítulos anteriores. En cada punto de este capítulo estudiaremos las similitudes y diferencias con los modelos de regresión.","code":""},{"path":"anova.html","id":"bancos-de-datos-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.1 Bancos de datos","text":"Si las variables explicativas son de tipo factor, el análisis preliminar se realiza mediante gráficos de cajas. Estos gráficos sirven para visualizar relaciones entre una variable continua y un factor. En estos casos, hablar de relación / asociación equivale hablar de diferencias en la variable continua respecto de los niveles (categorías) del factor. Tales diferencias se detectan cuando los diagramas de cajas muestran solapamientos en la escala de la variable respuesta. En los modelos de dos vías podremos realizar además un gráfico de interacción, que es un gráfico de líneas donde se representan las medias de las diferentes combinaciones de niveles del factor, y que nos permite establecer una primera conclusión sobre el comportamiento conjunto de ambos factores.Veamos los diferentes bancos de datos que iremos analizando lo largo de la unidad.Ejemplo 1. Datos de Insecticidas. Este ejemplo contiene los datos de un experimento agronómico para conocer el efecto de diferentes insecticidas (spray) sobre el número de insectos vivos (count) tras un periodo de tratamiento.Cargamos los datos y realizamos el gráfico descriptivo:Se aprecia como el número de insectos vivos es superior cuando usamos los sprays , B, y F (con medias muy similares). El resto de sprays muestran (medias) una supervivencia inferior. Se observan como dos grupos de sprays, uno con medias altas y otro con medias bajas.Ejemplo 2. Datos de Envasado. Se desea estudiar la fabricación de cuatro tipos de máquinas automáticas (maquina) en el cortado de piezas de embutido para envasado. Para ello se toman datos del número de envases sin defecto (produccion) que produce cada máquina durante periodos de una hora.Cargamos los datos y realizamos el gráfico descriptivo:Cada máquina muestra un nivel de envases sin defecto diferente. La máquina M4 es la que muestra mejores resultados y la M3 los peores.Ejemplo 3. Datos de venenos. Se ha realizado un experimento para comprobar la efectividad de diferentes antídotos (AA, AB, AC y AD) frente diferentes venenos (VA, VB y VC). Para ello se recoge el tiempo de reacción (tiempo) que cada antídoto tarda en hacer efecto para cada veneno.Cargamos los datos y realizamos el gráfico descriptivo:En el gráfico de interacción se puede apreciar que antídoto es más efectivo para cada tipo de veneno. demás se puede ver que las curvas del tiempo de reacción (líneas de colores) tiene el mismo comportamiento lo que indica habitualmente que hay cierto efecto de interacción entre los factores considerados, es decir, existe dos combinaciones antídoto - veneno que presentan resultados significativos (medias distintas).","code":"\ndata(InsectSprays)\ninsecticidas <- InsectSprays\nggplot(insecticidas,aes(x = spray, y = count)) + \n   geom_boxplot()\nmaquina <- c(rep(\"M1\", 4),rep(\"M2\", 4),rep(\"M3\", 4),rep(\"M4\", 4))\nproduccion <- c(103, 115, 101, 105, 109, 106, 116, 124, 104, 98, 117, \n                99, 128, 117, 121, 130)\nenvasado <- data.frame(maquina, produccion)\nggplot(envasado,aes(x = maquina, y = produccion)) + \n   geom_boxplot()\ntiempo <- c(0.31, 0.45, 0.46, 0.43, 0.36, 0.29, 0.4, 0.23, \n          0.22, 0.21, 0.18, 0.23, 0.82, 1.1, 0.88, 0.72, 0.92, 0.61, 0.49, \n          1.24, 0.3, 0.37, 0.38, 0.29, 0.43, 0.45, 0.63, 0.76, 0.44, 0.35, \n          0.31, 0.4, 0.23, 0.25, 0.24, 0.22, 0.45, 0.71, 0.66, 0.62, 0.56, \n          1.02, 0.71, 0.38, 0.3, 0.35, 0.31, 0.33)\nantidoto <- factor(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, \n                     2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, \n                     3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4), \n                   labels=c(\"AA\", \"AB\", \"AC\", \"AD\"))\nveneno <- factor(c(1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 2, \n                   2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, \n                   3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3), \n                 labels=c(\"VA\", \"VB\", \"VC\"))\nvenenos <- data.frame(tiempo, antidoto, veneno)\n# Diagrama de cajas\nggplot(venenos,aes(x = antidoto, y = tiempo, color = veneno)) + \n  geom_boxplot() \n# Gráfico de interacción de medias\nggplot(venenos, \n  aes(x = antidoto, y = tiempo, group = veneno, color = veneno)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") "},{"path":"anova.html","id":"modelos-anova-1-vía","chapter":"Unidad 8 Modelos ANOVA","heading":"8.2 Modelos ANOVA 1 vía","text":"El modelo ANOVA de una vía se plantea cuando tenemos una única variable predictora de tipo categórico y una respuesta de tipo numérico. Dicho modelo se puede describir de forma general, para una muestra de tamaño \\(n\\), como:Una variable respuesta, \\(Y\\), de tipo numérico con observaciones \\(y_1, \\ldots ,y_n\\).Una variable predictora, \\(F\\), de tipo categórico con \\(\\) grupos o niveles distintos de tamaños muestrales \\(n_1, n_2, \\ldots , n_I\\), de forma que \\(n = n_1 + n_2 + \\ldots + n_I\\), y el vector de observaciones de la respuesta se puede escribir como:\\[\ny_{11},\\ldots,y_{1n_1},y_{21},\\ldots,y_{2n_2},\\ldots,y_{I1},\\ldots,y_{In_I}\n\\]\ndonde el primer subíndice indica el nivel del factor y el segundo la posición dentro del conjunto de datos de dicho nivel del factor.Conjunto \\(\\mu_i\\) de medias de todas las observaciones de la respuesta asociadas con el nivel \\(\\) del factor, es decir:\\[\n\\mu_i = \\frac{\\sum_{j = 1}^{n_j} y_{ij}}{n_i}; \\text{ = 1, 2,..., }\n\\]Media global de la respuesta, \\(\\mu\\), que se puede obtener como:\\[\n\\mu = \\frac{\\sum_{j = 1}^{} \\mu_{j}}{}\n\\]Incrementos, \\(\\alpha_i\\), de cada una de las medias de cada grupo con respecto la media global, es decir:\\[\n\\alpha_i= \\mu - \\mu_i; \\text{ = 1, 2,..., }\n\\]El objetivo básico en este tipo de modelos es la comparación de las medias \\(\\mu_i\\) para detectar posibles diferencias entre os niveles del factor, es decir, plateamos el contraste de comparación de medias:\n\\[\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\mu_1 = \\mu_2 = \\ldots = \\mu_I\\\\\nH_a: & \\mbox{Existen dos grupos al menos con medias distintas}\n\\end{array}\n\\right.\n\\]Este contraste es equivalente al que se puede escribir en términos de los incrementos de las medias:\n\\[\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_I = 0\\\\\nH_a: & \\mbox{Al menos hay un incremento distinto de cero}\n\\end{array}\n\\right.\n\\]que tiene una estructura similar al test \\(F\\) de la regresión, donde queremos contrastar si existe algún coeficiente distinto de cero. Esto implica que el modelo ANOVA se puede escribir en términos de un modelo lineal sin más que considerar un conjunto de variables ficticias \\(X_1,...,X_{}\\) que toman los valores siguientes:\\[\nX_i= \n\\left\\{ \n\\begin{array}{ll}\n1 & \\mbox{si la respuesta pertenece al grupo }\\\\\n0 & \\mbox{en otro caso}\n\\end{array}\n\\right.\n\\]y un efecto común para todos los niveles del factor que viene dado por \\(\\mu\\) que denotamos por \\(\\alpha_0\\). La única diferencia entre ambas estructuras de comparación es que la referida las medias \\(\\mu_i\\) tiene \\(\\) parámetros, mientras que la de los incrementos tiene \\(+1\\) parámetros. Para hacer equivalentes ambas estructuras añadimos la conocida como restricción de identificabilidad que establece de partida que uno de los incrementos debe ser igual cero, es decir, que una media de un grupo coincide con la media global. Este es el nivel de referencia que utilizamos como base de comparación con el resto de niveles o grupos del factor.En esta situación si consideramos \\(Y_i = \\{y_{i1},...,y_{in_i}\\}\\), \\(1_{n_i} = \\{1,...,1\\}\\) un vector de \\(n_i\\) unos, \\(0_{n_i} = \\{0,...,0\\}\\) un vector de \\(n_i\\) ceros, para cada uno de los niveles \\(\\) del factor, \\(\\mu = \\alpha_0\\), y la restricción de identificabilidad \\(\\alpha_I = 0\\) (podríamos elegir cualquier otro \\(\\alpha_j =0\\) y obtendríamos el mismo modelo), podemos escribir el modelo ANOVA como:\\[\nY = \\left( \n\\begin{array}{c}\n   Y_1 \\\\\n   Y_2 \\\\\n   \\ldots \\\\\n   Y_I \\\\\n\\end{array}\n\\right)\n=\n\\left( \n\\begin{array}{ccccc}\n   1_{n_1} & 1_{n_1} & 0_{n_1} & \\ldots & 0_{n_1} \\\\\n   1_{n_2} & 0_{n_2} & 1_{n_2} & \\ldots & 0_{n_2} \\\\\n   \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\\n   1_{n_I} & 0_{n_I} & 0_{n_I} &\\ldots & 1_{n_I} \\\\\n\\end{array}\n\\right)\n\\left( \n\\begin{array}{c}\n   \\alpha_0 \\\\\n   \\alpha_1 \\\\\n  \\alpha_2 \\\\\n \\ldots \\\\\n   \\alpha_{I_1} \\\\\n\\end{array}\n\\right)\n+\n\\left( \n\\begin{array}{c}\n   e_1 \\\\\n   e_2 \\\\\n   \\ldots \\\\\n   e_n \\\\\n\\end{array}\n\\right) = X\\beta + \\epsilon\n\\]\no equivalentemente en términos de las medias como:\\[\\begin{equation}\n\\mu_i = \\alpha_0 + \\alpha_i + \\epsilon_i, \\text{ para } = 1, 2,...,k, \\text{ con } \\alpha_k = 0\n\\tag{8.1}\n\\end{equation}\\]Las hipótesis de este modelo es que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante \\(\\sigma^2\\) para cada uno de los grupos que determina la variable predictora. En esta situación el contraste sobre los incrementos es equivalente al test \\(F\\) de regresión de los modelos RLM.","code":""},{"path":"anova.html","id":"especificacion-del-modelo-en-r","chapter":"Unidad 8 Modelos ANOVA","heading":"8.2.1 Especificacion del modelo en R","text":"continuación se presenta la especificación en R de los modelos anteriores y de todos los anidados que pueden surgir partir de ellos, así como su interpretación en términos de contraste de hipótesis o coeficientes asociados cada efecto presente en el modelo. La especificación en R del modelo dado en la ecuación @ref{eq:aovonefac} para su estimación se realiza través de la expresión reducida del modelo dada por:\n\\[\nY \\sim F\n\\]En este tipo de modelos se plantean varias situaciones de modelos anidados que deberemos estudiar:Modelo con efecto de \\(F\\):\\[\\begin{equation}\nY \\sim F,\n\\tag{8.2}\n\\end{equation}\\]Si \\(F\\) resulta significativo, tenemos que se detectan diferencias entre al menos dos medias para los niveles del factor \\(F\\).Modelo sin efectos:\\[\\begin{equation}\nY \\sim 1\n\\tag{8.3}\n\\end{equation}\\]En este caso \\(F\\) resulta significativo y podmeos conluir que las medias de todos los niveles del factor se pueden considerar iguales.En la práctica el ajuste de este tipo de modelos se puede plantear como una comparación entre los modelos (8.2) y (8.3), dado que están anidados, de forma similar al proceso de comparación de modelos planteado en los modelos RLM y MP.","code":""},{"path":"anova.html","id":"modelos-anova-dos-vías","chapter":"Unidad 8 Modelos ANOVA","heading":"8.3 Modelos ANOVA dos vías","text":"El modelo ANOVA de dos vías se presenta cuando consideramos dos variables predictoras de tipo categórico. Si tenemos dos factores \\(F_1\\) y \\(F_2\\) con \\(\\) y \\(J\\) niveles respectivamente (y utilizando lo visto en el punto anterior), el modelo expresado en términos de las medias de las combinaciones de los diferentes niveles de los factores considerados viene dado por:\\[\\begin{equation}\n\\mu_{ij} = \\alpha_0 + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ij}, \\ \\mbox{para} \\ = 1,\\ldots,; \\ j = 1,\\ldots,J \\ \\mbox{con} \\ \\alpha_1 = \\beta_1 = \\alpha\\beta_{11} = 0\n\\tag{8.4}\n\\end{equation}\\]donde:\\(\\mu_{ij}\\) es la media de la respuesta para el nivel \\(\\) de \\(F_1\\) y el nivel \\(j\\) de \\(F_2\\).\\(\\alpha_0\\) es el efecto común todas las combinaciones de niveles para ambos factores.\\(\\alpha_i\\) son los incrementos asociados cada uno de los niveles del factor \\(F_1\\).\\(\\beta_j\\) son los incrementos asociados cada uno de los niveles del factor \\(F_2\\).\\(\\alpha\\beta_{ij}\\) son los incrementos asociados la combinación de los niveles \\(\\) de \\(F_1\\) y \\(j\\) de \\(F_2\\). Es lo que se denomina efecto de interacción que valora los cambios en las combinaciones de las medias para ambos factores. Si este efecto es significativo, la media para una combinación de niveles de los factores se construye sumando los incrementos de cada nivel de forma independiente.Dado que tenemos dos factores se hace necesario añadir más restricciones de identificabildiad. Añadimos una por cada factor y la que aparece de forma inmediata en la interacción de los dos efectos que estamos tomando como cero.En este caso escribimos la matriz de diseño del modelo pero se puede escribir considerado la matriz de coeficientes de dimensiones \\(IJ \\times IJ\\) que surge para el modelo (8.4). Bastará con sustituir los unos y ceros por vectores de unos y ceros de dimensiones adecuadas en función de los tamaños de cada uno de los niveles de los dos factores considerados. Dicha matriz de coeficientes se puede extraer partir de las ecuaciones completas del modelo (8.4) (teniendo en cuenta las restricciones de identificabilidad) dadas por:\\[\\begin{array}{lll} \n\\mu_{11} & = \\alpha_0 + \\alpha_{1} + \\beta_{1} + \\alpha\\beta_{11} + \\epsilon_{11}& = \\alpha_0 + \\epsilon_{11}\\\\\n\\mu_{12} & = \\alpha_0 + \\alpha_{1} + \\beta_{2} + \\alpha\\beta_{12} + \\epsilon_{12}& = \\alpha_0 + \\beta_{2} + \\alpha\\beta_{12} + \\epsilon_{12}\\\\\n\\ldots & \\ldots &\\ldots\\\\\n\\mu_{1J} & = \\alpha_0 + \\alpha_{1} + \\beta_{J} + \\alpha\\beta_{1J} + \\epsilon_{1J}& = \\alpha_0 + \\beta_{J} + \\alpha\\beta_{1J} + \\epsilon_{1J}\\\\\n\\mu_{21} & = \\alpha_0 + \\alpha_{2} + \\beta_{1} + \\alpha\\beta_{21} + \\epsilon_{21}& = \\alpha_0 + \\alpha_{2} + \\alpha\\beta_{21} +\\epsilon_{11}\\\\\n\\mu_{22} & = \\alpha_0 + \\alpha_{2} + \\beta_{2} + \\alpha\\beta_{22} + \\epsilon_{22}& = \\alpha_0 +  \\alpha_{2} + \\beta_{2} + \\alpha\\beta_{22} + \\epsilon_{22}\\\\\n\\ldots & \\ldots &\\ldots\\\\\n\\mu_{2J} & = \\alpha_0 + \\alpha_{2} + \\beta_{J} + \\alpha\\beta_{2J} + \\epsilon_{2J}& = \\alpha_0 +  \\alpha_{2} + \\beta_{J} + \\alpha\\beta_{2J} + \\epsilon_{2J}\\\\\n\\ldots & \\ldots &\\ldots\\\\\n\\ldots & \\ldots &\\ldots\\\\\n\\ldots & \\ldots &\\ldots\\\\\n\\mu_{I1} & = \\alpha_0 + \\alpha_{} + \\beta_{1} + \\alpha\\beta_{I1} + \\epsilon_{I1}& = \\alpha_0 +  \\alpha_{} +  \\alpha\\beta_{I1} + \\epsilon_{I1}\\\\\n\\mu_{I2} & = \\alpha_0 + \\alpha_{} + \\beta_{2} + \\alpha\\beta_{I2} + \\epsilon_{I2}& = \\alpha_0 +  \\alpha_{} + \\beta_{2} + \\alpha\\beta_{I2} + \\epsilon_{I2}\\\\\n\\ldots & \\ldots &\\ldots\\\\\n\\mu_{IJ} & = \\alpha_0 + \\alpha_{} + \\beta_{J} + \\alpha\\beta_{IJ} + \\epsilon_{IJ}& = \\alpha_0 +  \\alpha_{} + \\beta_{J} + \\alpha\\beta_{IJ} + \\epsilon_{IJ}\\\\\n\\end{array}\\]En este caso el objetivo vuelve ser la comparación de medias pero el contraste resolver depende de la especificación del modelo considerado. Hay que tener en cuenta que en este caso tenemos una única variable predictora, y además hay que valorar la posibilidad de la interacción entre ambos factores.","code":""},{"path":"anova.html","id":"especificacion-del-modelo-en-r-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.3.1 Especificacion del modelo en R","text":"La especificación en R del modelo dado en la ecuación @ref{eq:aovtwofac} para su estimación se realiza través de la expresión reducida del modelo dada por:\\[\\begin{equation}\nY \\sim F_1 + F_2 + F_1 : F_2,\n\\tag{8.5}\n\\end{equation}\\]donde \\(F_1 : F_2\\) representa el efecto de interacción, y \\(F_1\\) y \\(F_2\\) son los denominados efectos principales asociados con los factores considerados. En este tipo de modelos se plantean varias situaciones de modelos anidados que deberemos estudiar para establecer cual es el tipo de contraste sobre las medias que estamos resolviendo. Alternativamente ese modelo se puede escribir de forma más reducida como \\(Y \\sim F_1*F_2\\), que es un modelo con los efectos principales de cada factor y con la interacción.Modelo saturado o modelo con todos los factores y la interacción dado por:\\[\\begin{equation}\nY \\sim F_1 + F_2 + F_1 : F_2\n\\tag{8.6}\n\\end{equation}\\]Si la interacción está presente en el modelo el contraste planteado es:\n\\[\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\mbox{hay diferencias entre las medias conjuntas de ambos niveles del factor}\\\\\nH_a: & \\mbox{Al menos hay dos combinaciones de ambos factores con medias distintas}\n\\end{array}\n\\right.\n\\]que nos permite concluir sobre si el efecto de interacción es necesario para explicar el comportamiento de la respuesta. En caso de que dicho contraste resulte significativo podemos plantear el modelo con ambos efectos principales.Modelo con ambos efectos principales:\\[\nY \\sim F_1 + F_2\n\\]donde se parte de que la interacción es significativa y se plantean dos contrastes (uno por cada factor)\\[\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\mbox{hay diferencias entre las medias de } F_1\\\\\nH_a: & \\mbox{Al menos hay dos medias  de } F_1 \\mbox{ distintas}\n\\end{array}\n\\right.\n\\]\nSi rechazamos este contraste deberemos considerar que hay diferencias en las medias de los niveles establecidos en \\(F_1\\) y debemos considerar un modelo con efecto principal de \\(F_2\\).\\[\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\mbox{hay diferencias entre las medias de } F_2\\\\\nH_a: & \\mbox{Al menos hay dos medias  de } F_2 \\mbox{ distintas}\n\\end{array}\n\\right.\n\\]\nSi rechazamos este contraste deberemos considerar que hay diferencias en las medias de los niveles establecidos en \\(F_2\\) y debemos considerar un modelo con efecto principal de \\(F_1\\).Estos contrastes nos permiten concluir sobre cada factor de forma independiente (hay interacción entre ellos).Modelo con efecto principal de \\(F_1\\):\\[\nY \\sim F_1\n\\]donde se parte de que la interacción y el efecto principal de \\(F_2\\) son significativos y se plantea el contraste\\[\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\mbox{hay diferencias entre las medias de } F_1\\\\\nH_a: & \\mbox{Al menos hay dos medias  de } F_1 \\mbox{ distintas}\n\\end{array}\n\\right.\n\\]Si rechazamos este contraste deberemos considerar que hay diferencias en las medias de los niveles establecidos en \\(F_1\\) y nuestro modelo final contiene ningún tipo de efecto predictivo.Modelo con efecto principal de \\(F_2\\):\\[\nY \\sim F_2\n\\]donde se parte de que la interacción y el efecto principal de \\(F_1\\) son significativos y se plantea el contraste\\[\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\mbox{hay diferencias entre las medias de } F_2\\\\\nH_a: & \\mbox{Al menos hay dos medias  de } F_2 \\mbox{ distintas}\n\\end{array}\n\\right.\n\\]\nSi rechazamos este contraste deberemos considerar que hay diferencias en las medias de los niveles establecidos en \\(F_2\\) y nuestro modelo final contiene ningún tipo de efecto predictivo.Modelo sin efectos:\\[\nY \\sim 1\n\\]donde concluimos que todas las medias consideradas son iguales, es decir, los niveles de los factores considerados tienen efecto sobre la respuesta.Esta combinación de modelos nos da una estructura secuencial de modelización (modelos alternativos) que deberemos estudiar para elegir el modelo con una mayor capacidad predictiva. Dicha estructura viene dada por:\\[\\begin{array}{lll} \nM1: & Y \\sim F_1 + F_2 + F_1:F_2\\\\\nM2: & Y \\sim F_1 + F_2 \\\\\nM3.1: & Y \\sim F_1 \\\\\nM3.2: & Y \\sim F_2 \\\\\nM4: & Y \\sim 1\\\\\n\\end{array}\\]Más adelante veremos como llevar cabo esta comparación secuencial y como interpretarla para determinar los efectos que finalmente están presentes en el modelo y los que .","code":""},{"path":"anova.html","id":"estimación-y-bondad-de-ajuste-del-modelo","chapter":"Unidad 8 Modelos ANOVA","heading":"8.4 Estimación y bondad de ajuste del modelo","text":"Dado que los modelos ANOVA se pueden escribir como modelos lineales de regresión con una matriz de diseño que solo contiene unos y ceros, se pueden utilizar los procedimientos de estimación vistos en el capitulo anterior para estimar los par??metros del modelo. Podemos utilizar las ecuaciones normales para obtener los coeficientes del modelo que en este caso representarán los incrementos de cada nivel con respecto al de referencia (que hemos tomado como cero en la restricción de identificabilidad), que nos permiten obtener las estimaciones de las medias asociadas cada nivel o niveles del factor o factores. Además, podemos obtener los intervalos de confianza de dichos coeficientes.Sin embargo, en este caso se pueden interpretar de forma directa las significatividades de cada coeficiente en el modelo, ya que al contrario de los que ocurría en los modelos de regresión, donde cada coeficiente representaba el efecto de una predictora, en los modelos ANOVA el efecto de la predictora va asociado con todos los coeficientes estimados con el factor o interacción de factores. Por eso es necesario proporcionar una medida de bondad de ajuste con dichas estimaciones que en este caso es el test F ANOVA que se obtiene como el test F de la regresión de los modelos de unidades anteriores. Este test nos permite extraer conclusiones directas sobre los efectos presentes en el modelo y sobre los coeficientes estimados. En el punto siguiente veremos como utilizar el test F ANOVA para obtener el mejor modelo. En este caso utilizaremos el \\(R^2\\) como medida de bondad de ajusta ya que se encuentra diseñada para modelos de regresión donde las predictoras son de tipo numérico.Para resolver la estimación y la bondad de ajuste en este tipo de modelos utilizaremos las funciones utilizadas en la unidad anterior. En algunos casos modificaremos la sintaxis para adaptarnos las variables predictoras de tipo factor.","code":""},{"path":"anova.html","id":"ejemplos-9","chapter":"Unidad 8 Modelos ANOVA","heading":"8.4.1 Ejemplos","text":"Pasamos estudiar cada uno de los ejemplos ajustando el modelo saturado, es decir, aquel que contiene todos los efectos posibles en el modelo. Por defecto el programa R utiliza como restricción de identificabilidad que el incremento asociado con la primera categoría (por orden numérico o texto de los niveles del factor) es igual cero, es decir, que la estimación de dicho incremento corresponde con la interceptación o efecto común del modelo.En las tablas de estimaciones eliminamos el p-valor individual asociado cada incremento, dado que en este caso esos efectos deben interpretarse de forma individual.","code":""},{"path":"anova.html","id":"datos-de-insecticidas","chapter":"Unidad 8 Modelos ANOVA","heading":"8.4.1.1 Datos de Insecticidas","text":"Obtenemos el ajuste del modelo de ANOVA de un factor para el banco de datos de Insecticidas. La expresión del modelo viene dado por:\\[\\mu_{spray,}=\\alpha_0+\\alpha_{spray,}+\\epsilon_{spray,}, \\text{para }=,B,C,D,E ,F\\]donde la restricción de identificabilidad es fijar el incremento del spray igual cero, es decir, \\(\\alpha_{spray,} = 0\\).Las ecuaciones de estimación de las medias del número de insectos vivos se pueden extraer partir de la tabla de coeficientes estimados. Sin embargo, podemos obtener las estimaciones y representar las medias de todos los sprays haciendo uso de la función update() quitando el término de interceptación del modelo. De esta forma se prescinde de la restricción de identificabilidad y se obtienen las medias directamente.En esta tabla podemos ver tanto el valor estimado con el intervalo de confianza al 95% del número de insectos vivos con cada uno de los sprays. De esta forma os sprays , B, y F son los menos efectivos (medias de insectos vivos más altas), mientras que los otros tres tienen medias significativamente más bajas. El spray C es el que se muestra más efectivo con una media de insectos vivos de 2 (redondeando entero). Podemos ver gráficamente la solución obtenida mediante:En cuanto las medidas de bondad de ajuste utilizaremos le test F de igual de medias para establecer si los diferentes sprays considerados tienen un comportamiento similar o al menos hay dos de ellos que se comportan de forma distinta.Podemos ver que el test F resulta significativo indicando que al menos hay dos medias distintas (algún incremento distinto de cero), lo que implica que al menos hay dos tipos de spray que se comportan de forma diferente con respecto al número de insectos vivos que quedan finalmente.Dado que el factor considerado es significativo resulta necesario conocer que medias son realmente distintas y cuales podemos considerar iguales. Para realizar esta tarea utilizaremos los denominados tests de comparaciones múltiples que nos permiten comparar dos dos todas las combinaciones del factor para determinar las que son significativamente distintas o . En R podemos utilizar la función TukeyHSD que realiza el test de comparaciones múltiples de Tukey. La salida proporciona la diferencia estimada entre cada para de niveles del factor considerado, así como el p-valor asociado con dicha comparación.Para interpretar la tabla resultante nos debemos fijar en la columna p adj. Todos aquellos inferiores 0.05 indican que las medias son distintas. Analizando todos los p-valores podemos establecer dos grupos de sprays, el formado por los tipos , B y F y el formado por los tipos C, D, y E. Dentro de cada grupo hay diferencias pero sin entres los sprays de grupos distintos.También podemos representar gráficamente las comparaciones realizadas para que resulte más fácil su interpretación:Los intervalos de confianza que contienen al cero muestran aquellos pares de sprays que pueden considerarse iguales. Los intervalos de confianza la izquierda de cero muestran las combinaciones de sprays donde el primer spray tiene un efecto menor que el segundo (media negativa), mientras que en la parte derecha tenemos las combinaciones donde el primer spray es superior al segundo. Por ejemplo, podemos concluir: los sprays y B tienen comportamiento similar; los sprays y C tienen comportamientos distintos con la media del número de supervivientes en superior que en con el spray C; los sprays F y E tienen comportamientos distintos con la media del número de supervivientes en F superior que en con el spray E. ¿Qué otras conclusiones podemos extraer?","code":"\n# Ajuste del modelo\nfit.insecticidas <- lm(count ~ spray, data = insecticidas)\n# Inferencia sobre los parámetros del modelo\ntab_model(fit.insecticidas, \n          show.r2 = FALSE, \n          show.p = FALSE)\n# Modelo sin interceptación\nm1 <- update(fit.insecticidas, . ~ spray - 1)\n# Inferencia sin identificabilidad\ntab_model(m1, show.r2 = FALSE, show.p = FALSE)\n# Gráfico de medias estimadas\nplot_model(m1, show.values = TRUE, vline.color = \"yellow\")\nglance(fit.insecticidas)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.724         0.704  3.92      34.7 3.18e-17     5  -197.  409.  425.    1015.          66\n## # … with 1 more variable: nobs <int>\n# Test de comparaciones múltiples\nTukeyHSD(fit.insecticidas) ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = x)\n## \n## $spray\n##            diff        lwr       upr     p adj\n## B-A   0.8333333  -3.866075  5.532742 0.9951810\n## C-A -12.4166667 -17.116075 -7.717258 0.0000000\n## D-A  -9.5833333 -14.282742 -4.883925 0.0000014\n## E-A -11.0000000 -15.699409 -6.300591 0.0000000\n## F-A   2.1666667  -2.532742  6.866075 0.7542147\n## C-B -13.2500000 -17.949409 -8.550591 0.0000000\n## D-B -10.4166667 -15.116075 -5.717258 0.0000002\n## E-B -11.8333333 -16.532742 -7.133925 0.0000000\n## F-B   1.3333333  -3.366075  6.032742 0.9603075\n## D-C   2.8333333  -1.866075  7.532742 0.4920707\n## E-C   1.4166667  -3.282742  6.116075 0.9488669\n## F-C  14.5833333   9.883925 19.282742 0.0000000\n## E-D  -1.4166667  -6.116075  3.282742 0.9488669\n## F-D  11.7500000   7.050591 16.449409 0.0000000\n## F-E  13.1666667   8.467258 17.866075 0.0000000\nplot(TukeyHSD(fit.insecticidas), las=1, tcl = -.3)"},{"path":"anova.html","id":"datos-de-envasado","chapter":"Unidad 8 Modelos ANOVA","heading":"8.4.1.2 Datos de Envasado","text":"Ajustamos un modelo ANOVA de una vía para el conjunto de datos de envasado. En este caso la restricción de identificabilidad es fijar el incremento de Máquina M1 igual cero, es decir, \\(\\alpha_{maquina,M1} = 0\\).\\[\\mu_{maquina,}=\\alpha_0+\\alpha_{maquina,}+\\epsilon_{maquina,}, \\text{para }=M1, M2, M3, M4\\]\ndonde la restricción de identificabilidad es fijar el incremento de Máquina M1 igual cero, es decir, \\(\\alpha_{maquina,M1} = 0\\).la vista de los coeficientes la máquina el orden de las máquinas (de mejor peor, es decir, de mayor número de envases sin defecto menor) es \\(M4 > M2 > M1 > M3\\), con media estimadas dadas por:Vemos la solución gráfica donde se aprecian comportamientos muy diferenciados entre las diferentes máquinas.Con respecto la bondad del ajuste podemos ver que el test \\(F\\) resulta significativo indicando que al menos alguna de las medias es distinta del resto (algún incremento distinto de cero). Por tanto, existe un efecto del tipo de máquina en el número de envases sin defecto.Realizamos las comparaciones múltiples para comparar todas las máquinas dos dos.Tan sólo las combinaciones M4-M1 y M4-M3 muestran resultados significativos indicando que la máquina M4 es comparable con la M2, es decir, la media de envases sin defecto es igual para dichas máquinas, pero distintas de la M1 y M3 que también puede considerarse iguales. Veamos los resultados gráficamente:","code":"\n# Ajuste del modelo\nfit.envasado <- lm( produccion ~ maquina, data = envasado)\n# Inferencia sobre los parámetros del modelo\ntab_model(fit.envasado, \n          show.r2 = FALSE, \n          show.p = FALSE)\n# Modelo sin interceptación\nm1 <- update(fit.envasado, . ~ maquina - 1)\n# Inferencia sin identificabilidad\ntab_model(m1, show.r2 = FALSE, show.p = FALSE)\nplot_model(m1, show.values = TRUE, vline.color = \"yellow\")\nglance(fit.envasado)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.596         0.496  7.35      5.91  0.0102     3  -52.3  115.  118.     648.          12\n## # … with 1 more variable: nobs <int>\nTukeyHSD(fit.envasado) ##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = x)\n## \n## $maquina\n##        diff        lwr       upr     p adj\n## M2-M1  7.75  -7.673887 23.173887 0.4716327\n## M3-M1 -1.50 -16.923887 13.923887 0.9911761\n## M4-M1 18.00   2.576113 33.423887 0.0210561\n## M3-M2 -9.25 -24.673887  6.173887 0.3283778\n## M4-M2 10.25  -5.173887 25.673887 0.2508228\n## M4-M3 19.50   4.076113 34.923887 0.0126965\nplot(TukeyHSD(fit.envasado), las=1, tcl = -.3)"},{"path":"anova.html","id":"datos-de-venenos","chapter":"Unidad 8 Modelos ANOVA","heading":"8.4.1.3 Datos de Venenos","text":"Ajustamos un modelo ANOVA de dos vías con interacción entre veneno y antídoto para el conjunto de datos de venenos cuya expresión viene dada por (el índice hace referencia al antídoto, y el j los venenos):\\[\\mu_{Antidoto:,Veneno:j} = \\alpha_0 + \\alpha_{Antidoto:} +\\alpha_{Veneno:j} + \\alpha\\beta_{Antidoto:,Veneno:j} + \\epsilon_{Antidoto:,Veneno:j}\\]\npara \\(=AA,AB,AC,AD\\), \\(j=VA,VB,VC\\) con las restricciones de identificabilidad:\\(\\alpha_{antidoto,AA} = 0\\),\\(\\beta_{veneno,VA} = 0\\),\\(\\alpha\\beta_{Antidoto,AA; Veneno,VA} = 0\\),de forma que el ajuste para este modelo viene dado por:Desde luego interpretar la tabla de coeficientes en este caso es más complicado, dada la gran cantidad de parámetros involucrados, por lo que resulta más fácil obtener las medias y el gráfico asociado. En este caso podemos actualizar el modelo eliminando la interceptación dado que tenemos tres restricciones y una. Modificamos la tabla de estimaciones y el gráfico para obtener todo el conjunto de medias. Además, como hemos descartado el efecto de interacción, en el gráfico sólo representaremos las medias asociadas con dicho efecto. Para representar el efecto de interacción añadimos el parámetro \"int\" la función plot_model.la vista de la tabla de estimaciones podemos concluir que:El antídoto AA es el menos tiempo tarda en hacer efecto sobre el veneno VA.El antídoto AA es el menos tiempo tarda en hacer efecto sobre el veneno VB.El antídoto AA es el menos tiempo tarda en hacer efecto sobre el veneno VC.De esta forma parece que el antídoto AA es el mejor funciona con cualquiera de los venenos pero será necesario estudiar la bondad de ajuste y realizar el proceso de selección para determinar que efectos deben aparecer el modelo, para poder concluir más precisamente sobre el comportamiento de los diferentes antídotos. En el gráfico de estimación del efecto de interacción podemos apreciar de forma más sencillo la relación entre ambos factores. Aunque el antídoto AA muestra los tiempos más bajos, también es cierto que los intervalos de confianza son muy diferentes de los obtenidos para AC y AD.Con respecto la bondad del ajuste podemos ver que el test \\(F\\) resulta significativo indicando que al menos alguna de las medias es distinta del resto (algún incremento distinto de cero). Por tanto, existe un efecto de interacción entre venenos y antídoto que proporciona un tiempo de reacción distinto.El p-valor asociado con la interacción resulta significativo, indicando que dicho efecto puede ser considerado nulo y se debería considerarse en el modelo. Vemos aquí la diferencia entre la interpretación del test F para el modelo ANOVA de una vía y el ANOVA de dos vías. En este último caso es necesario el estudio de la tabla ANOVA para determinar los efectos significativos y basta solo con la bondad del ajuste. En el punto siguiente vemos como realizar la selección del mejor modelo en este tipo de situaciones.","code":"\n# Ajuste del modelo\nfit.venenos <- lm( tiempo ~ antidoto*veneno, data = venenos)\n# Inferencia sobre los parámetros del modelo\ntab_model(fit.venenos,\n          show.r2 = FALSE, \n          show.p = FALSE)\n# Objeto gráfico\np <- plot_model(fit.venenos, \"int\", \n           show.stat = TRUE, \n           title =\"\")\n# Tabla de estimaciones\np$data## # Predicted values of tiempo\n## \n## # veneno = VA\n## \n## antidoto | Predicted | group_col |       95% CI\n## -----------------------------------------------\n##        1 |      0.41 |        VA | [0.27, 0.56]\n##        2 |      0.88 |        VA | [0.73, 1.03]\n##        3 |      0.57 |        VA | [0.42, 0.71]\n##        4 |      0.61 |        VA | [0.46, 0.76]\n## \n## # veneno = VB\n## \n## antidoto | Predicted | group_col |       95% CI\n## -----------------------------------------------\n##        1 |      0.32 |        VB | [0.17, 0.47]\n##        2 |      0.81 |        VB | [0.67, 0.96]\n##        3 |      0.38 |        VB | [0.23, 0.52]\n##        4 |      0.67 |        VB | [0.52, 0.81]\n## \n## # veneno = VC\n## \n## antidoto | Predicted | group_col |       95% CI\n## -----------------------------------------------\n##        1 |      0.21 |        VC | [0.06, 0.36]\n##        2 |      0.33 |        VC | [0.19, 0.48]\n##        3 |      0.24 |        VC | [0.09, 0.38]\n##        4 |      0.32 |        VC | [0.18, 0.47]\n# Gráfico de interacción estimado\np\nglance(fit.venenos)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC deviance df.residual\n##       <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>       <int>\n## 1     0.734         0.653 0.149      9.03 0.000000193    11   30.2 -34.3 -9.98    0.800          36\n## # … with 1 more variable: nobs <int>"},{"path":"anova.html","id":"comparación-y-selección-de-modelos-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.5 Comparación y selección de modelos","text":"La comparación de modelos se basa en determinar la capacidad explicativa de cada uno de los efectos presentes en el modelo. Mientras que en los modelos de regresión esos efectos van asociados con un único coeficiente del modelo (pendiente asociada la predictora), en los modelos ANOVA el efecto de un factor va asociado con todos los incrementos establecidos. En realidad, tendremos un efecto significativo cuando detectemos diferencias entre al menos dos medias de la respuesta para los diferentes niveles del factor. Dado que el número de modelos que podemos construir en estas situaciones es limitado, podríamos construirlos todos ellos y compararlos de golpe para decidir el mejor modelo. Esto se puede hacer con el test F de comparación de modelos que es una versión del test F para un contraste especifico, o bien utilizar el criterio del AIC o BIC y quedarnos con aquel modelo que de un valor más pequeño. En modelos más complejos optaremos por un procedimiento automático por pasos empezando por el modelo saturado.En los modelos ANOVA de una vía la selección del modelo se basa únicamente en el análisis de bondad de ajuste, ya que el test F que obtenemos es realmente la comparación entre los modelos que contienen el factor y el que lo contiene, mientras que los modelos de más de una vía es necesario realizar el proceso de selección de efectos para determinar el modelo final.continuación presentamos las diferentes opciones con el banco de datos de venenos ya que en los otros dos casos ya hemos resulto la selección mediante el análisis de la bondad del ajuste.","code":""},{"path":"anova.html","id":"ejemplos-10","chapter":"Unidad 8 Modelos ANOVA","heading":"8.5.1 Ejemplos","text":"Realizamos la selección le mejor modelo par el banco de datos de venenos. En primer lugar procederemos de forma manual considerando todos los posibles modelos con las combinaciones de los factores considerados:\\(tiempo ∼veneno∗antidoto\\): Modelo con interacción, donde hay al menos una combinación de veneno y antídoto con tiempo de reacción medio distinto de otras combinaciones.\\(tiempo∼veneno+antidoto\\): Modelo sin interacción con efectos principales, donde el efecto del veneno y el antídoto tiene un efecto aditivo en e tiempo medio de reacción.\\(tiempo∼veneno\\): Modelo con efecto principal marginal del veneno, donde hay almenos un tipo de veneno con un tiempo medio de reacción distinto pero los antidotos tienen efecto.\\(tiempo∼antidoto\\): Modelo con efecto principal marginal del antídoto, donde hay al menos un tipo de antídoto con un tiempo medio de reacción distinto pero los venenos tienen efecto.\\(tiempo∼1\\): Modelo sin efectos, donde el tiempo de reacción cambia con el antídoto y el tipo de veneno.En este caso la especificación de todos los posibles modelos así como su comparación resulta más costosa ya que están involucrados dos factores de clasificación. Los posibles modelos son:\\(tiempo \\sim veneno * antidoto\\): Modelo con interacción, donde hay al menos una combinación de veneno y antídoto con tiempo de reacción medio distinto de otras combinaciones.\\(tiempo \\sim veneno + antidoto\\): Modelo sin interacción con efectos principales, donde el efecto del veneno y el antídoto tiene un efecto aditivo en e tiempo medio de reacción.\\(tiempo \\sim veneno\\): Modelo con efecto principal marginal del veneno, donde hay almenos un tipo de veneno con un tiempo medio de reacción distinto pero los antidotos tienen efecto.\\(tiempo \\sim antidoto\\): Modelo con efecto principal marginal del antídoto, donde hay al menos un tipo de antídoto con un tiempo medio de reacción distinto pero los venenos tienen efecto.\\(tiempo \\sim 1\\): Modelo sin efectos, donde el tiempo de reacción cambia con el antídoto y el tipo de veneno.Consideramos en primer lugar la comparación entre los modelos con y sin interacción para determinar si dicho efecto es relevante o para explicar el comportamiento del tiempo medio de reacción. Obtenemos el contraste F parcial mediante:Dado que el p-valor es significativo (p-valor > 0.05) podemos concluir que ambos modelos pueden ser considerarse como iguales, es decir, el efecto de interacción resulta significativo. Vamos comparar las estimaciones de ambos modelos y representamos gráficamente el modelo sin interacción. En este caso obtendremos un gráfico por cada factor dado que actúan de forma conjunta sino de forma aditiva. Utilizamos el parámetro “pred” para obtener las estimaciones de las medias asociadas cada factor.Como era de esperar, dado que la interacción es significativa, los coeficientes estimados (incrementos respecto de las medias de referencia dadas por el antídoto AA y el veneno VA) varían sustancialmente entre ambos modelos. En el gráfico podemos ver que el veneno con un tiempo de reacción más pequeño es el asociado con el veneno VC, mientras que si nos fijamos en el antídoto, el menor tiempo de reacción se da para el AA. Dado que hay interacción el modelo sugiere que independientemente del veneno utilizado, el antídoto más efectivo es el AA, seguido por el AC, AD, y por último el AB. Se pueden obtener las estimaciones asociadas con el modelo sin interacción para verificar este hecho.Este proceso manual resulta complicado de llevar cabo cuando tenemos modelos más complejos (con más de dos vías de clasificación), por lo que es necesario utilizar procedimientos secuenciales automáticos para alcanzar el mejor modelo. continuación se muestra como realizar esta selección:Como era de esperar el proceso determina que podemos prescindir del efecto de interacción.Para utilizar el criterio AIC utilizamos la función step() porque la función ols_step_backward_aic esta diseñada específicamente para trabajar con preditores numéricos y con interacciones de factores lo que puede llevar soluciones erróneas. Por tanto, la única opción pasa por evaluar todos los modelos con la función ols_step_all_possible y elegir el mejor de ellos teniendo en cuenta la construcción del modelo. Sin embargo, esta forma de proceder será práctica en modelos más complejos.El proceso indica que podemos descartar el efecto de interacción contradiciendo los resultados obtenidos con el test F, aunque la diferencia entre los valores de AIC es muy pequeña. Habitualmente para este tipo de modelos se suele utilizar el criterio basado en el test F, dado que ambos pueden llevar soluciones distintas como ocurre en este caso.","code":"\n# Ajuste del modelo con interacción\nfit1 <- lm(tiempo ~ antidoto + veneno + antidoto:veneno, data = venenos)\n# Ajuste del modelo sin interacción\nfit2 <- lm(tiempo ~ antidoto + veneno, data = venenos) \n# Comparación entre modelos\nanova(fit2, fit1)## Analysis of Variance Table\n## \n## Model 1: tiempo ~ antidoto + veneno\n## Model 2: tiempo ~ antidoto + veneno + antidoto:veneno\n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     42 1.0504                           \n## 2     36 0.8001  6   0.25027 1.8768 0.1118\n# Estimaciones de ambos modelos\ntab_model(fit1, fit2, show.r2 = FALSE) \n# Gráfico del modelo sin interacción\nplot_model(fit2, \"pred\")## $antidoto## \n## $veneno\n# Selección basada en test F\nols_step_backward_p(fit.venenos, prem = 0.05)## \n## \n##                               Elimination Summary                               \n## -------------------------------------------------------------------------------\n##         Variable                         Adj.                                      \n## Step        Removed        R-Square    R-Square     C(p)       AIC        RMSE     \n## -------------------------------------------------------------------------------\n##    1    antidoto:veneno      0.6508      0.6092    5.2608    -33.2407    0.1581    \n## -------------------------------------------------------------------------------\n# Selección con función step y AIC\nstats::step(fit.venenos, direction = \"backward\")## Start:  AIC=-172.52\n## tiempo ~ antidoto * veneno\n## \n##                   Df Sum of Sq    RSS     AIC\n## <none>                         0.8001 -172.52\n## - antidoto:veneno  6   0.25027 1.0504 -171.46## \n## Call:\n## lm(formula = tiempo ~ antidoto * veneno, data = venenos)\n## \n## Coefficients:\n##         (Intercept)           antidotoAB           antidotoAC           antidotoAD  \n##              0.4125               0.4675               0.1550               0.1975  \n##            venenoVB             venenoVC  antidotoAB:venenoVB  antidotoAC:venenoVB  \n##             -0.0925              -0.2025               0.0275              -0.1000  \n## antidotoAD:venenoVB  antidotoAB:venenoVC  antidotoAC:venenoVC  antidotoAD:venenoVC  \n##              0.1500              -0.3425              -0.1300              -0.0850"},{"path":"anova.html","id":"diagnóstico-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.6 Diagnóstico","text":"Los procedimientos de diagnóstico son los mismos que en los modelos de regresión salvo por el hecho de que la igualdad de varianzas de los residuos se debe cumplir para cada uno de los grupos determinados por los niveles de los factores, es decir, varianzas residuales iguales. Para este tipo de modelos utilizaremos el test de Kolmogorov-Smirnov para el test de normalidad y el de Levene para la homogeneidad de varianzasTambién se pueden realizar los diagnósticos gráficos de residuos para cada hipótesis pero en estos modelos sólo tiene sentido realizar los diagramas de cajas de residuos versus niveles del factor para valorar linealidad y varianza constante, dado que en este tipo de modelos tenemos un único valor ajustado (media) para todos los sujetos de un mismo nivel del factor, y por tanto un mismo residuo para todos los sujetos de un mismo grupo. En cuanto al estudio de influencia, nos centraremos principalmente en la distancia de Cook dado que tiene sentido considerar la influencia sobre los coeficientes del modelo ni sobre los valores ajustados dada la estructura agrupada del banco de datos analizar, es decir, cada coeficiente representa una variable sino que debemos considerar el conjunto de ellos que representan el efecto de un factor.Para realizar el diagnóstico partiremos del modelo ajustado en el apartado anterior para cada banco de datos.","code":""},{"path":"anova.html","id":"ejemplos-11","chapter":"Unidad 8 Modelos ANOVA","heading":"8.6.1 Ejemplos","text":"Para realizar el diagnóstico partiremos del modelo ajustado en el apartado anterior para cada banco de datos. En caso de detectar algún problema con las hipótesis del modelo deberemos ajustar un nuevo modelo y verificar que cumple las hipótesis.","code":""},{"path":"anova.html","id":"datos-de-insecticidas-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.6.1.1 Datos de insecticidas","text":"Obtenemos los valores de diagnóstico y realizamos los correspondientes tests de hipótesis y análisis de influencia para el banco de datos de insecticidas. En primer lugar realizamos el gráfico cajas de residuos versus niveles del factor para valorar la hipótesis de linealidad y varianza constante.En el gráfico se observa como la variabilidad dentro de cada nivel del factor parece diferente, aunque los residuos parecen tener un comportamiento lineal alrededor del cero.El test de Kolmogorov-Smirnov resulta significativo indicando el cumplimiento de la hipótesis de Normalidad de los residuos.El test resulta significativo (p-valor inferior 0.05) indicando que se verifica la hipótesis de varianza constante, como ya sospechábamos del gráfico de los residuos. Podemos ver si el incumplimiento es debido algún valor influyente, pero sino es así deberemos plantear alguna transformación de la respuesta para tratar de corregir los problemas de diagnóstico. Se puede verificar que el análisis de la distancia de Cook presenta ninguna observación influyente, por lo que lo ideal sería plantear la transformación de Box-Cox para encontrar una solución esta situación pero dado que tenemos valores de cero en la respuesta esto resulta posible. Sin embargo puesto que la respuesta son datos de conteos podemos utilizar la función logaritmo añadiendo un uno los valores de la respuesta para evitar problemas en la transformación.¿qué ajuste tenemos con esa nueva variable? ¿Cómo es la bondad del ajuste?¿se verifican ahora las hipótesis del modelo?","code":"\n# Valores de diagnóstico\ndiagnostico <- fortify(fit.insecticidas)\n# Gráfico de residuos vs factor\nggplot(diagnostico,aes(x = spray, y = .stdresid)) + \n   geom_boxplot() +\n   geom_hline(yintercept = 0, col = \"red\")\n# Tests de normalidad\nols_test_normality(fit.insecticidas) ## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9601         0.0223 \n## Kolmogorov-Smirnov        0.1301         0.1747 \n## Cramer-von Mises          6.6076         0.0000 \n## Anderson-Darling          1.2015         0.0037 \n## -----------------------------------------------\n# Tests de homogeneidad de varianzas\nleveneTest(.stdresid ~ spray, data = diagnostico) ## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value   Pr(>F)   \n## group  5  3.8214 0.004223 **\n##       66                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Calculamos la nueva variable\ninsecticidas <- insecticidas %>% mutate(lcount = log(count+1))"},{"path":"anova.html","id":"datos-de-envasado-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.6.1.2 Datos de envasado","text":"Obtenemos los valores de diagnóstico y realizamos los correspondientes tests de hipótesis y análisis de influencia.Los tests de hipótesis resultan significativos indicando que el modelo propuesto verifica dichas hipótesis. se detectan además observaciones influyentes, con lo que el modelo propuesto y que fue analizado en la sección de estimación es el modelo final para este banco de datos.","code":"\n# Valores de diagnóstico\ndiagnostico <- fortify(fit.envasado)\n# Gráfico\nggplot(diagnostico,aes(x = maquina, y = .stdresid)) + \n   geom_boxplot() +\n   geom_hline(yintercept = 0, col = \"red\")\n# Tests de hipótesis\nols_test_normality(fit.envasado)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9074         0.1058 \n## Kolmogorov-Smirnov        0.176          0.6420 \n## Cramer-von Mises          1.5833         0.0000 \n## Anderson-Darling          0.5552         0.1269 \n## -----------------------------------------------\nleveneTest(.stdresid ~ maquina, data = diagnostico)## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  3  0.2091 0.8882\n##       12\n# Análisis de influencia\nols_plot_cooksd_chart(fit.envasado)"},{"path":"anova.html","id":"datos-de-venenos-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.6.1.3 Datos de venenos","text":"Utilizamos el modelo sin interacción que hemos obtenido tras el proceso de selección de efectos tratada en la sección anterior.El test de Kolmogorov-Smirnov resulta significativo indicando el cumplimiento de la hipótesis de normalidad.El test de Leven resulta significativo indicando que se cumple la hipótesis de varianza constante. Se plantea utilizar Box-Cox para obtener una posible transformación de la respuesta que permita verificar las hipótesis del modelo.Se puede ver que el intervalo de confianza para la transformación contiene el valor de -1. Esto nos da indicios de que podríamos utilizar la transformación inversa para corregir los problemas detectados de homogeneidad de varianzas. En ese caso habrá que tener en cuenta que todas nuestras conclusiones se basarán en dicha transformación. Comenzamos de nuevo con la construcción del modelo.El modelo resultante es aquel que contiene la interacción. Obtenemos los coeficientes de ese modelo, así como las medias estimadas para la combinación de veneno-antídoto. Expresamos las medias con la transformación propuesta.Las ecuaciones de estimación son:\\[\\begin{array}{lll} \n1/\\mu_{AA,VA} & = \\alpha_0 + \\alpha_{AA} + \\beta_{VA} & = 2.70 + 0 + 0 = 2.70\\\\\n1/\\mu_{AA,VB} & = \\alpha_0 + \\alpha_{AA} + \\beta_{VB} & = 2.70 + 0 + 0.47 = 3.17\\\\\n1/\\mu_{AA,VC} & = \\alpha_0 + \\alpha_{AA} + \\beta_{VC} & = 2.70 + 0 + 2.00 = 4.70\\\\\n1/\\mu_{AB,VA} & = \\alpha_0 + \\alpha_{AB} + \\beta_{VA} & = 2.70 - 1.66 + 0 = 1.04\\\\\n1/\\mu_{AB,VB} & = \\alpha_0 + \\alpha_{AB} + \\beta_{VB} & = 2.70 - 1.66 + 0.47 = 1.51\\\\\n1/\\mu_{AB,VC} & = \\alpha_0 + \\alpha_{AB} + \\beta_{VC} & = 2.70 - 1.66 + 2.00 = 3.04\\\\\n1/\\mu_{AC,VA} & = \\alpha_0 + \\alpha_{AC} + \\beta_{VA} & = 2.70 - 0.57 + 0 = 2.13\\\\\n1/\\mu_{AC,VB} & = \\alpha_0 + \\alpha_{AC} + \\beta_{VB} & = 2.70 - 0.57 + 0.47 = 2.60\\\\\n1/\\mu_{AC,VC} & = \\alpha_0 + \\alpha_{AC} + \\beta_{VC} & = 2.70 - 0.57 + 2.00 = 4.13\\\\\n1/\\mu_{AD,VA} & = \\alpha_0 + \\alpha_{AD} + \\beta_{VA} & = 2.70 - 1.35 + 0 = 1.35\\\\\n1/\\mu_{AD,VB} & = \\alpha_0 + \\alpha_{AD} + \\beta_{VB} & = 2.70 - 1.35 + 0.47 = 1.82\\\\\n1/\\mu_{AD,VC} & = \\alpha_0 + \\alpha_{AD} + \\beta_{VC} & = 2.70 - 1.35 + 2.00 = 3.35\\\\\n\\end{array}\\]Veamos si se verifican las hipótesis del modelo:Se verifican las hipótesis del modelo. La transformación planteada permite obtener un modelo válido para la fase de predicción, y nos permite extraer conclusiones sobre la relación entre veneno y antídoto con el tiempo de reacción.","code":"\n# Modelo\nfit.venenos <- lm (tiempo ~ veneno + antidoto, data = venenos)\n# Valores de diagnóstico\ndiagnostico <- fortify(fit.venenos)\n# Tests de hipótesis\nols_test_normality(fit.venenos)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9221         0.0035 \n## Kolmogorov-Smirnov        0.113          0.5353 \n## Cramer-von Mises         12.0813         0.0000 \n## Anderson-Darling          0.8952         0.0205 \n## -----------------------------------------------\nleveneTest(.stdresid ~ veneno*antidoto, data = diagnostico)## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value    Pr(>F)    \n## group 11  4.1582 0.0005539 ***\n##       36                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nMASS::boxcox(fit.venenos) \n# Creamos la variable transformada\nvenenos <- venenos %>% mutate(tiempoinv = 1/tiempo)\n# Ajustamos el modelo de nuevo\nfit.venenos.inv <- lm(tiempoinv ~ antidoto*veneno, data = venenos)\n# Selección utilizando el test F\nols_step_backward_p(fit.venenos.inv, prem = 0.05)## \n## \n##                              Elimination Summary                               \n## ------------------------------------------------------------------------------\n##         Variable                         Adj.                                     \n## Step        Removed        R-Square    R-Square     C(p)       AIC       RMSE     \n## ------------------------------------------------------------------------------\n##    1    antidoto:veneno      0.8454       0.827    0.4181    75.5483    0.4911    \n## ------------------------------------------------------------------------------\n# Modelo sin interacción\nfit.venenos.inv <- lm(tiempoinv ~ antidoto + veneno, data = venenos)\n# Estimaciones de ambos modelos\ntab_model(fit.venenos.inv, show.r2 = FALSE)\n# Valores de diagnóstico\ndiagnostico <- fortify(fit.venenos.inv)\n# Tests de hipótesis\nols_test_normality(fit.venenos.inv)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9789         0.5339 \n## Kolmogorov-Smirnov        0.1057         0.6192 \n## Cramer-von Mises          6.0617         0.0000 \n## Anderson-Darling          0.2948         0.5834 \n## -----------------------------------------------\nleveneTest(.stdresid ~ veneno*antidoto, data = diagnostico)## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group 11  1.1672 0.3429\n##       36\n# Análisis de influencia\nols_plot_cooksd_chart(fit.venenos.inv)"},{"path":"anova.html","id":"predicción-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.7 Predicción","text":"El proceso de predicción en este tipo de modelos es muy simple ya que los posibles valores de las predictoras coincide con las medias de los posibles niveles o combinación de los niveles del factor o factores que aparezcan en el modelo final. En este caso solo tiene sentido predecir los valores de la media ya que nuestro objetivo es el estudio de dichas medias. Veremos además como representar gráficamente los resultados de la predicción.","code":""},{"path":"anova.html","id":"ejemplos-12","chapter":"Unidad 8 Modelos ANOVA","heading":"8.7.1 Ejemplos","text":"Obtenemos las tablas de medias estimadas para cada uno de los ejemplos tratados en esta unidad.","code":""},{"path":"anova.html","id":"datos-de-insecticidas-2","chapter":"Unidad 8 Modelos ANOVA","heading":"8.7.1.1 Datos de insecticidas","text":"Estamos interesados en predcir el número de insectos muertos en función del tipo de insecticida utilizado. Podemos obtener la tabla de predicción y el gráfico asociado con el código siguiente:Para obtener el verdadero valor de la media debemos deshacer la transformación utilizada para el ajuste del modelo. Representamos la media predicha así como un intervalo de predicción para ella. Ordenamos los valores obtenidos de mayor menor predicción para una mejor visualización.¿Qué podemos decir del gráfico de predicción de medias obtenido? ¿ Qué insecticida es más efectivo y cual menos?","code":"\n# Objeto gráfico\np <- plot_model(fit.insecticidas, \"pred\", terms = \"spray\",\n           show.stat = TRUE)\n# Tabla de estimaciones\np$data## # Predicted values of count\n## \n## spray | Predicted | group_col |         95% CI\n## ----------------------------------------------\n##     1 |     14.50 |         1 | [12.28, 16.72]\n##     2 |     15.33 |         1 | [13.11, 17.55]\n##     3 |      2.08 |         1 | [-0.14,  4.30]\n##     4 |      4.92 |         1 | [ 2.70,  7.14]\n##     5 |      3.50 |         1 | [ 1.28,  5.72]\n##     6 |     16.67 |         1 | [14.45, 18.89]\n# Secuencia de valores de predicción\nnewdata <- data.frame(spray = unique(insecticidas$spray))\n# Predicción para la media de la respuesta\nnewdata <- data.frame(newdata, \n            predict(fit.insecticidas, newdata, interval = \"confidence\"))\n# Deshacemos la transformación\nnewdata <- newdata %>% mutate(prediccion = exp(fit) - 1,\n                              lower = exp(lwr) - 1, \n                              upper = exp(upr) - 1)\n# Gráfico ordenado del menor al mayor valor en función del tipo de spray\nggplot(newdata, aes(x = fct_reorder(spray, prediccion), y = prediccion)) + \n    geom_errorbar(aes(ymin = lower, ymax = upper), width = .1) +\n    geom_line() +\n    geom_point() +\n    labs(x = \"Spray\", y = \"Conteo\") +\n    coord_flip() "},{"path":"anova.html","id":"datos-de-envasado-2","chapter":"Unidad 8 Modelos ANOVA","heading":"8.7.1.2 Datos de envasado","text":"Podemos obtener la tabla de predicción y el gráfico asociado con el código siguiente:Podemos ver como la predicción para la máquina 4 es diferente de las predicciones para las máquinas 1 y 3 (sus intervalos de confianza se solapan). hay diferencia entre la M4 y la M2, ni entre la M2 y la M1 o M3. La máquina que podemos indicar como más efectiva es la M4 ya que es la que produce un mayor número de envases sin defectos.","code":"\n# Objeto gráfico\np <- plot_model(fit.envasado, \"pred\", terms = \"maquina\",\n           show.stat = TRUE, \n           title = \" \")\n# Tabla de estimaciones\np$data## # Predicted values of produccion\n## \n## maquina | Predicted | group_col |           95% CI\n## --------------------------------------------------\n##       1 |    106.00 |         1 | [ 98.80, 113.20]\n##       2 |    113.75 |         1 | [106.55, 120.95]\n##       3 |    104.50 |         1 | [ 97.30, 111.70]\n##       4 |    124.00 |         1 | [116.80, 131.20]\n# Gráfico\np"},{"path":"anova.html","id":"datos-de-venenos-2","chapter":"Unidad 8 Modelos ANOVA","heading":"8.7.1.3 Datos de venenos","text":"Comenzaremos obteniendo la predicción para el modelo estimado y posteriormente lo haremos para la variable respuesta en la escala original.Dado que hemos utilizado la transformación inversa para encontrar el antídoto que mejor funciona para cada veneno debemos buscar el valor más alto de valor predicho. El antídoto AA es que le muestra valores predichos más altos para cualquier tipo de venenos por lo que resulta el más efectivo. Podemos ver el resultado gráficamente:¿Cómo podemos interpretar los intervalos de confianza para la predicción que hemos obtenido? ¿Cómo se comportan los antídotos con cada veneno? ¿Este resultado es esperable en función del modelo ajustado?Obtenemos y representamos los valores de predicción en la escala original de la variable respuesta.Realizamos el gráfico de predicción en la escala original de la variable respuesta ordenando los intervalos de confianza desde el tiempo de reacción más pequeño al más grande, para así determinar las combinaciones antídoto-veneno más efectivas.Las conclusiones que podemos extraer son:Para el veneno VA los mejores antídotos son el AA y el AC con tiempos de reacción similares (intervalos de predicción disjuntos), y distintos de los que proporcionan el antídoto AD y el AB.Para el veneno VB los mejores antídotos son el AA y el AC con tiempos de reacción similares, y distintos de los que proporcionan el antídoto AD y el AB.Para el veneno VC los mejores antídotos son el AA y el AC con tiempos de reacción similares, y distintos de los que proporcionan el antídoto AD y el AB.la hora de elegir deberíamos hacerlo entre AA y AC para cualquiera de los venenos. Además, podemos ver que el veneno VC es el más resistente los antídotos ya que tiene mayores tiempos de reacción (sitúa más intervalos de predicción en la zona alta). El menos resistente sería el veneno VA (mayor número de intervalos de predicción en la parte inferior).","code":"\n# Objeto gráfico\np <- plot_model(fit.venenos.inv, \"pred\", terms = c(\"veneno\",\"antidoto\"),\n           show.stat = TRUE, \n           title =\"\")\n# Tabla de estimaciones en términos de la inversa\np$data## # Predicted values of tiempoinv\n## \n## # antidoto = AA\n## \n## veneno | Predicted | group_col |       95% CI\n## ---------------------------------------------\n##      1 |      2.70 |        AA | [2.36, 3.04]\n##      2 |      3.16 |        AA | [2.82, 3.50]\n##      3 |      4.70 |        AA | [4.36, 5.04]\n## \n## # antidoto = AB\n## \n## veneno | Predicted | group_col |       95% CI\n## ---------------------------------------------\n##      1 |      1.04 |        AB | [0.70, 1.38]\n##      2 |      1.51 |        AB | [1.17, 1.85]\n##      3 |      3.04 |        AB | [2.70, 3.38]\n## \n## # antidoto = AC\n## \n## veneno | Predicted | group_col |       95% CI\n## ---------------------------------------------\n##      1 |      2.12 |        AC | [1.78, 2.46]\n##      2 |      2.59 |        AC | [2.25, 2.93]\n##      3 |      4.13 |        AC | [3.78, 4.47]\n## \n## # antidoto = AD\n## \n## veneno | Predicted | group_col |       95% CI\n## ---------------------------------------------\n##      1 |      1.34 |        AD | [1.00, 1.68]\n##      2 |      1.81 |        AD | [1.47, 2.15]\n##      3 |      3.35 |        AD | [3.01, 3.69]\n# Gráfico de interacción estimado\np\n# Gráfico de interacción estimado\np\n# Creamos la combinación de grupos con ambos factores\nf1 <- unique(venenos$antidoto)\nf2 <- unique(venenos$veneno)\n# Generamos todas las posibles combinaciones de niveles\nnewdata <- data.frame(expand.grid(f1, f2))\ncolnames(newdata) <- c(\"antidoto\", \"veneno\")\n# Predicción para la media de la respuesta\nnewdata <- data.frame(newdata, predict(fit.venenos.inv, \n                               newdata, \n                               interval=\"confidence\")) \n# Deshacemos la transformación inversa\nnewdata <- newdata %>% \n             mutate(prediccion = 1/fit,\n                    lower = 1/lwr, \n                    upper = 1/upr)\n# Presentamos los datos en la escala transformada y la original\nnewdata##    antidoto veneno      fit       lwr      upr prediccion     lower     upper\n## 1        AA     VA 2.696003 2.3455815 3.046425  0.3709194 0.4263335 0.3282536\n## 2        AB     VA 1.038601 0.6881791 1.389022  0.9628339 1.4531102 0.7199308\n## 3        AC     VA 2.123868 1.7734460 2.474289  0.4708391 0.5638739 0.4041564\n## 4        AD     VA 1.344279 0.9938569 1.694700  0.7438934 1.0061810 0.5900748\n## 5        AA     VB 3.164644 2.8142227 3.515066  0.3159913 0.3553379 0.2844897\n## 6        AB     VB 1.507242 1.1568203 1.857664  0.6634635 0.8644385 0.5383106\n## 7        AC     VB 2.592509 2.2420873 2.942931  0.3857267 0.4460130 0.3397973\n## 8        AD     VB 1.812920 1.4624982 2.163342  0.5515964 0.6837615 0.4622479\n## 9        AA     VC 4.697388 4.3469667 5.047810  0.2128842 0.2300455 0.1981057\n## 10       AB     VC 3.039986 2.6895643 3.390408  0.3289489 0.3718074 0.2949498\n## 11       AC     VC 4.125253 3.7748312 4.475675  0.2424094 0.2649125 0.2234300\n## 12       AD     VC 3.345664 2.9952422 3.696086  0.2988943 0.3338628 0.2705565\n# Variable de combinaciones de factores\nnewdata <- newdata %>% mutate(grupo = paste(antidoto,veneno))\n# Gráfico\nggplot(newdata, aes(x = fct_reorder(grupo,fit), y = fit, colour = veneno)) + \n    geom_errorbar(aes(ymin = lwr, ymax = upr), width=.1) +\n    geom_line() +\n    geom_point() +\n    coord_flip() +\n    labs(y = \"Tiempo\", x = \"Antidoto-Veneno\")"},{"path":"anova.html","id":"ejercicios-1","chapter":"Unidad 8 Modelos ANOVA","heading":"8.8 Ejercicios","text":"Ejercicio 1. Se realiza una investigación para conocer los niveles de fosfato inorgánico en plasma (mg / dl) una hora después de una prueba de tolerancia la glucosa estándar para sujetos obesos, con o sin hiperinsulinemia, y controles. Los datos corresponden con la tabla 6.18 de Dobson (2002).Ejercicio 2. Se lleva cabo un estudio sobre el contenido promedio de grasa (en porcentaje) en la leche del ganado de cinco razas distintas canadienses. Para ello se consideran veinte ejemplares de pura raza (diez de dos años y diez maduras de más de cuatro años de cada una de cinco razas.Ejercicio 3. Los datos que se presentan continuación [@Dobson02] corresponden un estudio en el que semillas genéticamente iguales son asignadas aleatoriamente, bien un entorno enriquecido nutricionalmente (tratamiento), bien condiciones estándar (control). Una vez han crecido todas las plantas, se recolectan, secan y pesan. El interés es investigar el efecto del tratamiento utilizado sobre le peso seco (en gramos) de las plantas en cuestión.Ejercicio 4. Se realiza un estudio experimental para estudiar el rendimiento de un proceso químico en función de la concentración del compuesto base (, B, C y D), el catalizador usado (C1, C2, y C3), y la temperatura usada en el proceso (T1, T2). Se quiere estudiar como afectan estos factores en el rendimiento del proceso.Ejercicio 5. Se quiere evaluar la eficacia de distintas dosis de un fármaco contra la hipertensión arterial, comparándola con la de una dieta sin sal. Para ello se seleccionan al azar 25 hipertensos y se distribuyen aleatoriamente en 5 grupos. Al primero de ellos se le suministra ningún tratamiento (T1), al segundo una dieta con un contenido pobre en sal (T2), al tercero una dieta sin sal (T3), al cuarto el fármaco una dosis determinada (T4) y al quinto el mismo fármaco otra dosis (T5). Las presiones arteriales (Presion) sistólicas de los 25 sujetos al finalizar los tratamientos son:Ejercicio 6. La convección es una forma de transferencia de calor por los fluidos debido sus variaciones de densidad por la temperatura; las partes calientes ascienden y las frías descienden formando las corrientes de convección que hacen uniforme la temperatura del fluido. Se ha realizado un experimento para determinar las modificaciones de la densidad de fluido al elevar la temperatura en una determinada zona. Los resultados obtenidos han sido los siguientes:Ejercicio 7. Un laboratorio de reciclaje controla la calidad de los plásticos utilizados en bolsas. Se desea contrastar si existe variabilidad en la calidad de los plásticos que hay en el mercado. Para ello, se eligen al azar cuatro plásticos y se les somete una prueba para medir el grado de resistencia la degradación ambiental. De cada plástico elegido se han seleccionado ocho muestras y los resultados de la variable que mide la resistencia son los de la tabla adjunta.Ejercicio 8. Se realiza un estudio sobre el efecto que produce la descarga de aguas residuales de un planta sobre la ecología del agua natural de un río. En el estudio se utilizaron dos lugares de muestreo. Un lugar está aguas arriba del punto en el que la planta introduce aguas residuales en la corriente; el otro está aguas abajo. Se tomaron muestras durante un periodo de cuatro semanas y se obtuvieron los datos sobre el número de diatomeas halladas. Los datos se muestran en la tabla adjunta:Ejercicio 9. (Estos datos corresponden modelos ANOVA) Un fabricante de ropa que suministra uniformes militares debe cortar chaquetas, camisas, pantalones (variable Prenda) y otros complementos (en muchas tallas diferentes), de rollos de tela. La tela es cara, de modo que el desperdicio (Desperdicio) tiene un efecto muy grande en los beneficios. El fabricante tiene que elegir entre tres máquinas (Maquina) cortadoras asistidas por computadora: , B y C. El fabricante decide experimentar haciendo que cada máquina corte varios lotes de chaquetas, varios más de camisas otros más de pantalones y complementos para determinar que máquina es más eficiente en cada caso, es decir, tratamos de conocer el desperdicio que se producirá para cada prenda y máquina.","code":"\n# Lectura de datos\nejer01 <- read_csv(\"https://goo.gl/3L4EtK\", col_types = \"cd\")\n# Lectura de datos\nejer02 <- read_csv(\"https://goo.gl/J2ZKWK\", col_types = \"dcc\")\npeso <- c(4.17, 5.58, 5.18, 6.11, 4.50, 4.61, 5.17, 4.53, 5.33, 5.14, 4.81, 4.17, \n          4.41, 3.59, 5.87, 3.83, 6.03, 4.89, 4.32, 4.69)\nentorno <- c(rep(\"tratamiento\",10),rep(\"control\",10))\nejercicio03 <- data.frame(peso,entorno)\nejercicio04 <- read_csv(\"https://bit.ly/2GhFsl7\", col_types = \"dccc\")\nejercicio04 <- ejercicio04 %>% \n   mutate_if(sapply(ejercicio04,is.character),as.factor)\npresion <- c(180, 173, 175, 182, 181, 172, 158, 167, 160,\n             175, 163, 170, 158, 162, 170, 158, 146, 160,\n             171, 155, 147, 152, 143, 155, 160)\ntratamiento <- c(rep(\"T1\",5),rep(\"T2\",5),rep(\"T3\",5),\n                 rep(\"T4\",5),rep(\"T5\",5))\nejercicio05 <- data.frame(presion, tratamiento)\ndensidad <- c(21.8, 21.9, 21.7, 21.6, 21.7, 21.7, 21.4, 21.5, 21.4,\n              21.9, 21.8, 21.8, 21.6, 21.5, 21.9, 22.1, 21.85, 21.9)\ntemperatura <- c(rep(\"T100\",5),rep(\"T125\",4),rep(\"T150\",5),\n                 rep(\"T175\",4))\nejercicio06 <- data.frame(densidad, temperatura)\nresistencia <- c(135, 175, 97, 169, 213, 171, 115, 143,\n                 275, 170, 154, 133, 219, 187, 220, 185,\n                 169, 239, 184, 222, 253, 179, 280, 193,\n                 115, 105, 93, 85, 120, 74, 87, 63)\nplastico <- c(rep(\"PA\",8),rep(\"PB\",8),rep(\"PC\",8),\n                 rep(\"PD\",8))\nejercicio07 <- data.frame(resistencia, plastico)\ndiatomeas <- c(78, 94, 43, 58, 620, 760, 420, 913, 204, 333, 98, 89,\n               890, 655, 763, 562, 79, 87, 145, 522, 546, 652, 76, 94,\n               45, 69, 59, 62, 254, 86, 789, 267)\nsemana <- c(rep(\"S1\",4), rep(\"S2\",4), rep(\"S3\",4), rep(\"S4\",4),\n            rep(\"S1\",4), rep(\"S2\",4), rep(\"S3\",4), rep(\"S4\",4))\nlugar <- c(rep(\"Aguas arriba\",16), rep(\"Aguas abajo\",16))\nejercicio08 <- data.frame(diatomeas, semana, lugar)\n# Lectura de datos\nejer09 <- read_csv(\"https://bit.ly/2GcVn3R\", col_types = \"ccd\")\nejer09 <- ejer09 %>% \n  mutate_if(sapply(ejer09,is.character),as.factor)"},{"path":"ancova.html","id":"ancova","chapter":"Unidad 9 Modelos ANCOVA","heading":"Unidad 9 Modelos ANCOVA","text":"En este tema se presentan los modelos ANCOVA o modelos de análisis de la covarianza. Esto modelos surgen cuando entre las posibles variables predictoras de la respuesta (de tipo numérico) consideramos tanto variables numéricas como factores. El objetivo principal de este tipo de modelos es estudiar si la relación entre las predictoras numéricas y la respuesta viene condicionada por el factor o factores de clasificación considerados, es decir, si debemos construir:un único modelo entre la respuesta y las predictoras de tipo numérico,un único modelo entre la respuesta y las predictoras de tipo categórico (factores),un modelo diferente entre la respuesta y las predictoras numéricas para cada nivel o combinaciones de niveles de los factores.Este tipo de modelos permiten una versatilidad que nos posibilita el estudio de situaciones experimentales más complejas. Sin embargo, están exentos de dificultades sobre todo en lo que tiene que ver con el cumplimiento de las hipótesis del modelo. En función del modelo final las hipótesis de normalidad y homogeneidad varían en su aplicación. Además, el proceso de selección del mejor modelo requiere de un proceso de análisis más profundo debido la inclusión de diferentes tipos de variables en el conjunto de posibles predictoras.Para introducir los conceptos básicos de este tipo de modelos y mostrar todas sus posibilidades de análisis comenzaremos con el modelo ANCOVA más sencillo donde únicamente consideramos dos variables predictoras, una de tipo numérico y la otra un factor. La formulación presentada se puede generalizar rápidamente situaciones más complejas donde el número de predictoras sea mayor.","code":""},{"path":"ancova.html","id":"bancos-de-datos-2","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.1 Bancos de datos","text":"Veamos los diferentes bancos de datos que iremos analizando lo largo de la unidad.Ejemplo 1. Datos de tiempo de vida. Se desea estudiar el tiempo de vida de una pieza (vida) cortadora de dos tipos, y B (herramienta), en función de la velocidad del torno (velocidad) en el que está integrada (en revoluciones por segundo). El objetivo del análisis es describir la relación entre el tiempo de vida de la pieza y la velocidad del torno, teniendo en cuenta de qué tipo es la pieza.Cargamos los datos y realizamos el gráfico descriptivo:la vista de la Figura anterior se puede apreciar que el comportamiento del tiempo de vida con respecto la velocidad disminuye al aumentar esta última, y además ese descenso es distinto en función de la herramienta utilizada. la misma velocidad la herramienta proporciona un tiempo de vida inferior que la herramienta B. Para ambas máquinas se aprecia una tendencia lineal bastante clara que deberá ser objeto de estudio. Esto implicaría que nuestro modelo se desdoblaría en dos rectas de regresión lineal simple en función de la herramienta utilizada.Ejemplo 2. Datos de longevidad. Partridge y Farquhar realizan un experimento para relacionar la vida útil (longevidad) de las moscas de la fruta con su actividad sexual (actividad). La información recogida es la longevidad en días de 125 moscas macho, divididas en cinco grupos bajo diferentes condiciones ambientales para medir su actividad sexual. Asimismo, se recoge la longitud del tórax (thorax) ya que se sospecha que afecta directamente la longevidad de las moscas.Cargamos los datos y realizamos el gráfico descriptivo:Se puede ver como la longevidad aumenta cuando aumenta la longitud del thorax pero ese crecimiento parece distinto según actividad, dado que las nubes de puntos están bastante mezcladas. En este caso parece adecuado un modelo lineal para cada grupo de actividad.","code":"\n# Carga de datos\nvelocidad <- c(610, 950, 720, 840, 980, 530, 680, 540, 980, 730, 670, \n             770, 880, 1000, 760, 590, 910, 650, 810, 500)\nvida <- c(18.73, 14.52, 17.43, 14.54, 13.44, 25.39, 13.34, 22.71, \n          12.68, 19.32, 30.16, 27.09, 25.40, 26.05, 33.49, 35.62, \n          26.07, 36.78, 34.95, 43.67)\nherramienta <- gl(2, 10, 20, labels=c(\"A\", \"B\"))\ntiempovida <- data.frame(velocidad, vida, herramienta)\n# Gráfico\nggplot(tiempovida,  aes(x = velocidad,  y = vida,  color = herramienta)) + \n  geom_point() \n# Carga de datos\nthorax <- c(0.68, 0.68, 0.72, 0.72, 0.76, 0.76, \n            0.76, 0.76, 0.76, 0.8, 0.8, 0.8, 0.84, 0.84, \n            0.84, 0.84, 0.84, 0.84, 0.88, 0.88, 0.92, 0.92, \n            0.92, 0.94, 0.64, 0.7, 0.72, 0.72, 0.72, 0.76, \n            0.78, 0.8, 0.84, 0.84, 0.84, 0.84, 0.84, 0.88, \n            0.88, 0.88, 0.88, 0.88, 0.92, 0.92, 0.92, 0.92, \n            0.92, 0.92, 0.94, 0.64, 0.68, 0.72, 0.76, 0.76, \n            0.8, 0.8, 0.8, 0.82, 0.82, 0.84, 0.84, 0.84, 0.84, \n            0.84, 0.84, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, \n            0.88, 0.92, 0.92, 0.68, 0.68, 0.72, 0.76, 0.78, \n            0.8, 0.8, 0.8, 0.84,  0.84, 0.84, 0.84, 0.84, \n            0.84, 0.88, 0.88, 0.88, 0.9, 0.9, 0.9, 0.9, 0.9, \n            0.9, 0.92, 0.92, 0.64, 0.64, 0.68, 0.72, 0.72, \n            0.74, 0.76, 0.76, 0.76, 0.78, 0.8, 0.8, 0.82, \n            0.82, 0.84, 0.84, 0.84, 0.84, 0.88, 0.88, 0.88, \n            0.88, 0.88, 0.88, 0.92)\nlongevidad <- c(37, 49,  46,  63,  39,  46,  56,  63,  65,  \n                56,  65,  70,  63,  65, 70,  77,  81,  86,  \n                70,  70,  77,  77,  81,  77,  40,  37,  44, \n                47,  47,  47,  68,  47,  54,  61,  71,  75,  \n                89,  58,  59,  62, 79,  96,  58,  62,  70,  \n                72,  75,  96,  75,  46,  42,  65,  46, 58,  \n                42,  48,  58,  50,  80,  63,  65,  70,  70,  \n                72,  97,  46, 56,  70,  70,  72,  76,  90,  \n                76,  92,  21,  40,  44,  54,  36, 40,  56,  \n                60,  48,  53,  60,  60,  65,  68,  60,  81,  \n                81,  48, 48,  56,  68,  75,  81,  48,  68,  \n                16,  19,  19,  32,  33,  33, 30,  42,  42,  \n                33,  26,  30,  40,  54,  34,  34,  47,  47, \n                42, 47,  54,  54,  56,  60,  44)\nactividad <- c(rep(\"G1\",24),rep(\"G2\",25),rep(\"G3\",25),rep(\"G4\",25),rep(\"G5\",25))\nlongevidad <- data.frame(thorax,longevidad,actividad)\n# Gráfico\nggplot(longevidad, aes(x = thorax, y = longevidad, color = actividad)) + \n  geom_point() "},{"path":"ancova.html","id":"modelo-ancova-básico","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.2 Modelo ANCOVA básico","text":"Consideramos el modelo ANCOVA más sencillo donde consideramos dos variables predictoras: una numérica y otra un factor. Consideramos una muestra de tamaño \\(n\\) donde tenemos:Una variable respuesta, \\(Y\\), de tipo numérico con observaciones \\(y_1,...,y_n\\).Una variable predictora, \\(X\\), de tipo numérico con observaciones \\(x_1,...,x_n\\).Una variable predictora, \\(F\\), de tipo categórico con \\(\\) grupos o niveles distintos de tamaños muestrales \\(n_1,n_2,...,n_I\\), de forma que \\(n = n_1 + n_2 + ... + n_I\\), de forma que el vector de observaciones de la respuesta y de la predictora numérica se pueden escribir como:\\[(Y_1, Y_2,...,Y_I) = y_{11},\\ldots,y_{1n_1},y_{21},\\ldots,y_{2n_2},\\ldots,\ny_{I1},\\ldots,y_{In_I}\\]\\[\n(X_1, X_2,...,X_I) = x_{11},\\ldots,x_{1n_1},x_{21},\\ldots,x_{2n_2},\\ldots,x_{I1},\\ldots,x_{In_I}\n\\]donde el primer subíndice indica el nivel del factor y el segundo la posición dentro del conjunto de datos de dicho nivel del factor.\n* Conjunto \\(\\mu_i\\) de medias de todas las observaciones de la respuesta asociadas con el nivel \\(\\) del factor, es decir:\\[\n\\mu_i = \\frac{\\sum_{j = 1}^{n_j} y_{ij}}{n_i}; \\text{ = 1, 2,..., }\n\\]Media global de la respuesta, \\(\\mu\\), que se puede obtener como:\\[\n\\mu = \\frac{\\sum_{j = 1}^{} \\mu_{j}}{}\n\\]Incrementos, \\(\\alpha_i\\), de cada una de las medias de cada grupo con respecto la media global, es decir:\\[\n\\alpha_i= \\mu - \\mu_i; \\text{ = 1, 2,..., }\n\\]Pendiente común, \\(\\beta\\), que representa la posible relación entre las variables de tipo numérico.Pendiente común, \\(\\beta\\), que representa la posible relación entre las variables de tipo numérico.Pendientes diferentes entre las predictoras numéricas asociadas cada nivel del factor, \\(\\gamma_i\\) con \\(= 1,...,\\).Pendientes diferentes entre las predictoras numéricas asociadas cada nivel del factor, \\(\\gamma_i\\) con \\(= 1,...,\\).En esta situación el modelo que describe la posible relación entre respuesta y predictoras se puede escribir como:\n\\[\\begin{equation}\ny_{ij} = \\alpha_0 + \\alpha_i + \\beta x_{ij} + \\gamma_i x_{ij} + \\epsilon_{ij};\\quad  =1,...,\\quad \\text{con} \\quad \\alpha_I = 0\n\\tag{9.1}\n\\end{equation}\\]que en forma matricial se puede escribir fácilmente (de forma análoga al ANOVA de un vía) sin más que considerar \\(1_(n_i )={1,...,1}\\) un vector de \\(n_i\\) unos, \\(0_(n_i )={0,...,0}\\) un vector de \\(n_i\\) ceros, para cada uno de los niveles del factor la matriz de diseño viene dada por:\n\\[\nY = \\left( \n\\begin{array}{c}\n   Y_1 \\\\\n   Y_2 \\\\\n   \\ldots \\\\\n   Y_I \\\\\n\\end{array}\n\\right)\n=\n\\left( \n\\begin{array}{ccccccccc}\n   1_{n_1} & 1_{n_1} & 0_{n_1} & \\ldots & 0_{n_1} & X_1 & X_1 & \\ldots & 0_{n_1}\\\\\n   1_{n_2} & 0_{n_2} & 1_{n_2} & \\ldots & 0_{n_2} & X_2 & 0_{n_2} & \\ldots & 0_{n_2}\\\\\n   \\ldots & \\ldots & \\ldots & \\ldots & \\ldots & \\ldots & \\ldots & \\ldots & 0_{n_3}\\\\\n   1_{n_I} & 0_{n_I} & 0_{n_I} &\\ldots & 1_{n_I} & X_I & 0_{n_I} & \\ldots & X_I\\\\\n\\end{array}\n\\right)\n\\left( \n\\begin{array}{c}\n   \\alpha_0 \\\\\n   \\alpha_1 \\\\\n  \\alpha_2 \\\\\n \\ldots \\\\\n   \\alpha_{I_1} \\\\\n   \\beta \\\\\n   \\gamma_1\\\\\n   \\ldots\\\\\n   \\gamma_I\\\\\n\\end{array}\n\\right)\n+\n\\left( \n\\begin{array}{c}\n   e_1 \\\\\n   e_2 \\\\\n   \\ldots \\\\\n   e_n \\\\\n\\end{array}\n\\right) = X\\beta + \\epsilon\n\\]donde las primeras \\(\\) columnas representan el efecto del factor (ANOVA de una vía), la siguiente columna representa el efecto común entre predictoras numéricas (Regresión lineal simple), y las últimas \\(\\) columnas representan el efecto distinto de la predictora numérica para cada nivel del factor. Estas columnas se obtienen fácilmente multiplicando la columna de \\(X\\) por cada de las columnas desde la \\(2\\) la \\(\\).Las posibles modelos anidados que se pueden obtener partir de la ecuación (9.1), así como sus interpretaciones se presentan continuación. Además, se ofrece la interpretación de dichos modelos en términos de los coeficientes que resulten significativos.","code":""},{"path":"ancova.html","id":"i-rectas-de-regresión-perpendiculares","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.2.1 I rectas de regresión perpendiculares","text":"Este modelo es el más general y considera una pendiente distinta para la predictora numérica en función de los niveles del factor, que resulta equivalente plantear el contraste\\[\\begin{equation}\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\gamma_1 = \\gamma_2 = \\ldots = \\gamma_I = 0\\\\\nH_a: & \\mbox{Al menos hay una pendiente distinto de cero}\\\\\n\\end{array}\n\\right.\n\\tag{9.2}\n\\end{equation}\\]Si rechazamos el contraste (9.2) tendremos un modelo de regresión entre la respuesta y la predictora numérica para cada nivel del factor, es decir, rectas de regresión distintas con ecuaciones:\\[\\begin{equation}\n\\begin{array}{ll}\ny_{1j} &= (\\alpha_0 + \\alpha_1) + (\\beta + \\gamma_1) x_{1j} + \\epsilon_{1j}\\\\\ny_{2j} &= (\\alpha_0 + \\alpha_2) + (\\beta + \\gamma_2) x_{2j} + \\epsilon_{2j}\\\\\n\\ldots &= \\ldots \\\\\ny_{Ij} &= (\\alpha_0 + \\alpha_I) + (\\beta + \\gamma_I) x_{Ij} + \\epsilon_{Ij}\\\\\n\\end{array}\n\\tag{9.3}\n\\end{equation}\\]con interceptaciones \\(\\alpha_0 + \\alpha_i\\) y pendientes \\(\\beta + \\gamma_i\\) para cada uno de los \\(\\) niveles del factor. Tenemos modelos de regresión distintos (uno por cada nivel del factor), es decir, tenemos rectas perpendiculares o que se cortan.Ejemplo. Para el banco de datos de tiempo de vida podemos representar gráficamente el modelo asumiendo dos rectas de regresión perpendiculares de vida versus velocidad del torno en función de la herramienta utilizada.Se puede ver como las rectas tienen interceptaciones distintas y pendientes algo diferentes. Las tendencias son perpendiculares en el sentido estricto sino que existe algún punto de corte entre ellas. La resolución del contraste nos indicara si el modelo es adecuado o si deberíamos optar por un modelo más sencillo.Ejemplo. Para el banco de datos de longevidad representamos gráficamente el modelo que asume 5 rectas de regresión perpendiculares de longevidad versus longitud del thorax en función del grupo de actividad.Se puede ver como las rectas tienen interceptaciones distintas y pendientes muy similares lo que puede ser un indicativo de que este modelo de rectas perpendiculares es adecuado.","code":""},{"path":"ancova.html","id":"i-rectas-de-regresión-paralelas","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.2.2 I rectas de regresión paralelas","text":"Si rechazamos el contraste (9.2) tendremos un único modelo de regresión entre la respuesta y la predictora pero con diferentes interceptaciones (la pendiente es la misma), es decir, rectas paralelas cuyas ecuaciones vienen dadas por:\\[\\begin{equation}\n\\begin{array}{ll}\ny_{1j} &= (\\alpha_0 + \\alpha_1) + \\beta  x_{1j} + \\epsilon_{1j}\\\\\ny_{2j} &= (\\alpha_0 + \\alpha_2) + \\beta  x_{2j} + \\epsilon_{2j}\\\\\n\\ldots &= \\ldots \\\\\ny_{Ij} &= (\\alpha_0 + \\alpha_I) + \\beta  x_{Ij} + \\epsilon_{Ij}\\\\\n\\end{array}\n\\tag{9.4}\n\\end{equation}\\]Ejemplo. Modelo de rectas paralelas para el banco de datos de tiempo de vida. E¿Se diferencia mucho este gráfico del de rectas perpendiculares visto antes?Ejemplo. Modelos de rectas paralelas para el banco de datos de longevidad.En este caso el gráfico es prácticamente idéntico indicando que para estos datos el modelo de rectas paralelas tiene la misma capacidad explicativa que el de rectas perpendiculares.En la situación donde el contraste (9.2) es significativo podemos definir diferentes modelos anidados en función de los incrementos del factor (`s) y la pendiente \\(\\beta\\). Los contrastes son:\\[\\begin{equation}\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_I = 0\\\\\nH_a: & \\mbox{Al menos hay un incremento distinto de cero}\\\\\n\\end{array}\n\\right.\n\\tag{9.5}\n\\end{equation}\\]\ny\n\\[\\begin{equation}\n\\left\\{ \n\\begin{array}{ll}\nH_0: & \\beta = 0\\\\\nH_a: & \\beta \\neq 0\n\\end{array}\n\\right.\n\\tag{9.6}\n\\end{equation}\\]La resolución de estos contrastes nos lleva nuevos modelos que pasamos describir.","code":""},{"path":"ancova.html","id":"una-recta-de-regresión","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.2.3 Una recta de regresión","text":"Si rechazamos la hipótesis nula de (9.5) pero si rechazamos la hipótesis nula de (9.6) diríamos que hay efecto del factor pero si que podemos establecer un modelo de regresión entre la respuesta y la predictora numérica (Modelo de Regresión Lineal Simple).Ejemplo. Modelo de recta de regresión para el banco de datos de tiempo de vida.¿Qué podemos decir de este modelo frente los anteriores?Ejemplo. Modelo de recta de regresión para el banco de datos de longevidad.¿Podríamos considerar este modelo como válido frente los considerados antes? Recuerda que siempre buscamos modelos con capacidades explicativas similares pero con menos efectos o variables.","code":""},{"path":"ancova.html","id":"anova-de-una-vía","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.2.4 ANOVA de una vía","text":"Si rechazamos la hipótesis nula de (9.5) pero rechazamos la hipótesis nula de (9.6) diríamos que hay efecto de la predictora numérica pero que si podemos establecer diferencias entre las medias de la respuesta dadas por los diferentes niveles del factor. Nos encontramos ante un modelo ANOVA de una vía. En esta situación asumimos que existe rectas de regresión que relacionen el comportamiento de la respuesta con la predictora numérica, es decir, ya tendremos una nube de puntos sino datos agrupados por el factor.Ejemplo. Modelo ANOVA de una vía para el banco de datos de tiempo de vida.Se observa claramente como la herramienta de tipo B produce un tiempo de vida superior al del tipo . Al menos el efecto del factor parece relevante para explicar el comportamiento del tiempo de vida.Ejemplo. Modelo ANOVA de una vía para el banco de datos de longevidad.El gráfico de cajas asociado con el factor muestra que los primeros cuatro grupos tienen un comportamiento muy similar, y tan sólo el grupo cinco parecer tener una longevidad inferior al resto.","code":""},{"path":"ancova.html","id":"modelos-sin-efectos","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.2.5 Modelos sin efectos","text":"Si rechazamos la hipótesis nula de (9.5) y rechazamos la hipótesis nula de (9.6) estaríamos ante un modelo nulo donde el comportamiento de la respuesta viene explicado por las predictoras consideradas.","code":""},{"path":"ancova.html","id":"especificación-del-modelo-en-r","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.2.6 Especificación del modelo en R","text":"El modelo ANCOVA planteado para un factor \\(F\\) y una variable predictora numérica \\(X\\), se puede escribir en R en su formato reducido como:\\[Y \\sim F + X + F:X\\]donde:\\(F\\) representa el efecto del factor, es decir, comparamos si las medias de la respuesta para cada grupo pueden considerarse iguales.\\(X\\) representa el efecto de regresión asociado con la variable numérica, es decir, la respuesta y X están relacionadas mediante una única pendiente que deberemos estimar.\\(F:X\\) representa el efecto de interacción entre predictoras, es decir, que la respuesta se relaciona con la predictora numérica mediante tantas curvas (generalmente líneas) como niveles tenga el factor \\(F\\).continuación, se presentan los modelos reducidos para diferentes situaciones experimentales en el número y tipo de variables predictoras:Modelo para dos factores (\\(F_1\\) y \\(F_2\\)) y una numérica (\\(X\\))\\[Y \\sim F_1 + F_2 + F_1:F_2 + X + F_1:X + F_2:X + F_1:F_2:X\\]\no en forma más simplificada \\(Y \\sim F_1*F_2*X\\)Modelo para un factor (\\(F\\)) y dos numéricas (\\(X_1\\) y \\(X_2\\))\\[Y \\sim F + X_1 + X_2 + F:X_1 + F:X_2\\]\no en forma más simplificada \\(Y \\sim F*(X_1 + X_2)\\)Modelo para dos factores (\\(F_1\\) y \\(F_2\\)) y dos numéricas (\\(X_1\\) y \\(X_2\\))\\[Y \\sim F_1*F_2*(X_1 + X_2)\\]Como se puede ver la complejidad del modelo aumenta sustancialmente con la consideración de más variables predictoras. La forma de expresar el modelo saturado debe contemplar tanto los efectos principales asociados cada predictora, como los efectos de interacción entre factores y entre factores y numéricas.","code":""},{"path":"ancova.html","id":"análisis-preliminar","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.3 Análisis preliminar","text":"En este tipo de modelos el análisis preliminar pasa por una representación de los diferentes modelos que pueden surgir desde el modelo saturado. En este caso se muestra como especificar los diferentes modelos y como obtener los gráficos correspondientes.Ejemplo. (Tiempo de vida). El objetivo del análisis es describir la relación entre el tiempo de vida de la pieza y la velocidad del torno, teniendo en cuenta de qué tipo es la pieza. Tenemos entonces los posibles modelos que involucran la velocidad:\\[\n\\left\\{ \n\\begin{array}{ll}\n\\mbox{M0}:& Vida \\sim velocidad\\\\\n\\mbox{M1}:& Vida \\sim velocidad + herramienta\\\\\n\\mbox{M2}:& Vida \\sim velocidad + herramienta + velocidad:herramienta\\\\\n\\end{array}\n\\right.\n\\]continuación se presenta el código para poder representar todos estos modleos en un único gráfico.También representamos el modelo donde únicamente tenemos el factor (diagrama de cajas):\\[\n\\begin{array}{ll}\n\\mbox{M}:& Vida \\sim herramienta\\\\\n\\end{array}\n\\]Ejemplo. Planterasmos los diferentes modelos para los datos de longevidad y los representamos gráficamente.\\[\n\\left\\{ \n\\begin{array}{ll}\n\\mbox{M0}:& longevidad \\sim thorax\\\\\n\\mbox{M1}:& longevidad \\sim thorax + actividad\\\\\n\\mbox{M2}:& longevidad \\sim thorax + herramienta + thorax:actividad\\\\\n\\end{array}\n\\right.\n\\]También representamos el modelo donde únicamente tenemos el factor (diagrama de cajas):\\[\n\\begin{array}{ll}\n\\mbox{M}:& longevidad \\sim actividad\\\\\n\\end{array}\n\\]","code":"\n# Comenzamos con el modelo más sencillo\n\n# Modelo con una única recta\nM0 <- lm(vida ~ velocidad, data = tiempovida)\n\n# M1: modelo con rectas paralelas\nM1 <- lm(vida ~ herramienta + velocidad, data = tiempovida)\n\n# M2: modelo con rectas no paralelas\nM2 <- lm(vida ~ herramienta + velocidad + herramienta:velocidad, data = tiempovida)\n\n# grid de valores para construir los modelos\ngrid <- tiempovida %>% data_grid(herramienta, velocidad) %>% \n   gather_predictions(M0, M1, M2)\n\n# Gráfico\nggplot(tiempovida,aes(velocidad, vida, colour = herramienta)) + \n  geom_point() + \n  geom_line(data = grid, aes(y = pred)) +\n  facet_wrap(~ model) +\n  labs(x = \"Velocidad del torno\", y = \"Tiempo de vida\") \nggplot(tiempovida, aes(x = herramienta, y = vida)) + \n   geom_boxplot() \n# Comenzamos con el modelo más sencillo\n\n# Modelo con una única recta\nM0 <- lm(longevidad ~ thorax, data = longevidad)\n\n# M1: modelo con rectas paralelas\nM1 <- lm(longevidad ~ actividad + thorax, data = longevidad)\n\n# M2: modelo con rectas no paralelas\nM2 <- lm(longevidad ~ actividad + thorax + actividad:thorax, data = longevidad)\n\n# grid de valores para construir los modelos\ngrid <- longevidad %>% data_grid(actividad, thorax) %>% \n   gather_predictions(M0, M1, M2)\n\n# Gráfico\nggplot(longevidad,aes(thorax, longevidad, colour = actividad)) + \n  geom_point() + \n  geom_line(data = grid, aes(y = pred)) +\n  facet_wrap(~ model) +\n  labs(x = \"Longitud del tórax\", y = \"Longevidad\") \nggplot(longevidad, aes(x = actividad, y = longevidad)) + \n   geom_boxplot() "},{"path":"ancova.html","id":"ajuste-y-selección-del-modelo","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.4 Ajuste y Selección del modelo","text":"Las hipótesis del modelo ANCOVA son que los errores se distribuyen de forma independiente mediante una distribución Normal de media cero y varianza constante σ^2 para cada uno de los grupos que determina la variable predictora de tipo factor. Estas hipótesis se adaptarán en función del tipo de modelo que finalmente alcancemos en el proceso de selección (rectas, una recta, o un ANOVA de una vía).Dado que hemos expresado el modelo ANCOVA como un modelo de tipo lineal con una ecuación similar los modelos de regresión múltiple, la estimación de los parámetros del modelo se puede realizar utilizando las ecuaciones normales. En el proceso de selección del mejor modelo actuaremos como en los modelos ANOVA, es decir partiremos del modelo saturado y veremos que efectos puede ser considerados como irrelevantes, y por tanto deben desaparecer del modelo. Esta selección nos permitirá elegir el modelo final resultante. En este caso más sencillo podemos escribir todos los modelos posibles y elegir el mejor de ellos, bien mediante la comparación con el test F o con el AIC, pero en la práctica se recurre los procedimientos secuenciales automáticos que son los que presentaremos aquí.Ejemplo. Realizamos la selección del mejor modleo apra el conjunto de datos de tiempo de vida. Para ello construimos el modelo saturado y utilizamos el procedimiento automático basado en el test F parcial.El proceso de selección identifica el efecto de interacción entre velocidad y herramienta como significativo, de forma que el modelo final viene dado por:\n\\[vida \\sim velocidad + herramienta\\]\nAjustamos el modelo con rectas paralelas, es decir, la pendiente de la recta entre el tiempo de vida y la velocidad stiene la misma pendiente, variando únicamenyte la interceptación en función del tipo de herramienta utilizada. Tenemos por tanto un modelo con dos rectas paralelas (una por cada tipo de herramienta).Las ecuaciones de estimación para este modelo vienen dadas por:\\[\n\\left\\{ \n\\begin{array}{lll}\n\\mbox{Herramienta }:& \\widehat{Vida_{}} = 36.93 + 0  - 0.03*velocidad &= 36.93 -0.03*velocidad\\\\\n\\mbox{Herramienta B}:& \\widehat{Vida_{B}} = 36.93 + 14.67 - 0.03*velocidad &= 51.60 - 0.03*velocidad \\\\\n\\end{array}\n\\right.\n\\]Tenemos una interceptación mayor para la herramienta B indicando que la recta asociada con dicha herramienta está por encima de la de la herramienta , lo que en térinos prácticos nos indica que el tiempo de vida con la herramienta B siempre será superior al del tipo para cualquier velocidad considerada. Además, la pendiente negativa asociada con la velocidad indica que conforme aumenta esta disminuye el tiempo de vida.Ejemplo. Analizamos ahora los datos de longevidad. Construimos el modelo saturado y seleccionamos mediante el test \\(F\\).El proceso de selección identifica el efecto de interacción entre thorax y actividad como significativo, de forma que el modelo final viene dado por:\n\\[longevidad \\sim thorax + actividad\\]\nAjustamos el modelo y estudiamos los parámetros obtenidos:Todas las rectas tienen la misma pendiente (coeficiente asociado tórax) pero con distinto punto de origen debido al tratamiento al que son sometidos los sujetos. La pendiente positiva indica que la longevidad aumenta cuando lo hace la longitud del tórax, mientras que podemos ver que el grupo G1 es el de mayor longevidad por tener la interceptación más grande. Las ecuaciones para cada uno de los grupos viene dada por:\\[\n\\left\\{ \n\\begin{array}{ll}\n\\mbox{G1}:& \\widehat{logevidad_{G1}} = - 44.61 + 134.34*thorax\\\\\n\\mbox{G2}:& \\widehat{logevidad_{G2}} = - 48.75 + 134.34*thorax \\\\\n\\mbox{G3}:& \\widehat{logevidad_{G3}} = - 46.11 + 134.34*thorax \\\\\n\\mbox{G4}:& \\widehat{logevidad_{G4}} = - 55.76 + 134.34*thorax \\\\\n\\mbox{G5}:& \\widehat{logevidad_{G5}} = - 68.75 + 134.34*thorax \\\\\n\\end{array}\n\\right.\n\\]El grupo con mayor longevidad es el G1, ya que tiene la interceptación más grande, mientras que el que tiene menor longevidad es G5. El orden vendría dado por \\(G1 > G3 > G2 > G4 > G5\\).","code":"\n# Modelo saturado\nfit.vida <- lm(vida ~ velocidad * herramienta, data = tiempovida)\n# Selección del modelo\nols_step_backward_p(fit.vida, prem = 0.05)## \n## \n##                                  Elimination Summary                                  \n## -------------------------------------------------------------------------------------\n##         Variable                               Adj.                                      \n## Step           Removed           R-Square    R-Square     C(p)       AIC        RMSE     \n## -------------------------------------------------------------------------------------\n##    1    velocidad:herramienta      0.8969      0.8847    3.9652    106.6591    3.0919    \n## -------------------------------------------------------------------------------------\n# Modelo saturado\nfit.vida <- lm(vida ~ velocidad + herramienta, data = tiempovida)\n# Parámetros estimados\ntab_model(fit.vida,\n          show.r2 = FALSE, \n          show.p = FALSE)\n# Modelos\nfit.longevidad <- lm(longevidad ~ thorax * actividad, data = longevidad)\n# Selección del modelo\nols_step_backward_p(fit.longevidad, prem = 0.05)## \n## \n##                                Elimination Summary                                 \n## ----------------------------------------------------------------------------------\n##         Variable                          Adj.                                        \n## Step        Removed         R-Square    R-Square     C(p)        AIC        RMSE      \n## ----------------------------------------------------------------------------------\n##    1    thorax:actividad      0.6527       0.638    -3.7881    943.8165    10.5394    \n## ----------------------------------------------------------------------------------\n# Modelos\nfit.longevidad <- lm(longevidad ~ thorax + actividad, data = longevidad)\n# Parámetros estimados\ntab_model(fit.longevidad,\n          show.r2 = FALSE, \n          show.p = FALSE)"},{"path":"ancova.html","id":"diagnóstico-del-modelo","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.5 Diagnóstico del modelo","text":"En este caso el diagnóstico es similar al de los modelos de regresión pero teniendo en cuenta que las hipótesis se deben verificar para los residuos asociados cada nivel del factor (si este está presente en el modelo). Las hipótesis son linealidad, normalidad y varianza constante. Para verificar la hipótesis de linealidad utilizamos el gráfico de residuos vs ajustados y residuos vs predictora numérica, mientras que usamos los tests de normalidad y homogeneidad para el resto de hipótesis. Utilizamos los mismos ejemplos de los puntos anteriores para mostrar como realizar el diagnóstico en este tipo de modelos.Ejemplo continuación, se presenta el diagnóstico para el modelo de tiempos de vida. Para realizar el diagnóstico partimos del modelo obtenido en la sección anterior.En la figura se observan problemas con los residuos, aunque sí se puede ver que para el tipo la variabilidad de los residuos aumenta cuando aumenta el valor ajustado, indicando posibles problemas con la homogeneidad de varianzas. Además, se observa ningún tipo de tendencia que pueda indicar falta de linealidad. Procedemos con los tests de hipótesis.Puesto que ambos tests resultan significativo se verifican las hipótesis del modelo, por lo que estamos en condiciones de afirmar que el modelo resultante es adecuado para explicar el comportamiento del tiempo de vida en función del tipo de pieza y de la velocidad considerada.La distancia de Cook muestra ninguna observación influyente (valor mayor que 1). Dado que se verifican las hipótesis el modelo obtenido parece adecuado para estudiar el tiempo de vida en función de la velocidad y la herramienta utilizada.Ejemplo. Analizamos ahora el modelo correspondiente los datos de longevidad. En primer lugar, realizamos el análisis gráfico de los residuos. En este caso hay interacción presente en el modelo y todos los gráficos deben identificar cada uno de los niveles de actividad.En la figura se puede observar que para algún nivel del factor los residuos aumentan o disminuyen en función del valor ajustado, por ejemplo para el grupo G3 (mayor variabilidad en el centro que en los extremos) indicando posibles problemas con la homogeneidad de varianzas. Pasamos valorar las hipótesis del modelo.Aunque se verifican las hipótesis del modelo y se detectan observaciones influyentes, si que es cierto que los gráficos de residuos muestran cierto comportamiento de embudo con variabilidades más pequeñas en valores más pequeños de thorax, y mayor dispersión al aumentar la longitud del thorax. Sería recomendable probar Box-Cox para tratar de obtener una transformación de la respuesta que nos permita obtener gráficos sin esos efectos indeseables.La transformación raíz cuadrada parece adecuada en esta situación. Obtenemos la nueva variable y ajustamos de nuevo el modelo.De nuevo el modelo seleccionado prescinde del efecto de interacción. Ajustamos y estudiamos el nuevo modelo.¿Cuáles son las ecuaciones de estimación en este caso?El proceso de diagnóstico para el nuevo modelo permite verificar el cumplimiento de las hipótesis y la leve mejora de los gráficos de residuos.","code":"\n# Valores de diagnóstico\ndiagnostico <- fortify(fit.vida)\n# Gráfico\nggplot(diagnostico,aes(x = velocidad, y = .stdresid, colour = herramienta)) + \n   geom_point() +\n   geom_hline(yintercept = 0, col = \"red\") +\n   facet_wrap(. ~ herramienta)\n# Tests de hipótesis\nols_test_normality(fit.vida)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9715         0.7858 \n## Kolmogorov-Smirnov        0.1232         0.8859 \n## Cramer-von Mises          1.4412          2e-04 \n## Anderson-Darling          0.2652         0.6555 \n## -----------------------------------------------\nleveneTest(.stdresid ~ herramienta, data = diagnostico)## Levene's Test for Homogeneity of Variance (center = median)\n##       Df F value Pr(>F)\n## group  1  1.3888  0.254\n##       18\n# Análisis de influencia\nols_plot_cooksd_chart(fit.vida)\n# Valores de diagnóstico\ndiagnostico <- fortify(fit.longevidad)\n# Gráfico\nggplot(diagnostico,aes(x = thorax, y = .stdresid, colour = actividad)) + \n   geom_point() +\n   geom_hline(yintercept = 0, col = \"red\")+\n   facet_wrap(. ~ actividad)\n# Tests de hipótesis\nols_test_normality(fit.longevidad)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9916         0.6607 \n## Kolmogorov-Smirnov        0.0538         0.8654 \n## Cramer-von Mises         10.2413         0.0000 \n## Anderson-Darling          0.3224         0.5241 \n## -----------------------------------------------\nleveneTest(.stdresid ~ actividad, data = diagnostico)## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(>F)\n## group   4  1.2925 0.2769\n##       119\n# Análisis de influencia\nols_plot_cooksd_chart(fit.longevidad)\nMASS::boxcox(fit.longevidad)\n# Transformación\nlongevidad <- longevidad %>% mutate(rlongevidad = sqrt(longevidad)) \n# Modelo saturado\nfit.longevidad <- lm(rlongevidad ~ thorax * actividad, data = longevidad)\n# Selección del modelo\nols_step_backward_p(fit.longevidad, prem = 0.05)## \n## \n##                                Elimination Summary                                \n## ---------------------------------------------------------------------------------\n##         Variable                          Adj.                                       \n## Step        Removed         R-Square    R-Square     C(p)        AIC        RMSE     \n## ---------------------------------------------------------------------------------\n##    1    thorax:actividad      0.6868      0.6736    -3.0694    267.2943    0.6888    \n## ---------------------------------------------------------------------------------\n# Modelos\nfit.longevidad <- lm(rlongevidad ~ thorax + actividad, data = longevidad)\n# Parámetros estimados\ntab_model(fit.longevidad,\n          show.r2 = FALSE, \n          show.p = FALSE)\n# Valores de diagnóstico\ndiagnostico <- fortify(fit.longevidad)\n# Gráfico\nggplot(diagnostico,aes(x = thorax, y = .stdresid, colour = actividad)) + \n   geom_point() +\n   geom_hline(yintercept = 0, col = \"red\")+\n   facet_wrap(. ~ actividad)\n# Tests de hipótesis\nols_test_normality(fit.longevidad)## -----------------------------------------------\n##        Test             Statistic       pvalue  \n## -----------------------------------------------\n## Shapiro-Wilk              0.9951         0.9465 \n## Kolmogorov-Smirnov        0.0474         0.9432 \n## Cramer-von Mises         10.7431         0.0000 \n## Anderson-Darling          0.2137         0.8484 \n## -----------------------------------------------\nleveneTest(.stdresid ~ actividad, data = diagnostico)## Levene's Test for Homogeneity of Variance (center = median)\n##        Df F value Pr(>F)\n## group   4  0.6856 0.6033\n##       119\n# Análisis de influencia\nols_plot_cooksd_chart(fit.longevidad)"},{"path":"ancova.html","id":"predicción-2","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.6 Predicción","text":"El proceso de predicción en este tipo de modelos es muy simple partir de las ecuaciones de los modelos obtenidos. De hecho, en las secciones anteriores ya hemos visto gráficamente la predicción para todos estos modelos en los ejemplos que hemos ido trabajando. Básicamente, si queremos obtener una predicción especifica deberemos dar un valor del factor y otro de la predictora numérica para calcular el valor de predicción y su correspondiente intervalo. En este caso nos imitamos representar las bandas de predicción que podemos obtener para cada modelo.Ejemplo. continuación, se presentan las rectas de predicción para el modelo ajustado en el banco de datos de tiempo de vida.En la figura se presentan los resultados obtenidos donde queda patente el comportamiento distinto para tipo de herramienta, mostrando que la de tipo B tiene un tiempo de vida superior, pero que se va reduciendo cuando aumentamos la velocidad.Ejemplo. continuación, se presentan las rectas de predicción para el modelo ajustado en el banco de datos de longevidad. En la figura se presentan los resultados obtenidos donde queda patente el comportamiento paralelo para cada tipo de actividad, mostrando que el grupo G1 es el que muestra una mayor longevidad que aumenta además con la longitud del tórax. Además, podemos ver como el único grupo que muestra un comportamiento distinto es el del grupo G5, mientras que los otros grupos muestran predicciones muy similares.Obtenemos ahora el gráfico de predicción en la escala original. Deshacemos el cambio de raíz cuadrada y representamos de nuevo las bandas de confianza.En la figura siguiente se presentan los resultados obtenidos donde se aprecia cierta curvatura en las bandas de predicción debida al cambio en la escala de la raíz cuadrada la escala original de la variable.","code":"\nplot_model(fit.vida, \"pred\", terms = c(\"velocidad\", \"herramienta\"),\n           title =\"Predicción de la media del tiempo de vida\")\nplot_model(fit.longevidad, \"pred\", terms = c(\"thorax\", \"actividad\"),\n           title =\"Predicción de la raíz cuadrada de la media de longevidad\")\n# Creamos grid de predicción\nnewdata <- data.frame(thorax = rep(seq(min(longevidad$thorax), \n                                       max(longevidad$thorax), \n                                       .01),\n                                   each=5), \nactividad = factor(c(\"G1\", \"G2\", \"G3\", \"G4\", \"G5\")))\n# Obtenemos la predicción para el modelo ajusatdo\nnewdata <- data.frame(newdata, \n             predict(fit.longevidad, newdata, interval=\"confidence\"))\n# Eliminamos la raíz cuadrada de las predicciones \nnewdata$fit <- newdata$fit^2\nnewdata$lwr <- newdata$lwr^2\nnewdata$upr <- newdata$upr^2\n# Gráfico de la predicción\nggplot(newdata, aes(x = thorax, y = fit, color = actividad)) +\n  geom_line() +\n  geom_ribbon(aes(ymax = upr, ymin = lwr, fill = actividad), alpha = 1/5) +\n  labs(x = \"Longitud del thorax\", y = \"Longevidad\", title = \"Bandas de predicción\")"},{"path":"ancova.html","id":"ejercicios-modelos-ancova","chapter":"Unidad 9 Modelos ANCOVA","heading":"9.7 Ejercicios modelos ANCOVA","text":"continuación se presenta la colección de ejercicios de esta unidad.Ejercicio 1. Disponemos de los datos de peso de 24 niños recién nacidos (peso), su sexo (sexo; “H” = Hombres y “M” = Mujeres) y la edad de sus madres (edad). Nos gustaría ser capaces de determinar un modelo que explique el peso de los niños recién nacidos en función de su sexo y de la edad de sus madres.Ejercicio 2. Se lleva cabo una investigación sobre diversas malformaciones del sistema nervioso central registradas en nacidos vivos en Gales del Sur, Reino Unido. El estudio fue diseñado para determinar el efecto de la dureza del agua sobre la incidencia de tales malformaciones. La información registrada son: NoCNS = recuento de nacimientos sin problema CNS; = conteo de nacimientos de Anencephalus; Sp = conteo de nacimientos de espina bífida; Otro = recuento de otros nacimientos del SNC; Agua = endurecimiento del agua; Trabajo = un factor con niveles Manual manual en función del tipo de trabajo realizado por los padres Se está interesado en predecir el número total de malformaciones en función de la calidad del agua y el trabajo realizado por los padres.Ejercicio 3. Se ha realizado un estudio para establecer la calidad de los vinos de la variedad Pino Noir en función de un conjunto de características analizadas. Las características analizadas son claridad, aroma, cuerpo, olor y matiz. Para medir la calidad se organiza una cata ciega un conjunto de expertos y se calcula la puntuación final de cada vino partir de la información de todos ellos. Además se registra la región (region) de procedencia del vino por si puede influir en la calidad del vino.Ejercicio 4. Una empresa recibe cargamentos de material para procesar en sus almacenes. El objetivo básico del estudio es determinar el tiempo de procesado de los cargamentos recibidos como función del tamaño del cargamento y el tipo de almacén.Ejercicio 5. Una empresa dedicada la fabricación de aislantes térmicos y acústicos establece un experimento que mide la pérdida de calor (Calor) través de cuatro tipos diferentes de cristal para ventanas (Cristal) utilizando cinco graduaciones diferentes de temperatura exterior (TempExt). Se prueban tres hojas de cristal en cada graduación de temperatura, y se registra la pérdida de calor para cada hoja.Ejercicio 6. El grupo de asesores LearnStatistics ha realizado un estudio para comprobar si las empresas destinan parte de los beneficios de sus ventas en la formación de sus empleados para mejorar su competitividad. Para ellos se recoge la información sobre ventas (Ventas) en miles de euros, capital invertido en formación (Capital) en miles de euros, y el nivel de productividad de la empresa establecido por un asesor externo (Productividad).","code":"\n# Lectura de datos\nedad <- c(40, 38, 40, 35, 36, 37, 41, 40, 37, 38, 40, 38,\n          40, 36, 40, 38, 42, 39, 40, 37, 36, 38, 39, 40)\npeso <- c(2968, 2795, 3163, 2925, 2625, 2847, 3292, 3473, \n          2628, 3176, 3421, 2975, 3317, 2729, 2935, 2754, \n          3210, 2817, 3126, 2539, 2412, 2991, 2875, 3231)\nsexo <- gl(2,12, labels=c(\"H\", \"M\"))\nejer01 <- data.frame(edad, peso, sexo)\n# Lectura de datos\nprevio <- read_csv(\"https://goo.gl/bNOSxt\", col_types = \"cdddddc\")\n# Calculamos el número total de malformaciones\nejer02 <- previo %>% mutate(CNS = An + Sp + Other)\n# Lectura de datos\nejer03 <- read_csv(\"https://goo.gl/OX9wgM\", col_types = \"ddddddc\")\n# Carga de datos \nejer04 <- read.table(\"https://goo.gl/kuMNpD\", header = TRUE)\nejer04 <- as_tibble(ejer04)\n# Lectura de datos\nejer05 <- read_csv(\"https://goo.gl/V6hyVW\", col_types = \"ddc\") \nejer05 <- ejer05 %>%\n  mutate_if(sapply(ejer05,is.character),as.factor) \n# Lectura de datos\nejer06 <- read_csv(\"https://bit.ly/2rCATaO\", col_types = \"dcd\")\nejer06 <- ejer06 %>%  \n  mutate_if(sapply(ejer06,is.character),as.factor) "},{"path":"smooth.html","id":"smooth","chapter":"Unidad 10 Modelos aditivos lineales","heading":"Unidad 10 Modelos aditivos lineales","text":"En esta unidad se presentan los modelos aditivos lineales. Esto modelos surgen cuando la relación entre la predictora y la respuesta (en el caso de variables numéricas) se puede escribir de forma lineal, sino más bien través de una función desconocida. En unidades anteriores utilizamos los modelos polinómicos para poder capturar comportamientos lineales entre predictora y respuesta, pero en este caso utilizaremos funciones de suavizado que permiten capturar todo tipo de comportamiento entre ambas.La mayor dificultad en este tipo de modelos es que tenemos una forma explícita para la función de suavizado, y por tanto es necesario utilizar las funciones específicas de predicción proporcionadas por la librería de ajuste para obtener el modelo resultante.En este tema sólo se pretende dar una versión introductoria de los modelos de suavizado por o que se recomienda la lectura de textos más avanzados para completar lo visto en esta unidad.El modelo aditivo más básico con una variable predictora y una respuesta Normal viene dado por:\\[Y = f(X) + \\epsilon\\] donde \\(f()\\) se denomina función suave o de suavizado para la variable \\(X\\).Las ventajas de este tipo de modelos es que son muy flexibles ya que permiten modelizar, través de dichas funciones suaves, relaciones de tipo lineal entre la variable respuesta y las predictoras. Sin embargo, todo son ventajas ya que el proceso de selección del mejor modelo se complica al añadir la elección de la función de suavizado utilizar.En situaciones con dos variables predictoras, \\(X_1\\) y \\(X_2\\), de tipo numérico se podrían plantear los modelos saturados siguientes:\\[\n\\begin{array}{ll}\nM0: & Y \\sim X_1 + X_2\\\\\nM1: & Y \\sim f(X_1) + X_2\\\\\nM2: & Y \\sim f(X_1) + f(X_2)\\\\\n\\end{array}\n\\]También resulta posible plantear este tipo de modelos donde se incluyen variables predictoras de tipo factor. En este caso debemos plantear una ecuación de suavizado para cada uno de los grupos determinados por el factor al igual que ocurría con el efecto de interacción en los modelos ANCOVA.Para modelizar este tipo de datos es necesario instalar y cargar la librería mgcv.","code":""},{"path":"smooth.html","id":"bancos-de-datos-3","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.1 Bancos de datos","text":"Veamos los diferentes ejemplos con los que vamos trabajar. Muchos de ellos ya los hemos utilizado en modelos anteriores.Ejemplo 1. Datos de calidad del aire. Este diseño experimental contiene la información recogida sobre el estudio de calidad del aire que ya presentamos en la Unidad 2. El objetivo de este estudio era tratar de predecir la calidad del aire, medida en términos del nivel de ozono (Ozone), en función de la radiación solar (Solar.R), velocidad del viento (Wind), y temperatura (Temp). Además, se recogen las variables mes (Month) y día (Day) de la recogida de datos. En base las variables experimentales recogidas cabría pensar que un modelo de regresión lineal múltiple de la forma\\[Ozone \\sim Solar.R + Wind + Temp\\]continuación, se presenta el código para la carga de datos y el gráfico de la respuesta versus cada predictora. En la figura se pueden ver los modelos lineales ajustados, y la aparente falta de ajuste de estos. Tan solo la relación entre Ozono y Wind parece de tipo lineal, mientras que en los otros dos parece necesario realizar algún tipo de transformación para linealizar la relación. Podemos intentar encontrar dicha transformación de respuesta o predictoras pero podemos ver que los métodos de suavizado nos proporcionan una solución rápida esta situación sin necesidad de perder el tiempo buscando transformaciones adecuadas.Como veremos más adelante la solución en términos de modelo de suavizado se puede apreciar en el gráfico siguiente:donde podemos ver el cambio en la asociación entre las diferentes predictoras y el nivel de ozono. Las tendencias de suavizado obtenidas reflejan el comportamiento gran escala de la predictora versus cada respuesta. ¿Qué conclusiones podemos extraer de cada uno de los suavizados obtenidos?Ejemplo 2. Datos de producción. Se realiza un ensayo agrícola para estudiar la producción de cierto tipo de planta en dos localidades en función de la densidad de plantas en la parcela de producción. Las variables recogidas en el experimento son la densidad de plantas (Densidad), la producción global obtenida (Produccion), y la localidad donde se encuentra la parcela de producción (Localidad). El banco de datos obtenido se presenta continuación:la vista de la información recogida se podría plantear un modelo ANCOVA, por lo que realizamos un gráfico de dispersión identificando cada punto según la localidad de procedencia.Se puede ver como la producción en la localidad queda por encima de la de la localidad B, con un descenso asociado con el aumento de la densidad de plantas. La única diferencia con los modelos ANCOVA clásicos es que el descenso parece ajustarse un modelo lineal, sino más bien un modelo con una caída curvilínea. Aunque existe la posibilidad de plantear una transformación de la respuesta y/o predictora, o incluso un modelo polinómico, resulta difícil plantear un modelo tan rígido dado que ambas localidades parecen comportarse de forma distinta.Vemos el resultado del modelo de suavizado (asumiendo que cada localidad puede tener un comportamiento distinto)El gráfico muestra las tendencias ajustadas para cada localidad reflejando una curva distinta para cada localidad, lo que permite obtener un modelo muy flexible que se adapta al comportamiento global de la producción versus la densidad de plantas en función de la localidad de procedencia. Más tarde presentaremos todas las posibilidades de modelización para este conjunto de datos.Ejemplo 3. Datos de infiltración. Se conoce como infiltración el proceso por el cual el agua (riego o lluvia) se va introduciendo bajo la superficie de un terreno cultivado. Este proceso es vital para determinar las cantidades de agua de riego necesarias, para mantener el terreno en condiciones óptimas. Un parámetro habitual que sirve para estudiar dicho proceso es la carga hidráulica. Este depende tanto de la profundidad de la infiltración (profundidad) como del procedimiento de riego usado. Se diseña un experimento para estudiar la carga hidráulica (cargahid) de un terreno bajo diferentes condiciones de riego (denominados tratamiento). Los datos recogidos en el experimento se presentan continuación:Representamos los datos mediante un gráfico de dispersión identificando cada uno de los tratamientos:En este caso las tendencias observadas son bastante diferentes entre tratamientos y claramente lineales. Un posible modelo de suavizado para este conjunto de datos vendría dado por:En este caso los ajustes de suavizado se asemejan modelos polinómicos de grado 3 o 4, de forma que se podrían plantear ambas modelizaciones y compararlas para determinar el modelo que mejor ajusta la tendencia observada en los datos.","code":"\n# Carga de datos\ndata(\"airquality\")\n# Seleccionamos variables de interés\ndatos <- airquality[,c(\"Ozone\", \"Solar.R\", \"Wind\", \"Temp\")]\ndatacomp = melt(datos, id.vars='Ozone')\n# Representamos respuesta vs predictoras\nggplot(datacomp) +\n  geom_jitter(aes(value,Ozone, colour=variable),) +\n  facet_wrap(~variable, scales=\"free_x\") +\n  labs(x = \"\", y = \"Ozono\")\nggplot(datacomp) +\n  geom_jitter(aes(value,Ozone, colour=variable),) + \n  geom_smooth(aes(value,Ozone, colour=variable), \n              method=loess, se=FALSE) +\n  facet_wrap(~variable, scales=\"free_x\") +\n  labs(x = \"\", y = \"Ozono\") \nDensidad <- c(23.48, 26.22, 27.79, 32.88, 33.27, 36.79, \n              37.58, 37.58, 41.49, 42.66, 44.23, 44.23, \n              51.67, 55.58, 55.58, 57.93, 58.71, 59.5, \n              60.67, 62.63, 67.71, 70.06, 70.45, 73.98, \n              73.98, 78.67, 95.9, 96.68, 96.68,101.38, \n              103.72, 104.51, 105.68, 108.03,117.82, 127.21, \n              134.26, 137.39, 151.87, 163.61, 166.35, 184.75, \n              18.78, 21.25, 23.23, 27.18, 30.15, 31.63, 32.12, \n              32.62, 32.62, 33.61, 37.07, 38.55, 39.54, 39.54, \n              41.02, 42.5, 43.98, 45.47, 49.92, 50.9, 53.87, \n              57.82, 61.78, 61.78, 63.75, 67.71, 71.66, 77.59, \n              80.56, 86.49, 88.46, 89.45, 90.93, 92.91, 101.81, \n              103.78, 115.15, 123.06, 144.31, 155.68, 158.15, \n              180.39)\nProduccion <- c(5.41, 5.46, 5.4, 5.4, 5.29, 5.25, 5.35, 5.25, \n                5.05, 5.12, 5.29, 5.04, 5.03, 4.96, 4.84, 5.12, \n                4.97, 5.02, 4.87, 4.83, 4.74, 4.76, 4.79, 4.9, \n                4.74, 4.51, 4.62, 4.58, 4.62, 4.58, 4.47, 4.4, \n                4.34, 4.47, 4.44, 4.24, 4.17, 4.2, 4.14, 4.02, \n                4.14, 4, 5.61, 5.46, 5.2, 5.18, 4.95, 5.13, \n                4.93, 5.15, 4.72, 5.05, 4.92, 5.04, 4.82, 4.99, \n                4.66, 4.94, 5, 4.7, 4.51, 4.63, 4.68, 4.53, 4.57, \n                4.55, 4.6, 4.54, 4.5, 4.24, 4.3, 4.32, 4.29, 4.38, \n                4.37, 4.26, 4.11, 4.31, 3.9, 4.04, 3.87, 3.69, 3.66, \n                3.37) \nLocalidad <- as.factor(c(rep(\"A\", 42), rep(\"B\", 42)))\nplantas <- data.frame(Densidad, Produccion, Localidad)\nggplot(plantas, aes(x = Densidad, y = Produccion, colour = Localidad)) +\n  geom_point() +\n  labs(x = \"Densidad\", y = \"Producción\")\ntratamiento <- as.factor(c(rep(\"A\", 15), rep(\"B\", 15), rep(\"C\", 15)))\nprofundidad <-c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, \n                140, 150, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, \n                120, 130, 140, 150, 10, 20, 30, 40, 50, 60, 70, 80, 90, \n                100, 110, 120, 130, 140, 150)\ncargahid <- c(-406.90, -345.70, -335.50, -315.10, -304.90, -315.10, \n              -323.26, -335.50, -345.70, -362.02, -374.26, -386.50, \n              -421.18, -435.46, -447.70, -896.50, -737.38, -653.74, \n              -470.14, -406.90, -388.54, -396.70, -396.70, -396.70, \n              -406.90, -419.14, -437.50, -468.10, -466.06, -492.58, \n              -896.50, -855.70, -818.98, -788.38, -678.22, -590.50, \n              -545.62, -515.02, -498.70, -496.66, -517.06, -555.82, \n              -619.06, -623.14, -623.14)\ninfiltracion <- data.frame(tratamiento, profundidad, cargahid)\nggplot(infiltracion, aes(x = profundidad, y = cargahid, colour = tratamiento)) +\n  geom_point() +\n  labs(x = \"Profundidad\", y = \"Carga Hidráulica\")"},{"path":"smooth.html","id":"funciones-de-suavizado","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.2 Funciones de suavizado","text":"Las funciones de suavizado son los denominados splines que consisten en funciones definidas sobre bases de polinomios. En nuestro caso utilizaremos los denominados splines penalizados o p-splines. Para el ajuste de este tipo de mosdelos utilizaremos la función gam de la libreria mgcv.La función de suavizado tiene la estructura siguiente:\\[s(variable, k = , m = , bs =  , = factor)\\]donde \\(k\\) es el tamaño de la base de polinomios, \\(m\\) es el orden de los polinomios, \\(bs\\) es el tipo de la base de splines utilizados y \\(\\) identifica un factor para el ajuste de las curvas de suavizado (efecto de interacción).","code":""},{"path":"smooth.html","id":"splines-de-regresión","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.2.1 Splines de regresión","text":"Las funciones de suavizado con las que se empieza trabajar asumen que la función de suavizado \\(f(X)\\) se puede escribir como:\\[\nf(X) = \\sum_{=1}^q \\beta_j b_j(X)\n\\]siendo \\(\\beta_j\\) parámetros desconocidos y \\(b_1(X),\\ldots,b_q(X)\\) una base de funciones de polinomios de dimensión \\(q\\) (modelo polinómico de orden \\(q\\)), de forma que el modelo lineal para la respuesta \\(Y\\) se podría expresar como:\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\ldots + \\beta_q x_i^q + \\epsilon_i, \\quad =1,...,n\n\\]El problema principal con esta propuesta es que la base de polinomios nos llevaría modelos donde aparecían problemas de multicolinealidad, y que exigen un grado bastante alto para poder adaptarse los cambios entre respuesta y predictora.Una generalización de estos modelos son los denominados “splines de regresión” que son curvas definidas trozos mediante polinomios, es decir, dividimos el rango de \\(X\\) en trozos y sobre cada trozo ajustamos un modelo polinómico con la única restricción que loas funciones obtenidas en cada trozo deben unirse formando una curva suave. La ventaja principal es que podemos ajustar utilizando polinomios de grado bajo en cada trozo, consiguiendo representar curvas con formas complicadas, evitando grandes oscilaciones en la tendencia que aparecen cuando utilizamos polinomios de grados muy altos.La desventaja principal es que debemos elegir el número de trozos o knots y el orden de la base de polinomios que debemos utilizar.","code":""},{"path":"smooth.html","id":"splines-cúbicos","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.2.2 Splines cúbicos","text":"Un “spline cúbico” es una curva construida partir de trozos de polinomios de grado 3 que se ensamblan perfectamente de forma que la curva que forma es continua hasta la segunda derivada.Son los más utilizados dentro de los splines de regresión porque utilizan polinomios muy sencillos que se adaptan perfectamente cambios en el comportamiento entre respuesta y predictora en los diferentes trozos en que se divide el rango de \\(X\\).","code":""},{"path":"smooth.html","id":"splines-penalizados","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.2.3 Splines penalizados","text":"La elección del grado de suavización de la función que ajusta la tendencia entre respuesta y predictora es un tema muy importante, y está asociado al grado de la base de polinomios utilizada. Las posibilidades que tenemos la hora de elegir el grado de suavizado pasan por utilizar los denominados “splines penalizados” que son splines de regresión en los que se introduce una penalización al realizar el ajuste del modelo. Dicha penalización viene controlada por el parámetro de suavizado \\(\\lambda\\).Si \\(\\lambda = 0\\) estamos en el caso particular en el que hay penalización y medida que \\(\\lambda\\) aumenta, aumentamos la intensidad de la penalización. Cuando\\(\\lambda\\) tiende 1 el modelo se convierte prácticamente en un modelo de regresión lineal simple. Se recomienda la lectura de la bibliografía recomendada para completar la información sobre el parámetro de suavizado y la penalización utilizada.Para ajustar este tipo de modelos en R utilizaremos la función anterior donde se toman valores:\\(k = 10\\) cuando nuestra muestra es pequeña y \\(k = 20\\) cuando nuestra muestra es grande (aunque se puede variar en función de los datos analizados).\\(m = 2\\) como el orden de los polinomios.Se toma como base de splines los splines penalizados (\\(bs = ps\\)).","code":""},{"path":"smooth.html","id":"bondad-del-ajuste-y-selección-del-modelo","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.3 Bondad del ajuste y selección del modelo","text":"La bondad del ajuste de este tipo de modelos se basa en los estadísticos AIC y GCV. El segundo de estos es específico de este tipo de modelos ya que se utiliza para estimar la curva de suavizado asociada con la variable predictora. En ambos casos cuanto menor es el valor mejor será el modelo obtenido. En la mayoría de situaciones se utilizan ambos criterios para comparar el modelo lineal con el suavizado y determinar cual es el mejor de los dos. Para este tipo de modelos se puede obtener además la capacidad explicativa del modelo construido través de la desvianza explicada, que representa el porcentaje de variabilidad de la respuesta que viene explicada por el modelo, de forma similar al \\(R^2\\) en los modelos de regresión.En cuanto la selección del modelo, la principal diferencia con respecto los modelos tratados hasta ahora es que existen procedimientos automáticos, con lo que la construcción y validación de los modelos requiere la construcción de todos los que consideremos que pueden ser adecuados.Las posibilidades más habituales de modelización y comparación consisten en:Comparar diferentes tipos de modelos lineales (RLS, RLM, o MP) con el modelo suavizado.Comparar las componentes del modelo aditivo, es decir, comprobar si podemos eliminar algunos de los efectos presentes en el modelo. En este sentido, cuando tenemos más de dos variables predictoras se pueden trabajar con todos los modelos que presentamos al inicio de esta unidad.Para la selección del mejor modelo se usan como referencia el AIC. El criterio basado en GCV se utiliza para seleccionar la función de suavizado (en cuanto los parámetros que se usan para su construcción).Pasamos realizar un análisis completo de los tres ejemplos presentados al inicio de la unidad. En este tipo de modelos utilizaremos la función summary en lugar de tab_model para obtener la información del modelo ajustado.Ejemplo. Estudaimos las diferentes posibilidades de modelización para el banco de datos de calidad del aire. Para construir el mejor modelo para este banco de datos vamos proceder ajustando individualmente cada posible predictora para proceder posteriormente con modelos más complejos. Para cada modelo individual probaremos un modelo polinómico (MP) de orden 3 (para poder captar los cambios de tendencia) y el correspondiente modelo de suavizado. Los ajustaremos todos y valoraremos la capacidad explicativa de cada uno.Se proponen los modelos siguientes para la asociación entre calidad del aire y la radiación solar:\\[\n\\begin{array}{ll}\nOzone & \\sim Solar.R + Solar.R^2 + Solar.R^3\\\\\nOzone & \\sim f(Solar.R)\\\\\n\\end{array}\n\\]con \\(f()\\) una función de suavidado basada en p-splines penalizdos con \\(k = 10\\) nodos y polinomios de orden \\(2\\).Realizamos el ajuste de ambos modelos utilizando la función gam(), y comparamos ambos modelos con el estadístico AIC.Resultados para el modelo polinómicoLa tabla de coeficientes muestra que el grado 3 del polinomio es necesario (p-valor asociado significativo), aunque la capacidad explicativa (Deviance explained) es muy baja (24.6%). En cuanto al modelo suavizado tendríamos:La función de suavizado es relevante (p-valor de s(Solar.R) significativo), pero la capacidad explicativa también es muy baja (24.6%), aunque del mismo orden que la obtenida con el modelo polinómico. Realizamos la comparación de ambos modelos mediante AIC:Ambos modelos tienen un AIC del mismo orden por lo que resulta difícil establecer cual de ellos resultaría más conveniente. Veamos gráficamente los ajustes obtenidos donde se aprecia el comportamiento de ambos modelos.Analizamos ahora el efecto de la velocidad del viento en el nivel de ozono. Se proponen los modelos siguientes:\\[\n\\begin{array}{ll}\nOzone & \\sim Wind + Wind^2 + Wind^3\\\\\nOzone & \\sim f(Wind)\\\\\n\\end{array}\n\\]Ajustamos ambos modelosResultados para el modelo polinómicoLa tabla de coeficientes muestra que es suficiente con considera el grado 2 en el polinomio. La capacidad explicativa de este modelo se sitúa casi en el 50% (49.8%), indicando que individualmente está variable contribuye más la explicación del nivel de ozono. En cuanto al modelo de suavizado:La función se suavizado es relevante (p-valor significativo), y la capacidad explicativa también es del 50%. Si comparamos con el AIC podemos ver que ambos modelos son muy similares. En realidad deberíamos ajustar de nuevo el modelo polinómico considerando el grado 2, pero los resultados son muy similares.En cuanto los ajustes obtenidos con cada modelo podemos ver lo que se parecen las dos soluciones propuestas.Analizamos ahora el efecto de la tempreratura sobre el nivel de ozono. Proponemos los modelos:\\[\n\\begin{array}{ll}\nOzone & \\sim Temp + Temp^2 + Temp^3\\\\\nOzone & \\sim f(Temp)\\\\\n\\end{array}\n\\]Ajustamos ambos modelosResultados para el modelo polinómicoLa tabla de coeficientes muestra que es necesario el grado 3 en el polinomio. La capacidad explicativa de este modelo se sitúa casi en 54.1% (la más alta de forma individual). En cuanto al modelo de suavizado:La función se suavizado es relevante (p-valor significativo), y la capacidad explicativa también es algo superior que en el modelo polinómico al alcanzar el 57%. En este caso el AIC para el modelo de suavizado es algo inferior al polinómico pero ambos proporcionan soluciones muy similares.La solución con cada modelo es:la vista de los análisis individuales las variables predictoras más relvantes por orden de importancia serían temperatura, velocidad del viento, y por último la radiación solar. En cuanto la elección de modelos polinómicos o suavizados los resultados son muy similares en los tres casos ya que únicamente se observa cierta preferencia del suavizado con la variable temperatura.Pasamos analizar los modelos más complejos con dos o tres predictoras. En primer lugar comparamos modelos con estructuras polinómicas frente modelos de suavizado. Veamos todos los modelos:\\[\n\\begin{array}{lll}\nM1:& Ozone & \\sim Solar.R + Solar.R^2 + Solar.R^3 + Wind + Wind^2\\\\\nM2:& Ozone & \\sim Solar.R + Solar.R^2 + Solar.R^3 + Temp + Temp^2 + Temp^3\\\\\nM3:& Ozone & \\sim Wind + Wind^2 + Temp + Temp^2 + Temp^3\\\\\nM4:& Ozone & \\sim Solar.R + Solar.R^2 + Solar.R^3 + Wind + Wind^2 + Temp + Temp^2 + Temp^3\\\\\nM5:& Ozone & \\sim f(Solar.R) + f(Wind)\\\\\nM6:& Ozone & \\sim f(Solar.R) + f(Temp)\\\\\nM7:& Ozone & \\sim f(Wind) + f(Temp)\\\\\nM8:& Ozone & \\sim f(Solar.R) + f(Wind) + f(Temp)\\\\\n\\end{array}\n\\]En primer lugar ajustamos todos los modelos (con el indicador de la tabla anterior):En lugar de estudiar con detalle todos los modelos propuestos, utilizamos el AIC para ordenarlos (de mejor peor) y analizamos los dos más relevantes.Los dos mejores modelos (menor valor del AIC) son el M4 y el M8 que corresponden con los modelos que incorporan las tres predictoras. El M4 expresado como modelo polinómico y el M8 como modelo aditivo. De hecho, el modelo aditivo con las tres predictoras es el mejor de todos. Dado que individualmente tanto para la variable Solar.R y Wind el modelo polinómico se comportaba al mismo nivel que el suavizado, se propone una última alternativa de modelización que consiste en asumir polinomios en estas dos variables y suavizado en Temp:\\[\n\\begin{array}{lll}\nM9:& Ozone & \\sim Solar.R + Solar.R^2 + Solar.R^3 + Wind + Wind^2 + f(Temp)\\\\\n\\end{array}\n\\]En lugar de estudiar con detalle todos los modelos propuestos, utilizamos el AIC para ordenarlos (de mejor peor) y analizamos los dos más relevantes.El AIC para este modelo es superior al obtenido para los modelos M4 y M8, de forma que el modelo preferido sería el M8. En primer lugar estudiamos el modelo obtenido:Los tres suavizados resultan significativos y la capacidad explicativa del modelo alcanza el 74.4%. Como alternativa este modelo podríamos considerar modelos aditivos donde aumentamos el número de nodos. Probamos este nuevo modelo duplicando el número de nodos (pasamos de 10 20). En este caso dado que se trata de comparar diferentes opciones de suavizado utilizaremos el criterio GCV para decidirnos entre los dos.El valor de GCV con 10 nodos es 339.03 mientras que con 20 nodos dicho estadístico es 289.38. Por tanto, el modelo con 20 nodos es preferido. De hecho, la capacidad explicada ha crecido hasta el 83.1% (un 10% superior). El valor de AIC para este modelo es 941 mostrando ser mejor que el modelo polinómico. El modelo final es un suavizado independiente para cada una de las predictoras consideradas.Ejemplo. Analizamos ahora el banco de datos de producción. La estructura de este banco de datos nos hace pensar que un modelo ANCOVA podría ser adecuado pero el gráfico descriptivo deja muy claro si el comportamiento es de tipo lineal o si hay que considerar efectos de interacción entre localidad y densidad de plantas de tipo lineal.El conjunto de modelos que se pueden plantear son:M1: Modelo lineal sin efecto de interacción.M2: Modelo lineal sin efecto de interacción.M3: Modelo polinómico de grado 2 sin interacción.M4: Modelo polinómico de grado 2 con interacción.M5: Modelo suavizado sin interacción.M6: Modelo suavizado con interacción.La representación gráfica de estos modelos se puede obteenr mediante:Veamos el código necesario para ajustar cada uno de los modelos propuestos y valoremos el AIC y el GCV para seleccionar el mejor modelo.Con ambos criterios de selección el modelo preferido es el M6, es decir, el modelo de suavizado con interacción con el factor localidad. Estudiamos con más detalle dicho modelo:Todos los efectos del modelo resultan significativos y la capacidad explicativa alcanza el 95%, indicando que el modelo obtenido es muy adecuado para estudiar la producción en las dos localidades.Ejemplo. Análisis de los datos de infoltración. El gráfico descriptivo de este conjunto de datos muestra relaciones lineales distintas entre carga hidráulica y profundidad en función del tratamiento, por lo que un modelo ANCOVA con interacción podría resultar demasiado rígido. Utilizamos un modelo suavizado para estos datos.Los suavizados para cada tratamiento resultan significativos y la capacidad explicativa alcanza el 99%, indicando que el modelo parece capturar adecuadamente la tendencia observada en los datos.","code":"\n# M1: modelo polinómico\nfit1.solarr <- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) , \n                   data = airquality)\n# M1: modelo suavizado\nfit2.solarr <- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = \"ps\"), \n                   data = airquality)\nsummary(fit1.solarr)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3)\n## \n## Parametric coefficients:\n##                Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)   1.432e+01  1.244e+01   1.151   0.2522  \n## Solar.R      -1.245e-01  3.265e-01  -0.381   0.7038  \n## I(Solar.R^2)  3.594e-03  2.212e-03   1.625   0.1072  \n## I(Solar.R^3) -9.661e-06  4.292e-06  -2.251   0.0264 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## R-sq.(adj) =  0.225   Deviance explained = 24.6%\n## GCV = 890.23  Scale est. = 858.15    n = 111\nsummary(fit2.solarr)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ s(Solar.R, k = 10, m = 2, bs = \"ps\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   42.099      2.783   15.13   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##              edf Ref.df     F  p-value    \n## s(Solar.R) 2.943  3.569 9.074 9.74e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.224   Deviance explained = 24.4%\n## GCV = 891.28  Scale est. = 859.62    n = 111\n# Bondad de ajuste de cada modelo (AIC)\ng1 <- glance(fit1.solarr)\ng2 <- glance(fit2.solarr)\nas_tibble(rbind(g1, g2))## # A tibble: 2 × 7\n##      df logLik   AIC   BIC deviance df.residual  nobs\n##   <dbl>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <int>\n## 1  4.00  -530. 1071. 1084.   91822.        107    111\n## 2  3.94  -530. 1071. 1084.   92029.        107.   111## $Solar.R## $Solar.R\n# M1: modelo polinómico\nfit1.wind <- gam(Ozone ~ Wind + I(Wind^2) + I(Wind^3) , \n                 data = airquality)\n# M1: modelo suavizado\nfit2.wind <- gam(Ozone ~ s(Wind, k = 10, m = 2, bs = \"ps\"), \n                 data = airquality)\nsummary(fit1.wind)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ Wind + I(Wind^2) + I(Wind^3)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 200.28778   27.01403   7.414  2.5e-11 ***\n## Wind        -32.26471    8.29374  -3.890  0.00017 ***\n## I(Wind^2)     1.92084    0.78733   2.440  0.01627 *  \n## I(Wind^3)    -0.03771    0.02305  -1.636  0.10465    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## R-sq.(adj) =  0.484   Deviance explained = 49.8%\n## GCV = 581.27  Scale est. = 561.23    n = 116\nsummary(fit2.wind)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ s(Wind, k = 10, m = 2, bs = \"ps\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   42.129      2.193   19.21   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##           edf Ref.df     F p-value    \n## s(Wind) 2.889  3.534 30.77  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.487   Deviance explained =   50%\n## GCV = 577.41  Scale est. = 558.05    n = 116\n# Bondad de ajuste de cada modelo (AIC)\ng1 <- glance(fit1.wind)\ng2 <- glance(fit2.wind)\nas_tibble(rbind(g1,g2))## # A tibble: 2 × 7\n##      df logLik   AIC   BIC deviance df.residual  nobs\n##   <dbl>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <int>\n## 1  4.00  -530. 1069. 1083.   62858.        112.   116\n## 2  3.89  -529. 1069. 1082.   62564.        112.   116## $Wind## $Wind\n# M1: modelo polinómico\nfit1.temp <- gam(Ozone ~ Temp + I(Temp^2) + I(Temp^3) , \n                 data = airquality)\n# M1: modelo suavizado\nfit2.temp <- gam(Ozone ~ s(Temp, k = 10, m = 2, bs = \"ps\"), \n                 data = airquality)\nsummary(fit1.temp)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ Temp + I(Temp^2) + I(Temp^3)\n## \n## Parametric coefficients:\n##               Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  0.0910285  0.0663384   1.372   0.1727  \n## Temp         2.2972754  1.6743100   1.372   0.1728  \n## I(Temp^2)   -0.0732824  0.0430854  -1.701   0.0917 .\n## I(Temp^3)    0.0006372  0.0002747   2.319   0.0222 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Rank: 3/4\n## R-sq.(adj) =  0.533   Deviance explained = 54.1%\n## GCV = 522.02  Scale est. = 508.52    n = 116\nsummary(fit2.temp)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ s(Temp, k = 10, m = 2, bs = \"ps\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   42.129      2.043   20.62   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##           edf Ref.df     F p-value    \n## s(Temp) 3.771  4.524 32.04  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.555   Deviance explained =   57%\n## GCV = 505.06  Scale est. = 484.29    n = 116\n# Bondad de ajuste de cada modelo (AIC)\ng1 <- glance(fit1.temp)\ng2 <- glance(fit2.temp)\nas_tibble(rbind(g1,g2))## # A tibble: 2 × 7\n##      df logLik   AIC   BIC deviance df.residual  nobs\n##   <dbl>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <int>\n## 1  3.00  -525. 1057. 1068.   57462.        113.   116\n## 2  4.77  -521. 1053. 1069.   53867.        111.   116## $Temp## $Temp\nfit.M1 <- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + \n                Wind +I(Wind^2), \n              data = airquality)\nfit.M2 <- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + \n                Temp +I(Temp^2) + I(Temp^3), \n              data = airquality)\nfit.M3 <- gam(Ozone ~ Wind +I(Wind^2) + \n                Temp +I(Temp^2) + I(Temp^3), \n              data = airquality)\nfit.M4 <- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + \n                Wind +I(Wind^2) + \n                Temp +I(Temp^2) + I(Temp^3), \n              data = airquality)\nfit.M5 <- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = \"ps\") + \n                s(Wind, k = 10, m = 2, bs = \"ps\"), \n              data = airquality)\nfit.M6 <- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = \"ps\") + \n                s(Temp, k = 10, m = 2, bs = \"ps\"), \n              data = airquality)\nfit.M7 <- gam(Ozone ~ s(Wind, k = 10, m = 2, bs = \"ps\") + \n                s(Temp, k = 10, m = 2, bs = \"ps\"), \n              data = airquality)\nfit.M8 <- gam(Ozone ~ s(Solar.R, k = 10, m = 2, bs = \"ps\") +\n                s(Wind, k = 10, m = 2, bs = \"ps\") + \n                s(Temp, k = 10, m = 2, bs = \"ps\"), \n              data = airquality)\n# Bondad de ajuste de cada modelo (AIC)\ng1 <- glance(fit.M1)\ng2 <- glance(fit.M2)\ng3 <- glance(fit.M3)\ng4 <- glance(fit.M4)\ng5 <- glance(fit.M5)\ng6 <- glance(fit.M6)\ng7 <- glance(fit.M7)\ng8 <- glance(fit.M8)\nas_tibble(rbind(g1, g2, g3, g4, g5, g6, g7, g8))## # A tibble: 8 × 7\n##      df logLik   AIC   BIC deviance df.residual  nobs\n##   <dbl>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <int>\n## 1  5.00  -523. 1057. 1074.   79944.        106.   111\n## 2  6.00  -497. 1008. 1027.   50331.        105.   111\n## 3  5.00  -506. 1024. 1041.   41853.        111.   116\n## 4  8.00  -477.  971.  996.   34858.        103.   111\n## 5  6.06  -495. 1003. 1022.   48176.        105.   111\n## 6  7.51  -493. 1003. 1026.   46669.        103.   111\n## 7  7.69  -501. 1018. 1042.   38028.        108.   116\n## 8  9.97  -470.  963.  992.   31176.        101.   111\nfit.M9 <- gam(Ozone ~ Solar.R + I(Solar.R^2) + I(Solar.R^3) + \n                Wind +I(Wind^2) + \n                s(Temp, k = 10, m = 2, bs = \"ps\"), \n              data = airquality)\n# Bondad de ajuste de cada modelo (AIC)\nglance(fit.M9)## # A tibble: 1 × 7\n##      df logLik   AIC   BIC deviance df.residual  nobs\n##   <dbl>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <int>\n## 1  8.30  -495. 1008. 1033.   48296.        103.   111\nsummary(fit.M8)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ s(Solar.R, k = 10, m = 2, bs = \"ps\") + s(Wind, k = 10, \n##     m = 2, bs = \"ps\") + s(Temp, k = 10, m = 2, bs = \"ps\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   42.099      1.667   25.25   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##              edf Ref.df      F p-value    \n## s(Solar.R) 2.622  3.209  4.142 0.00672 ** \n## s(Wind)    2.747  3.374 14.773 < 2e-16 ***\n## s(Temp)    3.600  4.327 12.738 < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.721   Deviance explained = 74.4%\n## GCV = 339.03  Scale est. = 308.58    n = 111\nfit.M10 <- gam(Ozone ~ s(Solar.R, k = 20, m = 2, bs = \"ps\") +\n                 s(Wind, k = 20, m = 2, bs = \"ps\") + \n                 s(Temp, k = 20, m = 2, bs = \"ps\"), \n               data = airquality)\nsummary(fit.M10)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Ozone ~ s(Solar.R, k = 20, m = 2, bs = \"ps\") + s(Wind, k = 20, \n##     m = 2, bs = \"ps\") + s(Temp, k = 20, m = 2, bs = \"ps\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   42.099      1.445   29.13   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##               edf Ref.df      F p-value    \n## s(Solar.R)  1.068  1.132 10.118 0.00175 ** \n## s(Wind)     3.588  4.416 20.900 < 2e-16 ***\n## s(Temp)    16.392 17.430  7.082 < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.791   Deviance explained = 83.1%\n## GCV = 289.38  Scale est. = 231.9     n = 111\nggplot(plantas, aes(x = Densidad, y = Produccion, colour = Localidad)) +\n  geom_point() +\n  labs(x = \"Densidad\", y = \"Producción\")\nfit.plantas.M1 <- gam(Produccion ~ Localidad + Densidad, \n                      data = plantas)\nfit.plantas.M2 <- gam(Produccion ~ Localidad * Densidad, \n                      data = plantas)\nfit.plantas.M3 <- gam(Produccion ~ Localidad + Densidad + I(Densidad^2), \n                      data = plantas)\nfit.plantas.M4 <- gam(Produccion ~ Localidad * (Densidad + I(Densidad^2)), \n                      data = plantas)\nfit.plantas.M5 <- gam(Produccion ~ Localidad + s(Densidad, k = 10, m = 2, bs = \"ps\"),\n                      data = plantas)\nfit.plantas.M6 <- gam(Produccion ~ Localidad + \n                        s(Densidad, k = 10, m = 2, bs = \"ps\", by = Localidad),\n                      data = plantas)\n### Valores de AIC\nrbind(glance(fit.plantas.M1),glance(fit.plantas.M2),glance(fit.plantas.M3),\n      glance(fit.plantas.M4),glance(fit.plantas.M5),glance(fit.plantas.M6))## # A tibble: 6 × 7\n##      df logLik    AIC    BIC deviance df.residual  nobs\n##   <dbl>  <dbl>  <dbl>  <dbl>    <dbl>       <dbl> <int>\n## 1  3      46.6  -85.2  -75.4    1.62         81      84\n## 2  4      49.5  -89.0  -76.9    1.51         80      84\n## 3  4.00   63.7 -117.  -105.     1.08         80      84\n## 4  6.00   66.0 -118.  -101.     1.02         78      84\n## 5  6.34   70.9 -127.  -109.     0.909        77.7    84\n## 6  9.09   77.0 -134.  -109.     0.786        74.9    84\n### Valores de GCV\ncbind(fit.plantas.M1$gcv.ubre, fit.plantas.M2$gcv.ubre, fit.plantas.M3$gcv.ubre,\n      fit.plantas.M4$gcv.ubre, fit.plantas.M5$gcv.ubre, fit.plantas.M6$gcv.ubre)##              [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n## GCV.Cp 0.02077199 0.01985587 0.01415785 0.01410118 0.01266822 0.01177339\nsummary(fit.plantas.M6)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## Produccion ~ Localidad + s(Densidad, k = 10, m = 2, bs = \"ps\", \n##     by = Localidad)\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  4.84394    0.01605  301.88   <2e-16 ***\n## LocalidadB  -0.32929    0.02272  -14.49   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                          edf Ref.df     F p-value    \n## s(Densidad):LocalidadA 2.541  3.101 216.4  <2e-16 ***\n## s(Densidad):LocalidadB 4.550  5.333 165.3  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.951   Deviance explained = 95.6%\n## GCV = 0.011773  Scale est. = 0.010499  n = 84\nfit.cargahid <- gam(cargahid ~ tratamiento + s(profundidad, k = 10, m = 2, \n                                               bs = \"ps\", \n                                               by = tratamiento), data = infiltracion)\nsummary(fit.cargahid)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## cargahid ~ tratamiento + s(profundidad, k = 10, m = 2, bs = \"ps\", \n##     by = tratamiento)\n## \n## Parametric coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -363.652      3.238 -112.32   <2e-16 ***\n## tratamientoB -131.920      4.579  -28.81   <2e-16 ***\n## tratamientoC -277.848      4.579  -60.68   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                               edf Ref.df      F p-value    \n## s(profundidad):tratamientoA 4.031  4.797  38.27  <2e-16 ***\n## s(profundidad):tratamientoB 7.224  7.928 248.40  <2e-16 ***\n## s(profundidad):tratamientoC 6.862  7.584 215.45  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.994   Deviance explained = 99.7%\n## GCV = 296.27  Scale est. = 157.24    n = 45"},{"path":"smooth.html","id":"diagnóstico-2","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.4 Diagnóstico","text":"Una vez ajustado un modelo el estudio de los residuos nos permite realizar su diagnóstico en las mismas condiciones que los modelos lineales habituales. Las hipótesis de este modelo son las mismas que las del modelo de regresión, y por tanto, el diagnóstico se centra en los mismos procedimientos gráficos. Recordemos que debemos verificar la normalidad y varianza constante de los residuos del modelo. Sin embargo, en este tipo de modelos se añade el diagnóstico sobre el grado de suavizado del modelo considerado para saber si es necesario modificarlo.Para este tipo de modelos resulta posible obtener todos los gráficos de interés con la función gam.check(). Esta función ofrece dos tipos de salidas:Una tabla que permite contrastar los parámetros del suavizado utilizado. Los p-valores obtenidos deben resultar significativos.gráficos de diagnóstico (qq, residuos versus ajustados, histograma de los residuos, valores observados versus valores ajustados). El gráfico qq y el histograma nos permiten verificar la hipótesis de normalidad, mientras que los residuos versus ajustados nos permite verificar la hipótesis de varianza constante. El último gráfico nos permite conocer lo bueno que es el ajuste realizado, ya que si los punto se distribuyen lo largo de la diagonal significará que el modelo predice adecuadamente la respuesta.Ejemplo. Realizamos el diagnóstico el modelo ajustado para los datos de calidad del aire.Los test de suavizado indican que el modelo es adecuado, pero mientras que los gráficos para validar normalidad parecen indicar que hay problema con dicha hipótesis, si parece haberlo con la de homogeneidad de varianzas. Se aprecia un efecto de embudo (aumenta la variabilidad cuando aumenta el valor ajustado) en el gráfico de residuos versus ajustados. En este caso podemos utilizar las transformaciones de Box-Cox que están diseñadas para los modelos estudiados en unidades anteriores, pero si podemos probar alguna transformación habitual para ver si corregimos ese defecto observado. Probamos con la raíz cuadrada. Dado que vamos modificar la escala de la respuesta es necesario reajustar el número de nodos (ya que hemos comprimido la escala). Asumimos 10 nodos y verificamos dicho modelo.El modelo ajustado resulta significativo con una capacidad explicativa del 76.6%. En cuanto las hipótesis del modelo podemos ver como se ha corregido el efecto de embudo obteniendo un modelo que verifica las hipótesis de partida.Ejemplo. Realizamos el diagnóstico del modelo obtenido para los datos de producción.se detecta ningún problema con las hipótesis del modelo y podemos pasar establecer la predicción para el modelo ajustado.Ejemplo. Realizamos el diagnóstico del modelo para los datos de infiltración.Todos los gráficos de diagnóstico parecen indicar que los residuos cumplen con las hipótesis del modelo (qq-plot, Resids vs linear pred., histogram residuals, Response vs. ftted values). Por otro lado, el análisis del parámetro de suavizado indica que el ajuste obtenido es adecuado, encontrando problemas importantes.","code":"\ngam.check(fit.M10)## \n## Method: GCV   Optimizer: magic\n## Smoothing parameter selection converged after 17 iterations.\n## The RMS GCV score gradient at convergence was 5.831016e-05 .\n## The Hessian was positive definite.\n## Model rank =  58 / 58 \n## \n## Basis dimension (k) checking results. Low p-value (k-index<1) may\n## indicate that k is too low, especially if edf is close to k'.\n## \n##               k'   edf k-index p-value\n## s(Solar.R) 19.00  1.07    1.06    0.72\n## s(Wind)    19.00  3.59    1.22    0.99\n## s(Temp)    19.00 16.39    1.09    0.81\n# Modelo\nfit.M11 <- gam(sqrt(Ozone) ~ s(Solar.R, k = 10, m = 2, bs = \"ps\") + \n                 s(Wind, k = 10, m = 2, bs = \"ps\") +\n                 s(Temp, k = 10, m = 2, bs = \"ps\"), data = airquality)\n# Bondad del ajuste\nsummary(fit.M11)## \n## Family: gaussian \n## Link function: identity \n## \n## Formula:\n## sqrt(Ozone) ~ s(Solar.R, k = 10, m = 2, bs = \"ps\") + s(Wind, \n##     k = 10, m = 2, bs = \"ps\") + s(Temp, k = 10, m = 2, bs = \"ps\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   6.0165     0.1165   51.62   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##              edf Ref.df      F  p-value    \n## s(Solar.R) 2.145  2.655  7.684 0.000295 ***\n## s(Wind)    2.461  3.049 12.871 5.32e-07 ***\n## s(Temp)    3.862  4.622 15.929  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.747   Deviance explained = 76.6%\n## GCV = 1.6482  Scale est. = 1.5076    n = 111\n# Diagnóstico\ngam.check(fit.M11)## \n## Method: GCV   Optimizer: magic\n## Smoothing parameter selection converged after 7 iterations.\n## The RMS GCV score gradient at convergence was 1.329944e-06 .\n## The Hessian was positive definite.\n## Model rank =  28 / 28 \n## \n## Basis dimension (k) checking results. Low p-value (k-index<1) may\n## indicate that k is too low, especially if edf is close to k'.\n## \n##              k'  edf k-index p-value  \n## s(Solar.R) 9.00 2.15    0.97    0.35  \n## s(Wind)    9.00 2.46    0.98    0.35  \n## s(Temp)    9.00 3.86    0.84    0.03 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ngam.check(fit.plantas.M6)## \n## Method: GCV   Optimizer: magic\n## Smoothing parameter selection converged after 5 iterations.\n## The RMS GCV score gradient at convergence was 3.261881e-07 .\n## The Hessian was positive definite.\n## Model rank =  20 / 20 \n## \n## Basis dimension (k) checking results. Low p-value (k-index<1) may\n## indicate that k is too low, especially if edf is close to k'.\n## \n##                          k'  edf k-index p-value\n## s(Densidad):LocalidadA 9.00 2.54    1.08    0.75\n## s(Densidad):LocalidadB 9.00 4.55    1.08    0.73\ngam.check(fit.cargahid)## \n## Method: GCV   Optimizer: magic\n## Smoothing parameter selection converged after 7 iterations by steepest\n## descent step failure.\n## The RMS GCV score gradient at convergence was 1.786612e-05 .\n## The Hessian was positive definite.\n## Model rank =  30 / 30 \n## \n## Basis dimension (k) checking results. Low p-value (k-index<1) may\n## indicate that k is too low, especially if edf is close to k'.\n## \n##                               k'  edf k-index p-value\n## s(profundidad):tratamientoA 9.00 4.03     1.1    0.78\n## s(profundidad):tratamientoB 9.00 7.22     1.1    0.70\n## s(profundidad):tratamientoC 9.00 6.86     1.1    0.70"},{"path":"smooth.html","id":"predicción-3","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.5 Predicción","text":"Una vez analizados y elegido el mejor modelo nos resta la fase de predicción. En este caso al disponer de una función paramétrica que relaciona la respuesta con las predictoras podemos obtener las ecauciones de predicción mediante una ecuación específica. Utilizamos las herramientas gráficas para ver los diferentes tipos de predicciones posibles.Ejemplo. Obtenemos la predicción para el modelo de calidad del aire. En primer lugar obtenemos las predicciones marginales asociadas cada una de las predictoras presentes en el modelo.En estos gráficos podemos ver el efecto de cada predictora con respecto la respuesta partir del suavizado estimado. En ellos podemos ver:el nivel de ozono aumenta con la radiación solar (hasta el valor de 250 aproximadamente), y luego empieza caer lentamente.El nivel de ozon disminuye con el aumento de la velocidad del viento hasta casi hacerse constante.El nivel de ozono se mantiene casi constante hasta una temperatura de 70, momento en el que empieza crecer hasta la temperatura de 90, caundo comienza ser casi constante de nuevo.Además, de los gráficos marginales podemos obtener gráficos conjuntos para dos variables sin más que estudiar los efectos combinados de ambas variables. En estos gráficos se crean escenarios con respecto una de las variables numéricas (valor bajo, valor intermedio, y valor alto) y se representa la evolución de la otra predictora.Podemos ver cada uno de los escenarios obtenidos lo que nos permite estudiar claramente el comportamiento de la respuesta. En realidad se reproducen los resultados independientes pero para diferentes valores (escenarios) de una de las predictoras numéricas consideradas. ¿Qué información obtenemos de dichos gráficos?Ejemplo. Para los datos de producciónos nos limitamos representar gráficamente la predicción del modelo ajustado, junto con sus intervalos de confianza.Se puede ver como la producción disminuye conforme aumenta la densidad de plantas, siendo el comportamiento lineal y distinto en cada localidad. De hecho, la producción siempre es inferior en la localidad B frente la en todo el rango de densidades de plantas.Ejemplo. Obtenemos y analizamos las funciones de predicción para los datos de infiltración.Los suavizados obtenidos se comportan adecuadamente al reproducir la tendencia observada. Se pueden ver las diferentes curvas de predicción en función del tratamiento utilizado.","code":"\nplot_model(fit.M11, \"pred\",\n           title =\"Predicción de la media de la raiz cuadarada de ozono\")## $Solar.R## \n## $Wind## \n## $Temp\nplot_model(fit.M11, \"pred\", terms = c(\"Solar.R\", \"Wind\"),\n           title =\"Predicción de la media de la raiz cuadarada de ozono\")\nplot_model(fit.M11, \"pred\", terms = c(\"Solar.R\", \"Temp\"),\n           title =\"Predicción de la media de la raiz cuadarada de ozono\")\nplot_model(fit.M11, \"pred\", terms = c(\"Wind\", \"Temp\"),\n           title =\"Predicción de la media de la raiz cuadarada de ozono\")"},{"path":"smooth.html","id":"ejercicios-2","chapter":"Unidad 10 Modelos aditivos lineales","heading":"10.6 Ejercicios","text":"En todos los ejercicios se debe proponer un modelo alterantivo basado en modelos de suavizado frente al modelo que sería de uso habitual. Se deberan comaparar ambos modelos para establecer si el modelo de suavizado es necesario.Ejercicio 1. Una empresa dedicada la fabricación de aislantes térmicos y acústicos establece un experimento que mide la pérdida de calor (Calor) través de cuatro tipos diferentes de cristal para ventanas (Cristal) utilizando cinco graduaciones diferentes de temperatura exterior (TempExt). Se prueban tres hojas de cristal en cada graduación de temperatura, y se registra la pérdida de calor para cada hoja.Ejercicio 2. Treinta aleaciones del tipo 90/10 Cu-Ni, cada una con un contenido específico de hierro son estudiadas bajo un proceso de corrosión. Tras un período de 60 días se obtiene la pérdida de peso (en miligramos al cuadrado por decímetro y día) de cada una de las aleaciones debido al proceso de corrosión. El objetivo es estudiar el nivel de corrosión en función del contenido de hierro. continuación se presenta el banco de datos y se realiza la primera inspección gráfica.Ejercicio 3. Se ha realizado un experimento para tratar de conocer la viscosidad de cierto compuesto en función de la cantidad de un tipo der aceite que se usa en su fabricación. Se asume una relación de tipo lineal entre la viscosidad y la cantidad de aceite utilizada.Ejercicio 4. Se realiza un estudio de campo para conocer el desarrollo de cierta especie de pez del lago lakemary en EEUU. Para medir el desarrollo se establece la edad de cada pez capturado mediante un procedimiento proporcionado por los biólogos. Además se mide la longitud del pez para tratar de establecer el estado de maduración de cada ejemplar. La investigación trata de relacionar la edad el pez partir de su longitud para determinar el número de capturas permitidas. Las variables recogidas son: “Age” (edad del pez), y “Length” (longitud del pez en mm).Ejercicio 5. Es bien sabido que la concentración de colesterol en el suero sanguíneo aumenta con la edad, pero es menos claro si el nivel de colesterol también está asociado con el peso corporal. Los datos muestran para una treinta de mujeres el colesterol sérico (milimoles por litro), la edad (años) y el índice de masa corporal (peso dividido por la altura al cuadrado, donde el peso se midió en kilogramos y la altura en metros). Se trata de construir un modelo que explique el nivel de colesterol en función de la edad y del índice de masas corporal. Los datos corresponden con la tabla 6.17 de Dobson (2002).Ejercicio 6. Los datos muestran el porcentaje de calorías totales obtenidas de carbohidratos complejos, para veinte diabéticos dependientes de insulina que habían seguido una dieta alta en carbohidratos durante seis meses. Se consideró que el cumplimiento del régimen estaba relacionado con la edad (en años), age, el peso corporal (relativo al peso “ideal” para la altura), weight, y otros componentes de la dieta como el porcentaje de proteínas ingeridas. Los datos corresponden con la tabla 6.3 de Dobson (2002).","code":"\n# Lectura de datos\nejer01 <- read_csv(\"https://goo.gl/V6hyVW\", col_types = \"ddc\") \nejer01 <- ejer01 %>%\n  mutate_if(sapply(ejer01,is.character),as.factor) \nhierro <- c(0.01, 0.48, 0.71, 0.95, 1.19, 0.01, 0.48, 1.44, 0.71, \n            1.96, 0.01, 1.44, 1.96)\npeso <- c(127.6, 124, 110.8, 103.9, 101.5, 130.1, 122, 92.3, 113.1, \n          83.7, 128, 91.4, 86.2)\nejer02 <- data.frame(hierro,peso)\naceite <- c(0, 12, 24, 36, 48, 60, 0, 12, 24, 36, 48, 60, 0, 12, 24, \n            36, 48, 60, 12, 24, 36, 48, 60)\nviscosidad <- c(26, 38, 50, 76, 108, 157, 17, 26, 37, 53, 83, 124, \n                13, 20, 27, 37, 57, 87, 15, 22, 27, 41, 63)\nejer03 <- data.frame(aceite, viscosidad)\ndata(\"lakemary\")\nejer04 <-lakemary\nejer05 <- read_csv(\"https://goo.gl/EKXWRc\", col_types = \"ddd\")\nejer06 <- read_csv(\"https://goo.gl/Grm8xM\", col_types = \"dddd\")"},{"path":"mmixed.html","id":"mmixed","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"Unidad 11 Modelos Lineales Mixtos","text":"Los modelos mixtos lineales son los más generales de los vistos hasra ahora ya que permiten estudio de las diferentes componentes de variabilidad que determinan el comportamiento de una variable respuesta, es decir, nos permite tner en cuenta efectos intrínsecos del diseño experimental que son de interés directo para el estudio del comportamiento de la respuesta. Aplicaciones de este tipo de modelos son:Estructuras anidadas de datos.Estructuras con medidas repetidas.Otros modelos con diferentes fuentes de variabilidad.Los modelos mixtos surgen por la necesidad de contemplar las diferentes fuentes de variabilidad presentes en un diseño experimental cuando las variables asociadas son de interese principal para estudiara el comportamiento de la respuesta. También se utilizan para representar la estructura jerárquica inherente en muchos diseños experimentales.Estos modelos parten de la idea que los coeficientes del modelo se pueden descomponer en una parte determinista (coeficiente del modelo) y una parte aleatoria, de forma que:\\[V_{Total} = V_{modelo} + V_{diseño} + V_{Residual},\\]que en términos del diseño se escribe como:\\[Y = X\\beta + Zb + \\epsilon\\] donde \\(Z\\) es la matriz de efectos aleatorios asociados al diseño experimental y \\(b\\) el conjunto de coeficientes asociado. De esta forma la variabilidad residual asociada al modelo con efectos fijos se descompone en varaibildiad deñ diseño y la varaibilidad que queda por explicar. Como ocurría en los modelos lineales se deben establecer las hipótesis sobre las componenetes aleatorias del modelo, en este caso:\\[b \\sim N(0, H)\\]\\[\\epsilon \\sim N(0, \\sigma^2 )\\]Para el análisis de estos modelos es necesario instalar las librerías lme4, nlme, y lattice.Antes de presentar los modelos de efectos mixtos lineales más habituales vamos proceer presntar los diferentes ejemplos con los que trabajaremos.","code":"## \n## Attaching package: 'lme4'## The following object is masked from 'package:nlme':\n## \n##     lmList## The following object is masked from 'package:mosaic':\n## \n##     factorize"},{"path":"mmixed.html","id":"ejemplos-13","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.1 Ejemplos","text":"Los ejemplos se han extraido de los disponibles en las linbrerías lme4 y nlme.","code":""},{"path":"mmixed.html","id":"estudio-del-sueño","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.1.1 Estudio del sueño","text":"Este ejmplo contiene los resultados sobre un estudio sobre el tiempo de reacción medio por día de un conjunto de sujetos sometidos diferentes procesos de privación de sueño. En el día 0 los sujetos tuvieron su cantidad normal de sueño. partir de esa noche se les restringió 3 horas de sueño por noche. Las observaciones representan el tiempo de reacción medio en una serie de pruebas realizadas cada día cada sujeto. Las variables consideradas son:Reaction: tiempo medio de reacción de un sejeto en un día específico.Days: dia del ensayo.Subject: identificador del sujeto bajo estudio.En estye caso existe un variable predictora (Days) que trata de explicar el comportamiento de la respuesta, pero el diseño experimental introduce una fuente de variabilidad asociada cada uno de los sujetos considerados que debería ser tenida en cuenta. Dado que los sujetos son una muestra aleatoria deberíamos introducir ese efecto como aleatorio y como fijo. Analizamos gráficamente el comportamiento de los datos del estudio. En primer lugar consideramos que cada observación recogida es independiente, es decir, nos olvidamos de la variable sujeto. Se puede ver como el tiempo de reacción aumneta con el paso de los días.Veamos que pasa cuando consideramos el efecto aleatorio del sujeto;Ahora se puede ver que todos los sujetos se comportan de la misma forma, ya que hay algunos donde la evolución con los días es más pronunciada que en otros. Además se puede ver que al incio del estudio los tiempos de reacción también son diferentes entre los sujetos considerados. Por stos motivos es necesario contemplar la introducción de un efecto aleatorio que represente los sujetos y permita estimar de forma correcta el comportamiento de cada uno de ellos.","code":"\ndata(sleepstudy)\nstr(sleepstudy)## 'data.frame':    180 obs. of  3 variables:\n##  $ Reaction: num  250 259 251 321 357 ...\n##  $ Days    : num  0 1 2 3 4 5 6 7 8 9 ...\n##  $ Subject : Factor w/ 18 levels \"308\",\"309\",\"310\",..: 1 1 1 1 1 1 1 1 1 1 ...\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  xlab(\"Días\") +\n  ylab(\"Tiempo de reacción\") \nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  xlab(\"Días\") +\n  ylab(\"Tiempo de reacción\") +\n  facet_wrap(~ Subject, ncol = 6)"},{"path":"mmixed.html","id":"estudio-de-ortodoncia","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.1.2 Estudio de ortodoncia","text":"El conjunto de datos de Ortodoncia contiene 108 filas y 4 variables. El objetivo es estudair el cambio en una medida de ortodoncia lo largo del tiempo para varios sujetos jóvenes. En concreto las variables consideradas son:distance: distancia desde la pituitaria hasta la fisura pterigomaxilar (en mm). Estas distancias se miden en imágenes de rayos X del cráneo.age: edad del sujeto ( en años). Tenemso cuatro mediciones por sujeto e las edades de 8, 10, 12 y 14 años.Subject: factor ordenado indicando el sujeto en el que se realizó la medición. Los niveles están etiquetados de M01 M16 para los hombres y de F01 F13 para las mujeres. La ordenación es por distancia media creciente dentro del sexo.Sex: un factor con niveles Masculino y FemeninoEn este experimento tenemos de nuevo un posible efecto del sujeto considerado. Veamos continuación los datos del experimento y su representación gráfica.Se puede apreciar una tendencia creciente de la distancia con la edad considerando cada fila del banco de datos como independiente del resto. Podemos añadir el efecto del sexo:Ahora además podemos apreciar un posible efecto del sexo. Procedemoe con el gráfico por sujeto. En este caso vamos introducir una aproximación de lo que sería la tendencia para cada uno de los sujetos:Como ocurría en el ejemplo anterior podemos observar que todos los sujetos tienen un comportamiento similar, reforzando el hecho de que hay que tener en cuenta el efecto aleatorio del sujeto en el análisis de este modelo.","code":"\ndata(\"Orthodont\")\nstr(Orthodont)## Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':   108 obs. of  4 variables:\n##  $ distance: num  26 25 29 31 21.5 22.5 23 26.5 23 22.5 ...\n##  $ age     : num  8 10 12 14 8 10 12 14 8 10 ...\n##  $ Subject : Ord.factor w/ 27 levels \"M16\"<\"M05\"<\"M02\"<..: 15 15 15 15 3 3 3 3 7 7 ...\n##  $ Sex     : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 1 1 1 1 1 1 ...\n##  - attr(*, \"outer\")=Class 'formula'  language ~Sex\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##  - attr(*, \"formula\")=Class 'formula'  language distance ~ age | Subject\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##  - attr(*, \"labels\")=List of 2\n##   ..$ x: chr \"Age\"\n##   ..$ y: chr \"Distance from pituitary to pterygomaxillary fissure\"\n##  - attr(*, \"units\")=List of 2\n##   ..$ x: chr \"(yr)\"\n##   ..$ y: chr \"(mm)\"\n##  - attr(*, \"FUN\")=function (x)  \n##   ..- attr(*, \"source\")= chr \"function (x) max(x, na.rm = TRUE)\"\n##  - attr(*, \"order.groups\")= logi TRUE\nggplot(Orthodont, aes(x = age, y = distance)) +\n  geom_point() +\n  xlab(\"Edad\") +\n  ylab(\"Distancia\") \nggplot(Orthodont, aes(x = age, y = distance, color = Sex)) +\n  geom_point() +\n  xlab(\"Edad\") +\n  ylab(\"Distancia\") \nggplot(Orthodont, aes(x = age, y = distance)) +\n  geom_point() +\n  geom_smooth(method = gam, se = FALSE) +\n  xlab(\"Edad\") +\n  ylab(\"Distancia\") +\n  facet_wrap(~ Subject, ncol = 6)## `geom_smooth()` using formula 'y ~ x'"},{"path":"mmixed.html","id":"estudio-de-pixel","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.1.3 Estudio de pixel","text":"El conjunto de datos de pixel tiene 102 filas y 4 columnas donde la variable objetivo registra la intensidad de los píxeles de las tomografías de los perros lo largo del tiempo. Las varaibles consideradas son:Dog: un factor con niveles de 1 10 que designa el perro en el que se realizó la exploración.Side: un factor con niveles L y R que designa el lado escaneado del perro.day: un valor numérico que indica el día posterior la inyección del contraste en el que se realizó la exploración.pixel: un valor numérico que indica la intensidad del píxel.En este caso tenemos dos fuentes de variabilidad aleatoria. La primera hace referencia al sujeto (perro en este caso), y la segunda al lado del perro escaneado. Este segundo efecto está anidado dentro del efecto perro y se deberá teenr en cuenta de esta forma en el modeelo. Veasmo los datos y su representación gráfica:En este caso realizamos únicamente el gráfico que considera todos los efectos.Podemos ver como existe un efecto asociado con e perro y con el lado escaneado. Por ejemplo los perros 7 y 9 tienen comportamiento muy diferente, y dicho comportamiento está asociado únicamente con el día sino con el propio sujeto utilizado.","code":"\ndata(\"Pixel\")\nstr(Pixel)## Classes 'nmGroupedData', 'groupedData' and 'data.frame': 102 obs. of  4 variables:\n##  $ Dog  : Factor w/ 10 levels \"1\",\"10\",\"2\",\"3\",..: 1 1 1 1 1 1 1 3 3 3 ...\n##  $ Side : Factor w/ 2 levels \"L\",\"R\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ day  : num  0 1 2 4 6 10 14 0 1 2 ...\n##  $ pixel: num  1046 1044 1043 1050 1045 ...\n##  - attr(*, \"formula\")=Class 'formula'  language pixel ~ day | Dog/Side\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##  - attr(*, \"formulaList\")=List of 2\n##   ..$ Dog :Class 'formula'  language ~Dog\n##   .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##   ..$ Side:Class 'formula'  language ~Side\n##   .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##  - attr(*, \"labels\")=List of 2\n##   ..$ x: chr \"Time post injection\"\n##   ..$ y: chr \"Pixel intensity\"\n##  - attr(*, \"units\")=List of 1\n##   ..$ x: chr \"(days)\"\n##  - attr(*, \"order.groups\")=List of 2\n##   ..$ Dog : logi TRUE\n##   ..$ Side: logi TRUE\n##  - attr(*, \"FUN\")=function (x)  \n##   ..- attr(*, \"source\")= chr \"function (x) max(x, na.rm = TRUE)\"\nggplot(Pixel, aes(x = day, y = pixel, col = Side)) +\n  geom_point() +\n  xlab(\"Día\") +\n  ylab(\"pixel\") +\n  facet_wrap(~ Dog, ncol = 5)"},{"path":"mmixed.html","id":"estudio-avena","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.1.4 Estudio Avena","text":"Este conjunto de datos hace referencia un estudio para analizar el redimiento de producción de diferentes varaiedades de avena sujetas diferentes condiciones experimentales. El area de estudio se distribuyó en 6 bloques de 3 parcelas principales, cada una de ellas dividida en 4 subparcelas. Cada variedad se aplico en cada una de las parcelas pricipales , mientras que los tratamiento de abonos se aplicaron en cada una de las subparcelas definidas. El banco de datos contiene las siguientes columnas:Block: Bloques considerados con niveles , II, III, IV, V y VI.Variety: Variedades de avena consideradas con 3 niveles.nitro: Tratamiento de nitrógeno (manurial) con niveles 0.0cwt, 0.2cwt, 0.4cwt y 0.6cwt, mostrando la aplicación en cwt/acre.yield: Rendimiento en 1/4lbs por subparcela, cada una de ellas con una superficie de 1/80 acres.En este caso tenemos como fuentes de variabilidad extra el bloque y la variedad aplicada dentro de cada bloque, ya que el objetivo principal es la influencia del tratamiento en la producción final de cada subparcela.Analizamos gráficamente los datos del experimento:¿Qué comportamiento observamos dentro de cada bloque?","code":"\ndata(\"Oats\")\nstr(Oats)## Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':   72 obs. of  4 variables:\n##  $ Block  : Ord.factor w/ 6 levels \"VI\"<\"V\"<\"III\"<..: 6 6 6 6 6 6 6 6 6 6 ...\n##  $ Variety: Factor w/ 3 levels \"Golden Rain\",..: 3 3 3 3 1 1 1 1 2 2 ...\n##  $ nitro  : num  0 0.2 0.4 0.6 0 0.2 0.4 0.6 0 0.2 ...\n##  $ yield  : num  111 130 157 174 117 114 161 141 105 140 ...\n##  - attr(*, \"formula\")=Class 'formula'  language yield ~ nitro | Block\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv> \n##  - attr(*, \"labels\")=List of 2\n##   ..$ y: chr \"Yield\"\n##   ..$ x: chr \"Nitrogen concentration\"\n##  - attr(*, \"units\")=List of 2\n##   ..$ y: chr \"(bushels/acre)\"\n##   ..$ x: chr \"(cwt/acre)\"\n##  - attr(*, \"inner\")=Class 'formula'  language ~Variety\n##   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>\nggplot(Oats, aes(x = nitro, y = yield, col = Variety)) +\n  geom_point() +\n  xlab(\"Concentración\") +\n  ylab(\"Rendimiento\") +\n  facet_wrap(~ Block, ncol = 3)"},{"path":"mmixed.html","id":"estudio-penicilina","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.1.5 Estudio Penicilina","text":"Los datos se describen en Davies y Goldsmith (1972) como procedentes de una investigación para “evaluar la variabilidad entre muestras de penicilina por el método de B. subtilis. En este método de prueba se vierte un medio de agar nutritivo inoculado granel en una placa de Petri de aproximadamente 90 mm. de diámetro, conocida como placa. Cuando el medio ha fraguado, se pegan en la superficie seis pequeños cilindros huecos o macetas (de unos 4 mm. de diámetro) intervalos igualmente espaciados. Se colocan unas gotas de las soluciones de penicilina que se van comparar en los respectivos cilindros, y toda la placa se coloca en una incubadora durante un tiempo determinado. La penicilina se difunde desde los botes al agar, y esto produce una clara zona circular de inhibición del crecimiento de los organismos, que puede medirse fácilmente. El diámetro de la zona está relacionado de forma conocida con la concentración de penicilina en la solución.En concreto se probaron seis muestras de penicilina utilizando el método de la placa de B. subtilis en cada una de 24 placas. La respuesta es el diámetro (mm) de la zona de inhibición del crecimiento del organismo. El banco de datos contiene 144 filas y tres variables:diameter: diámetro (en mm) de la zona de inhibición del crecimiento del organismo.plate: placa de ensayo. Un factor con niveles de hasta x.sample: muestra de penicilina. Un factor con niveles de hasta F.En este caso hay variables predictoras, ya que tanto la muestra y la placa deben ser considerados efectos aleatorios. Veamos los datos y su representación gráfica:","code":"\ndata(\"Penicillin\")\nstr(Penicillin)## 'data.frame':    144 obs. of  3 variables:\n##  $ diameter: num  27 23 26 23 23 21 27 23 26 23 ...\n##  $ plate   : Factor w/ 24 levels \"a\",\"b\",\"c\",\"d\",..: 1 1 1 1 1 1 2 2 2 2 ...\n##  $ sample  : Factor w/ 6 levels \"A\",\"B\",\"C\",\"D\",..: 1 2 3 4 5 6 1 2 3 4 ...\nggplot(Penicillin, aes(x = diameter, y = plate, col = sample)) +\n  geom_point() +\n  xlab(\"Diameter\") +\n  ylab(\"Sample\") "},{"path":"mmixed.html","id":"modelos-teóricos","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.2 Modelos teóricos","text":"En este punto mostramos los modelos lineales de efctos aletorios más extendidos en la literatura. Comenzamos con la situación más sencilla donde tenemos una única variable predictora y donde se integran diferentes efectos aleatorios. Para el ajuste de estos modelos usamos la función lmer de la librería lme4.","code":""},{"path":"mmixed.html","id":"modelo-con-interceptación-aleatoria","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.2.1 Modelo con interceptación aleatoria","text":"Los modelos de interceptación aleatoria son aquellos donde se considera que exite una relación lineal entre \\(Y\\) y \\(X\\) para cada una de las unidades experimentales, pero donde la recta asociada cada sujeto parte de un punto diferente (interceptación aleatoria). En este caso consideramos como efecto aleatorio la variable subject. La expresión del modelo en R viene dada por:","code":"\nlmer(Y ~ X + (1 | subject), datos)"},{"path":"mmixed.html","id":"modelo-con-interceptación-y-pendiente-aleatoria","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.2.2 Modelo con interceptación y pendiente aleatoria","text":"Los modelos de interceptación y pendiente aleatoria son aquellos donde se considera que exite una relación lineal entre \\(Y\\) y \\(X\\) para cada una de las unidades experimentales, pero donde la recta asociada cada sujeto parte de un punto diferente (interceptación aleatoria) y tiene una pendiente distinta (pendiente aleatoria). Consideramos de nuevo el efecto aleatorio subject. La expresión del modelo en R viene dada por:","code":"\nlmer(Y ~ X + (X | subject), datos)"},{"path":"mmixed.html","id":"modelo-con-interceptación-aletaoria-y-efectos-anidados","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.2.3 Modelo con interceptación aletaoria y efectos anidados","text":"Los modelos de interceptación aleatoria y efectos anidados son aquellos donde se considera que exite una relación lineal entre \\(Y\\) y \\(X\\) para cada una de las unidades experimentales, pero donde la recta asociada cada sujeto parte de un punto diferente (interceptación aleatoria) asociado un dos efectos aleatorios donde uno de ellos está anidado en el otro. En este caso consideramos como efecto aleatorio la variable grupo y dentro la variable subject, es decir los sujetos dentro de cada grupo pueden comportarse de forma diferente. La expresión del modelo en R viene dada por:","code":"\nlmer(Y ~ X + (1 | grupo/subject), datos)"},{"path":"mmixed.html","id":"modelo-con-interceptación-pendiente-aleatoria-y-efectos-anidados","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.2.4 Modelo con interceptación, pendiente aleatoria y efectos anidados","text":"Como el modelo anterior pero considerando que las pendientes también son aleatorias. La expresión del modelo en R viene dada por:","code":"\nlmer(Y ~ X + (X | grupo/subject), datos)"},{"path":"mmixed.html","id":"modelo-con-interceptación-aletaoria-y-efectos-aditivos","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.2.5 Modelo con interceptación aletaoria y efectos aditivos","text":"Los modelos de interceptación aleatoria y efectos aditivos son aquellos donde se considera que exite una relación lineal entre \\(Y\\) y \\(X\\) para cada una de las unidades experimentales, pero donde la recta asociada cada sujeto parte de un punto diferente (interceptación aleatoria) asociado la suma de varios efectos aletorios. En este caso consideramos como efectos aleatorios la variable grupo y la variable muestra, es decir se suma el efcto del grupo al efecto de la muestra. La expresión del modelo en R viene dada por:","code":"\nlmer(Y ~ X + (1 | grupo) + (1 | muestra), datos)"},{"path":"mmixed.html","id":"ajuste-y-comparación-de-modelos","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.3 Ajuste y comparación de modelos","text":"En este apartado obtenemos el ajuste de los diferentes modelos que podemos plantear para cada uno de los ejemplos presentados en apartados anteriores. En concreto, veremos la estimación e interpretación de las componentes de cada uno de los modelos planteados y veremos como comparalos utilizando el test F.","code":""},{"path":"mmixed.html","id":"estudio-del-sueño-1","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.3.1 Estudio del sueño","text":"Para los datos de estudio de sueño, y la vista de los gráficos realizados podemos plantear los modelos siguientes:Modelo sin efecto aletorio (Reaction vs Days).Modelo con interceptación aletoria por sujeto. Ajustamos una recta para cada sujeto donde partímos de interceptaciones diferentes.Modelo con interceptación y pendientes aletorias por sujeto. Ajustamos una recta para cada sujeto donde partímos de interceptaciones diferentes y la evolución (pendiente) para cada uno de ellos puede ser diferente.Analizamos individualmente cada uno de los modelos antes de proceder con una comparación formal.El ajuste obtenido nos proporciona la recta:\\[\\widehat{Reaction} = 251.41 + 10.47*Days\\]mostrando que el tiempo de reacción aumenta con el paso de los días y la reducción del número de horas de sueño, mientras que la desviación típica del error se sitúa en el valor 47.71, lo que indica que la parte sin explicar del modelo es muy superior la parte explicada (\\(R^2\\) pequeño). La solución gráfica de este modelo viene dada por:Para el modelo con interceptación aleatoria (fit2) tenemos que:En la parte de efectos fijos (comportamiento promedio de todos los sujetos) podemos ver que el modelo ajustado viene dado por la misma expresión que en el modelo anterior:\\[\\widehat{Reaction} = 251.41 + 10.47*Days\\]mientras que con respecto los efectos aleatorios podemos ver que la desviación típica residual del error se ha reducido hasta 30.99, y la desviación tipica asociada al efecto alaetorio de la interceptación es de 37.12, es decir, explicamos el 54.5% de la variabilidad total observada través de la introducción del efecto aleatorio lo que redundará en mejores estimaciones y un modelo para cada sujeto.Representamos loe efectos fijos y aleatorios asociados cada sujeto (ahora tenemos un modelo individual por sujeto).En el gráfico de efectos fijos podemos ver que el coeficiente asociado Days es constante, mientars que la interceptación es diferente por sujeto, dado que hemos introducido un efecto aleatorio sobre dicho coeficiente. Este efecto se ve claramente en el gráfico de efectos aleatorios donde podemos ver los cambios en la interceptación de cada uno de los sujetos, indicando que cada sujeto parte de un punto distinto.Veamos ahora la predicción del modelo:Podemos ver que todas las rectas de predicción tienen la misma pendiente pero parten de interceptaciones distintas.Para el modelo con interceptación y pendiente aleatoria (fit3) tenemos que:De nuevo la ecaución asociada con los efectos fijos es la misama que en los modelos anteriores pero podemos ver que la descomposición de la variabilidad asociada con las componentes aleatorias nos indica que la variabilidad asociada con el error es 25.592, mientras que con respecto la interceptación es de 24.741, y la pendiente de 5.922. Por tanto, la varaibildiad asociada con los efctos aletaorios reepresenta más del 50% de la varaibilidad observada, lo que indica que ambos efectos son necesarios, ya que mejoramos los resultados del modelo anterior. También podemos ver que los efectos aleatorios pueden considerarse independientes ya que su correlación es de 0.07.Representamos loe efectos fijos y aleatorios asociados cada sujeto (ahora tenemos un modelo individual por sujeto).Podemos ver como la introducción de ambos efectos aleatorios nos porporcionan interceptaciones y pendientes distintas para cada sujeto. De hecho, podemos ver que hay sujetos con pendientes que son prácticamente cero, y otros donde la pendiente es muy pronunciada (vinculada con los efectos aletorios postivos más grandes). El gráfico de predicción para este modelo viene dado por:En el gráfico se aprecia claramente que las rectas estimadas por cada sujeto varían tanto en interceptación como en pendiente.Podemos comparar los modelos obtenidos medainte el estadístico AIC, donde podremos ver que el mejor modelo es el fit3 (menor valor de AIC):Por último, podemos comparar los modelos de efectos aleatorios para determinar estadísticamente si es preferible el modelo con pendeientes aleatorias frente al modelo sin pendientes aleatorias.El p-valor obtenido muestra que es necesario considerar el modelo más complejo (interceptaciones y pendientes aleatorias).","code":"\nfit1 <- lm(Reaction ~ Days, sleepstudy)\nfit2 <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)\nfit3 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\nsummary(fit1) ## \n## Call:\n## lm(formula = Reaction ~ Days, data = sleepstudy)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -110.848  -27.483    1.546   26.142  139.953 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  251.405      6.610  38.033  < 2e-16 ***\n## Days          10.467      1.238   8.454 9.89e-15 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 47.71 on 178 degrees of freedom\n## Multiple R-squared:  0.2865, Adjusted R-squared:  0.2825 \n## F-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15\nsleepstudy$predic1 <- predict(fit1)\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(cex = 0.7) +\n  geom_line(aes(Days,predic1), col =\"red\") +\n  xlab(\"Días\") +\n  ylab(\"Tiempo de reacción\") \nsummary(fit2) ## Linear mixed model fit by REML ['lmerMod']\n## Formula: Reaction ~ Days + (1 | Subject)\n##    Data: sleepstudy\n## \n## REML criterion at convergence: 1786.5\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.2257 -0.5529  0.0109  0.5188  4.2506 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  Subject  (Intercept) 1378.2   37.12   \n##  Residual              960.5   30.99   \n## Number of obs: 180, groups:  Subject, 18\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept) 251.4051     9.7467   25.79\n## Days         10.4673     0.8042   13.02\n## \n## Correlation of Fixed Effects:\n##      (Intr)\n## Days -0.371\n# Efectos fijos\ndotplot(coef(fit2))## $Subject\n# Efectos aleatorios\ndotplot(ranef(fit2))## $Subject\nsleepstudy$predic2 <- predict(fit2)\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(cex = 0.7) +\n  geom_line(aes(Days,predic2), col =\"red\") +\n  xlab(\"Días\") +\n  ylab(\"Tiempo de reacción\") +\n  facet_wrap(~ Subject, ncol = 6)\nsummary(fit3) ## Linear mixed model fit by REML ['lmerMod']\n## Formula: Reaction ~ Days + (Days | Subject)\n##    Data: sleepstudy\n## \n## REML criterion at convergence: 1743.6\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.9536 -0.4634  0.0231  0.4634  5.1793 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr\n##  Subject  (Intercept) 612.10   24.741       \n##           Days         35.07    5.922   0.07\n##  Residual             654.94   25.592       \n## Number of obs: 180, groups:  Subject, 18\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept)  251.405      6.825  36.838\n## Days          10.467      1.546   6.771\n## \n## Correlation of Fixed Effects:\n##      (Intr)\n## Days -0.138\n# Efectos fijos\ndotplot(coef(fit3))## $Subject\n# Efectos aleatorios\ndotplot(ranef(fit3))## $Subject\nsleepstudy$predic3 <- predict(fit3)\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point(cex = 0.7) +\n  geom_line(aes(Days,predic3), col =\"red\") +\n  xlab(\"Días\") +\n  ylab(\"Tiempo de reacción\") +\n  facet_wrap(~ Subject, ncol = 6)\nAIC(fit1); AIC(fit2); AIC(fit3)## [1] 1906.293## [1] 1794.465## [1] 1755.628\nanova(fit2, fit3)## refitting model(s) with ML (instead of REML)## Data: sleepstudy\n## Models:\n## fit2: Reaction ~ Days + (1 | Subject)\n## fit3: Reaction ~ Days + (Days | Subject)\n##      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \n## fit2    4 1802.1 1814.8 -897.04   1794.1                         \n## fit3    6 1763.9 1783.1 -875.97   1751.9 42.139  2  7.072e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"mmixed.html","id":"estudio-de-ortodoncia-1","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.3.2 Estudio de ortodoncia","text":"Para estos datos queremos explicar el comportamiento de la distancia en función de la edad, teniendo en cuenta que el crecimiento para cada sujeto puede ser diferente. Dicho efecto puede actuar sobre la interceptación, o sobre la interceptación y la pendiente. Consideramos los modelos:Comparamos ambos modelos antes de proceder con su análisisDado que el p-valor resulta significativo podemos considerar que ambos modelos son iguales y optamos por quedarnos con el más sencillo, que sólo contempla un efecto aleatorio sobre la interceptación. Analizamos el modelo considerado:El modelo para el comportamiento promedio (obtenido mediante los efectos fijos del modelo) viene dado por:\\[\\widehat{distance} = 16.76 + 0.66*age\\] Por otro lado, la variabilidad asociada con el efecto aletorio considerado (2.12) es superior la asociado con los residuos (1.43), con lo que la consideración del efecto aletorio es muy conveniente en este modelo, ya que reducimos sensiblemente el error aleatorio del modelo.Representamos loe efectos fijos y aleatorios asociados cada sujeto (ahora tenemos un modelo individual por sujeto).En los efectos aletorios obtenidos podemos ver las diferencias entre los diferentes sujetos del estudio, con sujetos con valores muy bajos y otros con valores muy altos.El gráfico de predicción para este modelo viene dado por:Todas las rectas estimadas tienen la msima pendiente pero parten de puntos iniciales diferentes.","code":"\nfit1 <- lmer(distance ~ age + (1 | Subject), Orthodont)\nfit2 <- lmer(distance ~ age + (age | Subject), Orthodont)\nanova(fit1, fit2)## refitting model(s) with ML (instead of REML)## Data: Orthodont\n## Models:\n## fit1: distance ~ age + (1 | Subject)\n## fit2: distance ~ age + (age | Subject)\n##      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\n## fit1    4 451.39 462.12 -221.69   443.39                     \n## fit2    6 451.21 467.30 -219.61   439.21 4.1779  2     0.1238\nsummary(fit1)## Linear mixed model fit by REML ['lmerMod']\n## Formula: distance ~ age + (1 | Subject)\n##    Data: Orthodont\n## \n## REML criterion at convergence: 447\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.6645 -0.5351 -0.0129  0.4874  3.7218 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  Subject  (Intercept) 4.472    2.115   \n##  Residual             2.049    1.432   \n## Number of obs: 108, groups:  Subject, 27\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept) 16.76111    0.80240   20.89\n## age          0.66019    0.06161   10.72\n## \n## Correlation of Fixed Effects:\n##     (Intr)\n## age -0.845\n# Efectos fijos\ndotplot(coef(fit1))## $Subject\n# Efectos aleatorios\ndotplot(ranef(fit1))## $Subject\nOrthodont$predic <- predict(fit1)\nggplot(Orthodont, aes(x = age, y = distance)) +\n  geom_point(cex = 0.7) +\n  geom_line(aes(age,predic), col =\"red\") +\n  xlab(\"Edad\") +\n  ylab(\"Distancia\") +\n  facet_wrap(~ Subject, ncol = 6)"},{"path":"mmixed.html","id":"estudio-de-pixel-1","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.3.3 Estudio de pixel","text":"Para estos datos queremos explicar el comportamiento de la intensidad del pixel en función del día, y consideramos el efecto aletorio del perro y del lado del perro muestreado como un efectoa anidado dentro de él. Además como la tendencia media se correspondía exactamente con una recta sino más bien como un polinomio en gtado dos para el día, consideraremos también la incorporación de este efecto. Los modelos considerados son:En este caso podemos comparar todos los modelos de golpe mediante el test F porque están anidados entre ellos. Utilizamos el estadístico AIC para determinar el mejor de ellos:El valor más bajo se obtiene para el modelo polinómico con interceptación y pendiente aleatoria. Analizamos el modelo obtenido:El modelo para la pobalción media viene dado por:\\[\\widehat{pixel} = 1073.36 + 6.13*Day - 0.37*Day^2\\]Mientars que podemos ver como la suma de las variabildiades asociadas todos los efectos aleatorios es muy superior la variabilidad residual. Las variabildiades más grandes corresponden la interceptación en ambos efectos (Dog y Dog/Side), indicando que la mayor diferencia se da entre los diferentes perros, pero que también el lado escaneado es relevante. Los efectos sobre las pendientes son mucho más pequeños debido que la consideración del polinomio sobre la tendencia media hace que dicho efecto sea mucho más pequeño.Representamos los efectos aleatorios asociados cada perro.Por último vemos la predicción conseguida:¿qué podemos decir sobre la predicción obtenida?","code":"\n# Modelo lineal con interceptación aleatoria\nfit1 <- lmer(pixel ~ day + (1 | Dog/Side), Pixel)\n# Modelo lineal con interceptación y pendiente aleatoria\nfit2 <- lmer(pixel ~ day + (day | Dog/Side), Pixel)## boundary (singular) fit: see help('isSingular')\n# Modelo polinómico con interceptación aleatoria\nfit3 <- lmer(pixel ~ day + I(day^2) + (1 | Dog/Side), Pixel)\n# Modelo polinómico con interceptación y pendiente aleatoria\nfit4 <- lmer(pixel ~ day + I(day^2) + (day | Dog/Side), Pixel)## boundary (singular) fit: see help('isSingular')\nAIC(fit1);AIC(fit2);AIC(fit3);AIC(fit4)## [1] 899.5381## [1] 908.8714## [1] 876.839## [1] 842.0051\nsummary(fit4)## Linear mixed model fit by REML ['lmerMod']\n## Formula: pixel ~ day + I(day^2) + (day | Dog/Side)\n##    Data: Pixel\n## \n## REML criterion at convergence: 822\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.95926 -0.52629 -0.00073  0.47547  2.87733 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev. Corr \n##  Side:Dog (Intercept) 108.2123 10.4025       \n##           day           0.9771  0.9885  1.00 \n##  Dog      (Intercept) 894.1972 29.9031       \n##           day           2.9232  1.7097  -0.67\n##  Residual              75.7829  8.7053       \n## Number of obs: 102, groups:  Side:Dog, 20; Dog, 10\n## \n## Fixed effects:\n##              Estimate Std. Error t value\n## (Intercept) 1073.3610    10.1563 105.684\n## day            6.1309     0.8652   7.086\n## I(day^2)      -0.3679     0.0329 -11.182\n## \n## Correlation of Fixed Effects:\n##          (Intr) day   \n## day      -0.516       \n## I(day^2)  0.181 -0.657\n## optimizer (nloptwrap) convergence code: 0 (OK)\n## boundary (singular) fit: see help('isSingular')\n# Efectos aleatorios\ndotplot(ranef(fit4))## $`Side:Dog`## \n## $Dog\nPixel$predic <- predict(fit4)\nggplot(Pixel, aes(x = day, y = pixel)) +\n  geom_point(aes(col = Side)) +\n  geom_line(aes(day, predic, col = Side)) +\n  xlab(\"Día\") +\n  ylab(\"pixel\") +\n  facet_wrap(~ Dog, ncol = 5)"},{"path":"mmixed.html","id":"estudio-de-avena","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.3.4 Estudio de avena","text":"Para estos datos queremos explicar el comportamiento del rendimiento en función de la concentración de nitrógeno y la variedad teniendo en cuenta el diseño experimental que establece un efecto aleatorio por Bloque y la variedad dentro del bloque. En la parte de eectos fijos debemos tener en cuenta que la concentración de Nitrógeno es una factor ordenado, y que debemos valorar la posible interacción entre la varaiedad y la concentración de nitrógeno. Para idicar que tenemos un factor ordenado utilizaremos la función ordered. Consideramos los modelo siguientes:Valoramos con el test F ambos modelos:Dado que el p-valor resulta significativo nos quedamos con el modelo más simple que considera el efecto de interacción. Analizamos el modelo obtenido:Como tenemos cuatro dosis nitrógeno el modelo ajusta tres efectos polinómicos asociados con dicho efecto (L = lineal, Q = cuadrático, y C = cúbico). Además podemos ver las diferencias entre las tres variaedades consideradas. En cuanto los efectos aletorios podemos ver que la varainza residual es la mitad de las varainzas que obtenemos al introducir los efectos aleatorios.En este caso vamos ver directamente la predicción del modelo obtenida:","code":"\n# Modelo lineal sin interacción con interceptación aleatoria\nfit1 <- lmer(yield ~ ordered(nitro) + Variety + (1 | Block/Variety), Oats)\n# Modelo lineal con interacción con interceptación aleatoria\nfit2 <- lmer(yield ~ ordered(nitro) * Variety + (1 | Block/Variety), Oats)\nanova(fit1, fit2)## refitting model(s) with ML (instead of REML)## Data: Oats\n## Models:\n## fit1: yield ~ ordered(nitro) + Variety + (1 | Block/Variety)\n## fit2: yield ~ ordered(nitro) * Variety + (1 | Block/Variety)\n##      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\n## fit1    9 616.04 636.53 -299.02   598.04                     \n## fit2   15 625.91 660.06 -297.95   595.91 2.1375  6     0.9066\nsummary(fit1)## Linear mixed model fit by REML ['lmerMod']\n## Formula: yield ~ ordered(nitro) + Variety + (1 | Block/Variety)\n##    Data: Oats\n## \n## REML criterion at convergence: 569.5\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -1.84135 -0.66280 -0.06694  0.63822  1.66067 \n## \n## Random effects:\n##  Groups        Name        Variance Std.Dev.\n##  Variety:Block (Intercept) 109.7    10.47   \n##  Block         (Intercept) 214.5    14.65   \n##  Residual                  162.6    12.75   \n## Number of obs: 72, groups:  Variety:Block, 18; Block, 6\n## \n## Fixed effects:\n##                   Estimate Std. Error t value\n## (Intercept)       104.5000     7.7975  13.402\n## ordered(nitro).L   32.9447     3.0052  10.963\n## ordered(nitro).Q   -5.1667     3.0052  -1.719\n## ordered(nitro).C   -0.4472     3.0052  -0.149\n## VarietyMarvellous   5.2917     7.0789   0.748\n## VarietyVictory     -6.8750     7.0789  -0.971\n## \n## Correlation of Fixed Effects:\n##             (Intr) or().L or().Q or().C VrtyMr\n## ordrd(nt).L  0.000                            \n## ordrd(nt).Q  0.000  0.000                     \n## ordrd(nt).C  0.000  0.000  0.000              \n## VartyMrvlls -0.454  0.000  0.000  0.000       \n## VarityVctry -0.454  0.000  0.000  0.000  0.500\nOats$predic <- predict(fit1)\nggplot(Oats, aes(x = nitro, y = yield)) +\n  geom_point(aes(col = Variety)) +\n  geom_line(aes(nitro, predic, col = Variety)) +\n  xlab(\"Concentración\") +\n  ylab(\"Rendimiento\") +\n  facet_wrap(~ Block, ncol = 3)"},{"path":"mmixed.html","id":"estudio-de-penicilina","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.3.5 Estudio de penicilina","text":"Para estos datos existe un variable predictora definida ya que tanto sample como plate corresponden al diseño experimental pero pensamos que incidan directamente en la respuesta. que trata d explicar el comportamiento de la respuesta. Las variabilidades consideradas son las asociadas con la estructura experimental y en este caso sólo piden influir en el efecto común dado que hay variables predictora.¿cómo analizamos los resultados de este modelo?","code":"\nfit1 <- lmer(diameter ~ 1 + (1 | plate) + (1 | sample), Penicillin)\nsummary(fit1)## Linear mixed model fit by REML ['lmerMod']\n## Formula: diameter ~ 1 + (1 | plate) + (1 | sample)\n##    Data: Penicillin\n## \n## REML criterion at convergence: 330.9\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.07923 -0.67140  0.06292  0.58377  2.97959 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  plate    (Intercept) 0.7169   0.8467  \n##  sample   (Intercept) 3.7311   1.9316  \n##  Residual             0.3024   0.5499  \n## Number of obs: 144, groups:  plate, 24; sample, 6\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept)  22.9722     0.8086   28.41\n# gráfico de predicción\nPenicillin$predic <- predict(fit1)\nggplot(Penicillin, aes(x = sample, y = diameter)) +\n  geom_point() +\n  geom_point(aes(x = sample, y = predic), col = \"red\") +\n  xlab(\"Sample\") +\n  ylab(\"Diameter\") +\n  facet_wrap(~ plate, ncol = 6)"},{"path":"mmixed.html","id":"diagnóstico-3","chapter":"Unidad 11 Modelos Lineales Mixtos","heading":"11.4 Diagnóstico","text":"Las hipótesis sobre los residuos de este tipo d emodelos son las mismas que sobre el resto de modelos lineales (independencia, linealidad y normalidad). En este caso utilizamos procedimientos gráficos únicamente. Mostramos su uso mediante un ejemplo y dejamos el resto como ejercicios prácticos. Utilizamos los datos del sueño tomando el modelo obtenido en el punto anterior:Obtenemos los gráficos de linealidad y normalidad para determianr si se verifican las hipótesis del modelo:En el primer gráfico se aprecía gran aleatoriedad en los residuos pero con algunos valores bastante grandes. Se podría analizar de nuevo este modelo prescindiendo de esas observaciones. En el segundo gráfico vemos como la distribución de los puntos es alrededor de la hipótetica linea de normalidad. Parece que se verifican ambas hipótesis y por tanto el modelo considerado es válido.","code":"\nfit <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\nplot(fit, type = c(\"p\", \"smooth\"))\nqqmath(fit, id = 0.05)"},{"path":"glm.html","id":"glm","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"Unidad 12 Modelos Lineales Generalizados","text":"Todos los modelos estudiados hasta ahora comparten los mismos supuestos teóricos, dado que el tipo de variable respuesta es siempre de tipo numérico. Aunque este tipo de modelos abarcan un gran conjunto de situaciones experimentales, también es cierto que en algunos casos resulta difícil cumplir las hipótesis establecidas. De hecho, este tipo de modelos se asocian habitualmente con variables respuesta de tipo Normal, pero ¿qué ocurre cuando la variable puede considerarse Normal? ¿Cómo podemos modelizar situaciones experimentales donde la variable respuesta es de tipo Binomial o Poisson?Son este tipo de situaciones las que motivan la introducción de un nuevo tipo de modelos que como caso particular engloban todos los vistos en los temas anteriores. Concretamente estudiaremos situaciones donde:la variable respuesta es de tipo Binomial, es decir, hemos observado si se cumple o cierta condición experimental como resultado de un conjunto de variables predictoras que pueden ser de tipo numérico o categórico.la variable respuesta es de tipo Poisson, es decir, hemos observado el número de ocasiones que ha ocurrido un evento en un tiempo o espacio determinado como resultado de un conjunto de variables predictoras que pueden ser de tipo numérico o categórico.la variable respuesta es de tipo Exponencial, es decir, hemos observado el tiempo transcurrido hasta que ocurre un evento de interés como resultado de un conjunto de variables predictoras que pueden ser de tipo numérico o categórico.la variable respuesta es de tipo numérico, pero sólo puede tomar valores positivos de forma asimétrica, es decir, se encuentra concentrada en un conjunto de valores y su frecuencia disminuye cuando aumenta el valor de la respuesta. Este tipo de variable se conoce como Gamma.La característica común todas estas situaciones es que la respuesta se puede englobar dentro de un conjunto de variables aleatorias cuya función de densidad puede ser escrita través de una misma expresión. Todo este conjunto de variables se conocen con el nombre de familia exponencial.","code":""},{"path":"glm.html","id":"modelo-teórico","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1 Modelo Teórico","text":"Supongamos que hemos observado una situación experimental que involucra \\(n\\) sujetos cuyo conjunto de datos dispone de una variable respuesta \\(Y\\) y \\(p\\) variables predictoras \\(X_1,...,X_p\\). Denotamos por \\(Y_i\\) al valor de la respuesta para el sujeto \\(\\), y \\(x_{i1},...,x_{ip}\\) al valor de las predictoras para el mismo sujeto. Como ya vimos anteriormente el objetivo de cualquier modelo es estudiar el comportamiento de la respuesta partir de la información contenida en las variables predictoras, es decir:\\[E(Y_i|x_{i1},...,x_{ip}) = \\mu_i\\]Los supuestos del GLM son:\\(Y_i\\) son observaciones aleatorias e independientes cuya distribución de probabilidad corresponde la familia exponencial con \\(E(Y_i) = \\mu_i\\).Las variables predictoras proporcionan un conjunto de predictores lineales \\(\\eta_i\\):\\[\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip}\\]Existe una función \\(g()\\) denominada función link (enlace) que establece que\\[g(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip}\\]Es decir, tenemos un modelo lineal donde modelizamos la media de la respuesta sino una función de dicha media. continuación se muestran las funciones link más habituales para los diferentes tipos de variable respuesta.","code":""},{"path":"glm.html","id":"distribución-binomial-logit-link","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1.1 Distribución Binomial (logit link)","text":"En esta situación el valor esperado de la respuesta viene dado por \\(E(Y_i) = n \\pi_i\\), donde \\(\\pi_i\\) es la probabilidad de éxito, y la función link más habitual es:\\[g(\\mu_i) = log\\left(\\frac{\\mu_i}{n - \\mu_i}\\right)\\]\nSi sustituimos el valor de \\(\\mu_i\\) y simplificamos tenemos que:\\[g(\\mu_i) = log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right)\\]\ndonde podemos ver que el objeto de nuestra modelización es una función de la probabilidad de éxito, que en realidad es nuestro parámetro de interés en este tipo de situaciones.","code":""},{"path":"glm.html","id":"ejemplo-3","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1.1.1 Ejemplo","text":"En un experimento se sometió cierto número de cucarachas cinco horas de exposición disulfato de carbono gaseoso varias concentraciones. Se pretendía investigar la relación existente entre la dosis de disulfato administrada y la resistencia de los insectos. La variable respuesta es \\(Y_i\\) = número de escarabajos muertos en un total de ni sometidos una misma dosis de pesticida \\(x_i\\). La distribución habitual es binomial, \\(Y_i ~ Bi(n_i; \\pi_i)\\) para \\(= 1,...,n\\), con \\(\\pi_i\\) = probabilidad de muerte dosis \\(x_i\\).","code":""},{"path":"glm.html","id":"distribución-poisson-log-link","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1.2 Distribución Poisson (log link)","text":"En esta situación el valor esperado de la respuesta viene dado por \\(E(Y_i) = \\mu_i\\), donde \\(\\mu_i\\) es la tasa de ocurrencia, y la función link más habitual es:\\[g(\\mu_i) = log(\\mu_i)\\]donde podemos ver que el objeto de nuestra modelización es una función de la tasa de ocurrencia, que en realidad es nuestro parámetro de interés en este tipo de situaciones.","code":""},{"path":"glm.html","id":"ejemplo-4","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1.2.1 Ejemplo","text":"Se realiza un estudio en pacientes con una forma de cáncer de piel llamado melanoma maligno. En dicho estudio se recogió información sobre la localización del tumor y su tipo histológico. Los datos son el número de pacientes en cada combinación de tipo de tumor y localización. El interés básico del análisis es investigar la relación entre el tipo de tumor y su localización.","code":""},{"path":"glm.html","id":"distribución-normal-link-identidad","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1.3 Distribución Normal (link identidad)","text":"En esta situación el valor esperado de la respuesta viene dado por \\(E(Y_i) = \\mu_i\\), donde \\(\\mu_i\\) es la media de la población, y la función link más habitual es:\\[g(\\mu_i) = \\mu_i\\]que corresponde exactamente con la expresión de los modelos lineales que hemos trabajo en temas anteriores.","code":""},{"path":"glm.html","id":"distribución-gamma-link-recíproco","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1.4 Distribución Gamma (link recíproco)","text":"En esta situación la función link es:\\[g(\\mu_i) = \\frac{1}{\\mu_i}\\]","code":""},{"path":"glm.html","id":"ejemplo-5","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.1.4.1 Ejemplo","text":"Se realiza un ensayo clínico donde se registra el tiempo de supervivencia (en semanas) para pacientes de leucemia y su correspondiente conteo inicial de células blancas en la sangre (en escala log10). El interés del análisis es intentar predecir el tiempo de supervivencia \\(Y\\) en función del número inicial de células blancas \\(x_i\\). Una distribución usual en la modelización de tiempos de superveniencia es la exponencial, que es un caso particular de la distribución Gamma.","code":""},{"path":"glm.html","id":"estimación-de-los-glm","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.2 Estimación de los GLM","text":"Es evidente que debemos modificar los procedimientos de estimación de los parámetros ya que la estructura lineal se asume sobre la transformación de la media de la respuesta y sobre los valores de la respuesta directamente como ocurría en los modelos lineales. Los métodos de máxima verosimilitud para la estimación de los parámetros se basan en el Método de Scoring o el Método de Newton-Raphson. Se trata de métodos de estimación iterativos, y directos como en el caso de los modelos lineales, que finalizan cuando se alcanzan las condiciones de convergencia del método. En la mayoría de situaciones experimentales dichos métodos convergen en pocas iteracciones y proporcionan las estimaciones del parámetro del modelo. Haciendo uso del Teorema Central del Límite resulta posible obtener además intervalos de confianza para los parámetros del modelo.Para la estimación de este tipo de modelos utilizamos la función glm() cuya estructura viene dada por:donde modelo representa la ecuación del modelo en la forma \\(respuesta \\sim predictoras\\), distri representa la distribución de probabilidad asociada con la respuesta, tipo la función link utilizada y data_set el conjunto de dato sobre el que se ajusta el modelo. En temas sucesivos veremos como especificar esta función en cada una de las situaciones experimentales planteadas.Para estudiar en profundidad los procedimientos de estimación se puede leer el capítulo 2 de MayMor01.","code":"\nglm(modelo,family = distri(link = tipo),data_set)"},{"path":"glm.html","id":"bondad-del-ajuste-2","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.3 Bondad del ajuste","text":"Un aspecto importante en el ajuste de un modelo es determinar si describe adecuadamente o los datos observados. Cuando ajustamos un modelo lineal generalizado, juzgamos la adecuación del modelo comparando la verosimilitud del modelo ajustado con la verosimilitud del modelo saturado.El modelo saturado es un modelo de forma similar al modelo propuesto que describe de modo perfecto los datos. Por tanto, tiene poca utilidad desde el punto de vista de ajuste de un modelo. Sin embargo, es útil para medir cómo un ajuste concreto se parece un ajuste “perfecto”.Ejmplo. Considerando \\(Y_1,...,Y_n\\) v.. normales, con \\(E(Y_i) = \\mu_i\\) y varianza común \\(\\sigma^2\\), se propone un modelo en el que todas las medias \\(\\mu_i\\) son iguales,\n\\[\\mu_1 = ... = \\mu_n = \\mu.\\]En el modelo saturado se usa la misma distribución (normal), el mismo link (identidad), y hay un parámetro estimar por cada dato \\((\\mu_i \\y_i)\\):\\[\\text{Modelo propuesto } E(Y_i) = \\mu, =1,...,n \\\\widehat{\\mu}_i = \\bar{y}\\]\n\\[\\text{Modelo saturado } E(Y_i) = \\mu_i, =1,...,n \\\\widehat{\\mu}_i = y_i\\]En los GLM el estadístico utilizado para valorar la bondad del ajuste obtenido se denomina D = Deviance. Dicho estadístico mide como de grande es la desviación del modelo ajustado respecto de los datos (modelo saturado). Si \\(D\\) es grande, el modelo ajustado proporciona un ajuste pobre. Un valor pequeño de \\(D\\) es indicador de un buen ajuste. Para decidir qué se considera “D grande” y qué “D pequeño”, es preciso utilizar la distribución en el muestreo del estadístico.Cuando el modelo propuesto proporciona un buen ajuste de los datos, entonces\n\\[D \\text{ } \\dot{\\sim} \\text{ } \\chi^2_{n-p}, \\text{ asintóticamente}\\]\ndonde \\(n\\) es el número de datos ajustar y \\(p\\) el número de parámetros del modelo.En la práctica comparamos el valor de la deviance con el cuantil 0.95 de una \\(Chi^2\\) con \\(n-p\\) grados de libertad. Si \\(D\\) es menor que el cuantil diremos que tenemos un buen ajuste. En la práctica obtendremos el p-valor asociado con dicha deviance y lo comparemos con el valor de referencia 0.05. Si el p-valor obtenido es superior 0.05 diremos que tenemos un buen ajuste.","code":""},{"path":"glm.html","id":"comparación-de-modelos","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.4 Comparación de modelos","text":"Aunque un modelo con más parámetros proporcione un ajuste mejor que otro con menos parámetros, cabe plantearse si en realidad todos los parámetros estimados son necesarios. Esta cuestión se puede resolver con tests basados en la deviance y en el estadístico de Wald, siempre y cuando se trate de modelos anidados. Otros estadísticos como el AIC permiten la comparación más general de modelos necesariamente anidados.Dos modelos en competencia pueden ser comparados mediante la deviance cuando tienen la misma distribución y función link y sólo difieren en el número de parámetros, es decir, cuando se trata de modelos anidados. Supongamos dos modelos \\(M1\\) con \\(p_1\\) parámetros y \\(M2\\) con \\(p_2\\) parámetros, de forma que el primero está anidado en el segundo, es decir, \\(p_1 < p_2\\). En esta situación el estadístico para valorar si ambos modelos pueden considerarse iguales, es decir, debemos elegir \\(M1\\) ya que es más simple, sigue una distribución \\(\\chi^2_{p_2-p_1}\\). Para resolver la comparación debemos obtener el cuantil 0.95 de la distribución chi cuadrado y compararlo con el valor del estadístico. Si el pvalor asociado es inferior 0.05 rechazaremos el modelo \\(M1\\) en favor del modelo \\(M2\\), mientras que si es superior 0.05 elegiremos el modelo \\(M1\\).Para la selección automática de efectos en la construcción de un modelo se pueden utilizar los estadísticos AIC o el BIC. En ambos casos seleccionaremos el modelo con menor valor en dichos estadísticos. Procedemos de igual forma los que hacíamos en los modelos lineales que involucran más de una variable predictora.","code":""},{"path":"glm.html","id":"diagnóstico-de-los-glm","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.5 Diagnóstico de los GLM","text":"Los residuos se pueden utilizar para explorar la adecuación del ajuste de un modelo respecto la elección de la función de varianza, la función link y los términos incluir en el predictor lineal. Los residuos pueden también ser útiles para detectar observaciones influyentes o valores anómalos que requieran una investigación más intensa. Para modelos lineales generalizados, requerimos una definición algo más amplia de residuos, que sea aplicable todas las distribuciones que pueden reemplazar la Normal.Los gráficos de residuos pueden ser muy útiles la hora de detectar fallos sistemáticos en un modelo para ajustar unos datos. Detectados los fallos, la labor siguiente consiste en solventar las deficiencias y reajustar el modelo. Sin embargo, cuando la respuesta toma sólo unos pocos valores, como ocurre en pruebas Bernoulli, la utilidad de los residuos es limitada.La especificación incorrecta de un modelo se puede deber una serie de factores:elección incorrecta de la distribución de probabilidad,especificación incorrecta de la forma en que la respuesta media cambia con las variables explicativas, ya sea porque:\nla componente sistemática \\(\\eta()\\) está mal especificada,\no la función link \\(g()\\) es apropiada,\nla componente sistemática \\(\\eta()\\) está mal especificada,o la función link \\(g()\\) es apropiada,variables explicativas incorporadas en el modelo,funciones incorrectas de las variables explicativas en el modelo, incluyendo interacciones desconocidas entre ellas, o disponibilidad de suficientes funciones diferentes,dependencia entre las observaciones, por ejemplo, lo largo del tiempo (autocorrelación).Se definen en este caso los residuos deviance estandarizados que son los utilizados para diagnosticar el modelo, obtenidos como los residuos asociados con el predictor lineal. Sin embargo, dada la estructura de los GLM podemos definir también los residuos de la respuesta que son los obtenidos entre el valor de la respuesta y el proporcionado al deshacer el cambio involucrado en la función link.Utilizaremos los procedimientos gráficos vistos en temas anteriores para proceder con el diagnóstico de este tipo de modelos.","code":""},{"path":"glm.html","id":"predicción-de-los-glm","chapter":"Unidad 12 Modelos Lineales Generalizados","heading":"12.6 Predicción de los GLM","text":"La predicción en este tipo de modelos se divide en dos fases:predicción del predictor linealpredicción de la respuestaDado un GLM con predictor lineal dado por \\(\\eta_i = X_i\\beta\\) y función link \\(g(\\mu_i)\\), el proceso de estimación del modelo nos proporciona los valores estimados de \\(\\beta\\), de forma que obtendríamos el valor ajustado del predictor lineal como:\n\\[\\widehat{\\eta}_i = X_i\\widehat{\\beta}\\]y el valor ajustado de como\\[\\mu_i = g^{-1}(\\widehat{\\eta}_i) = g^{-1}(X_i\\widehat{\\beta})\\]","code":""},{"path":"glmbinomial.html","id":"glmbinomial","chapter":"Unidad 13 GLM respuesta binomial","heading":"Unidad 13 GLM respuesta binomial","text":"Consideramos en este tema la modelización de variables respuesta de tipo binario, es decir, para cada individuo, la variable respuesta para cada uno de los sujetos puede tomar únicamente dos posibles valores, que denotaremos por 0 (fracaso) y 1 (éxito),\\[Pr(Y_i = 0) = 1 - \\pi_i; \\text{ } Pr(Y_i = 1) = \\pi_i\\]Si además se han observado una serie de covariables continuas o de tipo factor, el objetivo del análisis con el modelo lineal generalizado será predecir la probabilidad asociada al éxito, \\(\\widehat{\\pi_i}\\), (o equivalentemente el fracaso), en función de dichas covariables.","code":""},{"path":"glmbinomial.html","id":"tipos-de-datos-binomiales","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.1 Tipos de datos binomiales","text":"Existen dos formas habituales en las que se presenta o se recoge la información experimental sobre variables que miden éxito o fracaso. continuación detallamos dichas situaciones experimentales y vemos un ejemplo de cada una de ellas.","code":""},{"path":"glmbinomial.html","id":"información-individualizada-por-sujeto","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.1.1 Información individualizada por sujeto","text":"En este caso el banco de datos recoge la información de cada sujeto de la muestra con una variable que indica el éxito o el fracaso. Dicha variable se suele codificar 1 (éxito) y 0 (fracaso). Asociada con esta variable binaria se pueden recoger variables predictoras, en cada uno de los sujetos, para tratar de explicar el comportamiento de la respuesta.","code":""},{"path":"glmbinomial.html","id":"datos-de-kifosis","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.1.1.1 Datos de Kifosis","text":"En este banco de datos aparecen los datos de 81 niños que fueron intervenidos quirúrgicamente para corregirles problemas en la columna vertebral. La variable binaria kifosis indica la presencia o ausencia de una deformidad postoperatoria en la columna, denominada kifosis. Las otras tres variables son Age, edad del niñoo en meses, Number, número de vértebras intervenidas en la operación, y Start, que define la primera vértebra involucrada en la operación. Es de interés en el análisis investigar cómo están relacionadas dichas variables la hora de predecir la incidencia de la kifosis en el postoperatorio. En este caso todas las variables predictoras son de tipo numérico.En este caso la variable respuesta se puede representar mediante el modelo \\(Y_i \\sim Bi(n = 1, \\pi_i)\\), dado que cada observación corresponde un único sujeto con \\(\\pi_i\\) la probabilidad de sufrir una deformidad postoperatoria para el sujeto \\(\\).Cargamos los datos partir de la librería rpart y los representamos gráficamente. En este caso realizamos diagramas de caja para cada variable predictora. La variable respuesta viene codificada con las opciones absent y present, por lo que al cargar los datos codificaremos dichos valores como 0 y 1. En este caso el éxito viene marcado por tener la enfermedad.En dichos gráficos podemos ver como hay más incidencia de la enfermedad para los bebes más grandes y cuando el número de vértebras intervenidas es inferior, mientras que disminuye cuando la primera vértebra involucrada en la operación tiene un valor más bajo, es decir, se sitúa más arriba en la columna vertebral.","code":"\nKyphosis <- factor(c(1, 1, 2L, 1, 1, 1,  1, 1, 1, 2, \n                     2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n                     2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, \n                     1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, \n                     1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, \n                     1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, \n                     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n                     2, 1, 1, 2, 1), labels = c(\"absent\", \"present\"))\nAge <- c(71, 158, 128, 2, 1, 1, 61, 37, 113, 59, 82, 148, \n         18, 1, 168, 1, 78, 175, 80, 27, 22, 105, 96, 131, \n         15, 9, 8, 100, 4, 151, 31, 125, 130, 112, 140, 93, \n         1, 52, 20, 91, 73, 35, 143, 61, 97, 139, 136, 131, \n         121, 177, 68, 9, 139, 2, 140, 72, 2, 120, 51, 102, \n         130, 114, 81, 118, 118, 17, 195, 159, 18, 15, 158, \n         127, 87, 206, 11, 178, 157, 26, 120, 42, 36) \nNumber <- c(3, 3, 4, 5, 4, 2, 2, 3, 2, 6, 5, 3, 5, 4, 3, 3, 6, \n            5, 5, 4, 2, 6, 3, 2, 7, 5, 3, 3, 3, 2, 3, 2, 5, 3, \n            5, 3, 3, 5, 6, 5, 5, 3, 9, 4, 3, 3, 4, 5, 3, 2, 5, \n            2, 10, 2, 4, 5, 3, 5, 7, 3, 4, 7, 4, 3, 4, 4, 2, 4, \n            4, 5, 5, 4, 4, 4, 3, 4, 3, 7, 2, 7, 4) \nStart <- c(5, 14, 5, 1, 15, 16, 17, 16, 16, 12, 14, 16, 2, 12, \n           18, 16, 15, 13, 16, 9, 16, 5, 12, 3, 2, 13, 6, 14, \n           16, 16, 16, 11, 13, 16, 11, 16, 9, 6, 9, 12, 1, 13, \n           3, 1, 16, 10, 15, 13, 3, 14, 10, 17, 6, 17, 15, 15, \n           13, 8, 9, 13, 1, 8, 1, 16, 16, 10, 17, 13, 11, 16, \n           14, 12, 16, 10, 15, 15, 13, 13, 13, 6, 13)\nkyphosis <- data.frame(Kyphosis, Age, Number, Start)\n# Recodificación a 0 - 1\nkyphosisb = kyphosis %>% mutate(Kyphosis = 1*(Kyphosis==\"present\"))\n# Gráficos de predictoras y variable\ndatacomp = melt(kyphosis, id.vars='Kyphosis')\nggplot(datacomp) +\n  geom_boxplot(aes(Kyphosis,value, colour=variable)) + \n  facet_wrap(~variable, scales =\"free_y\") +\n  labs(x = \"\", y = \"Kyphosis\") "},{"path":"glmbinomial.html","id":"información-agrupada-por-sujetos","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.1.2 Información agrupada por sujetos","text":"Los bancos de datos de este tipo suelen identificar por fila el “tratamiento” al que se ven sometidos un grupo de sujetos, registrándose el número total de sujetos en esa combinación y el número de éxitos (en algunos casos se recogen los éxitos y fracasos para dicha combinación). El “tratamiento” puede ser la combinación de una o más predictoras. Estos bancos de datos son muy habituales en ensayos de dosis-repuesta donde únicamente queremos valorar eficacia, ya que es necesario recoger una información exhaustiva sobre los sujetos bajo estudio. En estos casos estimamos la probabilidad de éxito individual sino del grupo de sujetos que se ven sometidos al mismo tratamiento. La información muestral debe recoger la probabilidad de éxito asociada cada combinación.","code":""},{"path":"glmbinomial.html","id":"datos-dosis-respuesta","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.1.2.1 Datos dosis-respuesta","text":"Collet (1991) presenta un experimento sobre la toxicidad de distintas dosis (en microgramos) del piretroide trnas-cipemetrín en los capullos de gusano del tabaco. Se había comenzado detectar resistencia de esas polillas dicho tóxico. El experimento consistía en exponer, durante tres días y distintas dosis de tóxico, series de 20 polillas de cada sexo (total). Se anotaron el número de polillas muertas en cada serie (dead). Es de interés en el análisis investigar si efectivamente se demostraba tal resistencia al tóxico en función del sexo de la polilla (sex) y partir de qué dosis (dosis). También se desea determinar la dosis la cual es posible garantizar el exterminio del 50% de los insectos.En este caso tenemos un modelo \\(Y_i \\sim Bi(n = 20, \\pi_i)\\), con \\(\\pi_i\\) la probabilidad de morir para una polilla en la combinación dosis-sexo.Veamos los datos para este experimento y la representación gráfica. Utilizamos la variable log(dosis) como es habitual en este tipo de experimentos para linealizar la relación entre probabilidad de éxito y dosis .Se observa que para la misma dosis la probabilidad de morir en las hembras es menor que en los machos. Además, hay una tendencia creciente con la dosis, es decir, cuanto aumenta la dosis aumenta la probabilidad de morir.","code":"\nDosis = read_csv(\"https://goo.gl/w23RGz\", col_types = \"cdii\")\nDosis## # A tibble: 12 × 4\n##    sex   dosis total  dead\n##    <chr> <dbl> <int> <int>\n##  1 M         1    20     1\n##  2 M         2    20     4\n##  3 M         4    20     9\n##  4 M         8    20    13\n##  5 M        16    20    18\n##  6 M        32    20    20\n##  7 F         1    20     0\n##  8 F         2    20     2\n##  9 F         4    20     6\n## 10 F         8    20    10\n## 11 F        16    20    12\n## 12 F        32    20    16\n# Calculamos los vivos, la probabilidad de morir, y el \n# logaritmo de dosis que es la forma habitual de medir \n# en este tipo de situaciones\nDosis = Dosis %>%  \n  mutate(alive = total - dead, probabilidad = dead/total, ldosis = log(dosis))\nDosis## # A tibble: 12 × 7\n##    sex   dosis total  dead alive probabilidad ldosis\n##    <chr> <dbl> <int> <int> <int>        <dbl>  <dbl>\n##  1 M         1    20     1    19         0.05  0    \n##  2 M         2    20     4    16         0.2   0.693\n##  3 M         4    20     9    11         0.45  1.39 \n##  4 M         8    20    13     7         0.65  2.08 \n##  5 M        16    20    18     2         0.9   2.77 \n##  6 M        32    20    20     0         1     3.47 \n##  7 F         1    20     0    20         0     0    \n##  8 F         2    20     2    18         0.1   0.693\n##  9 F         4    20     6    14         0.3   1.39 \n## 10 F         8    20    10    10         0.5   2.08 \n## 11 F        16    20    12     8         0.6   2.77 \n## 12 F        32    20    16     4         0.8   3.47\n# Representamos la probabilidad de morir en función de \n# las covariables sex y ldosis\nggplot(Dosis,aes(x = ldosis, y = probabilidad, color = sex)) + \n  geom_point() +\n  labs(x = \"Logaritmo Dosis\", y = \"Probabilidad de morir\") "},{"path":"glmbinomial.html","id":"especificación-e-hipótesis-de-modelo","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.2 Especificación e hipótesis de modelo","text":"Las hipótesis que debe verificar los modelos de este tipo son:independencia entre las observacioneslinealidad entre transformaciones de la proporción de éxitos y de las variables explicativas continuas (función link)consistencia entre la modelización y la interpretación física.Si \\(\\pi_i\\) es la probabilidad de éxito asociado al -ésimo experimento binomial con respuesta \\(Y_i\\), y condiciones de experimentación observadas en las variables predictoras \\(X_1;X_2,...,X_p\\), la formulación del GLM viene dada por:\\[g(\\pi_i) = \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip}\\]donde \\(g()\\) es la función link asociada con este tipo de datos. En este tipo de modelos los coeficientes \\(\\beta_i\\) representan el incremento o decremento sobre la función de enlace que nos permite linealizar la relación entre la probabilidad de éxito y las variables predictora o predictor lineal.continuación, presentamos las diferentes posibilidades de funciones link para este tipo de datos.","code":""},{"path":"glmbinomial.html","id":"funciones-link","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.2.1 Funciones link","text":"En este tipo de modelos se contemplan diferentes funciones link. De forma habitual se suelen ajustar los modelos obtenidos para función de enlace, y nos quedamos con aquel modelo, y por tanto función de enlace, con mejor capacidad explicativa.","code":""},{"path":"glmbinomial.html","id":"link-logit","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.2.1.1 link logit","text":"El link logit proporciona los comúnmente conocidos como modelos de regresión logística y su expresión viene dada por:\\[g(\\pi_i) = log\\left( \\frac{\\pi_i}{1-\\pi_i}\\right) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip}\\]En esta situación la probabilidad de éxito se puede escribir en términos de las variables predictoras (despejando de la ecuación anterior) como:\\[\\pi_i = \\frac{exp(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip})}{1+exp(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip})} \\]Si \\(\\pi_i (x_1)\\) y \\(\\pi_i (x_2)\\) denotan las probabilidades de éxito para dos valores de la predictora (\\(x_1\\) y \\(x_2\\)), el cociente \\((\\pi_i (x_1)/(1-\\pi_i (x_1))) / (\\pi_i (x_2)/(1-\\pi_i (x_2)))\\) se denomina odds ratio y valora la relación entre probabilidad de éxito y fracaso para dos valores de la covaraible, de forma que:un valor mayor que 1 indica un aumento de la probabilidad de éxito en \\(x_1\\) con respecto \\(x_2\\),un valor menor que 1 indica un decremento de la probabilidad de éxito en \\(x_1\\) con respecto \\(x_2\\).En términos del modelo ajustado se puede valorar el incremento o decremento del odds ratio con el valor de \\(exp(\\beta_i)\\), para cada uno de los efectos (variables numéricas o niveles de un factor) presentes en el modelo. Valores negativos de \\(\\beta_i\\) dan lugar odds ratios menores que 1, mientras que valores de \\(\\beta_i\\) positivos dan odds ratios positivos.De hecho, esta propiedad es la que motiva que esta función de enlace sea la más utilizada en este tipo de modelos.En R podemos obtener el valor de la función enlace para cualquier probabilidad con la sentenciay el valor de la probabilidad de éxito partir del valor del predictor lineal con","code":"\nbinomial(link = logit)$linlfun(probabilidad)\nbinomial(link = logit)$linlinv(predictor)"},{"path":"glmbinomial.html","id":"link-probit","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.2.1.2 link probit","text":"El link probit proporciona los comúnmente conocidos como modelos de regresión probit y su expresión viene dada por:\\[g(\\pi_i) = \\Phi^{-1}(\\pi_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip}\\]\ndonde \\(\\Phi^{-1}\\) es la función inversa de la función de distribución Normal estándar. En esta situación la probabilidad de éxito se puede escribir en términos de las variables predictoras (despejando de la ecuación anterior) como:\\[\\pi_i = \\Phi(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} + ... + \\beta_p x_{ip})\\]\ndonde \\(\\Phi\\) es la función de distribución Normal estándar.","code":""},{"path":"glmbinomial.html","id":"link-cloglog","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.2.1.3 link cloglog","text":"El link cloglog es el menos habitual en la práctica y su expresión viene dada por:\\[g(\\pi_i) = log(-log(1-\\pi_i)) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{21} +...+\\beta_p x_{ip}\\]\n¿Cuál es la probabilidad de éxito en este caso?La diferencia entre un link y otro hace referencia como modelizamos las probabilidades más extremas, es decir, las muy bajas o muy altas, pero en la práctica proporcionan resultados muy similares. Por eso casi siempre el modelo utilizado es el de regresión logística.","code":""},{"path":"glmbinomial.html","id":"representación-de-las-funciones-link","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.2.2 Representación de las funciones link","text":"Para poder comparar los diferentes links presentados veamos cual es su comportamiento en función de la probabilidad de éxito, lo que se denomina en los modelos de dosis-respuesta función de tolerancia. Respectivamente se representan el link logit (línea continua), link probit (línea discontinua), link cloglog (puntos):En este gráfico se puede apreciar las pequeñas diferencias entre las funciones de enlace. Podemos ver que para un valor del predictor lineal de 2.5 la probabilidad de éxito en los modelos probit y cloglog es 1 mientras que para el modelo logit está próxima 0.9. La elección de un tipo de enlace u otro dependerá por tanto del comportamiento de los datos observados.Representamos continuación las diferentes funciones link para los datos de dosis-respuesta. Este gráfico sólo se puede hacer para datos en formato agrupado.Se aprecia como las tres funciones de enlace tienen un comportamiento muy similar. En el resto de esta unidad mostraremos los resultados correspondientes al modelo de regresión logística (link logit), e indicaremos como obtener los modelos para el resto de funciones link.","code":""},{"path":"glmbinomial.html","id":"estimación-y-bondad-de-ajuste","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.3 Estimación y bondad de ajuste","text":"Para la estimación utilizamos la función glm con las especificaciones siguientes:Sin embargo, la especificación del modelo varía en función del tipo de datos (individualizados o conteos). En el caso de nuestro banco de datos contenga información individualizada para cada sujeto la variable respuesta se debe codificar con un 1 para el éxito y 0 para el fracaso, y el modelo se especifica como:En el caso de disponer del número de éxitos y fracasos el modelo se especifica e la forma siguiente:","code":"\n# Modelo de regresión logística\nglm(modelo,family = binomial(link = logit),data_set)\n# Modelo de regresión probit\nglm(modelo,family = binomial(link = probit),data_set)\n# Modelo cloglog\nglm(modelo,family = binomial(link = cloglog),data_set)\nrespuesta ~ predictoras\ncbind(exitos,fracasos) ~ predictoras"},{"path":"glmbinomial.html","id":"ejemplos-14","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.3.1 Ejemplos","text":"Estudiamos ahora el proceso de estimación del modelo completo (con todos los efectos posibles) para cada uno de los ejemplos. Utilizamos la función tab_model() con diferentes especificaciones para obtener las estimaciones de los parámetros del modelo (predictor lineal) y de los odds ratios.","code":""},{"path":"glmbinomial.html","id":"kyphosis","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.3.1.1 Kyphosis","text":"En este caso ya hemos creado un variable para identificar el éxito (tener malformación postoperatoria) o el fracaso (tener malformación) para cada sujeto de la muestra. El banco de datos es kyphosisb con variable respuesta Kyphosis y con tres posibles variables predictoras de tipo numérico: Age, Number, Start.\nPara ajustar el modelo de regresión logística escribimos:Veamos las estimaciones del modelo ajustado:la vista de los coeficientes estimados se desprende que la única preditora que parece tener influencia en la probabilidad de observar una malformación es la variable Start. Dado que el coeficiente es negativo dicha probabilidad disminuye cuando aumenta dicha variable. La ecuación del modelo viene dada por:\\[log\\left( \\frac{\\hat{\\pi}_i}{1-\\hat{\\pi}_i}\\right) = -2.04 + 0.01 *Age_{} + 0.41 *Number_{1} - 0.21 *Start_{}\\]donde la probabilidad de éxito es:\\[\\pi_i = \\frac{exp(-2.04 + 0.01 * Age_{} + 0.41* Number_{} - 0.21* Start_{})}{1 -exp(2.04 + 0.01* Age_{} + 0.41* Number_{} - 0.21* Start_{})}\\]Los odds ratios asociados cada variable presente en el modelo vienen dados por\\[\n\\begin{array}{ll}\nodds(Age) = & exp(0.01) = 1.01\\\\\nodds(Number) = & exp(0.41) = 1.51\\\\\nodds(Start) = & exp(-0.21) = 0.81\\\\\n\\end{array}\n\\]En términos de los odds ratios podemos ver que la probabilidad de sufrir una malformación aumenta 1.01 y 1.51 al aumentar respectivamente las variables Age y Number, es decir, la probabilidad de sufrir una malformación al incrementar en una unidad el número de vértebras intervenidas es 1.5 (odds ratio) veces superior que de padecerla. Con la variable Start podemos ver que la relación entre sufrir la malformación es 0.81 veces la probabilidad de sufrirla, es decir, disminuye al aumentar la vértebra de inicio.Con la formulación siguiente podemos obtener y representar de forma directa los odds ratios asociados al modelo analizado:De todas formas hay que ser cautelosos en la interpretación dado que todavía hemos validado el modelo ni hemos realizado el proceso de selección de efectos relevantes en el modelo.Para realizar la bondad de ajuste del modelo obtenemos los estadísticos asociados (función glance) y el contraste de bondad de ajuste utilizando la deviance explicada y sus grados de libertad asociados utilizando el test \\(\\chi^2\\).El p-valor obtenido es superior 0.05 indicando que el modelo considerado tiene una buena capacidad explicativa. En el punto siguiente estudiaremos la selección de variables para quedarnos con el mejor modelo posible.","code":"\nfit.kyphosis <- glm(Kyphosis  ~ Age + Number + Start,\n          family = binomial(link = logit),\n          data = kyphosisb)\n# Resumen del predictor lineal\ntab_model(fit.kyphosis, \n          transform = NULL, \n          string.est = \"Estimate\",\n          show.r2 = FALSE)\n# Gráfico\nplot_model(fit.kyphosis, \n           transform = NULL, \n           axis.title = c(\"Estimate\",\"\"),\n           show.values = TRUE)\n# Resumen del predictor lineal\ntab_model(fit.kyphosis, \n          show.r2 = FALSE)\n# Gráfico\nplot_model(fit.kyphosis, \n           show.values = TRUE)\n# P-valor del contraste\n1-pchisq(fit.kyphosis$deviance,fit.kyphosis$df.residual)## [1] 0.9033442\n# Bondad del ajuste\nglance(fit.kyphosis)## # A tibble: 1 × 8\n##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n##           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n## 1          83.2      80  -30.7  69.4  79.0     61.4          77    81"},{"path":"glmbinomial.html","id":"dosis-respuesta","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.3.1.2 Dosis-Respuesta","text":"En este caso trabajamos con datos agrupados que contabilizan los éxitos (número de polillas muertas) y los fracasos (número de polillas vivas) para cada combinación considerada en el diseño experimental. Se consideran las variables predictoras sex (categórica) y dosis (numérica).El banco de datos es Dosis y contiene además las polillas totales, muertas y vivas (calculadas como muertas - vivas), y el logaritmo de la dosis suministrada, ya que de forma habitual en este tipo de modelos se suele trabajar en dicha escala logarítmica. Para ajustar el modelo de regresión logística consideramos los efectos asociados con cada predictora, así como la posible interacción entre ambos:Analizamos el modelo obtenido:De los resultados obtenidos parece desprenderse que existe efecto de interacción entre sexo y dosis. Antes de tratar de explicar el resto de efectos deberemos estudiar la posibilidad de eliminar dicho efecto del modelo. Las ecuaciones de este modelo vienen dadas por:\\[\nlog\\left(\\frac{\\widehat{\\pi_i}}{1-\\widehat{\\pi_i}}\\right)_{Machos} = -2.82 + 1.82 * ldosis_{} \n\\]\\[\nlog\\left(\\frac{\\widehat{\\pi_i}}{1-\\widehat{\\pi_i}}\\right)_{Hembras} = -2.99 + 1.31 * ldosis_{}\n\\]En el punto siguiente seleccionaremos el mejor modelo e interpretaremos con detalle los coeficientes y los odds-ratios estimados.Por otro lado, el p-valor para valorar la bondad del ajuste:parece indicar que el ajuste obtenido es bueno, dado que es superior 0.05.","code":"\nYres <- cbind(Dosis$dead,Dosis$alive)\nfit.dosis <- glm(Yres  ~ sex * ldosis,\n          family = binomial(link = logit),\n          data = Dosis)\n# Resumen del predictor lineal\ntab_model(fit.dosis, \n          transform = NULL,\n          string.est = \"Estimate\",\n          show.r2 = FALSE)\n# Gráfico\nplot_model(fit.dosis, \n           transform = NULL, \n           axis.title = c(\"Estimate\",\"\"),\n           show.values = TRUE)\n1-pchisq(fit.dosis$deviance,fit.dosis$df.residual)## [1] 0.7582464"},{"path":"glmbinomial.html","id":"selección-del-modelo","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.4 Selección del modelo","text":"Para la construcción del mejor modelo utilizaremos los mismos procedimientos secuenciales de selección de efectos que vimos en unidades anteriores. En este caso disponemos del criterio Deviance, del AIC y del test \\(\\chi^2\\) (p-valor significativo o ) para valorar los efectos del modelo. En este caso utilizaremos el AIC dado que los p-valores son aproximados en este tipo de modelos, y tienen que ser interpretados con mucha cautela.Para realizar la selección del modelo utilizaremos el procedimiento:","code":"\nmodelo.final <- step(modelo)"},{"path":"glmbinomial.html","id":"ejemplos-15","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.4.1 Ejemplos","text":"Estudiamos continuación cada uno de los ejemplos.","code":""},{"path":"glmbinomial.html","id":"kyphosis-1","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.4.1.1 Kyphosis","text":"Realizamos la selección del modelo:El proceso de selección de efectos indica que podemos eliminar ninguna variable del modelo (todos los AIC asociados con las predictoras aumentan al eliminarlas). Los valores de Deviance siempre aumentan (deviance asociada cada variable) cuando eliminamos cualquiera de las variables con respecto la del modelo que se queda con todas ellas (<none>). Lo mismo ocurre con el estadístico AIC.Por tanto, el modelo final es el obtenido en el punto anterior. Todo el análisis de los coeficientes y de los odds ratios ya fue detallado y se repite aquí.","code":"\nstats::step(fit.kyphosis)## Start:  AIC=69.38\n## Kyphosis ~ Age + Number + Start\n## \n##          Df Deviance    AIC\n## <none>        61.380 69.380\n## - Age     1   64.536 70.536\n## - Number  1   65.299 71.299\n## - Start   1   71.627 77.627## \n## Call:  glm(formula = Kyphosis ~ Age + Number + Start, family = binomial(link = logit), \n##     data = kyphosisb)\n## \n## Coefficients:\n## (Intercept)          Age       Number        Start  \n##    -2.03693      0.01093      0.41060     -0.20651  \n## \n## Degrees of Freedom: 80 Total (i.e. Null);  77 Residual\n## Null Deviance:       83.23 \n## Residual Deviance: 61.38     AIC: 69.38"},{"path":"glmbinomial.html","id":"dosis-respuesta-1","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.4.1.2 Dosis-respuesta","text":"Realizamos la selección para el modelo de dosis-respuesta:El proceso de selección indica que debemos considerar el efecto de interacción ya que el valor de deviance es inferior cuando está presente que cuando lo eliminamos, el AIC aumenta al eliminarlo. El resto de efectos del modelo pueden ser eliminados. Esto implica que al haber efecto de interacción la probabilidad de morir se puede obtener mediante comportamientos paralelos para machos y hembras.Ajustamos el modelo sin interacción y realizamos el análisis de dicho modelo:La inferencia sobre el modelo viene dada por:de forma que podemos escribir el predictor lineal mediante las expresiones siguientes (atendiendo los diferentes valores de sex):\\[\nlog\\left(\\frac{\\widehat{\\pi_i}}{1-\\widehat{\\pi_i}}\\right)_{Machos} = -2.37 + 1,54* ldosis_{}\n\\]\\[\nlog\\left(\\frac{\\widehat{\\pi_i}}{1-\\widehat{\\pi_i}}\\right)_{Hembras} = -3.47 + 1.54* ldosis_{}\n\\]donde \\(\\pi_i\\) es la probabilidad de muerte de la polilla. Dado que la interceptación es más pequeña para las hembras que para los machos tenemos un indicador de que las hembras son más resistentes que los machos. La pendiente positiva refleja que la probabilidad de muerte aumenta cuando lo hace la dosis.En términos de los odds ratios:podemos ver que el odds ratio de los machos frente las hembras es 3.01 veces (coeficiente de sexo) superior, y es 4.64 veces mayor (coeficiente de log(dosis)) cuando aumentamos en una unidad el logaritmo de la dosis.El odds ratio por sexo se obtiene como:\n\\[\n\\frac{\\left(\\widehat{\\pi_i}/(1-\\widehat{\\pi_i})\\right)_{Machos}}{\\left(\\widehat{\\pi_i}/(1-\\widehat{\\pi_i})\\right)_{Hembras}} = \\frac{exp(-2.37 + 1,54 * ldosis_{})}{exp(-3.47 + 1.54 * ldosis_{})} = exp(3.47 - 2.37) = exp(1.1)\n\\]mientras que el odds ratio por ldosis, tanto para machos como para hembras, se obtiene como (tomamos los machos):\n\\[\n\\frac{\\left(\\widehat{\\pi_i}/(1-\\widehat{\\pi_i})\\right)_{Machos}}{\\left(\\widehat{\\pi_i}/(1-\\widehat{\\pi_i})\\right)_{Machos}} = \\frac{exp(-2.37 + 1,54 * (ldosis_{}+1))}{exp(-2.37 + 1.54 * ldosis_{})} = exp(1.54) \n\\]La bondad de ajuste de este modelo:muestra que el ajuste obtenido puede considerarse como bueno dado que el p-valor resulta significativo.Para verificar el efecto de la dosis por sexo podemos calcular cuál es la dosis necesaria aplicar cada sexo para conseguir una probabilidad de muerte del 50%, o como habitualmente se conoce con el nombre de dosis letal al 50% (LD50). Para ello basta con sustituir en las ecuaciones anteriores el valor de \\(\\pi_i\\) por 0.5 y despejar el valor de ldosis:\\[ldosis50_{Machos} = \\frac{2.372}{1.535} = 1.54277 \\dosis50_{Machos} = exp(1.54277) = 4.69 \\]\n\\[ldosis50_{Hembras} = \\frac{3.473}{1.535} = 2.262541 \\dosis50_{Hembras} = exp(1.54277) = 9.61 \\]De los resultados obtenidos podemos ver que hay que aplicar el doble de dosis en las hembras para conseguir la misma probabilidad de muerte del 50%.Para conseguir cualquier otra dosis letal basta con sustituir \\(\\pi_i\\) por la correspondiente probabilidad y despejar en las ecuaciones obtenidas, o utilizar el código siguiente para obtener la dosis letal una probabilidad “prob”:Utilizamos el código anterior para representar y estudiar las curvas de dosis letales para ambos sexos (machos = azul, hembras = rojo):","code":"\nstats::step(fit.dosis)## Start:  AIC=43.1\n## Yres ~ sex * ldosis\n## \n##              Df Deviance    AIC\n## - sex:ldosis  1   6.7571 42.867\n## <none>            4.9937 43.104\n## \n## Step:  AIC=42.87\n## Yres ~ sex + ldosis\n## \n##          Df Deviance     AIC\n## <none>         6.757  42.867\n## - sex     1   16.984  51.094\n## - ldosis  1  118.799 152.909## \n## Call:  glm(formula = Yres ~ sex + ldosis, family = binomial(link = logit), \n##     data = Dosis)\n## \n## Coefficients:\n## (Intercept)         sexM       ldosis  \n##      -3.473        1.101        1.535  \n## \n## Degrees of Freedom: 11 Total (i.e. Null);  9 Residual\n## Null Deviance:       124.9 \n## Residual Deviance: 6.757     AIC: 42.87\nfit.dosis <- glm(Yres ~ sex + ldosis,\n                 family = binomial(link = logit),\n                 data = Dosis)\n# Resumen del predictor lineal\ntab_model(fit.dosis, \n          transform = NULL,\n          string.est = \"Estimate\",\n          show.r2 = FALSE)\n# Resumen del predictor lineal\ntab_model(fit.dosis, \n          show.r2 = FALSE)\n1-pchisq(fit.dosis$deviance,fit.dosis$df.residual)## [1] 0.6623957\n# prob = Probabilidad buscada\npredictor <- binomial(link = logit)$linkfun(prob)\nmachos <-exp((predictor+2.372)/1.535)\nhembras <- exp((predictor+3.473)/1.535)\nprob <- seq(0.05,0.95,0.01)\n# Trabajamos con dosis \npredictor <- binomial(link = logit)$linkfun(prob)\nmachos <-exp((predictor + 2.372) / 1.535)\nhembras <- exp((predictor + 3.473) / 1.535)\ndosis <- data.frame(prob, machos, hembras)\nggplot(dosis) + \n  geom_line(aes(x = prob, y = machos), color = \"blue\") + \n  geom_line(aes(x = prob, y = hembras), color = \"red\") + \n  scale_x_continuous(breaks = seq(0,1,0.05)) +\n  scale_y_continuous(breaks = seq(0,70,5)) +\n  labs(x = \"Probabilidad de muerte\", y = \"Dosis\") + \n  theme_bw() "},{"path":"glmbinomial.html","id":"diagnóstico-4","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.5 Diagnóstico","text":"Antes de comenzar el proceso de diagnóstico hay que recordar que en este tipo de modelos tenemos dos tipos de valores ajustados y dos tipos de residuos:Los correspondientes al predictor lineal obtenidos con las estimaciones de los parámetros del modelo.Los correspondientes los valores originales que se obtienen al deshacer la transformación de la función de enlace.El diagnóstico de los GLM se basa en los procedimientos gráficos, que vimos en temas anteriores, aplicado los residuos deviance obtenidos partir del predictor lineal considerado (\\(X\\widehat{\\beta}\\)). Tanto para obtener los valores ajustados y residuos del predictor lineal utilizaremos la función fortify. Al resultado de dicha función podemos añadir sin muchos problemas los residuos entre valores originales de la respuesta y predichos. En el caso del modelo de regresión logística el código necesario viene dado por:Los gráficos de diagnóstico que vamos utilizar son:Residuos vs Ajustados (predictor lineal)Residuos vs variables en el modelo (predictor lineal)Valores influyentes","code":"\n# Sólo hay que sustituir \"modelo\" por el modelo ajustado a nuestros datos\ndiagnostico <- fortify(\"modelo\")\ndiagnostico$fitoriginal <- predict.glm(\"modelo\", type = \"response\")\ndiagnostico$residoriginal <- residuals.glm(\"modelo\", type = \"response\")"},{"path":"glmbinomial.html","id":"ejemplos-16","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.5.1 Ejemplos","text":"Pasamos analizar cada uno de los ejemplos.","code":""},{"path":"glmbinomial.html","id":"kyphosis-2","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.5.1.1 Kyphosis","text":"Obtenemos todas las cantidades de interés del modeloRealizamos los gráficos de diagnósticoComo se puede observar en el gráfico de ajustados versus residuos la interpretación en este tipo de modelos se hace bastante complicada. Esto es debido que la variable respuesta sólo toma valores 0 y 1, lo que motiva que el gráfico tenga esa pinta tan extraña. Si valoraros las distancias de Cook obtenidas (diagnostico$.cooksd) podemos ver que hay ninguna superior 1, y por tanto tenemos ninguna observación influyente.En cuanto los gráficos con respecto las variables predictoras parece apreciarse ciertas tendencias con respecto ellas. Este comportamiento parece más evidente con la variable edad donde se ve una parábola indicando la posible existencia de un efecto polinómico de grado 2 con respecto ella. En las otras dos predictoras el efecto es menos apreciable.Para solucionar este problema la opción más habitual sería ajustar un nuevo modelo indicando dicha tendencia de grado 2 (modelo polinómico con Age), pero se podría optar también por la inclusión de efectos de suavizado sobre cada una de ellas para evitar esas tendencias observadas. Utilizamos esta última opción y optamos por incluir efectos de suavizado para Age y Start (ambos con tendencias parabólicas) y comparamos con en el modelo sin suavizado.pesar de que la respuesta es numérica pode os utilizar la función gam sin más que identificar el tipo de respuesta y la función de enlace utilizar. Veamos el ajuste y la comparación entre ambos modelos:El menor valor de AIC indica que el modelo con efectos de suavizado es preferible al que los tiene. Estudiamos con un poco más detalle dicho modelo:En los efectos paramétricos se comprueba que el efecto asociado con Number resulta significativo, mientras que en los paramétricos vemos que el suavizado con edad resulta significativo al 95%. Construimos todos los posibles modelos y valoramos cual de ellos es el mejor:El menor valor de AIC corresponde al modelo con los dos suavizados y la variable Number. Hay que tener en cuenta que las significatividades que parecen en las tablas son aproximaciones asintóticas, y por tanto siempre se deben tomar con cautela y proceder con otro tipo de comparación. El estadístico GCV también proporciona la misma conclusión.Por tanto, el modelo final ajustado viene dado por:\\[log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right) = -3.60 + 0.33 * Number_i + s(Age_i) + s(Start_i)\\]Realizamos el diagnóstico del modelo de suavizado utilizando los procedimientos vistos en dicha unidad.Ningún p-valor resulta significativo indicando que el suavizado utilizado es adecuado. Además, los gráficos de diagnóstico reportan ninguna incidencia.","code":"\n# Obtención de valores para el diagnóstico\ndiagnostico <- fortify(fit.kyphosis)\ndiagnostico$fitoriginal <- predict.glm(fit.kyphosis, type = \"response\")\ndiagnostico$residoriginal <- residuals.glm(fit.kyphosis, type = \"response\")\n#Gráfico de residuos vs ajustados (predictor lineal)\nggplot(diagnostico, aes(x = .fitted, y = .stdresid)) +\n  geom_point() + \n  geom_smooth(method=loess, se=FALSE) +\n  labs(x = \"Ajustados\", y = \"Residuos\")\n# Gráficos de residuos vs predictoras (preditor lineal)\nplot_model(fit.kyphosis, type = \"resid\",\n           show.data = TRUE,\n           ci.lvl = NA) + \n  geom_point(color = \"black\", size = 0.8)\n# Modelo con suavizado en edad y start\nfit.kyphosis.gam <- gam(Kyphosis  ~ s(Age, k = 10, m = 2, bs = \"ps\")  + \n                          Number + \n                          s(Start, k = 10, m = 2, bs = \"ps\"),\n                        family = binomial(link = logit),kyphosisb)\n# Comparamos utilizando el estadístico AIC\nAIC(fit.kyphosis,fit.kyphosis.gam)##                        df      AIC\n## fit.kyphosis     4.000000 69.37993\n## fit.kyphosis.gam 6.107437 62.71679\n# Resumen del predictor lineal\nsummary(fit.kyphosis.gam)## \n## Family: binomial \n## Link function: logit \n## \n## Formula:\n## Kyphosis ~ s(Age, k = 10, m = 2, bs = \"ps\") + Number + s(Start, \n##     k = 10, m = 2, bs = \"ps\")\n## \n## Parametric coefficients:\n##             Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  -3.6011     1.1482  -3.136  0.00171 **\n## Number        0.3333     0.2324   1.434  0.15160   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##            edf Ref.df Chi.sq p-value  \n## s(Age)   2.151  2.671  6.345  0.0699 .\n## s(Start) 1.956  2.409  9.747  0.0130 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.354   Deviance explained = 39.3%\n## UBRE = -0.22572  Scale est. = 1         n = 81\n# Modelo con suavizados y number\nM1 <- gam(Kyphosis  ~ s(Age, k = 10, m = 2, bs = \"ps\")  + \n            Number + \n            s(Start, k = 10, m = 2, bs = \"ps\"),\n          family = binomial(link = logit),kyphosisb)\n# Modelo con suavizados y sin number\nM2 <- gam(Kyphosis  ~ s(Age, k = 10, m = 2, bs = \"ps\") + \n            s(Start, k = 10, m = 2, bs = \"ps\"),\n          family = binomial(link = logit),kyphosisb)\n# Modelo con suavizados con Start unicamente\nM3 <- gam(Kyphosis  ~ s(Start, k = 10, m = 2, bs = \"ps\"),\n          family = binomial(link = logit),kyphosisb)\n# Valores de AIC\nAIC(M1,M2,M3)##          df      AIC\n## M1 6.107437 62.71679\n## M2 5.237969 62.98679\n## M3 3.066073 68.34242\n# Valores de GCV\nc(M1$gcv.ubre,M2$gcv.ubre,M3$gcv.ubre)##     GCV.Cp     GCV.Cp     GCV.Cp \n## -0.2257187 -0.2223853 -0.1562664\n# Gráficos de diagnóstico\ngam.check(fit.kyphosis.gam)## \n## Method: UBRE   Optimizer: outer newton\n## full convergence after 4 iterations.\n## Gradient range [-5.641377e-09,6.154328e-08]\n## (score -0.2257187 & scale 1).\n## Hessian positive definite, eigenvalue range [0.007746516,0.01021528].\n## Model rank =  20 / 20 \n## \n## Basis dimension (k) checking results. Low p-value (k-index<1) may\n## indicate that k is too low, especially if edf is close to k'.\n## \n##            k'  edf k-index p-value\n## s(Age)   9.00 2.15    1.11    0.83\n## s(Start) 9.00 1.96    1.14    0.90"},{"path":"glmbinomial.html","id":"dosis---respuesta","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.5.1.2 Dosis - respuesta","text":"Realizamos ahora el diagnóstico para el modelo de dosis-respuesta partir del modelo sin interacción obtenido en el apartado de selección del modelo. Obtenemos las cantidades de interés y realizamos los gráficos correspondientes.Obtenemos todas las cantidades de interés del modeloRealizamos el gráfico de residuos versus ajustados identificando por el factor:En este caso hemos ajustado las tendencias, ya que dado el tamaño de muestra tan pequeño en cada uno de los grupos, los resultados podrían mostrar tendencias ficticias que nos podrían hacer dudar de la validez del modelo. En este caso se puede afirmar que exista ningún tipo de tendencia destacable. Tampoco se registra ninguna observación influyente.En los gráficos de residuos versus predictoras ocurre algo similar, ya que el número de observaciones es tan bajo que las posibles tendencias observadas implican una modificación del modelo planteado. Dado el número tan bajo de observaciones el planteamiento de modelos con suavizado sobre dosis resulta una opción.","code":"\n# Obtención de valores para el diagnóstico\ndiagnostico <- fortify(fit.dosis)\ndiagnostico$fitoriginal <- predict.glm(fit.dosis, type = \"response\")\ndiagnostico$residoriginal <- residuals.glm(fit.dosis, type = \"response\")\n#Gráfico de residuos vs ajustados (predictor lineal)\nggplot(diagnostico, aes(x = .fitted, y = .stdresid, color = sex)) +\n  geom_point() + \n  labs(x = \"Ajustados\", y = \"Residuos\")\n# Gráficos de residuos vs predictoras (preditor lineal)\nplot_model(fit.dosis, type = \"resid\",\n           show.data = TRUE,\n           ci.lvl = NA) + \n  geom_point(color = \"black\", size = 0.8)"},{"path":"glmbinomial.html","id":"predicción-4","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.6 Predicción","text":"La predicción en este tipo de modelos se divide en dos fases: ) predicción del predictor lineal, y ii) predicción de la respuesta, aunque evidentemente la más interesante es la correspondiente la respuesta.Sin embargo, cuando la respuesta viene individualizada, la predicción obtenida es un valor comprendido entre 0 y 1, con lo que hay que definir una regla para clasificar finalmente al sujeto con un 1 o un 0, identificando el éxito o el fracaso. En esta situación se toma la regla:si la predicción es mayor o igual 0.5 asignamos un éxito como resultado de la predicciónsi la predicción es menor 0.5 asignamos un fracaso como resultado de la predicciónEl resultado de la predicción es una tabla de doble entrada donde comparamos los valores observados frente los predichos calculados con la regla anterior. En esta situación estamos interesados en el porcentaje de observaciones que son correctamente clasificadas como éxitos o fracasos.Este problema aparece cuando tenemos datos agrupados ya que la variable respuesta ya es un valor entre 0 y 1, ya que modelizamos directamente la probabilidad de éxito.En los modelos de suavizado utilizaremos la función gam.predict para conseguir las predicciones.","code":""},{"path":"glmbinomial.html","id":"ejemplos-17","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.6.1 Ejemplos","text":"continuación, presentamos los procedimientos para obtener y representar la predicción de los modelos obtenidos en apartados anteriores.","code":""},{"path":"glmbinomial.html","id":"kyphosis-3","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.6.1.1 Kyphosis","text":"Dado que en este caso tenemos datos individualizamos, en primer lugar calculamos la predicción de la respuesta y utilizamos la regla de clasificación presentada anteriormente, para obtener el porcentaje de clasificación correcta.Tenemos un 86.42% de clasificar correctamente un individuo como éxito o fracaso en función de las variables predictoras consideradas. El modelo construido tiene alta capacidad predictiva ya que identifica bastante bien los sujetos clasificados como éxitos y como fracasos.Estudiamos ahora los efectos marginales individuales de cada efecto presente en el modelo para valorar como influyen en la probabilidad de éxito, es decir, probabilidad de sufrir una malformación.Se observa la parábola que describe la variable Age, la función creciente con Number y la función decreciente con Start. En conclusión, la probabilidad de malformación:es más alta en valores de Age próximos los 100 meses (probabilidad superior 0.2).crece conforme aumenta el número de vértebras involucradas hasta superar el 0.6 con 10 vértebras.disminuye con la primera vértebra operada (si es partir de la quinta) hasta hacerse prácticamente cero si empezamos en la vértebra 20.Construimos ahora los gráficos conjuntos para las tres predictoras mediante la creación de escenarios. Concretamente seleccionamos valores para Start y Age:Valores Start: 1, 5, 10, 15.Valores Age: 0, 50, 100, 150, 200, 250, 300Podemos ver el efecto de los escenarios con Number para predecir la probabilidad de malformación. se aprecian muchas diferencias entre los perfiles de predicción para Start en los tres primeros niveles considerados, donde la probabilidad de malformación es muy similar, pero si se aprecian cambios cuando Start = 15. Esto implica directamente que cuando dicha variable toma valores más grandes la probabilidad de malformación disminuye drásticamente.","code":"\n# Obtención de predicción de la respuesta\nprediccion <- predict.gam(fit.kyphosis.gam, type = \"response\")\n# Clasificacmos a cada sujeto como éxito o fracaso\nclasificado <- 1*(prediccion>=0.5)\ntabla <- table(kyphosis$Kyphosis,clasificado)\ntabla##          clasificado\n##            0  1\n##   absent  60  4\n##   present  7 10\n# Porcentaje de clasificación correcta\nround(100*sum(diag(tabla))/sum(tabla),2)## [1] 86.42\n# Gráficos marginales\nplot_model(fit.kyphosis.gam, \"pred\")## $Age## \n## $Number## \n## $Start\n# Gráficos marginales\nplot_model(fit.kyphosis.gam, \"pred\", \n           terms =c(\"Number\", \"Age [0, 50, 100, 150, 200, 250, 300]\", \"Start [1,5,10,15]\"),\n           ci.lvl = NA)"},{"path":"glmbinomial.html","id":"dosis-respuesta-2","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.6.1.2 Dosis-respuesta","text":"Dado que los datos están en formato agrupado obtenemos los gráficos de predicción asociados los efectos del modelo:En los perfiles obtenidos se aprecia claramente el paralelismo entre ambos sexos, mostrando que las hembras son más resistentes (probabilidad de morir más baja) para todos los niveles de dosis considerados, y que la probabilidad de morir aumenta con la dosis suministrada.","code":"\nplot_model(fit.dosis,\"pred\",\n           terms = c(\"ldosis\",\"sex\"),\n           title = \"Porcentaje de insectos muertos\",\n           axis.title  = c(\"Log(dosis)\",\"\"))"},{"path":"glmbinomial.html","id":"ejercicios-3","chapter":"Unidad 13 GLM respuesta binomial","heading":"13.7 Ejercicios","text":"Ejercicio 1. En un experimento se sometió cierto número de cucarachas (number) cinco horas de exposición disulfato de carbono gaseoso varias concentraciones. Se pretendía investigar la relación existente entre la dosis (dose) de disulfato administrada y la resistencia de los insectos (dead); si existe tal relación, determinar la dosis la cual es posible garantizar el exterminio del 50% de los insectos.Ejercicio 2. Se realiza un experimento vitro para estimar el número de anteras embriogénicas de las especies de plantas Datura innoxia Mill bajo dos condiciones experimentales. El primer tratamiento consiste en almacenar 3° C durante 48 horas, y el segundo consiste en un control donde se aplica ningún tratamiento. Además se considera una variable que representa los tres valores de fuerza de centrifugación. Las variables registradas son total, embryogenic, storage, centrifuge. Es de interés en el análisis investigar si efectivamente se demostraba un mayor número de anteras para las diferentes condiciones experimentales.Ejercicio 3. Se realiza un ensayo clínico para determinar en un grupo de personas mayores su estado psiquiátrico. Para cada sujeto se realiza un análisis completo y se clasifica cada uno en función de si muestra rasgos de senilidad (senility) (calificados como 1) o (calificados como 0). Por otro lado se les pasa el test de escala de inteligencia de adultos (score) para saber si la puntuación obtenida puede ser un indicador de si la persona tiene rasgos de senilidad o .Ejercicio 4. Se realiza un estudio para conocer que el impacto de la bomba de hiroshima en al aparición de caso de leucemia. Para ello se registro para todos los sujetos que presentaron algún tipo de cáncer el grado de radiación al que fue sometida la persona. Las variables que aparecen son los conteos del número de casos de leucemia (leukemia) y de otros tipos de cáncer () para los diferentes niveles de radiación (radiation). Se contabilizan también el número total de casos de cáncer registrados (total). Es de interés en el análisis investigar la influencia del grado de radiación en la aparición de un mayor número de casos de leucemia.Ejercicio 5. La Oficina Europea de Patentes puede proteger una patente de la competencia durante un cierto período de tiempo. La Oficina de Patentes tiene la tarea de examinar las invenciones y declarar patentes si se cumplen ciertos requisitos previos. El requisito más importante es que la invención sea algo realmente nuevo. Con el fin de analizar las objeciones contra las patentes, se recopiló un conjunto de datos con 4,866 patentes de los sectores biotecnología / farmacia y semiconductor / computadora. Las variables inclinadas son: opp = oposición de patente; biopharm = Patente del sector biotecnológico / farmacéutico; ustwin = existe la patente gemela estadounidense; patus = titular de la patente de los Estados Unidos; patgsgr = titular de la patente de Alemania, Suiza o Gran Bretaña; año = Año de concesión; ncit = Número de citas de la patente; ncountry = Número de estados designados para la patente; nclaims = Número de declaraciones. ¿Cómo afecta cada una de las variables consideradas en la posible objeción una patente?Ejercicio 6. Los datos siguientes describen los patrones de comportamiento en el consumo de drogas psicotrópicas en una muestra de individuos del Oeste de Londres. Los investigadores se plantean las preguntas siguientes:¿Hay diferencias por sexo en el consumo de drogas? ¿Cómo influye la edad para explicar el consumo de psicotrópicos? ¿La edad influye igual en hombres y en mujeres?Obtén la expresión y el valor de las predicciones sobre el consumo de psicotrópicos en hombres y en mujeres de 17 y 52 años con el modelo ajustado.Las variables usa y nousa contienen la información sobre el uso de drogas psicotrópicas.Ejercicio 7. Se desea estudiar la sensibilidad de un test basado en el diagnóstico de la tuberculosis través de una prueba basada en rayos X. la información registrdas viene agrupada para las variables tuberculosis, total y rayosx. En las dos primeras se registran los positivos y negativos detectados por rayox para los enfermos de tuberculosis y para todos los que se realziaron las pruebas.Ejercicio 8. En el Hospital de Yale-New Heaven, en Connecticut, se llevó cabo un estudio para investigar la relación entre los nacimientos prematuros (el niño nazca antes de 37 semanas de gestación o su peso sea inferior 2500 g.) y la edad de la madre. La población de estudio consistió en 175 madres de niños nacidos únicos y prematuros, y 303 madres de niños prematuros. Los datos agrupados en función de la edad de la madre se presentan continuación. ¿Hay alguna relación entre la edad de la madre y el hecho de que un niño nazca prematuro? ¿Cuál es el grupo de edad con mayor riesgo?Ejercicio 9. En el banco de datos siguiente se presentan los resultados de una encuesta realizada en 1998. cada sujeto de una muestra de 300 adultos se le pidió que opinara sobre qué política consideraba adecuada implantar respecto al uso de tabaco en lugares públicos. Las opciones planteadas son:Opción 1: Sin restriccionesOpción 2: Fumar sólo en áreas exclusivasOpción 3: fumar nuncaOpción 4: opina¿Hay alguna relación entre la actitud frente al tabaco y el nivel de estudios?","code":"\nejercicio01 = read_csv(\"https://goo.gl/E2MlSZ\", col_types = \"dii\")\n# Calculamos los vivos para el ajuste de modelos\nejercicio01 = ejercicio01 %>% \n  mutate(alive = number - dead)\nejercicio02 = read_csv(\"https://goo.gl/6P3zRr\", col_types = \"iici\")\n# Recodificación del factor y variable de no embryogenic\nejercicio02 = ejercicio02 %>%\n  mutate(storage=fct_recode(storage,\"Control\" = \"1\",\"treatment\" = \"2\"),\n         nembrig = total - embryogenic)\nejercicio03 = read_csv(\"https://goo.gl/6E8fhd\", col_types = \"ic\")\n# Convertimos la respuesta para el ajuste de modelos\nejercicio03 = ejercicio03 %>% \n  mutate(senility = 1*(senility==1))\nejercicio04 = read_csv(\"https://goo.gl/ZDIWVC\", col_types = \"ciii\")\nejercicio05 = read_csv(\"https://goo.gl/2EygLk\", col_types = \"ccccciiii\")\n# Recodificación de factores\nejercicio05 = ejercicio05 %>% \n  mutate(opp = fct_recode(opp,\"yes\" = \"1\",\"no\" = \"0\"), \n         biopharm=fct_recode(biopharm,\"yes\" = \"1\",\"no\" =\"0\"),\n         ustwin=fct_recode(ustwin,\"yes\" = \"1\",\"no\" = \"0\"),\n         patus=fct_recode(patus,\"yes\" = \"1\",\"no\" = \"0\"),\n         patgsgr=fct_recode(patgsgr,\"yes\" = \"1\",\"no\" = \"0\"))\nsexo <- c(rep(\"H\",4),rep(\"M\",4))\nedad <- c(\"16-29\",\"30-44\",\"45-64\",\"65-74\",\"16-29\",\"30-44\",\"45-64\",\"65-74\")\nusa <- c(21,32,70,43,46,89,169,51)\nnousa <- c(683,596,705,295,738,700,847,196)\nejercicio06 = data.frame(sexo, edad, usa, nousa)\ntuberculosis <- c(22,8)\ntotal <- c(51,1739)\nrayosx <- c(\"Positivo\",\"Negativo\")\nejercicio07 = data.frame(tuberculosis, total, rayosx)\nedad <- c(\"14-17\",\"18-19\",\"20-24\",\"25-29\",\"+30\")\ncasos <- c(15,22,47,56,35)\ncontroles <- c(16,25,62,122,18)\nejercicio08 = data.frame(edad, casos, controles)\nnivel <- c(\"Est. Superiores\", \"Secundaria\", \"Primaria\")\nopt1 <- c(5,15,15)\nopt2 <- c(44,100,40)\nopt3 <- c(23,30,10)\nopt4 <- c(3,5,10)\ntotal <- c(75,150,75)\nejercicio09 = data.frame(nivel, opt1, opt2, opt3, opt4, total)"},{"path":"glmpoisson.html","id":"glmpoisson","chapter":"Unidad 14 GLM Poisson","heading":"Unidad 14 GLM Poisson","text":"Son frecuentes los datos discretos que provienen de conteos de sucesos que se producen por azar con cierta frecuencia y son modelizables en términos de tasas de incidencia que dependen de ciertas variables predictoras. Para modelizar este tipo de datos se utiliza la distribución de Poisson, \\(X \\sim Po(\\mu)\\), donde \\(\\mu\\) representa el número medio de ocurrencias, de forma que:\\[E(X) = \\mu \\quad \\text{ y } \\quad  V(X) = \\mu.\\]El parámetro \\(\\mu\\) requiere una definición cuidadosa. menudo es necesario describirlo como una tasa; por ejemplo, el número promedio de clientes que compran un particular producto de cada 100 clientes que ingresan la tienda. De manera más general, la tasa se especifica en términos de unidades de “exposición” o en términos de años-persona ‘en riesgo’, por ejemplo, el número de personas que sufren cierta enfermedad sobre el total de personas para un instante de tiempo determinado. El efecto de las variables predictoras sobre la respuesta se modeliza valorando su efecto sobre \\(\\mu\\).Sea \\(Y_1, Y_2,..., Y_n\\) un conjunto de variables aleatorias Poisson, donde \\(Y_i\\) representa el número de eventos observados partir de la exposición \\(n_i\\) para el -ésimo patrón de covariables. En esta situación el valor esperado de \\(Y_i\\) se puede escribir como:\\[E(Y_i) = \\mu_i = n_i \\lambda_i,\\]\ndonde \\(\\lambda_i\\) representa la tasa de incidencia para el patrón -ésimo.En este tipo de modelos es muy habitual trabajar con variables categóricas de tipo ordinal. Dichas variables pueden ser incorporadas en el modelo como factores pero en ocasiones interesa introducirlas como numéricas para captar las posibles tendencias de la respuesta conforme aumenta la relevancia del factor. Recordemos que una variable categórica ordinal se puede obtener siempre partir de una variable de tipo numérico. Para poder hacer esto introducimos un código numérico artificial asociado con cada categoría. Disponemos de dos alternativas:Asignar un código continuo: 1, 2, 3,… donde los valores más bajos se asocian con los niveles más bajos de la variable categórica.Cuando la variable categórica viene dada en términos de un intervalo se puede usar el punto medio de dicho intervalo para establecer el código numérico.De hecho, en el análisis descriptivo inicial de los datos (sobre todo en los gráficos) esta forma de proceder permite analizar con más detalle la evolución del factor ordinal en la respuesta.","code":""},{"path":"glmpoisson.html","id":"bancos-de-datos-4","chapter":"Unidad 14 GLM Poisson","heading":"14.1 Bancos de datos","text":"continuación se epresentan los ancos de datos con los que iremos trabajando.","code":""},{"path":"glmpoisson.html","id":"telas","chapter":"Unidad 14 GLM Poisson","heading":"14.1.1 Telas","text":"Se recogen los resultados de un experimento para determinar el efecto del tipo de lana (o B) y la tensión (baja, media o alta) en el número de roturas de deformación en la fabricación de telas. Se recopilaron datos de nueve telares para cada combinación de configuraciones. Las variables consideradas son:breaks (número de roturas),wool (tipo de lana),tension (tensión de la lana).La pregunta de interés es si resulta posible predecir el número de roturas en función del tipo de lana y de la tensión.En este caso la exposición riesgo es constante (\\(n_i =1\\)) de forma que \\(log(n_i) = 0\\) desapareciendo de la especificación del modelo. Por tanto, se considera que las posibles predictoras influyen directamente sobre \\(\\mu_i = breaks_i\\).En primer lugar cargamos los datos de la librería datasetsPara la representación gráfica y análisis de este banco de datos introducimos una variable codificada para el factor ordinal tension mediante un código continuo:Representamos la incidencia (en este caso \\(n_i = 1\\)) que corresponde con la variable breaks, es decir, representamos el logaritmo de la incidencia con respecto las predictoras, para valorar la linealidad del efecto.En el gráfico se aprecia cierta tendencia con la tensión, pero tanto con el tipo de lana. Realizamos el gráfico de interacción con las medias para las combinaciones de ambos factores.Se puede ver un efecto conjunto entre tension y wool. Parece bastante claro que deberemos considerar un efecto de interacción para este modelo. De hecho, se aprecian curvas descendientes con tension (indicando que las telas sufren menos roturas mayor tensión) apreciándose efecto lineal lo que puede provocar que se verifiquen las hipótesis del modelo, y que sea necesario introducir ciertas tendencias en las variables predictoras. En el tipo de lana resulta imposible hacer esto porque es una variable categórica nominal, pero si podríamos hacerlo con tension si consideramos la variable numérica construida partir de ella.","code":"\nroturas <- warpbreaks\nstr(roturas)## 'data.frame':    54 obs. of  3 variables:\n##  $ breaks : num  26 30 54 25 70 52 51 26 67 18 ...\n##  $ wool   : Factor w/ 2 levels \"A\",\"B\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ tension: Factor w/ 3 levels \"L\",\"M\",\"H\": 1 1 1 1 1 1 1 1 1 2 ...\n# Creamos código asociado a cada nivel del factor y la variable asociada\nroturas <- roturas %>% \n  mutate(t1 = ifelse(tension == \"L\", 1, 0)) %>%\n  mutate(t2 = ifelse(tension == \"M\", 2, 0)) %>%\n  mutate(t3 = ifelse(tension == \"H\", 3, 0)) %>%\n  mutate(tension.num = t1 + t2 + t3) \n# Nos quedamos con las variables de interés\nroturas <- dplyr::select(roturas,c(\"breaks\", \"wool\", \"tension\", \"tension.num\"))\n# Utilizamos las dos variables categóricas\nggplot(roturas,aes(x = tension, y = log(breaks), color = wool)) + \n  geom_boxplot()\nggplot(roturas, aes(x = tension, y = log(breaks), group = wool, color = wool)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\")"},{"path":"glmpoisson.html","id":"barcos","chapter":"Unidad 14 GLM Poisson","heading":"14.1.2 Barcos","text":"En este banco de datos se analizan lo datos proporcionados por la aseguradora Lloyd’s con respecto al número de incidentes causados por las olas en cierto componente de los buques cargueros. El banco de datos contiene información sobre:type (el tipo de barco, codificado de hasta E),year (el año de construcción del barco en los periodos 1960-64, 65-69, 70-74, 75-79 codificados como 60, 65, 70, 75),period (período de operación del barco 1960-74, 75-79 codificados como 60, 75),service (meses de servicio),incidents (número de incidentes sufridos).La pregunta de interés es si resulta posible predecir el número de incidentes en función de las predictoras consideradas.En este caso la exposición riesgo es constante, ya que cuanto mayor sea el tiempo de servicio de un barco parece más probable que sufra un incidente. En este caso \\(n_i = service_i\\) de forma que \\(log(n_i) = log(service_i)\\) remarcando el hecho de que el riesgo aumenta cuando lo hace el tiempo de servicio. El offset se puede estimar ya que es fijo, y modelizamos la tasa de riesgo \\(\\lambda_i\\) con la información de las variables predictoras. En la práctica dicha tasa debe aproximarse \\(\\lambda_i = incidents_i/service_i\\), por lo que representamos dicha variable con respecto las predictoras para el planteamiento del modelo.Comenzamos con la lectura del banco de datos creando factores partir de los códigos numéricos de las variables year y period.Un análisis descriptivo inicial del banco de datos muestra que hay barcos que tienen 0 meses de servicio. Estos barcos deben ser eliminados del análisis, ya que al tratar de ajustar la tasa de incidentes dividiríamos por un valor de 0, lo que provocaría la imposibilidad de un ajuste adecuado. Seleccionamos todos los barcos con tiempo de servicio mayor que cero y obtenemos la tasa de incidencia partir de la relación entre el número de incidentes y las horas de servicio de cada barco.Realizamos el gráfico de la media del logaritmo de la tasa para type, year y period. En realidad trabajamos con los factores para representar adecuadamente el gráfico de interacción.Se observa que los tipos de barco se comportan de forma diferente en cuanto la tasa de incidentes. Los barcos del tipo “E” son los que tienen tasas más altas. En cuanto al año de fabricación y el periodo de funcionamiento varían con el tipo de barco. Mientras que para los tipos “” y “B” se observan comportamientos muy similares ocurre los mismo para el tipo “C”. De nuevo habrá que vigilar el efecto para asegurar la linealidad del predictor lineal asociado al GLM considerado.En este caso nos podemos plantear un modelo con tres predictoras de tipo factor y considerar sus posibles interacciones (lo que nos da un modelo bastante complejo), u optar por un modelo más sencillo donde consideramos year y period como numéricas. Obtenemos la tabla cruzada de ambos factores para ver si hay alguna combinación que se da, y por tanto desechar el modelo de interacción al que darían lugar.Se observa que la combinación year_f = 75 y period_f = 60 tiene ningún dato. Si consideráramos un modelo con dicha interacción resultaría imposible obtener la estimación de dicho modelo. Para evitar ese problema, y dado el carácter ordinal de los factores, consideramos sus versiones numéricas para la estimación del modelo correspondiente.","code":"\nbarcos <- ships\n# Generamos factores a partir de los códigos numéricos\nbarcos$year.f <- as.factor(barcos$year)\nbarcos$period.f <- as.factor(barcos$period)\nbarcos <- barcos %>% \n  filter(service>0) %>%\n  mutate(tasa = incidents/service)\nggplot(barcos, aes(x = year.f, y = log(tasa), group = period.f, color = period.f)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun = mean, geom = \"line\") +\n  facet_grid(~type) \ntable(barcos$year.f, barcos$period.f)##     \n##      60 75\n##   60  5  4\n##   65  5  5\n##   70  5  5\n##   75  0  5"},{"path":"glmpoisson.html","id":"modelo-teórico-1","chapter":"Unidad 14 GLM Poisson","heading":"14.2 Modelo Teórico","text":"El modelo lineal generalizado para variables aleatorias Poisson o más conocido como regresión de Poisson se puede especificar siguiendo la definición establecida en el punto anterior como:La distribución de cada variable aleatoria \\(Y_i\\) viene dada por\n\\[Y_i \\sim Po(\\mu_i)\\]El valor medio de cada variable viene dado por:\n\\[E(Y_i) = \\mu_i = n_i\\lambda_i\\]Si consideramos un conjunto \\(x_{i1}, x_{i2},...,x_{ip}\\) de \\(p\\) variables predictoras que pueden afectar el comportamiento de \\(Y_i\\) asumimos que:\n\\[\\lambda_i = exp(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ...+ \\beta_p x_{ip}),\\]\ndonde cada \\(\\beta\\) representa el efecto de la correspondiente variable predictora.En esta situación y tomando como función de enlace el logaritmo tenemos que:\\[log(E(Y_i)) = log(\\mu_i) = log(n_i\\lambda_i) = log(n_i) + log(\\lambda_i)= log(n_i) + \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ...+ \\beta_p x_{ip}\\]donde tenemos la expresión del modelo lineal generalizado. La única diferencia de este modelo con los vistos hasta ahora es la inclusión de un término fijo en el modelo (\\(log(n_i)\\)) que actúa como “offset” y hace referencia al logaritmo de sujetos en riesgo para la combinación de predictoras correspondiente al índice \\(\\). Este término se debe incorporar en la estimación del modelo como una constante para cada \\(\\).Respecto de la interpretación de dichos coeficientes se puede demostrar que \\(exp(\\beta_j)\\) representa el riesgo relativo (\\(RR\\)) sobre la tasa de incidencia de los sucesos asociado un incremento de una unidad en la covariable \\(x_j\\). Es similar la interpretación que hacíamos en los modelos lineales, salvo el cambio de escala debido la transformación logaritmo sobre la media de la respuesta.Para una variable de tipo numérico se define el riesgo relativo al incrementar una unidad el valor de la variable predictora \\(X\\) como:\\[RR(X) = \\frac{\\lambda(X = x+1)}{\\lambda(X = x+1)} = exp(\\beta_x)\\]mientras que el riesgo relativo para valorar el cambio entre dos niveles de una variable categórica (y B) se define como:\\[RR(X_{AB}) = \\frac{\\lambda(X = B)}{\\lambda(X = )} = exp(\\text{coeficiente asociado B} - \\text{coeficiente asociado })\\]En ambos casos, el riesgo relativo aumenta cuando \\(RR > 1\\) y disminuye cuando \\(RR < 1\\). Tenemos el mismo riesgo cuando \\(RR = 1\\). De esta forma tenemos un indicador de cuanto aumenta la respuesta en función de las predictoras consideradas.","code":""},{"path":"glmpoisson.html","id":"estimación-y-bondad-de-ajuste-del-modelo-1","chapter":"Unidad 14 GLM Poisson","heading":"14.3 Estimación y bondad de ajuste del modelo","text":"Como hemos hecho hasta ahora, en un primer momento estimamos el modelo más complejo posible para proceder posteriormente con la selección de aquellos efectos que de verdad resultan significativos. Para la estimación utilizamos la función glm con las especificaciones siguientes:La diferencia principal con los modelos de regresión logística es la introducción de los offsets en el ajuste del modelo. Si disponemos de offsets se especificarán en el ajuste del modelo.Para el estudio de bondad de ajuste utilizamos el test \\(\\chi^2\\) basado en la deviance del modelo. En este caso dejamos los calculo para el lector. También podemos saber la capacidad explicativa (explained deviance) del modelo que se puede obtener con la función summary(m) donde m es el modelo que deseamos estudiar.","code":"\nglm(modelo,family = poisson(), offset = value, data_set)"},{"path":"glmpoisson.html","id":"ejemplos-18","chapter":"Unidad 14 GLM Poisson","heading":"14.3.1 Ejemplos","text":"Procedemos con el análisis de cada uno de los ejemplos.","code":""},{"path":"glmpoisson.html","id":"telas-1","chapter":"Unidad 14 GLM Poisson","heading":"14.3.1.1 Telas","text":"Para este banco de datos ajustamos dos modelos completos considerando la tensión como factor (tension) o como variable numérica (tension.num).","code":""},{"path":"glmpoisson.html","id":"modelo-con-tension","chapter":"Unidad 14 GLM Poisson","heading":"14.3.1.1.1 Modelo con tension","text":"Para este banco de datos ajustamos dos modelos completos considerando la tensión como factor o como variable numérica. Comenzamos con el modelo considerando todas las predictoras como factores. Consideramos tanto los efectos asociados cada factor como la posible interacción entre ellos. En este caso hay offsets. El modelo propuesto es:\\[breaks \\sim wool*tension\\]que se puede ajustar medianteAl igual que ocurría con los modelos de regresión logística hacemos uso de la función tab_model para obtener las estimaciones del predictor lineal y de la tasa de incidencia. Analizamos en primer lugar el predictor lineal.Los coeficientes del modelo resultan significativos salvo en la combinación wool B y tension H. Ajustamos el modelo sin interacción de forma que las ecuaciones del predictor lineal para este nuevo modelo son:\\[\\left\\{\\begin{array}{ll} \nlog(breaks)_{woolA;tensionL} & = 3.80 \\\\ \nlog(breaks)_{woolA;tensionM} & = 3.80 - 0.62 = 3.18 \\\\\nlog(breaks)_{woolA;tensionH} & = 3.80 - 0.60 = 3.20 \\\\\nlog(breaks)_{woolB;tensionL} & = 3.80 - 0.46 = 3.34 \\\\ \nlog(breaks)_{woolB;tensionM} & = 3.80 - 0.46 - 0.62 + 0.64 = 3.36 \\\\\nlog(breaks)_{woolB;tensionH} & = 3.80 - 0.46 - 0.60 + 0.19 = 2.93 \\\\\n\\end{array}\\right.\\]El proceso de estimación nos proporciona que el mayor número de roturas se produce para la combinación wool = y tension = L, mientras que el menor número se estima para la combinación wool = B y tension = H.Obtenemos ahora las tasas de incidencia de rotura asociados con este modelo:Identificamos de esta forma los efectos del modelo que producen un mayor riesgo relativo (valores mayores que 1) con respecto la combinación que se utiliza como referencia en el modelo que es wool = y tension = L. En realidad la tabla presenta los valores de \\(exp(\\beta)\\) para cada uno de los efectos del modelo con las que podemos obtener las tasas de incidencia:\\[\\left\\{\\begin{array}{ll} \nbreaks_{woolA;tensionL} & = 44.56 \\\\ \nbreaks_{woolA;tensionM} & = 44.56 * 0.54 = 24.06 \\\\\nbreaks_{woolA;tensionH} & = 44.56 * 0.55 = 24.51 \\\\\nbreaks_{woolB;tensionL} & = 44.56 * 0.63 =28.07\\\\ \nbreaks_{woolB;tensionM} & = 44.56 * 0.63 * 0.54 * 1.89 = 28.65 \\\\\nbreaks_{woolB;tensionH} & = 44.56 * 0.63 * 0.55 * 1.21 = 18.68 \\\\\n\\end{array}\\right.\\]partir de las tasas de incidencia es posible estimar los riesgos relativos para la combinación de los dos factores. Comparamos las combinaciones {woolA;tensionL} y {woolA;tensionH} de forma que el riesgo relativo es:\\[\nRR_{(-L; -H)} = \\frac{44.56}{24.51} = 1.81\n\\]Se concluye que el riesgo de rotura de la combinación {woolA;tensionL} es casi el doble que el de la combinación {woolA;tensionH}.","code":"\nfit.telas.cat <-glm(breaks ~ wool*tension,\n                    family = poisson(), \n                    data = roturas)\ntab_model(fit.telas.cat, \n          transform = NULL, \n          show.r2 = FALSE)\ntab_model(fit.telas.cat, \n          show.r2 = FALSE)"},{"path":"glmpoisson.html","id":"modelo-con-tension.num","chapter":"Unidad 14 GLM Poisson","heading":"14.3.1.1.2 Modelo con tension.num","text":"Ajustamos ahora el modelo asumiendo la variable tensión con su código numérico (para valorar el efecto ordinal del factor). El modelo propuesto es:\\[breaks \\sim wool*tension.num\\]Ajustamos el modelo y estudíamos las ecuaciones de estimación obtenidasTodos los coeficientes estimados resultan significativos y las ecuaciones de estimación vienen dadas por:\\[\\left\\{\\begin{array}{ll} \nlog(breaks_{woolA}) & = 4.06 - 0.33*tension.num \\\\ \nlog(breaks_{woolB}) & = 3.60 - 0.19*tension.num\\\\ \n\\end{array}\\right.\\]Dado que los coeficientes asociados tension.num son negativos, el efecto de la tensión es menos relevante para wool = que para wool = B, de forma que el nivel de roturas disminuye al aumentar la tensión.Obtenemos las incidencias asociadas con el modelocon estimaciones para cada nivel de wool:\\[\\left\\{\\begin{array}{ll} \nbreaks_{woolA} & = 57.72 *0.72^{tension.num} \\\\ \nbreaks_{woolB} & = (57.72*0.63) *(0.72*1.15)^{tension.num} = 36.36*0.83^{tension.num}\\\\\n\\end{array}\\right.\\]de forma que el riesgo relativo entre los dos tipos de wool viene dado por:\\[\nRR_{(;B)} = \\frac{57.72*0.72^{tension.num}}{36.36*0.83^{tension.num}} = 1.59*0.87^{tension.num}\n\\]cuyos valores son 1.38, 1.20 y 1.05 para los valores de tension 1, 2, y 3 respectivamente. Esto implica que el riesgo de rotura entre wool = y wool = B se iguala (tiende 1) cuando aumenta la tensión, aunque siempre es superior en que en B.Otras alternativas este modelo serían considerar un modelo polinómico con tension.num para capturar la curvas observadas en los gráficos de medias o planteara un modelo de suavizado con ella y con interacción con wool. En estos materiales exploramos estas posibilidades.En el apartado de selección del modelo valoraremos los dos modelos construidos para determinar cual de ellos es mejor.","code":"\nfit.telas.num <-glm(breaks ~ wool*tension.num,\n                family = poisson(), \n                data = roturas)\ntab_model(fit.telas.num, \n          transform = NULL, \n          show.r2 = FALSE)                \ntab_model(fit.telas.num, \n          show.r2 = FALSE)                "},{"path":"glmpoisson.html","id":"barcos-1","chapter":"Unidad 14 GLM Poisson","heading":"14.3.1.2 Barcos","text":"Ajustamos el modelo utilizando las variables con los códigos numéricos en lugar de los factores. En este caso si disponemos de offset y el modelo planteado viene dado por:\\[incidents \\sim offset(service) + type *(year + period)\\]\nEn realidad la estimación de este modelo nos proporciona las tasas de incidentes \\(\\lambda_i = incidents_i/service_i\\). Realizamos el ajuste del modelo partir del cual podemos obtener las ecuaciones de estimación por los diferentes niveles de type:Las ecuaciones para las diferentes tasas son:\\[\\left\\{\\begin{array}{ll} \nlog(\\lambda_{typeA}) & = - 9.24  + 0.04*year + 0.02*period \\\\ \nlog(\\lambda_{typeB}) & = - 11.51 + 0.05*year + 0.03*period \\\\ \nlog(\\lambda_{typeC}) & = - 12.18 + 0.17*year - 0.07*period \\\\ \nlog(\\lambda_{typeD}) & = - 7.70  - 0.02*year + 0.05*period \\\\ \nlog(\\lambda_{typeE}) & =  4.16   - 0.16*year + 0.03*period \\\\ \n\\end{array}\\right.\\]Sin embargo, dado que el modelo presenta muchos coeficientes que parecen significativos, dejaremos la interpretación de este modelo hasta la selección del modelo definitivo.","code":"\nfit.barcos.num <-glm(incidents ~ type *(year + period),\n                 family = poisson(), \n                 offset = log(service), \n                 data = barcos)\ntab_model(fit.barcos.num, \n          transform = NULL, \n          show.r2 = FALSE)   "},{"path":"glmpoisson.html","id":"selección-del-modelo-1","chapter":"Unidad 14 GLM Poisson","heading":"14.4 Selección del modelo","text":"Para la construcción del mejor modelo utilizaremos los mismos procedimientos que presentamos en la unidad anterior basados en los procedimientos secuenciales con el estadístico AIC.","code":""},{"path":"glmpoisson.html","id":"ejemplos-19","chapter":"Unidad 14 GLM Poisson","heading":"14.4.1 Ejemplos","text":"Estudiamos continuación cada uno de los ejemplos.","code":""},{"path":"glmpoisson.html","id":"datos-de-telas","chapter":"Unidad 14 GLM Poisson","heading":"14.4.1.1 Datos de telas","text":"En este caso realizamos la selección tanto en el modelo con la variable tensión como factor y como numérica, y elegimos como mejor modelo el que tenga un menor valor de AIC.Comenzamos con la selección del modelo con factorNo se elimina ningún efecto del modelo y el AIC correspondiente es de 468.97. Procedemos ahora con el modelo con tensión como variable numérica.De nuevo se elimina ningún efecto pero el AIC es de 489.55, de forma que entre los dos modelos nos quedaríamos con el que toma la variable tension como factor.Recordamos las ecuaciones de estimación del número de roturas obtenidas en el apartado anterior, que venían dadas para cada combinación por:\\[\\left\\{\\begin{array}{ll} \nbreaks_{woolA;tensionL} & = 44.56 \\\\ \nbreaks_{woolA;tensionM} & = 24.06 \\\\\nbreaks_{woolA;tensionH} & = 24.51 \\\\\nbreaks_{woolB;tensionL} & = 28.07\\\\ \nbreaks_{woolB;tensionM} & = 28.65 \\\\\nbreaks_{woolB;tensionH} & = 18.68 \\\\\n\\end{array}\\right.\\]En la tabla siguiente se presentan los riesgos relativos asociados la comparación entra wool = y wool = B.\\[\\left\\{\\begin{array}{ll} \nRR\\left(\\frac{wool ; tension L}{wool B; tension L}\\right) = 44.56/28.07 = 1.59\\\\\nRR\\left(\\frac{wool ; tension L}{wool B; tension M}\\right) = 44.56/28.65 = 1.56\\\\\nRR\\left(\\frac{wool ; tension L}{wool B; tension H}\\right) = 44.56/18.68 = 2.39\\\\\nRR\\left(\\frac{wool ; tension M}{wool B; tension L}\\right) = 24.06/28.07 = 0.86\\\\\nRR\\left(\\frac{wool ; tension M}{wool B; tension M}\\right) = 24.06/28.65 = 0.84\\\\\nRR\\left(\\frac{wool ; tension M}{wool B; tension H}\\right) = 24.06/18.68 = 1.29\\\\\nRR\\left(\\frac{wool ; tension H}{wool B; tension L}\\right) = 24.51/28.07 = 0.87\\\\\nRR\\left(\\frac{wool ; tension H}{wool B; tension M}\\right) = 24.51/28.65 = 0.86\\\\\nRR\\left(\\frac{wool ; tension H}{wool B; tension H}\\right) = 24.51/18.68 = 1.31\\\\\n\\end{array}\\right.\\]¿Cómo interpretamos los valores obtenidos?Comparamos ahora para un mismo valor de wool atendiendo al orden del factor:\n\\[\\left\\{\\begin{array}{ll} \nRR\\left(\\frac{wool ; tension H}{wool ; tension M}\\right) = 24.51/24.06 = 1.02\\\\\nRR\\left(\\frac{wool ; tension M}{wool ; tension L}\\right) = 24.06/44.56 = 0.54\\\\\n\\end{array}\\right.\\]Mientras que el riesgo de sufrir una rotura para wool = es prácticamente el mismo para los valores H y M de tensión, este se duplica cuando comparamos los valores M y L. ¿Qué podemos decir si utilizamos wool = B?","code":"\n# Selección del modelo\nstats::step(fit.telas.cat)## Start:  AIC=468.97\n## breaks ~ wool * tension\n## \n##                Df Deviance    AIC\n## <none>              182.31 468.97\n## - wool:tension  2   210.39 493.06## \n## Call:  glm(formula = breaks ~ wool * tension, family = poisson(), data = roturas)\n## \n## Coefficients:\n##    (Intercept)           woolB        tensionM        tensionH  woolB:tensionM  woolB:tensionH  \n##         3.7967         -0.4566         -0.6187         -0.5958          0.6382          0.1884  \n## \n## Degrees of Freedom: 53 Total (i.e. Null);  48 Residual\n## Null Deviance:       297.4 \n## Residual Deviance: 182.3     AIC: 469\n# Selección del modelo\nstats::step(fit.telas.num)## Start:  AIC=489.55\n## breaks ~ wool * tension.num\n## \n##                    Df Deviance    AIC\n## <none>                  206.89 489.55\n## - wool:tension.num  1   211.63 492.30## \n## Call:  glm(formula = breaks ~ wool * tension.num, family = poisson(), \n##     data = roturas)\n## \n## Coefficients:\n##       (Intercept)              woolB        tension.num  woolB:tension.num  \n##            4.0555            -0.4620            -0.3279             0.1399  \n## \n## Degrees of Freedom: 53 Total (i.e. Null);  50 Residual\n## Null Deviance:       297.4 \n## Residual Deviance: 206.9     AIC: 489.6"},{"path":"glmpoisson.html","id":"datos-de-barcos","chapter":"Unidad 14 GLM Poisson","heading":"14.4.1.2 Datos de barcos","text":"Comenzamos con la selección del modeloEl proceso de selección elimina únicamente la interacción entre tipo y periodo, de forma que el modelo final viene dado por:Las estimaciones del predictor lineal vienen dadas por:de forma que las ecuaciones de estimación del logaritmo de la tasa de incidencia para cada combinación de tipo de barco vienen dadas por:\\[\\left\\{\\begin{array}{ll} \nlog(\\lambda_{typeA}) & = - 9.41  + 0.03*year + 0.02*period \\\\ \nlog(\\lambda_{typeB}) & = - 11.40 + 0.05*year + 0.02*period \\\\ \nlog(\\lambda_{typeC}) & = - 14.21 + 0.09*year + 0.02*period \\\\ \nlog(\\lambda_{typeD}) & = - 6.83  - 0.01*year + 0.02*period \\\\ \nlog(\\lambda_{typeE}) & =  4.21   - 0.16*year + 0.02*period \\\\ \n\\end{array}\\right.\\]Dado que el coeficiente asociado period es el mismo para todos los tipos de barco (efecto de interacción eliminado del modelo), el riesgo entre dos tipos de barco se asocia directamente con el año de fabricación y la interceptación (donde se incluye el tipo de barco también).Podemos obtener los riesgos partir de las ecuaciones anteriores. Por ejemplo, vamos obtener el riesgo relativo para los tipos C y E:\\[\nRR\\left(\\frac{type C}{type E}\\right) = \\frac{exp(- 14.21 + 0.09*year + 0.02*period)}{exp(4.21 - 0.16*year + 0.02*period)} = exp(-18.42 + 0.27*year)\n\\]que podemos representar gráficamente:Se aprecia como el riesgo relativo entre los tipos de barco C y E aumenta con el año de fabricación del barco, pero mientras que hasta el año 73 o 74 hay un mayor riesgo en los barcos de tipo E que en los de tipo C, en los años anteriores es al contrario. Se ha utilizado un línea horizontal en el 1 para identificar la igualdad de riesgo relativo.En el apartado de predicción representaremos la incidencia correspondiente todas las posibles combinaciones de predictoras.","code":"\n# Selección del modelo\nstats::step(fit.barcos.num)## Start:  AIC=167\n## incidents ~ type * (year + period)\n## \n##               Df Deviance    AIC\n## - type:period  4   45.138 165.00\n## <none>             39.136 167.00\n## - type:year    4   54.046 173.91\n## \n## Step:  AIC=165\n## incidents ~ type + year + period + type:year\n## \n##             Df Deviance    AIC\n## <none>           45.138 165.00\n## - type:year  4   59.375 171.24\n## - period     1   54.589 172.46## \n## Call:  glm(formula = incidents ~ type + year + period + type:year, family = poisson(), \n##     data = barcos, offset = log(service))\n## \n## Coefficients:\n## (Intercept)        typeB        typeC        typeD        typeE         year       period  \n##    -9.40739     -1.99170     -4.80174      2.57742     13.61537      0.03166      0.02474  \n##  typeB:year   typeC:year   typeD:year   typeE:year  \n##     0.02134      0.06103     -0.03907     -0.19193  \n## \n## Degrees of Freedom: 33 Total (i.e. Null);  23 Residual\n## Null Deviance:       146.3 \n## Residual Deviance: 45.14     AIC: 165\nfit.barcos.num <-glm(incidents ~ type + year + period + type:year,\n                 family = poisson(), \n                 offset = log(service), \n                 data = barcos)\ntab_model(fit.barcos.num, \n          transform = NULL, \n          show.r2 = FALSE)   \n# Asignamos valores al año\nsecyear <- 60:75\n# Calculamos el riesgo entre C y E como el cociente de tasas estimadas\nrr <- exp(-14.21 + 0.09*secyear)/exp(4.21 -0.16*secyear)\nriesgos <- data.frame(secyear,rr)\n# Gráfico\nggplot(riesgos,aes(x = secyear, y = rr)) +\n  geom_line() +\n  geom_hline(yintercept = 1, color = \"red\") +\n  labs(x =\"Año\", y = \"Riesgo relativo\")"},{"path":"glmpoisson.html","id":"diagnóstico-5","chapter":"Unidad 14 GLM Poisson","heading":"14.5 Diagnóstico","text":"Las herramientas de diagnóstico para este tipo de modelos son las mismas que el caso de los GLM con respuesta binaria. Los gráficos que vamos utilizar son:Residuos vs Ajustados (predictor lineal)Residuos vs variables en el modelo (predictor lineal)Valores influyentes","code":""},{"path":"glmpoisson.html","id":"ejemplos-20","chapter":"Unidad 14 GLM Poisson","heading":"14.5.1 Ejemplos","text":"Procedemos con el diagnóstico de los modelos obtenidos hasta el momento. En caso de encontrar problemas con alguno de ellos, procederemos como en casos anteriores.","code":""},{"path":"glmpoisson.html","id":"datos-de-telas-1","chapter":"Unidad 14 GLM Poisson","heading":"14.5.1.1 Datos de telas","text":"Obtenemos las cantidades de interés y realizamos los gráficos correspondientes.Obtenemos todas las cantidades de interés del modeloRealizamos el gráfico de residuos versus ajustados:Dado que las dos predictoras son factores el gráfico que aparece es bastante natural ya que solo resulta predecir cuatro valores posibles, que corresponden con las cuatro combinaciones de los dos factores. se aprecian comportamientos extraño en cuanto la tendencia o la homocedasticidad.Realizamos ahora los gráficos versus predictoras.se aprecian comportamientos que hagan dudar del incumplimiento de las hipótesis. El modelo obtenido es adecuado y puede ser usado para predecir el número de roturas debido los factores considerados.","code":"\n# Obtención de valores para el diagnóstico\ndiagnostico <- fortify(fit.telas.cat)\n#Gráfico de residuos vs ajustados (predictor lineal)\nggplot(diagnostico, aes(x = .fitted, y = .stdresid)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(x = \"Ajustados\", y = \"Residuos\")\n# Gráficos de residuos vs predictoras (preditor lineal)\nplot_model(fit.telas.cat, type = \"resid\",\n           show.data = TRUE,\n           ci.lvl = NA) + \n  geom_point(color = \"black\", size = 0.8)"},{"path":"glmpoisson.html","id":"datos-de-barcos-1","chapter":"Unidad 14 GLM Poisson","heading":"14.5.1.2 Datos de barcos","text":"Obtenemos las cantidades de interés y realizamos los gráficos correspondientes.Realizamos el gráfico de residuos versus ajustados:Los residuos se reparten aleatoriamente pero parece apreciarse cierto efecto de embudo con los valores ajustados. Realizamos los gráficos con respecto las predictoras:Mientras que respecto de period y type se aprecia ningún problema, si existe cierta tendencia con respecto year. Se podría optar por un modelo polinómico o plantear directamente un suavizado con es variable. Se propone utiliza el modelo de suavizado, pero dado que tenemos pocos datos para tipo de barco, rebajamos el valor del número de nodos 5 para conseguir el ajuste.Consideramos un modelo con interacción type:period y otro sin interacción para valorar cual de ellos se comporta mejor mediante el estadístico AIC:El modelo seleccionado es el que tiene la interacción. Empezamos el diagnóstico de dicho modelo asignando el modelo obtenido y realizamos el diagnóstico.Se observan dos observaciones alejadas del resto por lo que se podría considerar su eliminación. Sin embargo, dado que el banco de datos contiene pocas observaciones podríamos tener problemas para ajustar el modelo si las eliminamos. Puesto que el test del suavizado presenta ningún problema, utilizaremos el modelo obtenido como base para la predicción.El problema con las funciones de suavizado es que resulta más complicado la evaluación de los riesgos relativos, ya que al disponer de una forma específica para las predictoras que forman parte del suavizado, podemos obtener directamente dicho riesgo. En el apartado de predicción veremos como estimar dichos riesgos.","code":"\n# Obtención de valores para el diagnóstico\ndiagnostico <- fortify(fit.barcos.num)\n#Gráfico de residuos vs ajustados (predictor lineal)\nggplot(diagnostico, aes(x = .fitted, y = .stdresid)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\") +\n  labs(x = \"Ajustados\", y = \"Residuos\")\n# Gráficos de residuos vs predictoras (preditor lineal)\nplot_model(fit.barcos.num, type = \"resid\",\n           show.data = TRUE,\n           ci.lvl = NA) + \n  geom_point(color = \"black\", size = 0.8)\n# Modelo con interacción type:period\nM1 <- gam(incidents ~ type + period + \n            s(year, k = 5, m = 2, bs = \"ps\", by = type) + \n            type:period,\n          family = poisson(),\n          offset = log(service), \n          data = barcos)\n# Modelo sin interacción type:period\nM2 <- gam(incidents ~ type + period + \n            s(year, k = 5, m = 2, bs = \"ps\", by = type),\n          family = poisson(), \n          offset = log(service), \n          data = barcos)\n# Comparamos ambos modelos\nAIC(M1, M2)##          df      AIC\n## M1 16.98855 145.4310\n## M2 13.00991 143.3748\n# Asignamos el modelo \nfit.barcos.num <- M2\n# Gráficos de diagnóstico\ngam.check(fit.barcos.num)## \n## Method: UBRE   Optimizer: outer newton\n## full convergence after 10 iterations.\n## Gradient range [-2.348054e-07,1.788437e-12]\n## (score 0.3384798 & scale 1).\n## Hessian positive definite, eigenvalue range [1.827615e-07,0.02021138].\n## Model rank =  26 / 26 \n## \n## Basis dimension (k) checking results. Low p-value (k-index<1) may\n## indicate that k is too low, especially if edf is close to k'.\n## \n##                 k'  edf k-index p-value\n## s(year):typeA 4.00 1.17    1.17    0.81\n## s(year):typeB 4.00 1.97    1.17    0.89\n## s(year):typeC 4.00 1.00    1.17    0.83\n## s(year):typeD 4.00 1.87    1.17    0.83\n## s(year):typeE 4.00 1.00    1.17    0.86"},{"path":"glmpoisson.html","id":"predicción-5","chapter":"Unidad 14 GLM Poisson","heading":"14.6 Predicción","text":"continuación se presentan los resultados de predicción para los bancos de datos que hemos venido trabajando. Antes de proceder con la predicción para este tipo de modelos es necesario realizar la siguiente anotación. La inclusión de offset altera en cierta forma el proceso de predicción. En esta situación solo es posible predecir las tasas asociadas al modelo propuesto y la predicción de la variable original. Podemos obtener el valor estimado en la escala de la variable original sin más que multiplicar las tasas predichas por un valor del offset prefijado.","code":""},{"path":"glmpoisson.html","id":"bancos-de-datos-5","chapter":"Unidad 14 GLM Poisson","heading":"14.6.1 Bancos de datos","text":"Detallamos el procedimiento de predicción en cada ejemplo.","code":""},{"path":"glmpoisson.html","id":"datos-de-telas-2","chapter":"Unidad 14 GLM Poisson","heading":"14.6.1.1 Datos de telas","text":"En este caso tenemos offset y podemos realizar la predicción sin problemas. Obtenemos tanto el gráfico como la tabla de predicción.El cociente entre los valores predichos de la tabla nos permite realizar una evaluación del riesgo de dos combinaciones de los factores. El gráfico nos permite realizar una comparación aproximada del riesgo, y una evaluación de la combinación que produce un mayor número de roturas (wool = y tension = L), y la que menos (wool = B y tension = H).","code":"\n# Obtenemos el objeto de predicción\ngpred <- plot_model(fit.telas.cat,\"pred\",\n                    terms=c(\"wool\",\"tension\"),\n                    axis.labels = c(\"Wool\", \"Número de roturas\"),\n                    title = \"Predicción del numéro medio de roturas\")\ngpred$data## # Predicted counts of breaks\n## \n## # tension = L\n## \n## wool | Predicted | group_col |         95% CI\n## ---------------------------------------------\n##    1 |     44.56 |         L | [40.40, 49.14]\n##    2 |     28.22 |         L | [24.96, 31.92]\n## \n## # tension = M\n## \n## wool | Predicted | group_col |         95% CI\n## ---------------------------------------------\n##    1 |     24.00 |         M | [21.00, 27.42]\n##    2 |     28.78 |         M | [25.48, 32.50]\n## \n## # tension = H\n## \n## wool | Predicted | group_col |         95% CI\n## ---------------------------------------------\n##    1 |     24.56 |         H | [21.52, 28.02]\n##    2 |     18.78 |         H | [16.15, 21.83]\ngpred"},{"path":"glmpoisson.html","id":"datos-de-barcos-2","chapter":"Unidad 14 GLM Poisson","heading":"14.6.1.2 Datos de barcos","text":"En este caso la predicción hace referencia las tasas de incidencia del número de incidentes y sobre el número de incidentes.El gráfico de predicción nos permite apreciar los cambios en la tasa de incidencia para la combinación de las predictoras consideradas (incluso con el suavizado introducido). Los tipos de barco tienen perfiles similares al comparar por periodo, pero su evolución es diferente en cuanto al año de fabricación. En casi todos los tipos de barco hay una evolución ligeramente creciente, salvo en el tipo D donde hay un descenso en los últimos años, y en el tipo E que hay un descenso.Para obtener el número de incidentes podemos fijar un valor de tiempo de servicio y multiplicarlo por los valores predichos obtenidos en la tabla.","code":"\n# Obtenemos el objeto de predicción\ngpred <- plot_model(fit.barcos.num,\"pred\",\n                    terms=c(\"year\",\"period\",\"type\"),\n                    ci.lvl = NA,\n                    axis.labels = c(\"\", \"Tasa de incidentes\"),\n                    title = \"Predicción de la tasa de incidencia de acidentes\")\ngpred$data## # Predicted counts of incidents\n## \n## # period = 60\n## #   type = A\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |         0 |        60\n##   65 |         0 |        60\n##   70 |         0 |        60\n##   75 |         0 |        60\n## \n## # period = 75\n## #   type = A\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |      0.00 |        75\n##   65 |      0.00 |        75\n##   70 |      0.00 |        75\n##   75 |      0.01 |        75\n## \n## # period = 60\n## #   type = B\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |         0 |        60\n##   65 |         0 |        60\n##   70 |         0 |        60\n##   75 |         0 |        60\n## \n## # period = 75\n## #   type = B\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |         0 |        75\n##   65 |         0 |        75\n##   70 |         0 |        75\n##   75 |         0 |        75\n## \n## # period = 60\n## #   type = C\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |         0 |        60\n##   65 |         0 |        60\n##   70 |         0 |        60\n##   75 |         0 |        60\n## \n## # period = 75\n## #   type = C\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |         0 |        75\n##   65 |         0 |        75\n##   70 |         0 |        75\n##   75 |         0 |        75\n## \n## # period = 60\n## #   type = D\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |      0.00 |        60\n##   65 |      0.00 |        60\n##   70 |      0.01 |        60\n##   75 |      0.00 |        60\n## \n## # period = 75\n## #   type = D\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |      0.00 |        75\n##   65 |      0.00 |        75\n##   70 |      0.01 |        75\n##   75 |      0.00 |        75\n## \n## # period = 60\n## #   type = E\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |      0.02 |        60\n##   65 |      0.01 |        60\n##   70 |      0.00 |        60\n##   75 |      0.00 |        60\n## \n## # period = 75\n## #   type = E\n## \n## year | Predicted | group_col\n## ----------------------------\n##   60 |      0.03 |        75\n##   65 |      0.01 |        75\n##   70 |      0.01 |        75\n##   75 |      0.00 |        75\n## \n## Adjusted for:\n## * service = 7.05\ngpred"},{"path":"glmpoisson.html","id":"ejercicios-4","chapter":"Unidad 14 GLM Poisson","heading":"14.7 Ejercicios","text":"Ejercicio 1. En el banco de datos siguiente aparecen los datos correspondientes un pequeño experimento (grupo) en el que 7 de un total de 16 ratones fueron aleatoriamente seleccionados para recibir un nuevo tratamiento médico. Los 9 ratones restantes fueron asignados un grupo control en el que se administró ningún tipo de tratamiento. El objetivo del tratamiento era prolongar el tiempo de supervivencia (tiempo) después de una operación quirúrgica. La tabla muestra los tiempos de supervivencia (en días) tras la operación para los 16 ratones. El tratamiento, ¿prolongaba la vida de los ratones tras la operación?Ejercicio 2. Los datos siguientes contienen el tiempo de supervivencia, que es el tiempo (tiempo) hasta la muerte (en semanas) desde el diagnóstico de leucemia, y el número inicial de células blancas en sangre (lcelulas en escala log10), para un grupo de 17 sujetos que fueron diagnosticados de leucemia. ¿Cuáles son tus conclusiones sobre la utilidad de saber el número de células blancas en la sangre para predecir el tiempo de supervivencia? ¿Cuál es la probabilidad de sobrevivir de un paciente con 5000 células blancas en la sangre?Ejercicio 3. Los datos de este ejercicio provienen de un famoso estudio realizado por Sir Richard Doll y colegas. En 1951, todos los médicos británicos se les envió un breve cuestionario sobre si fumaban o . Desde entonces, la información sobre la causa de la muerte ha sido recogida desde entonces. El conjunto de datos muestra el número de muertes por enfermedad coronaria (deaths), si eran o fumadores (smoking), el grupo de edad al fallecimiento (age), y el número total de personas-años de observación en el momento del análisis (person-years). Para aprovechar el carácter ordinal de la variable age se sugiere utilizar una codificación numérica que identifique con un 1 al grupo de menor edad y con un 5 al grupo de mayor edad (otra opción sería utilizar el punto medio del intervalo). Representa la tasa de muertes por 100000 habitantes-año para el grupo de edad identificando si eran o fumadores y ajusta el modelo que consideres más oportuno atendiendo ese comportamiento. Los datos aparecen en Breslow Day (1987).Ejercicio 4. Los datos de este ejercicio contienen el número de pólizas de seguros de coches (n) y el número de reclamaciones o partes recibidos (y). Se registra además:el distrito donde se contrato la póliza (district), que toma valor 1 para las contratadas en la ciudad de Londres y 0 para el resto de ciudades,el nivel de la póliza contratada (car), registrada con un valor numérico donde 1 representa la categoría más baja y 4 la más alta,el grupo de edad del conductor (age), registrada con un valor numérico donde 1 indica los conductores de menor edad y 4 los de mayor edad.Estas dos últimas variables son categóricas ordinales que se codifican numéricamente para indicar el carácter ordinal de dicho factor. Analiza los datos teniendo en cuenta:Debes calcular el ratio entre pólizas y asegurados para cada categoría de las variables predictoras y representar sus posibles efectos.Ajusta un modelo adecuado la información recogida y explica las conclusiones obtenidas.Los datos provienen de Baxter, Coutts, Ross (1980).Ejercicio 5. En este ejemplo están recogidos los datos de un estudio sobre 44 médicos que trabajaban en el servicio de urgencias de un hospital Le (1998). El objetivo del estudio era determinar qué variables estaban relacionadas con el número de quejas que recibían los médicos lo largo de un año. Para ello se controló. para cada médico el número de consultas que realizó lo largo del año (consultas) y el número de quejas recibidas (quejas). Se tuvieron en cuanta además otras posibles características que podrían influir como:el salario anual del médico (en dólares por hora) (ingresos)la carga de trabajo (en horas) en el servicio de urgencias (horas)el sexo del médico (H = hombre y M = Mujer) (sexo)si el médico recibió o entrenamiento como residente en el servicio de urgenciasRepresenta la tasa de quejas para cada posible variable predictora y obtén el modelo correspondiente estos datos que te permita responde sobre la influencia de cada una de las características recogidas en el número de quejas recibidas.Ejercicio 6. Disponemos de los pesos (g) de dos pikas afganas preñadas, lo largo de 14 periodos de tiempo igualmente espaciados entre sí desde la concepción al parto. Ajusta una curva de crecimiento para describir estos datos ¿Puedes detectar alguna diferencia entre los dos animales?Ejercicio 7. Este problema se refiere datos de un estudio de cangrejos herradura anidados. Cada cangrejo de herradura hembra en el estudio tenía un cangrejo macho unido ella en su nido. El estudio investigó los factores que afectan si el cangrejo hembra tenía otros machos, llamados satélites, que residían cerca de ella. La característica que se cree que afecta el ancho del caparazón de la hembra. Se recogió información sobre las hembras y los machos, y estas se agruparon en grupos según su anchura. Cada grupo se caracterizo con la anchura media. Además, se registro el número de hembras correspondientes cada anchura y el número de satélites alrededor de ellas. Representa la tasa media de satélites por hembra en función de la anchura y construye un modelo que represente la información contenida en estos datos. Extrae todas las conclusiones derivadas de el.","code":"\ngrupo <- c(rep(\"T\",7),rep(\"C\",9)) \ntiempo <- c(94,38,23,197,99,16,141,52,104,146,10,51,30,40,27,46) \nejercicio01 <- data.frame(grupo,tiempo) \nlcelulas <- c(3.36,2.88,3.63,3.41,3.78,4.02,4.00,4.72,5.00,4.23,\n              3.73,3.85,3.97,4.51, 4.54,5.00,5.00) \ntiempo <- c(65,156,100,134,16,108,121,5,65,4,39,143,56,26,22,1,1) \nejercicio02 <- data.frame(lcelulas,tiempo) \nrequire(dobson) ## Loading required package: dobson## \n## Attaching package: 'dobson'## The following object is masked from 'package:survival':\n## \n##     leukemia## The following object is masked from 'package:lattice':\n## \n##     melanoma## The following object is masked from 'package:MASS':\n## \n##     housing\nejercicio03 <- doctors \n# Construimos el vector numérico de edad \nejercicio03$age_num <- c(1,2,3,4,5,1,2,3,4,5) \nejercicio03$age_num2 <- c(40,50,60,70,80,40,50,60,70,80) \nrequire(dobson) \nejercicio04 <- insurance \nejercicio04$car_f <- as.factor(ejercicio04$car) \nejercicio04$age_f <- as.factor(ejercicio04$age) \nconsultas <- c(2014, 3091, 879, 1780, 3646, 2690, 1864, 2782, \n               3071, 1502, 2438, 2278, 2458, 2269, 2431, 3010, \n               2234, 2906, 2043, 3022, 2123, 1029, 3003, 2178, \n               2504, 2211, 2338, 3060, 2302, 1486, 1863, 1661, \n               2008, 2138, 2556, 1451, 3328, 2927, 2701, 2046, \n               2548, 2592, 2741, 3763) \nquejas <- c(2, 3, 1, 1, 11, 1, 2, 6, 9, 3, 2, 2, 5, 2, 7, 2, 5, \n            4, 2, 7, 5, 1, 3, 2, 1, 1, 6, 2, 1, 1, 1, 0, 2, 2, \n            5, 3, 3, 8, 8, 1, 2, 1, 1, 10) \nresidente <- c(\"Si\", \"No\", \"Si\", \"No\", \"No\", \"No\", \"Si\", \"No\", \n               \"No\", \"Si\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Si\", \n               \"Si\", \"No\", \"Si\", \"No\", \"No\", \"Si\", \"Si\", \"No\", \n               \"Si\", \"No\", \"Si\", \"Si\", \"No\", \"Si\", \"Si\", \"No\", \n               \"No\", \"No\", \"No\", \"Si\", \"Si\", \"No\", \"No\", \"Si\", \n               \"Si\", \"No\", \"Si\", \"Si\") \nsexo <- c(\"M\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"M\", \"H\", \"M\", \n          \"H\", \"H\", \"M\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"M\", \"M\", \n          \"M\", \"H\", \"M\", \"M\", \"H\", \"H\", \"H\", \"M\", \"H\", \"H\", \"H\", \n          \"H\", \"H\", \"M\", \"H\", \"H\", \"H\", \"H\", \"H\", \"H\", \"M\", \"H\") \ningresos <-c(263.03, 334.94, 206.42, 226.32, 288.91, 275.94, 295.71, \n             224.91, 249.32, 269, 225.61, 212.43, 211.05, 213.23, \n             257.3, 326.49, 290.53, 268.73, 231.61, 241.04, 238.65, \n             287.76, 280.52, 237.31, 218.7, 250.01, 251.54, 270.52, \n             247.31, 277.78, 259.68, 260.92, 240.22, 217.49, 250.31, \n             229.43, 313.48, 293.47, 275.4, 289.56, 305.67, 252.35, \n             276.86, 308.84) \nhoras <- c(1287.25, 1588, 705.25, 1005.5, 1667.25, 1517.75, 967, \n           1609.25, 1747.75, 906.25, 1787.75, 1480.5, 1733.5, 1847.25, \n           1433, 1520, 1404.75, 1608.5, 1220, 1917.25, 1506.25, 589, \n           1552.75, 1518, 1793.75, 1548, 1446, 1858.25, 1486.25, 933.75, \n           1168.25, 877.25, 1387.25, 1312, 1551.5, 973.75, 1638.25, \n           1668.25, 1652.75, 1029.75, 1127, 1547.25, 1499.25, 1747.5) \nejercicio05 <- data.frame(consultas,quejas,residente,sexo,ingresos,horas)\nanimal <- c(rep(\"Pika1\",14),rep(\"Pika2\",14)) \npeso <-c(251, 254, 267, 267, 274, 286, 298, 295, 307, 318, 341,\n         342, 367, 370, 258, 263, 269, 266, 282, 289, 295, 308, \n         338, 350, 359, 382, 390, 400) \nsemana <- c(1:14,1:14) \nejercicio06 <- data.frame(animal,peso,semana) \nanchura <- c(22.69, 23.84, 24.77, 25.84, 26.79, 27.74, \n             28.67, 30.41) \nhembras <- c(14, 14, 28, 39, 22, 24, 18, 14) \nsatel <- c(14, 20, 67, 105, 63, 93, 71, 72) \nejercicio07 <- data.frame(anchura,hembras,satel) "},{"path":"glmtablascont.html","id":"glmtablascont","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"Unidad 15 GLM para tablas de contingencia","text":"Un tabla de contingencia se usa para estudiar una tabla de clasificación de un conjunto de factores donde se recogen el número de casos de cada una de las combinaciones de dichos factores.En este tipo de modelos las variables respuestas y predictoras son todas categóricas, medidas en una escala nominal u ordinal, aunque como veremos las construcción de este tipo de modelos se centran en estudiar las frecuencias de ocurrencia de cada combinación de los factores. El objetivo básico en el análisis de tablas de contingencia es estudiar si existe alguna relación/asociación entre los factores de clasificación considerados. posteriormente, si dicha relación existe, habrá que describir cómo es para tratar de predecirla. Para cuantificar el grado de asociación entre los factores de clasificación, los modelos log-lineales proporcionarán predicciones de las frecuencias observadas en las celdas y las probabilidades asociadas, en función de dichos factores y la interacción entre ellos.Para el análisis de estipo de datos es necesario que los datos vengan en un formato específico de forma que cada fila representa una combinación de los factores presentes en la tabla de contingencia y debe haber una columna que recoja los conteos o frecuencias de esa combinación. En ocasiones los datos deben ser trasnformados partir de la tabla de contingencia para conseguir el fromato adecaudo para el análisis. En este último caso utilizaremos la función gather() que nosa permite modificar de forma sencilla la configuración del banco de datos.Como ocurría en los modelos de regresión de Poisson, en el caso de trabajar con factores ordinales se debderna considerar sus versiones numéwricas utilizando el método de codificación presentado en esa unidad.","code":""},{"path":"glmtablascont.html","id":"bancos-de-datos-6","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.1 Bancos de datos","text":"continuación se presentan diferentes ejemplos de tablas de contingencia que iremos analizando lo largo de la unidad. Realizamos una primera aproximación gráfica la tabla de contingencia obtenida.","code":""},{"path":"glmtablascont.html","id":"melanoma","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.1.1 Melanoma","text":"Los datos siguientes provienen de un estudio de pacientes con una forma de cáncer de piel llamado melanoma maligno. En una muestra de 400 pacientes se recogió información sobre la localización del tumor y su tipo histológico. Los datos son el número de pacientes (frecuency) en cada combinación de tipo de tumor (type) y localización (site). El objetivo básico del análisis es investigar si determinados tipos de tumor están asociados ciertas localizaciones. La tabla de conteos o contingencia recogida es:EL código para cargar estos datos se presenta continaución. En este caso cada línea representa una combinación de factores. También representamos los datos:Podemos ver claramente que hay ciertos tumores que tienen una mayor predisposición aparecer en una parte del cuerpo que en otra.","code":"\n# Carga de datos\nMelanoma <- read_csv(\"https://goo.gl/yeCXsv\", col_types = \"cci\")\n# Gráfico\nggplot(Melanoma, aes(x = site, y = frequency, fill = type)) + \n  stat_identity(geom = \"bar\", position = \"dodge\") "},{"path":"glmtablascont.html","id":"gripe","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.1.2 Gripe","text":"En un estudio prospectivo sobre una nueva vacuna para la gripe, los pacientes fueron asignados aleatoriamente dos grupos. los pacientes de uno de los grupos se les trató con la nueva vacuna y los otros se les dio un placebo salino. Las respuestas fueron los niveles de anticuerpos inhibidores de hemoglutinina (HIA) encontrados en la sangre seis semanas después de la vacunación. Los datos se encuentran en el banco de datos siguiente. El objetivo del estudio es investigar el efecto de la nueva vacuna, esto es, comprobar si el hecho de dar placebo o vacuna provoca diferente respuesta HIA. Así, la variable HIA es la variable explicar (response) en función del tipo de tratamiento (treatment) que ha recibido el paciente y las frecuencias de las celdas (frequency).La tabla de contingencia obtenida es:Podemos cargar los datos y representarlos gráficamenteSe puede ver un comportamiento distinto entre el placebo y el tratamiento.","code":"\n# Lectura de datos\nGripe <- read_csv(\"https://goo.gl/jPcz7x\", col_types = \"cci\")\n# Gráfico\nggplot(Gripe, aes(x = treatment, y = frequency, fill = response)) + \n  stat_identity(geom = \"bar\", position = \"dodge\") "},{"path":"glmtablascont.html","id":"aspirina","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.1.3 Aspirina","text":"En un estudio retrospectivo de casos-controles, se reunió un grupo de pacientes con úlcera y se buscó otro grupo de individuos de similares características los anteriores en edad, sexo y estatus socio económico, sobre los que se sabía que tuvieran úlcera péptica. Los pacientes con úlcera fueron clasificados de acuerdo la localización de la úlcera: gástrica o duodenal. Se les preguntó todos si consumían aspirina. El objetivo del análisis es averiguar si existe alguna relación entre el consumo de aspirina y la existencia de algún tipo de úlcera. Si existe tal asociación, querríamos comprobar si el consumo de aspirina (aspirina) es más habitual en los pacientes con úlcera gástrica que en los de úlcera duodenal (locali). Interesa predecir el consumo de aspirina (variable explicar) en función de si un individuo tiene úlcera o (ulcera). y si la tiene, de qué tipo es. La variable frecuencia contiene los conteos de las combinaciones de tratamientos.La tabla de contingencia obtenida es:Podemos cargar los datos y representarlos gráficamenteDe nuevo se observan comportamientos distintos para la combinación de los factores considerados.","code":"\n# Carga de datos\nlocali <- gl(2, 2, 8, labels = c(\"Gástrica\", \"Duodenal\"))\nulcera <- gl(2, 1, 8, labels = c(\"Si\", \"No\"))\naspirina <- gl(2, 4, 8, labels = c(\"No\", \"Si\"))\nfrecuencia <- c(39, 62, 49, 53, 25, 6, 8, 8)\nAspirina <- data.frame(locali, ulcera, aspirina, frecuencia)\n# Gráfico\nggplot(Aspirina, aes(x = aspirina, y = frecuencia, fill = locali)) + \n  stat_identity(geom = \"bar\", position = \"dodge\") +\n  facet_wrap(~ ulcera) +\n  labs(xlab =\"Consumo de aspirina\", \n       title = \"Desarrollo de úlcera\", fill = \"Localización\")"},{"path":"glmtablascont.html","id":"aspiraciones","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.1.4 Aspiraciones","text":"En la tabla se muestran una parte de los resultados de un estudio en los Estados Unidos con el que se pretendía investigar el grado de asociación entre las aspiraciones de los estudiantes de bachillerato por proseguir con estudios universitarios (aspiraciones) y su entorno social, medido en términos del estatus socio-económico de su familia (estatus) y el hecho de que recibieron motivación en su familia para continuar estudiando (motivacion). La variable frecuencia contiene los conteos para todas las combinaciones. La tabla de contingencia original es:Dado que Estatus y Motivación personal son factores ordinales se debe considerar su codificación como variables numéricas. continuación se presenta la lectura del banco de datos y su representación gráfica.¿Qué podemos comentar del gráfico obtenido?","code":"\nestatus <- gl(4, 2, 16, labels = c(\"Baja\", \"Media-Baja\",\n                                   \"Media-Alta\", \"Alta\"))\nmotivacion <- gl(2, 1, 16, labels = c(\"Baja\", \"Alta\"))\naspiraciones <- gl(2, 8, 16, labels = c(\"No\", \"Si\"))\nfrecuencia <- c(749, 233, 627, 330, 420, 374, 153, 266, \n                35, 133, 38, 303, 37, 467, 26, 800)\nAspiraciones <- data.frame(estatus, motivacion, aspiraciones, frecuencia)\n# Código numérico para factores ordinales\nAspiraciones <- Aspiraciones %>% \n  mutate(estatus.num = as.numeric(gl(4, 2, 16)), \n         motivacion.num = as.numeric(gl(2, 1, 16)))\n# Represenatción gráfica\nggplot(Aspiraciones, aes(x = aspiraciones, y = frecuencia, fill = motivacion)) + \n  stat_identity(geom = \"bar\", position = \"dodge\") +\n  facet_wrap(~ estatus) +\n  labs(xlab =\"Conusmo de aspirina\", \n    title = \"Estatus social\", fill = \"Motivación\")"},{"path":"glmtablascont.html","id":"contraceptivos","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.1.5 Contraceptivos","text":"En la tabla aparecen los datos de un estudio sobre distintos usos contraceptivos en diferentes grupos generacionales. Entendiendo la variable método contraceptivo (metodo) como variable respuesta y la variable edad (edad) como predictiva, una cuestión de interés puede ser predecir el uso anticonceptivo de una mujer en función de su edad. La variable frecuencia contiene los conteos para todas las combinaciones.El código de lectura de datos (incluyendo la codificación del factor ordinal como el punto medio del intervalo) y el gráfico se muestran continuación:¿Qué conclusiones podemos obtener de este gráfico?","code":"\nedad <- gl(7, 1, 21, labels = c(\"15-19\", \"20-24\", \"25-29\", \n                                \"30-34\", \"35-39\", \"40-44\", \"45-49\"))\nmetodo <- gl(3, 7, 21, labels = c (\"Esterilización\", \"Otros\", \"Ninguno\"))\nfrecuencia <- c(3, 80, 216, 268, 197, 150, 91, 61, 137, 131, 76, \n                50, 24, 10, 232, 400, 301, 203, 188, 164, 183)\nContraceptivos <- data.frame(edad, metodo, frecuencia)\n# Código numérico para factor ordinales\nContraceptivos <- Contraceptivos %>% \n  mutate(edad.num = rep(c(17, 22, 27, 32, 37, 42, 47),3))\n# Gráfico\nggplot(Contraceptivos, aes(x = metodo, y = frecuencia, fill = edad)) + \n  stat_identity(geom = \"bar\", position = \"dodge\") "},{"path":"glmtablascont.html","id":"satisfacción-laboral","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.1.6 Satisfacción laboral","text":"Los datos que se presentan corresponden un estudio en el que se pretendía concluir sobre la relación entre el grado de satisfacción en el trabajo (Estado) y los ingresos percibidos (Ingresos). El objetivo es predecir el grado de satisfacción en el trabajo en función de los ingresos percibidos (en dólares). Construimos la variable frecuencia que contiene los conteos para todas las combinaciones.Cargamos los datos que vienen en formato de tabla, con lo que es necesario transformarlos adecuadamente. También se calculan las variables numéricas asociadas con los factores ordinales.¿Qué conclusiones podemos obtener de este gráfico?","code":"\nSatisfaccion <- read_csv(\"https://goo.gl/5T0nh0\", col_types = \"ciiii\")\n# Construimos la tabla donde cada fila recoge la frecuencia observada para combinación de las variables\nSatisfaccion <- Satisfaccion %>% \n  gather(`Muy insatisfecho`, `Poco insatisfecho`,\n         `Moderadamente satisfecho`, `Muy satisfecho`,\n         key = \"Estado\", value = frecuencia)\n# Código numérico para factores ordinales\nSatisfaccion <- Satisfaccion %>% \n  mutate(Ingresos.num = as.numeric(gl(4, 1, 16)), \n         Estado.num = as.numeric(gl(4, 4, 16)))\n# Gráfico\n# Creamos un vector con el orden predefinido\nords <- c(\"< 6000\", \"6000 - 15000\", \"15000 - 25000\", \"> 25000\")\nggplot(Satisfaccion, aes(x = Ingresos, y = frecuencia, fill = Estado)) + \n  stat_identity(geom = \"bar\", position = \"dodge\") + \n  scale_x_discrete(limits = ords)"},{"path":"glmtablascont.html","id":"modelos-log-lineales","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.2 Modelos Log-Lineales","text":"Las cuestiones básicas de interés en el análisis de tablas de contingencia son detectar asociación entre los factores de clasificación y predecir las frecuencias medias observadas. Dichas cuestiones se pueden resolver partir de la modelización log-lineal de las frecuencias esperadas.\nConsideramos, para presentar estos modelos, únicamente dos vías de clasificación, F1 y F2. Si los factores se consideran independientes el número de individuos que esperamos clasificar en la celda \\((, j)\\) (nivel \\(\\) de F1 y nivel \\(j\\) de F2) se puede calcular través del producto de los sucesos independientes “número esperado de individuos clasificados en el nivel de F1”, \\(\\mu_{+}\\), y “número esperado de individuos clasificados en el nivel j de F2”, \\(\\mu_{+j}\\) esto es,\\[E(Y_{ij}) = \\mu_{ij} = \\mu_{+}*\\mu_{+j}\\]El efecto multiplicativo de los factores para predecir la frecuencia observada da lugar, de modo natural, modelos logarítmicos aditivos (modelos log-lineales) de la forma:\\[log(E(Y_{ij})) = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij}\\]donde \\(\\alpha_i\\) representa el efecto principal del factor fila F1 , \\(\\beta_j\\) el correspondiente al factor columna F2 y el \\((\\alpha\\beta)_{ij}\\) efecto de interacción entre ambos. Este modelo se denomina modelo saturado, ya que tiene tantos parámetros como celdas estamos considerando en la combinación de los factores, y tiene poco interés desde el punto de vista estadístico ya que cada parámetro del modelo se estima con un único valor en la muestra de datos. Contrastar independencia versus asociación será equivalente contrastar interacción nula versus nula en el modelo log-lineal, es decir debemos estudiar los modelos:El modelo saturado dado por \\(Frecuencias \\sim F1 * F2\\)El modelo sin interacción dado por \\(Frecuencias \\sim F1 + F2\\)dados por las expresiones\\[\\begin{array}{ll}\nM_0: log(E(Y_{ij})) &= \\mu + \\alpha_i + \\beta_j\\\\ \nM_1: log(E(Y_{ij})) &= \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij}\\\\ \n\\end{array}\\]En la práctica para contrastar independencia ajustamos un modelo con únicamente los efectos principales y valoramos la bondad del ajuste conseguido mediante el test \\(\\chi^2\\). En conclusión, reconocer asociación entre los factores de clasificación de una tabla de contingencia es equivalente rechazar interacción nula entre ellos en el modelo log-lineal con el que se predicen las frecuencias observadas en las celdas.Cuando consideramos la tabla de contingencia asociada con tres factores el modelo saturado vendrá dado por:\\[Frecuencias \\sim F1 + F2 + F3 + F1:F2 + F1:F3 + F2:F3 + F1:F2:F3\\]\ndonde el efecto de interacción triple \\(F1:F2:F3\\) nos permite estudiar si existe asociación entre los tres factores conjuntamente. En caso de que el contraste de bondad de ajuste del modelo reducido\\[M_0: Frecuencias \\sim F1 + F2 + F3 + F1:F2 + F1:F3 + F2:F3\\]\nresulte significativo, podremos abordar el estudio de las interacciones dobles de la siguiente forma:la interacción \\(F1:F2\\) nos permite estudiar la asociación entre ambos factores dado F3la interacción \\(F1:F3\\) nos permite estudiar la asociación entre ambos factores dado F2la interacción \\(F2:F3\\) nos permite estudiar la asociación entre ambos factores dado F1Por tanto, debemos estudiar la bondad de ajuste de los modelos\n\\[\\begin{array}{ll}\nM_1:& Frecuencias \\sim F1 + F2 + F3 + F1:F3 + F2:F3\\\\ \nM_2:& Frecuencias \\sim F1 + F2 + F3 + F1:F2 + F2:F3\\\\\nM_3:& Frecuencias \\sim F1 + F2 + F3 + F1:F2 + F1:F3\\\\\n\\end{array}\\]para detectar las posibles asociaciones. En la pra´ctica siempre se fija un factor como varaible respuesta y los otros dos como predictoras, de forma que, si el factor F1 actúa como respuesta y los otros dos factores como predictoras, el modelo M3 tiene sentido práctico, ya que en realidad nuestro interés radica en la asociación entre respuesta y predictoras.Así podemos seguir aumentando el número de factores en el modelo y estudiando las posibles asociaciones entre ellos. Se trata pues de un procedimiento de comparación de efectos o modelos de tipo manual y automático como habíamos hecho en otro tipo de modelos.Para escribir de forma reducida lo modelos anteriores usamos la siguiente notación (que también nos vale para el ajuste de los modelos en R).\n\\[\\begin{array}{ll}\nM_0:& Frecuencias \\sim (F1+F2+F3)^2\\\\ \nM_1:& Frecuencias \\sim F2*F3 + F1 + F1:F3\\\\ \nM_2:& Frecuencias \\sim F2*F3 + F1 + F1:F2\\\\\nM_3:& Frecuencias \\sim F1*F3 + F2 + F1:F2\\\\\n\\end{array}\\]","code":""},{"path":"glmtablascont.html","id":"especificación-en-r","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.2.1 Especificación en R","text":"Para el ajuste de estos modelos utilizamos la función glm() con las especificaciones siguientes:\\[glm(modelo, family = poisson(), data_set)\\]donde \\(modelo\\) se establece según las notaciones reducidas que hemos presentado tanto para dos como para tres factores.","code":""},{"path":"glmtablascont.html","id":"ejemplos-21","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.2.2 Ejemplos","text":"continuación estudiamos de forma inicial la independencia o dependencia en los ejemplos del inicio. Para el análisis de independencia utilizamos el test \\(\\chi^2\\) de bondad de ajuste y concluímos en función del p-valor obtenido.","code":""},{"path":"glmtablascont.html","id":"melanoma-1","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.2.2.1 Melanoma","text":"En este caso la pregunta de interés es si ¿existe asociación entre el tipo de tumor y su localización?, que equivale estudiar si el efecto de interacción entre tipo de tumor y su localización resulta significativo, es decir, estudiar la bondad de ajuste del modelo sin interacción entre ambos.Dado que el valor es inferior 0.05 podemos concluir que hay evidencias estadísticas favor de la asociación entre los factores tipo de tumor y localización, es decir, determinados tipos de tumor se presentan en ciertas localizaciones con más frecuencia que en otras.","code":"\n# Ajuste del modelo sin interacción\nfit.melanoma <- glm(frequency ~ type + site, \n                    family = poisson(), \n                    data = Melanoma)\n# Bondad del asjute del modelo\n1-pchisq(fit.melanoma$deviance, fit.melanoma$df.residual)## [1] 2.050453e-09"},{"path":"glmtablascont.html","id":"aspirina-1","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.2.2.2 Aspirina","text":"En el estudio se controlaron las variables úlcera y localización. los pacientes clasificados en cada una de las combinaciones de estas dos variables se les preguntó sobre el consumo de aspirina, que es la variable aleatoria y por tanto la variable explicar en función de las otras dos. Las cuestiones que nos planteamos son:En el grupo gástrico ¿la úlcera está asociada con el consumo de aspirina? ¿y en el grupo duodenal?. Para ambas cuestiones estamos planteando si, dado un grupo especifico (locali), el consumo de aspirina (aspirina) está relacionado con la existencia de úlcera (ulcera). Ambas preguntas se pueden contestar estudiando el efecto de interacción entre el consumo de aspirina y el desarrollo de una úlcera (aspirina:ulcera).En el grupo gástrico ¿la úlcera está asociada con el consumo de aspirina? ¿y en el grupo duodenal?. Para ambas cuestiones estamos planteando si, dado un grupo especifico (locali), el consumo de aspirina (aspirina) está relacionado con la existencia de úlcera (ulcera). Ambas preguntas se pueden contestar estudiando el efecto de interacción entre el consumo de aspirina y el desarrollo de una úlcera (aspirina:ulcera).Si dicha asociación existe, cabe preguntarse si dicha relación es similar en los dos grupos de pacientes.Si dicha asociación existe, cabe preguntarse si dicha relación es similar en los dos grupos de pacientes.En este caso el estudio de la independencia total nos llevaría analizar un modelo donde eliminamos la interacción de orden tres. Dicho modelo se puede obtener y analizar con:Dado que el p-valor resulta significativo podemos concluir que existe asociación entre los factores considerados. Pasamos ahora estudiar cada una de las cuestiones planteadas ahora que sabemos que los factores están relacionados. Para saber si la existencia del desarrollo de una úlcera está asociado con el consumo de aspirina para cada tipo de úlcera, ajustamos un modelo sin la interacción aspirina:ulceraSe rechaza claramente independencia entre el consumo de aspirina y la existencia de úlcera para ambos tipos de úlcera.Pasamos ahora la segunda cuestión, es decir, tratamos de averiguar si existe asociación entre el consumo de aspirina y el tipo de úlcera desarrollada. Para ello debemos analizar la interacción entre aspirina:locali cuando sabemos que se ha desarrollado una úlceraRechazamos la hipótesis de independencia favor de que el consumo de aspirina es diferente en pacientes con distinto tipo de úlcera.","code":"\n# Ajuste del modelo sin interacción\nfit.aspirina <- glm(frecuencia ~ (locali + ulcera + aspirina)^2, \n                    family = poisson(), \n                    data = Aspirina)\n# Bondad del asjute del modelo\n1-pchisq(fit.aspirina$deviance, fit.aspirina$df.residual)## [1] 0.01219027\n# Ajuste del modelo sin interacción\nfit.aspirina <- glm(frecuencia ~ locali*ulcera + aspirina + aspirina:locali, \n                    family = poisson(), \n                    data = Aspirina)\n# Bondad del asjute del modelo\n1-pchisq(fit.aspirina$deviance, fit.aspirina$df.residual)## [1] 0.000143616\n# Ajuste del modelo sin interacción\nfit.aspirina <- glm(frecuencia ~ locali*ulcera + aspirina + aspirina:ulcera, \n                    family = poisson(), \n                    data = Aspirina)\n# Bondad del asjute del modelo\n1-pchisq(fit.aspirina$deviance, fit.aspirina$df.residual)## [1] 0.005147627"},{"path":"glmtablascont.html","id":"gripe-1","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.2.2.3 Gripe","text":"Estudiamos la posible asociación entre la respuesta tars la vacunación y si el sujeto eras tratado o .Podemos cargar los datos y representarlos gráficamentepuesto que el p-valor obtenido es significativo podemos concluir que hay asociación entre los factores considerados, es decir, la respuesta tras la vacunación se asocia con el grupo de tratamiento.","code":"\n# Ajuste del modelo sin interacción\nfit.gripe <- glm(frequency ~ treatment + response, \n                    family = poisson(), \n                    data = Gripe)\n# Bondad del asjute del modelo\n1-pchisq(fit.gripe$deviance, fit.gripe$df.residual)## [1] 8.95007e-05"},{"path":"glmtablascont.html","id":"aspiraciones-1","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.2.2.4 Aspiraciones","text":"Estamso interesados en conocer si existe asociación entre el estatus, la motivacion familiar y las aspiraciones universitarias. En este caso la variable que actuaría como respuesta son las aspiraciones universitarias, mientras que las otras dos se utilizan como varaible clasificadoras. Ajustamos el modelo sin interacción triple para establecer la posible asociación.en este caso parece existir uan asociación entre los tres factores, pero ¿podría alguno de ellos estar asociado con las aspiraciones? ¿Cómo interpretamos los modelos siguientes?Ejercicio. Realiza el análisis de independencia apra los bancos de datos Contraceptivos y Salud Laboral.","code":"\n# Ajuste del modelo sin interacción\nfit.aspiraciones <- glm(frecuencia ~ (estatus + motivacion + aspiraciones)^2, \n                    family = poisson(), \n                    data = Aspiraciones)\n# Bondad del asjute del modelo\n1-pchisq(fit.aspiraciones$deviance, fit.aspiraciones$df.residual)## [1] 0.664965\n# Modelo 1\nfit.aspiraciones <- glm(frecuencia ~ estatus*motivacion + aspiraciones + aspiraciones:motivacion, \n                        family = poisson(), \n                        data = Aspiraciones)\n# Modelo 2\nfit.aspiraciones <- glm(frecuencia ~ estatus*motivacion + aspiraciones + aspiraciones:estatus, \n                        family = poisson(),  \n                        data = Aspiraciones)"},{"path":"glmtablascont.html","id":"modelos-logit-multinomial","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.3 Modelos Logit-Multinomial","text":"El objetivo principal al analizar las tablas de contingenia es sólo determinar la posible asociación entre los factores, sino tratar de predecir el comportamiento de uno de los factores (que actúa como variable respuesta) en función del resto de factores (que actúan como predictoras) cuando se detecta esta. En esta situación interesará modelizar los datos para predecir la probabilidad de que se dé una u otra respuesta en la variable respuesta en función de las distintas combinaciones en las variables predictoras.Para modelizar este tipo de situación se establece una variante de los modelos log-lineales, que además de resultar útiles para investigar asociación, son equivalentes los modelos logit para respuesta Binomial, conocidos como modelos logit-multinomial.Para estudiar la especificación de este tipo de modleos nos planteamos la situacoçpn más sencilla donde consideremos una tabla de contingencia en la que están involucradas como variables de clasificación, una variable respuesta \\(R\\) con \\(\\) niveles de clasificación, y una variable respuesta \\(\\) con \\(J\\) niveles de clasificación. Además, denotamos por \\(\\pi_{|j}\\) las probabilidades de respuesta \\(R = \\) dada cierta clasificación en la variable predictora, \\((= j)\\), esto es,\\[\\pi_{|j} = P(R = | = j).\\]Los modelos logit-multinomial vienen definidos en términos de los log-odds (logaritmo del cociente) de las probabilidades condicionadas de dos niveles de respuesta de \\(R\\), dadas las diferentes combinaciones de niveles de las variables predictoras. En esta situación, y de forma similar lo que ocurre con los modelos ANOVA, se suele elegir uno de los niveles de \\(R\\) como nivel de referencia, generalmente el primero o el último, para predecir el cociente de probabilidades condicionadas. Si se toma como categoría de referencia la última de \\(R\\) (en este caso ), los log-odds predecir serán:\\[log\\left(\\frac{\\pi_{|j}}{\\pi_{|j}}\\right), \\quad \\text{para cada nivel } \\text{ de } R\\]El ajuste de estos modelos logit-multinomiales se realiza través de modelos log-lineales equivalentes con los que se predicen las frecuencias observadas \\(Y_{ij}\\) en la tabla de contingencia. Es preciso puntualizar que la equivalencia entre estos modelos logit y los log-lineales es cierta asumiendo que los totales marginales correspondientes las variables predictivas son fijos. Básicamente es decir que las variables predictoras han sido controladas en el estudio con el fin principal de predecir la variable respuesta \\(R\\). Al asumir esto, obligamos que los efectos asociados dichas variables predictoras hayan de aparecer siempre (junto con sus interacciones) en cualquier modelo log-lineal que ajustemos.El cociente de log-odds (o logit entre las categorías \\(\\) e \\(\\)) se puede expresar como:\\[log\\left(\\frac{\\pi_{|j}}{\\pi_{|j}}\\right) = log\\left(\\frac{E(Y_{ij})}{E(Y_{Ij})}\\right) = log\\{E(Y_{ij})\\} - log\\{E(Y_{Ij})\\}\\]donde los \\(E(Y_{ij})\\) para ca \\(\\) son los valores predichos del modelo log-lineal para cada combinación de los niveles de todas las variables involucradas. En realidad, esto significa que la hora de estimar el log-odds sólo debemos tener en cuenta los coeficientes del modelo que hagan referencia los niveles considerados de la variable respuesta, y la interacción (si está presente en el modelo) entre respuesta y predictora, es decir:\\[log\\left(\\frac{\\pi_{|j}}{\\pi_{|j}}\\right) = (\\alpha_i -\\alpha_I) + (\\alpha\\beta_{ij} - \\alpha\\beta_{Ij})\\]donde los \\(\\alpha\\) son los efectos asociados con los niveles de las categorías de la respuesta y los \\(\\alpha\\beta\\) son los efectos asociados con la interacción entre respuesta y predictora. Esta relación tiene implicaciones importantes ya que para estimar los logits debemos tener en cuenta únicamente los coeficientes del modelo que están relacionados con la respuesta, través de sus efectos principales o de sus interacciones con las predictoras. Este principio nos sirve para todas las situaciones de análisis que planteamos continuación en función de las características de la variable respuesta y la predictora o predictoras.Antes de comenzar, al igual que ocurria en modelizaciones anteriores, la utilización de variables ficticias numéricas para sustituir una variable categórica de tipo ordinal es una forma de proceder es muy habitual. Dichos valores numéricos se denominan en los modelos log-lineales como scores o valores numéricos asociados variables de carácter ordinal. Existen dos posibilidades de asignación de estos scores:Cuando la variable ordinal viene dada en intervalos se elige el punto medio como score asociado.Cuando la varaible ordinal viene dada en categorías nonuméricas se suele asociar un código numérico con el valor más bajo de 1 indicando la categoría más baja, y vamos aumentando de 1 en 1 hasta acabar con todos los niveles del factor.","code":""},{"path":"glmtablascont.html","id":"estimación","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4 Estimación","text":"continuación se presenta el proceso de estimación de los diferentes modelos logit-multinomial que pueden surgir en función de las características de la varaible respuesta y las predictoras. Se detalla cada caso siguiendo las pautas del punto anterior y se analizan los ejemplos correspondientes.","code":""},{"path":"glmtablascont.html","id":"respuesta-binaria-nominal-y-predictora-nominal","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.1 Respuesta binaria nominal y predictora nominal","text":"Imaginemos que la variable respuesta esta compuesta por dos categorías de carácter nominal (\\(=1, 2\\)=. Con la estructura de estimación definida en el punto anterior si la respuesta tiene únicamente dos categorías tendríamos que \\(\\pi_{1|j} = 1 - \\pi_{2|j}\\), de forma que tendríamos un único log-odds que se puede expresar como:\\[log\\left(\\frac{\\pi_{2|j}}{\\pi_{1|j}}\\right) = log\\left(\\frac{\\pi_{2|j}}{1-\\pi_{2|j}}\\right) = (\\alpha_2 -\\alpha_1) + (\\alpha\\beta_{2j} - \\alpha\\beta_{1j})\\]donde los \\(\\alpha\\) y los \\(\\alpha\\beta\\) son los definidos en el punto anterior. Si denotamos por \\(\\eta_{2|j}\\) al \\(log\\left(\\pi_{2|j}/\\pi_{1|j}\\right)\\), partir de la expresión anterior resulta posible obtener la probabilidad de cada nivel de la respuesta mediante la ecuación:\\[\\pi_{2|j} = \\frac{exp(\\eta_{2|j})}{1+exp(\\eta_{2|j})}\\]","code":""},{"path":"glmtablascont.html","id":"datos-aspirina","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.1.1 Datos Aspirina","text":"En primer lugar calculamos y evaluamos los logits empíricos asociados y más tarde realizamos el ajuste y estimación delm modelo. Tomamos como variable respuesta el consumo de aspirina, y usamos como referencia la categroria podemos obtener los log-odds como:Ajustamos un modelo con todas las interacciones dobles, ya que en este caso resulta imposible plantear el modelo con las interacción triple. Este modelo el que ya se planteo en el estudio de independencia.Para obtener los logits y probabilidades asociadas cada categoría de la respuesta extraemos los coeficientes del modelo ajustado. Dado que solo nos interesan los efectos que están relaciondos directamente con la variable respuesta vamos presentar únicamente dichos valores. Para saber los nombres de los coeficientes del modelo podemos utilizar el código siguiente identificando todos los efectos que están realcionados con la respuesta (efecto principal e interacciones).En estos modelos utilizamos la función La función tidy para presentar la estimación del modelo selecionando las columnas necesarias.Seleccionamos los efectos necesarios para el ajuste de los logits y la estimación de probabilidades. Estos efectos son los relacionados (principales e interacciones) con la variable aspirina.Dado que ambas variables son de tipo nominal y tomando como referencia la categoría “” de consumo de aspirina, podemos obtener el logit del consumo de aspirina versus la categoria de referencia para cualquier combinación der categorias de las predictoras mediante:\\[log\\left(\\frac{\\pi_{SI|jk}}{\\pi_{|jk}}\\right) = (\\alpha_{Si} - \\alpha_{}) + (\\theta_{Si,jk} - \\theta_{,jk})\\]donde \\(\\alpha\\) hace referencia al efecto principal asociado con la respuesta (aspirina), y \\(\\theta\\) hace referencia al coeficiente de interacción entre la respuesta y los niveles \\(j\\) y \\(k\\) de localización de la úlcera (locali) y tener úlcera (ulcera) respectivamente. Si denotamos por \\(\\) al consumo de aspirina, \\(L\\) la localización de la úlcera, y \\(U\\) tener o tener úlcera, para obtener el logit para un sujeto con \\(U = \\) y \\(L = Duodenal\\) tendríamos que:\\[\\eta_\\text{Si| U = ; L = Duodenal} = (\\alpha_{Si} - \\alpha_{}) + (\\theta_{Si,,Duodenal} - \\theta_{,,Duodenal})\\]donde \\(\\alpha_{Si} = -0.679\\), \\(\\alpha_{} = 0\\), y \\(\\theta_{Si,,Duodenal}\\), \\(\\theta_{,,Duodenal}\\) representan las interacciones de la respuesta y predictoras para dicha combinación que vienen dadas por:\\[\\begin{array}{ll}\n\\theta_{Si,,Duodenal} &= \\theta_{Si,} + \\theta_{Si,Duodenal} = - 1.14 - 0.70 = - 1.84\\\\\n\\theta_{,,Duodenal} &= \\theta_{,} + \\theta_{,Duodenal} = 0 + 0 = 0\\\\\n\\end{array}\\]Para la estimación de los logits hya que tener en cuenta las restricciones de identificabilidad \\(\\alpha_{} = 0\\) y \\(\\theta_{,jk} = 0\\) para cualquier combinación de \\(j\\) y \\(k\\), de forma que:\\[\\begin{array}{ll}\n\\eta_\\text{[Si| U = ; L = Duodenal]} &= \\alpha_{Si} + \\theta_{Si,} + \\theta_{Si,Duodenal} = - 0.679 - 1.84 = -2.519\\\\\n\\eta_\\text{[Si| U = ; L = Gástrica]} & = \\alpha_{Si} + \\theta_{Si,} + \\theta_{Si,Gástrica} = - 0.679 - 1.14 + 0 = -1.819\\\\\n\\eta_\\text{[Si| U = Si; L = Duodenal]} & = \\alpha_{Si} + \\theta_{Si,Si} + \\theta_{Si,Duodenal} = - 0.679 + 0 - 0.70 = -1.379\\\\\n\\eta_\\text{[Si| U = Si; L = Gástrica]} & = \\alpha_{Si} + \\theta_{Si,Si} + \\theta_{Si,Gástrica} = - 0.679 + 0 + 0 = - 0.679\\\\\n\\end{array}\\]Podemos obtener ahora las probabilidades de consumo de aspirina para cada combinación de las predictoras:\\[\\begin{array}{ll}\n\\pi_\\text{[Si| U = ; L = Duodenal]} & =  exp(-2.519)/(1+exp(-2.519)) = 0.0745\\\\\n\\pi_\\text{[Si| U = ; L = Gástrica]} & =  exp(-1.819)/(1+exp(-1.819)) = 0.1396\\\\\n\\pi_\\text{[Si| U = Si; L = Duodenal]} & =  exp(-1.379)/(1+exp(-1.379)) = 0.2012\\\\\n\\pi_\\text{[Si| U = Si; L = Gástrica]} & =  exp(-0.679)/(1+exp(-0.679)) = 0.3365\\\\\n\\end{array}\\]la vista de estos resultados podemos concluir que la probabilidad de consumir aspirina dado un enfermo de úlcera duodenal es de 0.2012, mientras que para un enfermo de úlcera gástrica dicha probabilidad es de 0.3365. Podemos concluir que la probabilidad del consumo de aspirina es superior en los enfermos con úlcera gástrica que en la duodenal, y mayor en los sujetos enfermos que en los controles.Las probabilidades anteriores se puden obtener de forma un poco más directa partir de los valores predichos del modelo.Podemos representar las probabilidades obtenidas para ver los efectos de la combinacion de predictoras en la respuesta.","code":"\n# Creamos variable con valores de referencia y obtenemos los log-odds asociados\nreferencia.val <- unlist(dplyr::select(filter(Aspirina, aspirina == \"No\"),\n                                       frecuencia))\n# Log-odds\nAspirina <- Aspirina %>% \n  mutate(referencia = rep(referencia.val, 2),\n         lodds = log(frecuencia / referencia))\n# Gráfico\n# Seleccionamos las categorías a representar eliminando la de referencia\n# Como la respuesta sólo tiene una categoría no hace falta utilizarla en el gráfico\ndatos <- filter(Aspirina, aspirina != \"No\")\nggplot(datos,aes(x = locali, y = lodds, group = ulcera, color = ulcera)) +\n  geom_point() + \n  geom_line()  +\n  labs(x = \"Localización\",\n       y = \"log-odds (Aspirina == Si/No)\",\n       col = \"Desarrollo de úlcera\") \n# Ajuste del modelo sin interacción\nfit.aspirina <- glm(frecuencia ~ (locali + ulcera + aspirina)^2, \n              family = poisson(), \n              data = Aspirina)\ntabla <- tidy(fit.aspirina)[,c(\"term\", \"estimate\")]\ntabla## # A tibble: 7 × 2\n##   term                      estimate\n##   <chr>                        <dbl>\n## 1 (Intercept)                 3.75  \n## 2 localiDuodenal              0.0698\n## 3 ulceraNo                    0.321 \n## 4 aspirinaSi                 -0.679 \n## 5 localiDuodenal:ulceraNo    -0.106 \n## 6 localiDuodenal:aspirinaSi  -0.700 \n## 7 ulceraNo:aspirinaSi        -1.14\n# Filas de la tabla de estimaciones relacionados con metodo\nselefect<- c(4, 6, 7)\ntabla[selefect,]## # A tibble: 3 × 2\n##   term                      estimate\n##   <chr>                        <dbl>\n## 1 aspirinaSi                  -0.679\n## 2 localiDuodenal:aspirinaSi   -0.700\n## 3 ulceraNo:aspirinaSi         -1.14\n# Calculamos los conteos predichos del modelo\nconteos <- predict(fit.aspirina, type = \"response\")\n# Combinamos los datos originales con los predichos\nnewdata <- cbind(Aspirina, conteos)\n# Calculamos el conteo total por las predictoras\nnewdata.sum <- newdata %>% group_by(locali, ulcera) %>% \n  summarise(suma = sum(conteos))\n# Calculamos las probabilidades asociadas a cada combinación\nnewdata <- newdata %>% \n  left_join(newdata.sum, by = c(\"locali\",\"ulcera\")) %>%\n  mutate(prob = round(conteos/suma,4))\n# Vemos las probabiliddes obtenidas\ndplyr::select(newdata,c(locali, ulcera, aspirina, prob))##     locali ulcera aspirina   prob\n## 1 Gástrica     Si       No 0.6635\n## 2 Gástrica     No       No 0.8608\n## 3 Duodenal     Si       No 0.7988\n## 4 Duodenal     No       No 0.9257\n## 5 Gástrica     Si       Si 0.3365\n## 6 Gástrica     No       Si 0.1392\n## 7 Duodenal     Si       Si 0.2012\n## 8 Duodenal     No       Si 0.0743\nggplot(newdata, aes(x = locali, y = prob, group = ulcera, color = ulcera)) +\n  geom_point() + \n  geom_line()  +\n  facet_wrap(~ aspirina) +\n  labs(title = \"Consumo de aspirina\",\n       x = \"Localización de la úlcera\",\n       y = \"Probabilidad\",\n       col = \"Úlcera\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1))"},{"path":"glmtablascont.html","id":"respuesta-multinomial-nominal-y-predictora-nominal","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.2 Respuesta multinomial nominal y predictora nominal","text":"Cuando tenemos más de dos categorías el procedimiento es bastante similar. En este caso tomamos la última categoria \\(\\) como referencia, de forma que los log-odds se pueden expresar como:\\[log\\left(\\frac{\\pi_{|j}}{\\pi_{|j}}\\right) = \\eta_{|j} = (\\alpha_i -\\alpha_I) + (\\alpha\\beta_{ij} - \\alpha\\beta_{Ij})\\]partir del logit anterior para cada categoría \\(\\),podemos obtener la probabilidad condicionada de dicha categoría \\(\\) como:\\[\\pi_{|j} = \\frac{exp(\\eta_{|j})}{\\sum_{l=1}^exp(\\eta_{l|j})}\\]Puesto que \\(exp(\\eta_{|j}) = 1\\) dicha probabilidad se puede expresar como:\\[\\pi_{|j} = \\frac{exp(\\eta_{|j})}{1 + \\sum_{l \\neq } exp(\\eta_{l|j})}\\]Esta expresión es similar la de dos categorías salvo por la modificación del denominador, donde se incluyen los predictores, \\(\\eta\\), asociados todas las categorías de la respuesta.","code":""},{"path":"glmtablascont.html","id":"respuesta-nominal-predictora-ordinal","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.3 Respuesta nominal, predictora ordinal","text":"Esta situación es miliar la anterior salvo por el carácter ordinal de la predictora. Para estimar dichos modelos debemos incluir una variable ficticia (\\(S\\)) que contendrá los scores asociados con las categorías de la predictora. Si \\(v_1=1, v_2=2,...,v_J=J\\) denota los scores asociados con la varaible ordinal, el logit asociado esta situación se puede expresar mediante:\\[log\\left(\\frac{\\pi_{|j}}{\\pi_{|j}}\\right) = \\eta_{|j} = (\\alpha_i -\\alpha_I) + (\\alpha\\beta_{ij} - \\alpha\\beta_{Ij})v_j\\]donde las interacciones entre la respuesta y la predictora se modifican través del score definido, para introducir el efecto ordinal entre las comninaciones, de forma que los cocientes de probabilidades se obtienen como\\[\\frac{\\pi_{|j}}{\\pi_{|j}} = exp(\\eta_{|j}) = exp\\{(\\alpha_i -\\alpha_I) + (\\alpha\\beta_{ij} - \\alpha\\beta_{Ij})v_j\\}\\]de forma que la probabilidad de una categoría \\(\\) viene dada por:\\[\\pi_{|j} = \\frac{exp(\\eta_{|j})}{1 + \\sum_{l \\neq } exp(\\eta_{l|j})}\\]","code":""},{"path":"glmtablascont.html","id":"datos-aspiraciones-universitarias","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.3.1 Datos Aspiraciones universitarias","text":"Si tomamos como variable respuesta las aspiraciones universitarias en función del estatus económico y de la motivación familiar, y usamos como referencia la categoría podemos obtener el log-odd de la categoría Si mediante:Se observa una tendencia creciente (puede incluso que lineal) conforme aumenta el estatus económico, que va relacionada directamente con una motivación más alta en el entorno familiar (comportamiento casi paralelo para ambas motivaciones).Para el ajuste de este modelo debemos considerar los scores asociados cada una de las predictoras ordinales (estatus.num y motivacion.num) tal y como los definimos al presentar este banco de datos. En primer lugar ajustamos el modelo para estudiar asociación eliminando la interacción de orden 3.podemos rechazar la interacción triple. Ajustamos un nuevo modelo eliminando la interacción doble entre estatus y motivacion dado que los prefiles de los logits parecían paralelos.El p-valor resulta significativo dando indicaciones de que la interacción estatus:motivacion es relevante pero este modelo si que es distinto del modelo saturado. Ajustamos este modelo teniendo en cuenta los scores:Obtenemos la tabla de estimación para el modelo:Seleccionamos los efectos necesarios para el ajuste de los logits y la estimación de probabilidades. Estos efectos son los relacionados (principales e interacciones) con la variable aspiraciones.Con una variable respuesta nominal y una predictora ordinal la ecuació del logit (tomando la categoría como referencia) se pueden obtener de forma sencilla partir de las ecuaciones descritas anteriormente. La expresión del logit en función de los niveles de las predictoras viene dada por:\\[log\\left(\\frac{\\pi_{\\text{Si,jk}}}{\\pi_{\\text{,jk}}}\\right) = (\\alpha_{\\text{Si}} - \\alpha_{\\text{}}) + (\\theta_{Si,j} - \\theta_{,j}) v_j + (\\gamma_{Si,k} - \\gamma_{,k}) w_k \\\\ = \\alpha_{\\text{Si}} + \\theta_{Si,j} v_j + \\gamma_{Si,k} w_k\\]donde los \\(\\alpha\\) representan los efectos principales de aspiraciones, \\(\\theta\\) los efectos de las interacciones entre aspiraciones y estatus, \\(\\gamma\\) los efectos de las interacciones entre aspiraciones y motivacion, los \\(v_j\\) son los scores de estatus, y los \\(\\gamma_k\\) son los scores de motivacion. Sustituyendo tendríamos:\\[log\\left(\\frac{\\pi_{\\text{Si,jk}}}{\\pi_{\\text{,jk}}}\\right) = - 7.81 + 0.806 v_j + 3.01 w_k\\]Dando valores \\(v_j\\) (de 1 4) y \\(w_k\\) (de 1 2) podemos obtener el logit asociado con la variable respuesta. En este caso pasamos directamente estimar las probabilidades de cada combinación.Podemos representar las probabilidades obtenidas para ver los efectos de la combinacion de predictoras en la respuesta.","code":"\n# Creamos variable con valores de referencia y obtenemos los log-odds asociados\nreferencia.val <- unlist(dplyr::select(filter(Aspiraciones, aspiraciones == \"No\"),\n                                       frecuencia))\nAspiraciones <- Aspiraciones %>% \n  mutate(referencia = rep(referencia.val, 2),\n         lodds = log(frecuencia / referencia))\n# Gráfico\n# Seleccionamos las categorías a representar eliminando la de referencia\n# Como la respuesta sólo tiene una categoría no hace falta utilizarla en el gráfico\ndatos <- filter(Aspiraciones, aspiraciones != \"No\")\nggplot(datos,aes(x = estatus, y = lodds, group = motivacion, color = motivacion)) +\n  geom_point() + \n  geom_line() +\n  labs(x = \"Estatus Socioeconómico\",\n       y = \"log-odds (Aspiraciones == Si / No)\",\n       col = \"Motivacion\")\n# Ajuste del modelo sin interacción triple\nfit.aspiraciones.aso <- glm(frecuencia ~ (estatus + motivacion + aspiraciones)^2, \n                          family = poisson(), \n                          data = Aspiraciones)\n# Bondad del asjute del modelo\n1-pchisq(fit.aspiraciones.aso$deviance, fit.aspiraciones.aso$df.residual)## [1] 0.664965\n# Ajuste del modelo sin interacción triple\nfit.aspiraciones.aso2 <- glm(frecuencia ~ estatus + motivacion + aspiraciones + estatus:aspiraciones + motivacion:aspiraciones, \n                          family = poisson(), \n                          data = Aspiraciones)\n# Bondad del ajuste\n1-pchisq(fit.aspiraciones.aso2$deviance, fit.aspiraciones.aso2$df.residual)## [1] 0\n# Comapramos con el modelo anterior\nanova(fit.aspiraciones.aso2, fit.aspiraciones.aso, test =\"Chisq\")## Analysis of Deviance Table\n## \n## Model 1: frecuencia ~ estatus + motivacion + aspiraciones + estatus:aspiraciones + \n##     motivacion:aspiraciones\n## Model 2: frecuencia ~ (estatus + motivacion + aspiraciones)^2\n##   Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n## 1         6    298.485                          \n## 2         3      1.575  3   296.91 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Debemos incluir el efecto de las predictoras como factor principal \n# para mantener la estructura del modelo\nfit.aspiraciones <- glm(frecuencia ~ estatus + motivacion + aspiraciones + aspiraciones*(estatus.num + motivacion.num), \n                          family = poisson(), \n                          data = Aspiraciones)\ntabla <- tidy(fit.aspiraciones)[,c(\"term\", \"estimate\")]\ntabla## # A tibble: 10 × 2\n##    term                          estimate\n##    <chr>                            <dbl>\n##  1 (Intercept)                     6.42  \n##  2 estatusMedia-Baja              -0.0318\n##  3 estatusMedia-Alta              -0.307 \n##  4 estatusAlta                    -0.780 \n##  5 motivacionAlta                 -0.482 \n##  6 aspiracionesSi                 -7.81  \n##  7 estatus.num                    NA     \n##  8 motivacion.num                 NA     \n##  9 aspiracionesSi:estatus.num      0.806 \n## 10 aspiracionesSi:motivacion.num   3.01\n# Filas de la tabla de estimaciones relacionados con metodo\ntabla[6:8,]## # A tibble: 3 × 2\n##   term           estimate\n##   <chr>             <dbl>\n## 1 aspiracionesSi    -7.81\n## 2 estatus.num       NA   \n## 3 motivacion.num    NA\n# Calculamos los conteos predichos del modelo\nconteos <- predict(fit.aspiraciones, type = \"response\")\n# Combinamos los datos originales con los predichos\nnewdata <- cbind(Aspiraciones, conteos)\n# Calculamos el conteo total por las predictoras\nnewdata.sum <- newdata %>% group_by(estatus, motivacion) %>% \n  summarise(suma = sum(conteos))\n# Calculamos las probabilidades asociadas a cada combinación\nnewdata <- newdata %>% \n  left_join(newdata.sum, by = c(\"estatus\", \"motivacion\")) %>%\n  mutate(prob = round(conteos/suma,4))\n# Vemos las probabiliddes obtenidas\ndplyr::select(newdata,c(estatus, motivacion, aspiraciones, prob))##       estatus motivacion aspiraciones   prob\n## 1        Baja       Baja           No 0.9819\n## 2        Baja       Alta           No 0.7282\n## 3  Media-Baja       Baja           No 0.9604\n## 4  Media-Baja       Alta           No 0.5448\n## 5  Media-Alta       Baja           No 0.9156\n## 6  Media-Alta       Alta           No 0.3484\n## 7        Alta       Baja           No 0.8289\n## 8        Alta       Alta           No 0.1928\n## 9        Baja       Baja           Si 0.0181\n## 10       Baja       Alta           Si 0.2718\n## 11 Media-Baja       Baja           Si 0.0396\n## 12 Media-Baja       Alta           Si 0.4552\n## 13 Media-Alta       Baja           Si 0.0844\n## 14 Media-Alta       Alta           Si 0.6516\n## 15       Alta       Baja           Si 0.1711\n## 16       Alta       Alta           Si 0.8072\nggplot(newdata, aes(x = estatus, y = prob, group = aspiraciones, color = aspiraciones)) +\n  geom_point() + \n  geom_line()  +\n  facet_wrap(~ motivacion) +\n  labs(title = \"Probabilidades estimadas por motivacion\",\n       x = \"Edad\",\n       y = \"Probabilidad\",\n       col = \"Método anticonceptivo\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1))"},{"path":"glmtablascont.html","id":"datos-de-contraceptivos","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.3.2 Datos de Contraceptivos","text":"Si tomamos como variable respuesta el método anticonceptivo utilizado, y usamos como referencia la categroria Ninguno podemos obtener los log-odds de la otras dos categorías como:Se observan claramente tendencias cuadráticas distintas para cada método asociadas con el grupo de edad considerado. Aprovecharemos el carácter ordinal de dicha variable para incluir una nueva variable que contenga la marca de clase de cada grupo de edad (score), que nos permita ajustar el modelo cuadrático con edad.Dado que sólo tenemos una predictora vamos ajustar un modelo sólo con efectos principales. Estudiamos la bondad del ajuste de dicho modelo para conocer el patrón de asociación entre las variables. Si desechamos la interacción doble concluiríamos que existe asociación entre edad y método contraceptivo. En caso contrario deberíamos establecer el modelo de asociación entre ambas variables. El gráfico de log-odds muestra cierto grado de asociación de tipo cuadrático con la edad. Se debe explorar la posibilidd de un modelo de interacción con la variable de scores de edad asumiendo un modelo cuadrático.Rechazamos la hipótesis de independencia. Ajustamos el modelo de interacción cuadrático con los scores de la variable edad que ya fijamos en la presentación del banco de datos (punto medio de los intervalos de edad). En este modelo incluimos la edad como factor principal, y los scores de la edad en interacción con la variable método anticonceptivo.Comparamos los dos modelos obtenidos para validar la inclusión de la variable de scores de edadDado que el p-valor resulta significativo podemos conluir que ambos modelos son distintos.Obtenemos la tabla de estimación para el modelo:Seleccionamos los efectos necesarios para el ajuste de los logits y la estimación de probabilidades. Estos efectos son los relacionados (principales e interacciones) con la variable metodo.Con una variable respuesta nominal y una predictora ordinal la ecuació del logit (tomando la categoría Ninguno como referencia) se pueden obtener de forma sencilla partir de las ecuaciones descritas anteriormente. Para simplificar las expresiones utilizamos la codificación de categorias de la variable metodo: Esterilizacion = E, Ninguno = N, y Otros = O. Las expresiones de los logits en función de la edad son:\\[log\\left(\\frac{\\pi_{\\text{E,j}}}{\\pi_{\\text{N,j}}}\\right) = (\\alpha_{\\text{E}} - \\alpha_{\\text{N}}) + (\\theta_{E,j} - \\theta_{N,j}) v_j + (\\theta^2_{E,j} - \\theta^2_{N,j}) v^2_j\\]\\[log\\left(\\frac{\\pi_{\\text{O,j}}}{\\pi_{\\text{N,j}}}\\right) = (\\alpha_{\\text{O}} - \\alpha_{\\text{N}}) + (\\theta_{O,j} - \\theta_{N,j}) v_j + (\\theta^2_{O,j} - \\theta^2_{N,j}) v^2_j\\]donde los \\(\\alpha\\) representan los efectos principales, \\(\\theta\\) los efectos de las interacciones con el efecto lineal del score, \\(\\theta^2\\) los efectos de las interacciones con el efecto cuadrático del score, y los \\(v_j\\) son los scores de la edad. Sustituyendo tendríamos:\\[log\\left(\\frac{\\pi_{\\text{E,j}}}{\\pi_{\\text{N,j}}}\\right) = (0 - 12.30) + (0 + 0.70) v_j + (0 - 0.00973) v^2_j \\\\ = - 12.30 + 0.70 v_j - 0.00973 v^2_j\\]\\[log\\left(\\frac{\\pi_{\\text{O,j}}}{\\pi_{\\text{N,j}}}\\right) = (7.85 - 12.30) + (-0.441 + 0.70) v_j + (0.00497 - 0.00973) v^2_j \\\\ = - 4.45 + 0.259 v_j - 0.00476 v^2_j\\]Podemos representar los logits obtenidos en función de la edad:Los logits estimados reproducen el comportamiento observado en los logits empíricos.Por último obtnemos las probabilidades de cada combinación, que en este caso su evolución como función de la edad. En lugar de evaluar los logits utilizaremos la función predict para evaluar dichas probabildiades y representarlas gráficamente.Podemos representar las probabilidades obtenidas para ver los efectos de la combinacion de predictoras en la respuesta.¿Qué conclusiones podemos extraer de este gráfico? ¿qué método predomina en el grupo de 20 24 años? ¿y en l grupo 40 44?","code":"\n# Creamos variable con valores de referencia y obtenemos los log-odds asociados\nreferencia.val <- unlist(dplyr::select(filter(Contraceptivos, metodo == \"Ninguno\"), frecuencia))\nContraceptivos <- Contraceptivos %>% \n  mutate(referencia = rep(referencia.val, 3),\n         lodds = log(frecuencia / referencia))\n# Gráfico\n# Seleccionamos las categorías a representar eliminando la de referencia\n# Como la respuesta sólo tiene una categoría no hace falta utilizarla en el gráfico\ndatos <- filter(Contraceptivos, metodo != \"Ninguno\")\nggplot(datos,aes(x = edad, y = lodds, group = metodo, color = metodo)) +\n  geom_point() + \n  geom_line() +\n  labs(x = \"Grupo de edad\",\n       y = \"log-odds (Ref: Método == Ninguno)\",\n       col = \"Método anticonceptivo\") \n# Ajuste del modelo sin interacción\nfit.contraceptivos.aso <- glm(frecuencia ~ edad + metodo, \n                          family = poisson(), \n                          data = Contraceptivos)\n# Bondad del asjute del modelo\n1-pchisq(fit.contraceptivos.aso$deviance, fit.contraceptivos.aso$df.residual)## [1] 0\n# Ajuste del modelo con efecto cuadrático en edad\n# Debemos incluir el efecto edad como factor principal \n# para mantener la estructura del modelo\nfit.contraceptivos <- glm(frecuencia ~ edad + metodo*(edad.num + I(edad.num^2)), \n                          family = poisson(), \n                          data = Contraceptivos)\nanova(fit.contraceptivos.aso, fit.contraceptivos, test = \"Chisq\")## Analysis of Deviance Table\n## \n## Model 1: frecuencia ~ edad + metodo\n## Model 2: frecuencia ~ edad + metodo * (edad.num + I(edad.num^2))\n##   Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n## 1        12     521.10                          \n## 2         8      20.47  4   500.63 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ntabla <- tidy(fit.contraceptivos)[,c(\"term\", \"estimate\")]\ntabla## # A tibble: 15 × 2\n##    term                        estimate\n##    <chr>                          <dbl>\n##  1 (Intercept)                  2.26   \n##  2 edad20-24                    2.14   \n##  3 edad25-29                    3.04   \n##  4 edad30-34                    3.28   \n##  5 edad35-39                    3.16   \n##  6 edad40-44                    2.78   \n##  7 edad45-49                    2.13   \n##  8 metodoOtros                  7.85   \n##  9 metodoNinguno               12.3    \n## 10 edad.num                    NA      \n## 11 I(edad.num^2)               NA      \n## 12 metodoOtros:edad.num        -0.441  \n## 13 metodoNinguno:edad.num      -0.700  \n## 14 metodoOtros:I(edad.num^2)    0.00497\n## 15 metodoNinguno:I(edad.num^2)  0.00973\n# Filas de la tabla de estimaciones relacionados con metodo\ntabla[8:13,]## # A tibble: 6 × 2\n##   term                   estimate\n##   <chr>                     <dbl>\n## 1 metodoOtros               7.85 \n## 2 metodoNinguno            12.3  \n## 3 edad.num                 NA    \n## 4 I(edad.num^2)            NA    \n## 5 metodoOtros:edad.num     -0.441\n## 6 metodoNinguno:edad.num   -0.700\n# Secuencia de edad\nedadsec <- seq(15,49,1)\n# Valores logit comparada con la de referencia\nEvsN <- - 12.30 + 0.70*edadsec - 0.00973*edadsec^2\nOvsN <- - 4.45 + 0.259*edadsec - 0.00476*edadsec^2\n# Configuración de datos para logits\nedad <- rep(edadsec,2)\ncombinacion <- c(rep(\"E versus N\",length(edadsec)), rep(\"O versus N\",length(edadsec)))\nlogits <- c(EvsN, OvsN)\ndatoscom <- data.frame(edad, combinacion, logits)\n# Gráfico\nggplot(datoscom, aes(x = edad, y = logits, group = combinacion, color = combinacion)) +\n  geom_line() +\n  labs(title = \"Logits por edad\", \n       y = \"Logits\", \n       x = \"Edad\",\n       col = \"Comparación\") \n# Calculamos los conteos predichos del modelo\nconteos <- predict(fit.contraceptivos, type = \"response\")\n# Combinamos los datos originales con los predichos\nnewdata <- cbind(Contraceptivos, conteos)\n# Calculamos el conteo total por las predictoras\nnewdata.sum <- newdata %>% group_by(edad) %>% \n  summarise(suma = sum(conteos))\n# Calculamos las probabilidades asociadas a cada combinación\nnewdata <- newdata %>% \n  left_join(newdata.sum, by = \"edad\") %>%\n  mutate(prob = round(conteos/suma,4))\n# Vemos las probabiliddes obtenidas\ndplyr::select(newdata,c(edad, metodo, prob))##     edad         metodo   prob\n## 1  15-19 Esterilización 0.0322\n## 2  20-24 Esterilización 0.1318\n## 3  25-29 Esterilización 0.3086\n## 4  30-34 Esterilización 0.4632\n## 5  35-39 Esterilización 0.5192\n## 6  40-44 Esterilización 0.4572\n## 7  45-49 Esterilización 0.2832\n## 8  15-19          Otros 0.1936\n## 9  20-24          Otros 0.2306\n## 10 25-29          Otros 0.2018\n## 11 30-34          Otros 0.1451\n## 12 35-39          Otros 0.0999\n## 13 40-44          Otros 0.0693\n## 14 45-49          Otros 0.0434\n## 15 15-19        Ninguno 0.7741\n## 16 20-24        Ninguno 0.6376\n## 17 25-29        Ninguno 0.4895\n## 18 30-34        Ninguno 0.3917\n## 19 35-39        Ninguno 0.3809\n## 20 40-44        Ninguno 0.4734\n## 21 45-49        Ninguno 0.6734\nggplot(newdata, aes(x = edad, y = prob, group = metodo, color = metodo)) +\n  geom_point() + \n  geom_line()  +\n  labs(title = \"Probabilidades estimadas\",\n       x = \"Edad\",\n       y = \"Probabilidad\",\n       col = \"Método anticonceptivo\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1))"},{"path":"glmtablascont.html","id":"respuesta-ordinal-predictora-nominal","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.4 Respuesta ordinal, predictora nominal","text":"Cuando la variable respuesta tiene carácter ordinal el proceso de estimación debe variar para tener en cuenta dicho carácter. En esta situación obtenemos los logit de una categoría con respecto la de referencia sino que procedemos construyendo los logit de dos categorías consecutivas\\[log\\left(\\frac{\\pi_{+1|j}}{\\pi_{|j}}\\right)\\]para valorar el incremento que sufre la probabilidad de un categoría con respecto su categoría superior.Para estimar dichos modelos debemos incluir una variable ficticia (\\(S\\)) que contendrá los scores asociados con las categorías de la respuesta. Si \\(u_1=1, u_2=2,...,u_n=n\\) denotan dichos scores y sólo tenemos una variable predictora, el logit anterior se puede obtener fácilmente mediante:\\[log(E(Y_{ij})) = \\mu +\\alpha_i + \\beta_j + \\theta_j S_i\\]donde \\(\\mu\\) es la interceptación del modelo, \\(\\alpha_i\\) es el efecto asociado con el nivel \\(\\) de la respuesta, \\(\\beta_j\\) es el efecto asociado con el nivel \\(j\\) de la predictora, y \\(\\theta\\) es el efecto asociado con la variable de scores. En la práctica el logit para dos categorías consecutivas de la respuesta dependen únicamente de los coeficientes del modelo que afectaban la respuesta y la interacción entre respuesta y predictora, es decir:\\[log\\left(\\frac{\\pi_{+1|j}}{\\pi_{|j}}\\right) = \\eta_{+1|j} =(\\alpha_{+1} - \\alpha_i)+ \\theta_j (u_{+1} - u_i)\\ \\]de forma que la relación entre ambas probabilidades viene dada por:\\[\\frac{\\pi_{+1|j}}{\\pi_{|j}} = exp(\\eta_{+1|j}) = exp\\{(\\alpha_{+1} - \\alpha_i) + \\theta_j (u_{+1} - u_i)\\}\\]Si queremos obtener la relación entre las probabilidades de las categorías \\(+2\\) e \\(\\) basta con considerar que:\\[\\frac{\\pi_{+2|j}}{\\pi_{|j}} = \\frac{\\pi_{+2|j}}{\\pi_{+1|j}}\\frac{\\pi_{+1|j}}{\\pi_{|j}}\\]\nque se puede calcular como:\\[\\frac{\\pi_{+2|j}}{\\pi_{|j}} = exp(\\eta_{+2|j}) = exp\\{(\\alpha_{+2} - \\alpha_i) + \\theta_j (u_{+2} - u_i)\\}\\]De esta forma podemos obtener la relación entre cualquier para de categorías de la respuesta.","code":""},{"path":"glmtablascont.html","id":"datos-de-gripe","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.4.1 Datos de gripe","text":"En primer lugar calculamos y evaluamos los logits empíricos asociados y más tarde realizamos el ajuste y estimación del modelo. Dado que la respuesta es ordinal debemos calcular los logits referenciados la categoría inferior tomando como referencia inicial la categoría small. Obtenemos los log-odds y los representamos mediante:Se observa casi un comportamiento paralelo entre los logits obtenidos. En este caso queremos estudiar un posible modelo de asociación entre la respuesta la vacuna y el tipo de tratamiento seguido. La variable respuesta es de tipo ordinal y asignamos la variable de scores de 1 3, donde 1 refleja efecto pequeño y 3 refleja un efecto grande. En este caso modelizaremos los logits de categorías consecutivas: Moderado vs Pequeño, y Grande vs Moderado. Planteamos el análisis de independencia (sin interacción entre el tratamiento y la variable de scores) frente al de asociación (interacción entre tratamiento y scores) dado los logits observados.En primer lugar creamos los scores asociados:Ajustamos los modelos sin (independencia) y con interacción (dependencia) de los scores con la predictora y valoramos sin ambos modleos son iguales:Rechazamos el modelo de independencia favor del modelo de asociación. Veamos las estimaciones del modelo.Obtenemos la tabla de estimación para el modelo:Seleccionamos los efectos necesarios para el ajuste de los logits y la estimación de probabilidades. Estos efectos son los relacionados (principales e interacciones) con la variable response.Para este modelo Los logits consecutivos asociados vienen dados por las expresiones siguientes donde utilizamos la codificación S = small, M = moderate, y L = large para la variable response:\\[log\\left(\\frac{\\pi_{\\text{M,j}}}{\\pi_{\\text{S,j}}}\\right) = (\\alpha_{\\text{M}} - \\alpha_{\\text{S}}) + \\theta_j(u_{M} - u_{S})\\]\n\\[log\\left(\\frac{\\pi_{\\text{L,j}}}{\\pi_{\\text{M,j}}}\\right) = (\\alpha_{\\text{L}} - \\alpha_{\\text{M}}) + \\theta_j(u_{L} - u_{M})\\]donde los \\(\\alpha_i\\) son los efectos asociados con el nivel \\(\\) de la respuesta, \\(\\theta\\) es el efecto de intereacción entre predictora y scores, y los \\(u_i\\) son los scores asociados con la variable respuesta. Al sustituir por las correspondientes estimaciones y scores proporciona:\\[log\\left(\\frac{\\pi_{\\text{M,j}}}{\\pi_{\\text{S,j}}}\\right) = (0.0928 + 0.490) + \\theta_j (2 -1) = 0.5828 + \\theta_j \\]\n\\[log\\left(\\frac{\\pi_{\\text{L,j}}}{\\pi_{\\text{M,j}}}\\right) = (0 - 0.0928) + \\theta_j (3 - 2) = -0.0928 + \\theta_j\\]con \\(\\theta_1 = -1.25\\) para el placebo y \\(\\theta_2 = 0\\) para el tratamiento. Si asignamos la codificación T = treatment y P = placebo los losgits obtenidos son:\\[log\\left(\\frac{\\pi_{\\text{M,T}}}{\\pi_{\\text{S,T}}}\\right) = 0.5828; \\quad log\\left(\\frac{\\pi_{\\text{M,P}}}{\\pi_{\\text{S,P}}}\\right) = - 0.6672\\]\n\\[log\\left(\\frac{\\pi_{\\text{L,T}}}{\\pi_{\\text{M,T}}}\\right)= - 0.0928; \\quad log\\left(\\frac{\\pi_{\\text{L,P}}}{\\pi_{\\text{M,P}}}\\right)= - 1.3428\\]partir de estos logits podemos obtner las probabiliddes asociadas cada combinación de respuesta y predictora. Calculamos y representamos dichas probabilidades:Podemos representar las probabilidades obtenidas para ver los efectos de la combinacion de predictoras en la respuesta.¿Cómo varían las probabilidades de la respuesta en función del tratamiento?","code":"\n# Creamos referencias de comparación consecutivas\nref.val1 <- unlist(dplyr::select(filter(Gripe, response == \"small\"), frequency))\nref.val2 <- unlist(dplyr::select(filter(Gripe, response == \"moderate\"), frequency))\n# Log-odds\nGripe <- Gripe %>% \n  mutate(referencia = c(rep(ref.val1, 2), ref.val2),\n         lodds = log(frequency / referencia))\n# Gráfico\n# Seleccionamos las categorías a representar eliminando la de referencia\n# Como la respuesta sólo tiene una categoría no hace falta utilizarla en el gráfico\ndatos <- filter(Gripe, response != \"small\")\nggplot(datos,aes(x = treatment, y = lodds, group = response, color = response)) +\n  geom_point() + \n  geom_line()  + \n  labs(x = \"Tratamiento\",\n       y = \"log-odds condicionados\",\n       col = \"Respuesta\") \n# Creamos variable de scores\nGripe$efecto.s <-c() \nGripe$efecto.s[Gripe$response == \"small\"] <- 1\nGripe$efecto.s[Gripe$response == \"moderate\"] <- 2\nGripe$efecto.s[Gripe$response == \"large\"] <- 3\n# Ajuste del modelo de independencia\nfit.gripe.ind <- glm(frequency ~ treatment + response + efecto.s, \n              family = poisson(), \n              data = Gripe)\n# Ajuste del modelo de asociación\nfit.gripe <- glm(frequency ~ treatment + response + treatment:efecto.s, \n               family = poisson(), \n               data = Gripe)\n# Comparamos ambos modelos\nanova(fit.gripe.ind, fit.gripe, test = \"Chisq\")## Analysis of Deviance Table\n## \n## Model 1: frequency ~ treatment + response + efecto.s\n## Model 2: frequency ~ treatment + response + treatment:efecto.s\n##   Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n## 1         2    18.6425                          \n## 2         1     4.3106  1   14.332 0.0001532 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ntabla <- tidy(fit.gripe)[,c(\"term\", \"estimate\")]\ntabla## # A tibble: 6 × 2\n##   term                      estimate\n##   <chr>                        <dbl>\n## 1 (Intercept)                 4.88  \n## 2 treatmentvaccine           -2.32  \n## 3 responsemoderate            0.0928\n## 4 responsesmall              -0.490 \n## 5 treatmentplacebo:efecto.s  -1.25  \n## 6 treatmentvaccine:efecto.s  NA\n# Filas de la tabla de estimaciones relacionados con metodo\ntabla[3:5,]## # A tibble: 3 × 2\n##   term                      estimate\n##   <chr>                        <dbl>\n## 1 responsemoderate            0.0928\n## 2 responsesmall              -0.490 \n## 3 treatmentplacebo:efecto.s  -1.25\n# Calculamos los conteos predichos del modelo\nconteos <- predict(fit.gripe, type = \"response\")\n# Combinamos los datos originales con los predichos\nnewdata <- cbind(Gripe, conteos)\n# Calculamos el conteo total por las predictoras\nnewdata.sum <- newdata %>% group_by(treatment) %>% \n  summarise(suma = sum(conteos))\n# Calculamos las probabilidades asociadas a cada combinación\nnewdata <- newdata %>% \n  left_join(newdata.sum, by = \"treatment\") %>%\n  mutate(prob = round(conteos/suma,4))\n# Vemos las probabiliddes obtenidas\ndplyr::select(newdata,c(treatment, response, prob))##   treatment response   prob\n## 1   placebo    small 0.6075\n## 2   placebo moderate 0.3113\n## 3   placebo    large 0.0812\n## 4   vaccine    small 0.2261\n## 5   vaccine moderate 0.4049\n## 6   vaccine    large 0.3690\nggplot(newdata, aes(x = treatment, y = prob, group = response, color = response)) +\n  geom_point() + \n  geom_line()  +\n  labs(title = \"Probabilidades estimadas\",\n       x = \"Tratamiento\",\n       y = \"Probabilidad\",\n       col = \"Respuesta\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1))"},{"path":"glmtablascont.html","id":"respuesta-ordinal-predictora-ordinal","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.5 Respuesta ordinal, predictora ordinal","text":"Imaginemos que tenemos una variable respuesta y otra predictora (ambas de carácter ordinal). La modelización en esta situación pasa por definir variables ficticias de scores tanto para la predictora como la respuesta. Denotamos por \\(S\\) y \\(U\\) las variables de scores para la respuesta y la predictora respectivamente. Calculamos la variable producto \\(P\\) en ambos scores que será introducida en el modelo para reflejar el posible efecto de interacción entre respuesta y predictora. En esta situación tenemos que:\\[log(E(Y_{ij})) = \\mu +\\alpha_i + \\beta_j + \\theta P_{ij} = \\mu +\\alpha_i + \\beta_j + \\theta S_iU_j\\]\ndonde \\(\\mu\\) es la interceptación del modelo, \\(\\alpha_i\\) es el efecto asociado con el nivel \\(\\) de la respuesta, \\(\\beta_j\\) es el efecto asociado con el nivel \\(j\\) de la predictora, y \\(\\theta\\) es el efecto asociado con la variable \\(P_{ij} = S_iU_j\\). En la práctica se puede demostrar que el logit para dos categorías consecutivas de la respuesta dependen únicamente de los coeficientes del modelo que afectan la respuesta y la interacción entre respuesta y predictora, es decir:\\[log\\left(\\frac{\\pi_{+1|j}}{\\pi_{|j}}\\right) = \\eta_{+1|j} = (\\alpha_{+1} - \\alpha_i) + \\theta (u_{+1} - u_i)v_j\\]de forma que la relación entre ambas probabilidades viene dada por:\\[\\frac{\\pi_{+1|j}}{\\pi_{|j}} = exp(\\eta_{+1|j}) = exp\\{(\\alpha_{+1} - \\alpha_i) + \\theta (u_{+1} - u_i)v_j\\}\\]Si queremos obtener la relación entre las probabilidades de las categorías \\(+2\\) e \\(\\) basta con considerar que:\\[\\frac{\\pi_{+2|j}}{\\pi_{|j}} = \\frac{\\pi_{+2|j}}{\\pi_{+1|j}}\\frac{\\pi_{+1|j}}{\\pi_{|j}}\\]que se puede calcular como:\\[\\frac{\\pi_{+2|j}}{\\pi_{|j}} = exp(\\eta_{+2|j}) = exp\\{(\\alpha_{+2} - \\alpha_i) + \\theta (u_{+2} - u_i)v_j\\}\\]De esta forma podemos obtener la relación entre cualquier par de categorías de la respuesta. Actuado de esta forma resulta posible calcular la probabilidad de cada categoría de la respuesta sin más que fijar una categoría de referencia, y obtener todas las probabilidades asociadas con la de referencia. Procederíamos entonces como en el caso multinomial.Todas estas ecuaciones de estimación se pueden generalizar sin problemas cuando existe más de una variable predictora, sin más que considerar las posibles interacciones entre la respuesta y las predictoras. En caso de tratarse de variables de tipo ordinal construiremos los scores correspondientes para analizar dichos efectos de interacción.","code":""},{"path":"glmtablascont.html","id":"datos-satisfacción-laboral","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.4.5.1 Datos Satisfacción laboral","text":"Si tomamos como variable respuesta el grado de satisfacción, y usamos como referencia la categroria Muy Insatisfecho podemos obtener los log-odds condicionales mediante:¿Cómo interpretamos los log-odds condicionales entre categorías consecutivas obtenidos? Recuerda que en este caso cada perfil se debe comparar con la categoria inmediatamente inferior.En este caso queremos estudiar un posible modelo de asociación entre la satisfacción laboral en función de los ingresos percibidos. Tanto la variable respuesta como la predictora se encuentran en escala ordinal. Debemos introducir los scores asociados ambas variables para poder considerarlos en el modelo de asociación. Creamos scores en escala continua de 1 4 para la satisfacción (\\(u_i\\)), y de 1 4 para los ingresos (\\(v_j\\)), ya que los intervalos considerados tienen la misma amplitud en este caso y podmeos utilizar el punto medio. La interacción entre ambas variables se reflejará mediante el producto de las variables de scores creadas.Rechazamos el modelo de independencia favor del modelo de asociación que obtenemos con los scores. Veamos el resumen del modelo:Seleccionamos los efectos necesarios para el ajuste de los logits y la estimación de probabilidades. Estos efectos son los relacionados (principales e interacciones) con la variable Estado.Para la obtención de los logits vinculados con este modelo usamos la codificación Muy Insatisfecho = S1, Poco Insatisfecho = S2, Moderadamente Satisfecho = S3, y Muy Satisfecho = S4 para la variable Estado, y < 6000 = I1, 6000 - 15000 = I2, 15000 - 25000 = I3, y > 25000 = I4 para la variable ingresos, de forma que las ecuaciones de los logit para este modelo vienen dadas por:\\[log\\left(\\frac{\\pi_{\\text{S4|j}}}{\\pi_{\\text{S3|j}}}\\right) = (\\alpha_{\\text{S4|j}} - \\alpha_{\\text{S3|j}} ) + \\theta (u_{\\text{S4|j}} - u_{\\text{S3|j}})v_j\\]\\[log\\left(\\frac{\\pi_{\\text{S3|j}}}{\\pi_{\\text{S2|j}}}\\right) = (\\alpha_{\\text{S3|j}} - \\alpha_{\\text{S2|j}} ) + \\theta (u_{\\text{S3|j}} - u_{\\text{S2|j}})v_j\\]\\[log\\left(\\frac{\\pi_{\\text{S2|j}}}{\\pi_{\\text{S1|j}}}\\right) = (\\alpha_{\\text{S2|j}} - \\alpha_{\\text{S1|j}} ) + \\theta (u_{\\text{S2|j}} - u_{\\text{S1|j}})v_j\\]donde los \\(\\alpha_i\\) son los efectos asociados con el nivel \\(\\) de la respuesta, \\(\\theta\\) es el efecto de intereacción entre scores (asociacion), los \\(u_i\\) son los scores asociados con Estado, y los \\(u_i\\) son los scores asociados con Ingresos. Al sustituir por las correspondientes estimaciones y scores proporciona:\\[log\\left(\\frac{\\pi_{\\text{S4|j}}}{\\pi_{\\text{S3|j}}}\\right) = (-0.0181 - 0) + 0.112*(4-3)* v_j = -0.0181 + 0.112 * v_j\\]\\[log\\left(\\frac{\\pi_{\\text{S3|j}}}{\\pi_{\\text{S2|j}}}\\right) = (0 + 0.823) + 0.112*(3-2)* v_j = 0.823 + 0.112 * v_j\\]\\[log\\left(\\frac{\\pi_{\\text{S2|j}}}{\\pi_{\\text{S1|j}}}\\right) = (-0.823 + 1.13) + 0.112*(2-1)* v_j = 0.307 + 0.112 * v_j\\]Sustiyuendo los \\(v_j\\) podemos establecer la relación entre las probabildiades de dos categorias consecutivas de la respuesta. Finalmente obtnemos las probabilidades asociadas al modelo ajustado.Podemos representar las probabilidades obtenidas para ver los efectos de la combinacion de predictoras en la respuesta.¿Cómo evolucionan las probabilidades del nivel de satisfacción en función de los ingresos percibidos?","code":"\n# Creamos referencias de comparación consecutivas\nref.val1 <- unlist(dplyr::select(filter(Satisfaccion, Estado == \"Muy insatisfecho\"),frecuencia))\nref.val2 <- unlist(dplyr::select(filter(Satisfaccion, Estado == \"Poco insatisfecho\"),frecuencia))\nref.val3 <- unlist(dplyr::select(filter(Satisfaccion, Estado == \"Moderadamente satisfecho\"), frecuencia))\n# Log-odds\nSatisfaccion <- Satisfaccion %>% \n  mutate(referencia = c(rep(ref.val1, 2), ref.val2, ref.val3),\n         lodds = log(frecuencia / referencia))\n# Gráfico\n# Seleccionamos las categorías a representar eliminando la de referencia\n# Como la respuesta sólo tiene una categoría no hace falta utilizarla en el gráfico\ndatos <- filter(Satisfaccion, Estado != \"Muy insatisfecho\")\nords <- c(\"< 6000\", \"6000 - 15000\", \"15000 - 25000\", \"> 25000\")\nggplot(datos,aes(x = Ingresos, y = lodds, group = Estado, color = Estado)) +\n  geom_point() + \n  geom_line()  + \n  scale_x_discrete(limits = ords) +\n  labs(x = \"Ingresos\",\n       y = \"log-odds condicionados\",\n       col = \"Grado de satisfacción\") \n# Generamos la variable de interacción\nSatisfaccion <- Satisfaccion %>%\n  mutate(asociacion = Ingresos.num * Estado.num)\n# Ajuste del modelo de independencia\nfit.satisfaccion.ind <- glm(frecuencia ~ Ingresos + Estado, \n              family = poisson(), data = Satisfaccion)\n# Ajuste del modelo de asociación\nfit.satisfaccion <- glm(frecuencia ~ Ingresos + Estado + asociacion, \n               family = poisson(), data = Satisfaccion)\n# Comparamos ambos modelos\nanova(fit.satisfaccion.ind, fit.satisfaccion, test = \"Chisq\")## Analysis of Deviance Table\n## \n## Model 1: frecuencia ~ Ingresos + Estado\n## Model 2: frecuencia ~ Ingresos + Estado + asociacion\n##   Resid. Df Resid. Dev Df Deviance Pr(>Chi)   \n## 1         9    12.0369                        \n## 2         8     2.3859  1   9.6509 0.001893 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ntabla <- tidy(fit.satisfaccion)[,c(\"term\", \"estimate\")]\ntabla## # A tibble: 8 × 2\n##   term                    estimate\n##   <chr>                      <dbl>\n## 1 (Intercept)               3.98  \n## 2 Ingresos> 25000          -1.26  \n## 3 Ingresos15000 - 25000    -0.577 \n## 4 Ingresos6000 - 15000     -0.0105\n## 5 EstadoMuy insatisfecho   -1.13  \n## 6 EstadoMuy satisfecho     -0.0181\n## 7 EstadoPoco insatisfecho  -0.823 \n## 8 asociacion                0.112\n# Filas de la tabla de estimaciones relacionados con metodo\ntabla[5:8,]## # A tibble: 4 × 2\n##   term                    estimate\n##   <chr>                      <dbl>\n## 1 EstadoMuy insatisfecho   -1.13  \n## 2 EstadoMuy satisfecho     -0.0181\n## 3 EstadoPoco insatisfecho  -0.823 \n## 4 asociacion                0.112\n# Calculamos los conteos predichos del modelo\nconteos <- predict(fit.satisfaccion, type = \"response\")\n# Combinamos los datos originales con los predichos\nnewdata <- cbind(Satisfaccion, conteos)\n# Calculamos el conteo total por las predictoras\nnewdata.sum <- newdata %>% group_by(Ingresos) %>% \n  summarise(suma = sum(conteos))\n# Calculamos las probabilidades asociadas a cada combinación\nnewdata <- newdata %>% \n  left_join(newdata.sum, by = \"Ingresos\") %>%\n  mutate(prob = round(conteos/suma,4))\n# Vemos las probabiliddes obtenidas\ndplyr::select(newdata,c(Ingresos, Estado, prob))##         Ingresos                   Estado   prob\n## 1         < 6000         Muy insatisfecho 0.0939\n## 2   6000 - 15000         Muy insatisfecho 0.0741\n## 3  15000 - 25000         Muy insatisfecho 0.0578\n## 4        > 25000         Muy insatisfecho 0.0447\n## 5         < 6000        Poco insatisfecho 0.1429\n## 6   6000 - 15000        Poco insatisfecho 0.1261\n## 7  15000 - 25000        Poco insatisfecho 0.1101\n## 8        > 25000        Poco insatisfecho 0.0952\n## 9         < 6000 Moderadamente satisfecho 0.3637\n## 10  6000 - 15000 Moderadamente satisfecho 0.3589\n## 11 15000 - 25000 Moderadamente satisfecho 0.3505\n## 12       > 25000 Moderadamente satisfecho 0.3390\n## 13        < 6000           Muy satisfecho 0.3995\n## 14  6000 - 15000           Muy satisfecho 0.4409\n## 15 15000 - 25000           Muy satisfecho 0.4816\n## 16       > 25000           Muy satisfecho 0.5210\nords <- c(\"< 6000\", \"6000 - 15000\", \"15000 - 25000\", \"> 25000\")\nggplot(newdata, aes(x = Ingresos, y = prob, group = Estado, color = Estado)) +\n  geom_point() + \n  geom_line()  + \n  scale_x_discrete(limits = ords) +\n  labs(title = \"Probabilidades estimadas\",\n       x = \"Ingresos\",\n       y = \"Probabilidad\",\n       col = \"Nivel de satisfacción\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1))"},{"path":"glmtablascont.html","id":"diagnóstico-6","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.5 Diagnóstico","text":"El diagnóstico de este tipo de modelos se parece al diagnóstico de los modelos de poisson. Dado que sólo tenemos un valor predicho y un residuo por cada combinación de niveles el diagnóstico nos limitaremos :Realizar el gráfico de residuos vs ajustados del predictor lineal, para detectar problema de linealidad y/o homogeneidad.Realizar el gráfico de valores predichos (frecuencias predichas) vs observados (frecuenias observadas), para conocer la precisión en la predicción de la frecuencia de interés.continuación pasamos analizar los ejemplos tratados en el punto anterior.","code":""},{"path":"glmtablascont.html","id":"ejemplos-22","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.5.1 Ejemplos","text":"Para el análisis diagnóstico utilizamos la función fortify que nos proporciona tanto los valores de predicción del preditor lineal y los residuos estandarizados, mientras que la predicción de la respuesta ya hemos visto en el punto anterior como obtenerla como una nueva columna denominada conteos.","code":""},{"path":"glmtablascont.html","id":"aspirina-2","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.5.1.1 Aspirina","text":"En primer luagr realizamos el gráfico de residuos versus ajustados (predictor lineal).Comprobamos la bondad del modelo representando los valores originales de frecuencias frente los valores ajustados.El ajuste conseguido es bastante bueno dado que los valores se presentan los largo de la diagonal, que representa el ajuste perfecto.","code":"\n# Valores de diganóstico\ndiagnostico <- fortify(fit.aspirina)\n# Gráfico Residuos vs ajustados\nggplot(diagnostico, aes(.fitted, .stdresid)) + \n  geom_point() +\n  labs(x = \"Ajustados\", y = \"Residuos\") + \n  geom_hline(yintercept = 0) \n# Valores predichos\nAspirina <- Aspirina %>% \n  mutate(conteo = predict(fit.aspirina, type = \"response\"))\n# Gráfico Residuos vs ajustados\nggplot(Aspirina, aes(conteo, frecuencia)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"Predicción frecuencia\", y = \"Frecuencia observada\") "},{"path":"glmtablascont.html","id":"aspiraciones-2","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.5.1.2 Aspiraciones","text":"En primer luagr realizamos el gráfico de residuos versus ajustados (predictor lineal).Comprobamos la bondad del modelo representando los valores originales de frecuencias frente los valores ajustados.El ajuste conseguido es bastante bueno dado que los valores se presentan los largo de la diagonal, que representa el ajuste perfecto.","code":"\n# Valores de diganóstico\ndiagnostico <- fortify(fit.aspiraciones)\n# Gráfico Residuos vs ajustados\nggplot(diagnostico, aes(.fitted, .stdresid)) + \n  geom_point() +\n  labs(x = \"Ajustados\", y = \"Residuos\") + \n  geom_hline(yintercept = 0) \n# Valores predichos\nAspiraciones <- Aspiraciones %>% \n  mutate(conteo = predict(fit.aspiraciones, type = \"response\"))\n# Gráfico Residuos vs ajustados\nggplot(Aspiraciones, aes(conteo, frecuencia)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"Predicción frecuencia\", y = \"Frecuencia observada\") "},{"path":"glmtablascont.html","id":"contraceptivos-1","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.5.1.3 Contraceptivos","text":"En primer luagr realizamos el gráfico de residuos versus ajustados (predictor lineal).¿Qué opinas sobre el gráfico?Comprobamos la bondad del modelo representando los valores originales de frecuencias frente los valores ajustados.El ajuste conseguido es bastante bueno dado que los valores se presentan los largo de la diagonal, que representa el ajuste perfecto.","code":"\n# Valores de diganóstico\ndiagnostico <- fortify(fit.contraceptivos)\n# Gráfico Residuos vs ajustados\nggplot(diagnostico, aes(.fitted, .stdresid)) + \n  geom_point() +\n  labs(x = \"Ajustados\", y = \"Residuos\") + \n  geom_hline(yintercept = 0) \n# Valores predichos\nContraceptivos <- Contraceptivos %>% \n  mutate(conteo = predict(fit.contraceptivos, type = \"response\"))\n# Gráfico Residuos vs ajustados\nggplot(Contraceptivos, aes(conteo, frecuencia)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"Predicción frecuencia\", y = \"Frecuencia observada\") "},{"path":"glmtablascont.html","id":"gripe-2","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.5.1.4 Gripe","text":"En primer luagr realizamos el gráfico de residuos versus ajustados (predictor lineal).Comprobamos la bondad del modelo representando los valores originales de frecuencias frente los valores ajustados.¿Qué opinas de la calidad del ajuste?","code":"\n# Valores de diganóstico\ndiagnostico <- fortify(fit.gripe)\n# Gráfico Residuos vs ajustados\nggplot(diagnostico, aes(.fitted, .stdresid)) + \n  geom_point() +\n  labs(x = \"Ajustados\", y = \"Residuos\") + \n  geom_hline(yintercept = 0) \n# Valores predichos\nGripe <- Gripe %>% \n  mutate(conteo = predict(fit.gripe, type = \"response\"))\n# Gráfico Residuos vs ajustados\nggplot(Gripe, aes(conteo, frequency)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"Predicción frecuencia\", y = \"Frecuencia observada\") "},{"path":"glmtablascont.html","id":"satisfacción-laboral-1","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.5.1.5 Satisfacción laboral","text":"En primer lugar realizamos el gráfico de residuos versus ajustados (predictor lineal).Comprobamos la bondad del modelo representando los valores originales de frecuencias frente los valores ajustados.¿Qué opinas de la calidad del ajuste?","code":"\n# Valores de diganóstico\ndiagnostico <- fortify(fit.satisfaccion)\n# Gráfico Residuos vs ajustados\nggplot(diagnostico, aes(.fitted, .stdresid)) + \n  geom_point() +\n  labs(x = \"Ajustados\", y = \"Residuos\") + \n  geom_hline(yintercept = 0) \n# Valores predichos\nSatisfaccion <- Satisfaccion %>% \n  mutate(conteo = predict(fit.satisfaccion, type = \"response\"))\n# Gráfico Residuos vs ajustados\nggplot(Satisfaccion, aes(conteo, frecuencia)) + \n  geom_point() +\n  geom_abline(intercept = 0, slope = 1) +\n  labs(x = \"Predicción frecuencia\", y = \"Frecuencia observada\") "},{"path":"glmtablascont.html","id":"predicción-6","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.6 Predicción","text":"La predicción en este tipo de modelos pasa por estimar:los conteos predichos para comninación de variables incluidas en el modelo,las probabilidades de ocurrencia de cada combinación.Dado que en puntos anteriores ya hemos visto como obtener dichos valores de interés repetoremos de nuevo dichos calculos.","code":""},{"path":"glmtablascont.html","id":"ejercicios-5","chapter":"Unidad 15 GLM para tablas de contingencia","heading":"15.7 Ejercicios","text":"Colección de ejercicios sobre modleos lineales generalizados con respuesta binomial. Los pasos seguir para la obtención del modelo son los que hemos ido desarrollando: representación gráfica y propuesta de modelo, ajuste, bondad de ajuste, diagnóstico y predicción. En caso de encontrar problemas con el diagnóstico se deberá proponer un nuevo modelo alternativo.olvides cargar las librerías para realizar los ejercicios.Ejercicio 1. En el banco de datos siguiente aparecen los datos de un estudio sobre la actividad sexual de los adolescentes entre 15 y 16 años. El objetivo básico de interés radica en conocer si han tenido o relaciones sexuales. Las datos son el número de adolescentes (frecuencia) que han tenido o relaciones sexuales (relaciones) en función del sexo del adolescente (sexo) y su raza (raza). Realiza un análisis preliminar de los datos recogidos y contesta las siguientes preguntas ajustando el modelo que corresponda en cada caso:¿Hay diferencias entre individuos de raza blanca y de raza negra respecto al hecho de haber tenido relaciones sexuales antes de los 16 años? ¿Y entre chicos y chicas?Predice la proporción de individuos que han tenido relaciones sexuales antes de los 16 años en función de su sexo y raza.Ejercicio 2. Los datos que se presentan corresponden una encuesta social en Dinamarca, en la que se clasificó 165 individuos según su estatus marital (soltero, casado, o divorciado alguna vez} y su edad. Se pretendía establecer una relación entre las variables estatus marital y edad, con el fin de concluir sobre el comportamiento social en Dinamarca.Identifica las variables involucradas en el análisis de estos datos. ¿Cuáles son de tipo cualitativo -nominal u ordinal- y cuáles cuantitativas?la vista de los datos y del gráfico de número de individuos versus edad para cada status marital, ¿aprecias algún tipo de tendencia entre edad y estatus marital? El efecto de la edad, ¿es similar para los solteros, los casados y los divorciados?¿Existe alguna relación entre el estatus marital y la edad? Tu conclusión, ¿es coherente con lo que comentaste en el apartado anterior?Si pretendemos llevar cabo un análisis predictivo y explicar la proporción de individuos en cada uno de los estatus maritales en función de la edad, ¿cuál es la variable explicar y cuál la explicativa? Incorporando toda la información de que dispones. propón. formula y ajusta un modelo log-lineal útil para concluir sobre la relación entre edad y estatus marital. Comenta las hipótesis que asumes y tus conclusiones.Construye la variable log(edad-16)y y representa el gráfico del punto 2 para esta nueva variable (sustituyendo la variable edad). Ajusta el modelo log-lineal más acertado para representar dicha información. Extrae las conclusiones de dicho modelo.Compara las probabilidades condicionadas de estatus dado edad para el modelo anterior. Para el grupo de edad 40-50, ¿cuál es la relación que predices entre las proporciones de individuos en cada estatus marital? ¿Qué es lo más común en ese grupo de edad?Ejercicio 3. Los datos que se presentan corresponden al status de supervivencia, tras tres años, de pacientes con cáncer de mama, clasificados por su edad (en tres grupos) y por el tipo de tumor (maligno o benigno). Formula al menos dos preguntas de interés científico para contestar partir de los datos anteriores.Formula el modelo ajustar suponiendo que existe independencia completa entre los tres factores de clasificación. Contrasta la hipótesis de independencia completa.Formula el modelo correspondiente la hipótesis de que el que una paciente sobreviva tiene nada que ver con su edad o con el tipo de tumor que padece, y resuelve el contraste.Formula el modelo correspondiente la hipótesis de que en cada grupo de edad, el que una paciente sobreviva o es independiente del tipo de tumor, y resuelve el contraste.Consigue el mejor ajuste para explicar los datos de supervivencia al cáncer de mama. Justifícalo y formula explícitamente el modelo que ajustado.¿Cómo están relacionados la supervivencia de una paciente, con su edad y con el tipo de tumor que padece? Identifica los coeficientes que explican dicha relación.Según el modelo que ajustado, calcula el número de pacientes que se espera que sobrevivan en función del grupo de edad al que pertenecen y el tipo de tumor que padecen.Ejercicio 4. En la Fiji Fertility Survey de 1975 se entrevistó 1607 mujeres casadas y embarazadas. Fueron clasificadas por edad actual (4 grupos), nivel de estudios (2 niveles), deseo de tener más hijos y uso de contraceptivos. ¿Están relacionados factores como la educación, la edad o simplemente el deseo de tener más hijos, con el uso de contraceptivos?Ejercicio 5. En el banco de datos siguiente recoge la información de un estudio sobre la vinculación del cáncer de pulmón y el consumo de tabaco. Se registra la información de hombres ingresados en hospitales enfermos de cáncer de pulmón y enfermos por otras causas (controles) y se les preguntó sobre su consumo diario de cigarros. ¿Existe relación entre el consumo de cigarros y tener cáncer de pulmón? Si es así ¿cómo cambia la probabilidad de tener cáncer de pulmón en función del consumo diario de cigarros?Ejercicio 6. En el banco de datos siguiente recoge la información de un estudio sobre el estudio de cierta sustancia farmacológica que potencialmente podría acarrear malformaciones en el feto. Para ello se toma una muestra de ratones embarazados, se les aplica diferente concentraciones de fármaco y se observan los resultados tras el embarazo (“Muerto”, “Malformaciones”,“Normal”). La concentración, medida en mg/kg por día, tiene los niveles: “N0” = 0, “N1” = 62.5, “N2” = 125, “N3” = 250, “N4” = 500. ¿Influye la concentración en las características finales del feto? Si utilizamos los valores numéricos de concentración ¿Como varía el estado del feto en función de dicha concentración?","code":"\nraza <- gl(2, 4, 8, labels = c(\"Blanca\", \"Negra\"))\nsexo <- gl(2, 2, 8, labels = c(\"Hombre\", \"Mujer\")) \nrelaciones <- gl(2, 1, 8, labels = c(\"Si\", \"No\"))\nfrecuencia <- c(43, 134, 26, 149, 29, 23, 22, 36)\nejercicio01 <- data.frame(raza, sexo, relaciones, frecuencia)\nedad <- gl(8, 3, 24, labels = c(\"17-21\", \"21-25\", \"25-30\", \"30-40\", \n                                \"40-50\", \"50-60\", \"60-70\", \"+70\"))\nestado <- gl(3, 1, 24, labels = c(\"Solteros\", \"Casados\", \"Divorciados\"))\nfrecuencia <- c(17, 1, 0, 16, 8, 0, 8, 17, 1, 6, 22, 4, 5, 21, \n                6, 3, 17, 8, 2, 8, 6, 1, 3, 5)\nedad.num <- rep(c(19, 23, 27.5, 35, 45, 55, 65, 75),\n                c(3, 3, 3, 3, 3, 3, 3, 3))\nejercicio02 <- data.frame(edad, estado, frecuencia, edad.num)\nedad <- gl(3, 2, 12, labels = c(\"<50\", \"50-69\", \">70\"))\nmaligno <- gl(2, 1, 12,labels = c(\"No\", \"Si\"))\nsupervivencia <- gl(2, 6, 12, labels = c(\"Si\", \"No\"))\nfrecuencia <- c(77, 51, 51, 38, 7, 6, 10, 13, 11, 20, 3, 3)\nedad.num <- c(40, 40, 60, 60, 80, 80, 40, 40, 60, 60, 80, 80)\nejercicio03 <- data.frame(edad, maligno, supervivencia, \n                          frecuencia, edad.num)\nedad <- gl(4, 4, 32, labels = c(\"<25\", \"25-29\", \"30-39\", \"40-49\"))\nestudios <- gl(2, 2, 32, labels = c(\"Baja\", \"Alta\"))\nhijos <- gl(2, 1, 32, labels = c(\"Si\", \"No\"))\ncontraceptivos <- gl(2, 16, 32, labels = c(\"Si\", \"No\"))\nfrecuencia <- c(53, 10, 212, 50, 60, 19, 155, 65, 112, 77, 118, \n                68, 35, 46, 8, 12, 6, 4, 52, 10, 14, 10, 54, 27, \n                33, 80, 46, 78, 6, 48, 8, 31)\nedad.num <- rep(c(22.5, 27.5, 35, 45, 22.5, 27.5, 35, 45), \n                c(4, 4, 4, 4, 4, 4, 4, 4))\nejercicio04 <- data.frame(edad, estudios, hijos, contraceptivos, \n                          frecuencia, edad.num)\nconsumo <- gl(6, 1, 12, labels = c(\"Ninguno\", \"<5\", \"5-14\", \n                                   \"15-24\", \"25-49\", \"50+\"))\nsujeto <- gl(2, 6, 12, labels = c(\"cáncer\", \"control\"))\nfrecuencia <- c(7, 55, 489, 475, 293, 38, 61, 129, 570, \n                431, 154, 12)\nedad.num <- rep(c(0, 2.5, 9.5, 19.5, 37, 55), 2)\nejercicio05 <- data.frame(consumo, sujeto, frecuencia, edad.num)\nconcentracion <- gl(5 ,1, 15,labels = c(\"N0\", \"N1\", \"N2\", \"N3\", \"N4\"))\nconcentracion.num <- rep(c(0, 62.5, 125, 250, 500), 3)\nestado <- gl(3, 5, 15, labels = c(\"Muerto\", \"Malformaciones\", \"Normal\"))\nejercicio06 <- data.frame(concentracion, concentracion.num, estado)"},{"path":"glmsuperv.html","id":"glmsuperv","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"Unidad 16 GLM para datos de supervivencia","text":"El análisis de supervivencia examina y modela el tiempo que tardan en producirse lo que denominamos “eventos” aleatorios. Habitualmente dicho evento se asocia con la muerte del sujeto bajo estudio lo que justifica su nombre como análisis de supervivencia. Sin embargo el ámbito de aplicación es mucho más amplio. Esencialmente, la misma metodología utilizada para el estudio de mortalidad se utiliza para el “análisis de eventos históricos” en sociología y el “análisis del tiempo de fallo” en ingeniería.Cuando se toman datos sobre tiempos de supervivencia en un conjunto de sujetos, se toma como resultado de dicha variable:el tiempo en que acontece el suceso (el sujeto muere, la máquina falla, la enfermedad remite,etc…o el tiempo transcurrido hasta la finalización del periodo de seguimiento del experimento.Tenemos por tanto, dos posibilidades para cada sujeto en función se si ocurre o el evento de interés. Los sujetos en los que al finalizar el periodo bajo estudio se ha registrado el evento de interés se denominan datos censurados o incompletos. Las observaciones censuradas aún contienen información importante. Se sabe que al final del período de observación, un individuo con dato censurado es un individuo sobre el que ha ocurrido el evento de interés, es decir, un individuo cuyo tiempo de vida supera al tiempo del periodo de observación.Los tipos de censura más habituales son:censura por tiempo, que ocurre cuando se deja de observar los individuos una vez que ha transcurrido un intervalo fijo de tiempo. En consecuencia, se sabe si actualmente ocurrido el evento de interés o . Es frecuente en estudios médicos.censura por fallo: si lo largo del período de estudio se “pierden” casos (pacientes que dejan ele asistir las revisiones, etc.), pesar ele que el estudio prosiga hasta conseguir información completa sobre un número fijo de casos. Es común en tests industriales, donde para modelizar el proceso de fallos se observa, en cada máquina, el tiempo transcurrido hasta que tienen un fallo; el objetivo en estos tests es predecir cada cuanto tiempo acontece un fallo.Sin embargo, los casos pueden desaparecer por razones relacionadas con el estudio o más allá del control del investigador. Esto puede estar o ligado la respuesta o las variables predictoras (por ejemplo, testando un fármaco, un paciente decide suspender el tratamiento debido los efectos secundarios que le ha provocado).Por tanto, el análisis de supervivencia se centra en estudiar la función de distribución del tiempo de supervivencia o tiempo hasta que ocurre el evento de interés. El caso más interesante aparece cunado disponemos de variables predictoras que pueden influir en el tiempo de supervivencia. En este tema estudiamos este tipo de modelos y más concretamente los conocidos como modelos de regresión de Cox.continuación se muestran los diferentes ejemplos que trabajaremos lo largo del tema.Ejemplo Leucemia. Los datos siguientes provienen de un estudio de supervivencia en pacientes con leucemia mielógena aguda. las variables consideradas son time (tiempo de supervivencia o censura en semanas), status (valor 1 cuando el paciente ha fallecido y 0 si el dato está censurado), y x que indica para cada sujeto si el ciclo estándar de quimioterapia debería extenderse (Maintained) para ciclos adicionales o (Nonmaintained). La pregunta de interés es si el matenimiento de la quimioteapia mejora la supervivencia de los pacientes.Ejemplo Cáncer de pulmón. Los datos siguientes provienen de un estudio de supervivencia en pacientes con cáncer de pulmón avanzado del North Central Cancer Treatment Group. cada paicnete se le pasan diferentes test para medir su capacidad para desempeñarse habitualmente, así ocm variables asociadas con su alimentación. El conjunot de variable consideradas son: inst (código de la institución), time (tiempo de supervivencia en días), status (estado de la censura 1=censurado, 2=muerto), age (edad años), sex (masculino=1 femenino=2), ph.ecog (puntuación del test ECOG (0=buen estado 5=muerto)), ph.karno (puntuación del test de Karnofsky (malo=0-bueno=100)), pat.karno (test de Karnofsky estandarizado por paciente), meal.cal (conusmo de calorias), wt.loss (peso pérdido en los últimos seis meses). Se desea estudiar la supervivencia de los pacientes en función de las variables registradas.Ejemplo Ratas. Los datos siguientes provienen de un estudio de supervivencia en ratas. El experimneto consistía en estudiar el efecto de un nuevo tratamiento para un tumor cerebral. Se elegían tres ratas de cada una de 100 camadas y se les generaba un tumor cerebral. Las ratas eran divididas en dos grupos de forma que unas eran tratados con radioterapía y las otras recibían ningún tratamiento. Las variables consideradas son: litter (camada de la que proviene la rata), rx (1 = radioterapia, 0 = control), time (supervivencia en días), status (0 = censurado, 1 = muerta), sex (f = femenino, m = masculino). El objetvo es estudiar la supervivencia de las ratas en función de las variables consideradas.Ejemplo Ovarian Los datos siguientes provienen de un estudio de supervivencia en mujeres con cáncer de ovarios sometidas dos tratamientos distintos. Las variables consideradas son: futime (tiempo de supervivencia en días), fustat (estado de la censura 0 = censurado, 1 = defunción), age (edad de la paciente), resid.ds (persistencia de la enfermedad tras el tratamiento 1 = , 2 = Si), rx (tratamiento la que es sometida la paciente), ecog.ps (puntuación del test ECOG 1= buen estado). El objetvo es estudiar la supervivencia de las pacientes en función de las variables consideradas.Ejemplo Retinopatia Se realiza un ensayo clínico para estudiar la coagulacióncon láser como tratamiento para retrasar la retinopatía diabética. Los 197 pacientes en este conjunto de datos fueron una muestra aleatoria del 50% de los pacientes con retinopatía diabética de “alto riesgo” tal como se define en el Estudio de Retinopatía Diabética (DRS). Cada paciente tenía un ojo asignado al azar al tratamiento con láser y el otro ojo recibió tratamiento, y tiene dos observaciones en el conjunto de datos. Para cada ojo, el evento de interés fue el tiempo desde el inicio del tratamiento hasta el momento en que la agudeza visual cayó por debajo de 5/200 dos visitas seguidas. Por lo tanto, hay un retraso incorporado de aproximadamente 6 meses (las visitas fueron cada 3 meses). Los tiempos de supervivencia en este conjunto de datos son el tiempo real de pérdida de la visión en meses, menos el tiempo mínimo posible para el evento (6,5 meses). Las variables consideradas son: id (identificación del sujeto), laser (tipo de láser usado xenon o argon), eye (que ojo se ha tratado: left, right ), age (edad al diagnóstico de diabetes), type (tipo de diabetes: juvenile adult), trt (0 = control eye, 1 = treated eye), futime (tiempo hasta de pérdida de visión o finalización del seguimiento), status (0 = censurado, 1 = pérdida de visión en el ojo), risk (un score de riesgo para el ojo, con un valor mayor que 6 indicando riesgo alto en al menos un ojo).Ejemplo Mieloma Datos de mieloma múltiple extraídos de datos de expresión génica disponibles públicamente (ID GEO: GSE4581). Las variables consideradas son: molecular_group (grupo molecular de pacientes), chr1q21_status (estado de amplificación del cromosoma 1q21), treatment (tratamiento), event (estado de supervivencia 0 = censurado, 1 = muerto), time (tiempo de supervivencia en meses), CCND1, CRIM1, DEPDC1, IRF4, TP53, WHSC1 son las expresiones de los genes.Ejemplo Veteranos Datos sobre un ensayo aleatorizado de dos regímenes de tratamiento para el cáncer de pulmón en veteranos de guerra. Las variables consideradas son: trt (tratamiento 1=standard 2=test), celltype (1=squamous, 2=smallcell, 3=adeno, 4=large), time (tiempo de supervivencia), status (estado censura 0=censurada), karno (puntuación del test de Karnofsky (100=good)), diagtime (meses desde el diagnóstico hasta la aleatorización), age (en años), prior (si se aplico una terapia anterior 0=, 1=yes.)","code":""},{"path":"glmsuperv.html","id":"funciones-de-supervivencia","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.1 Funciones de supervivencia","text":"Antes de presentar el modelos de regresión de Cox estableemos la notación y definiciones necesarias para el estudio de este tipo de modelos.Si \\(f(t)\\) denota la función de densidad de probabilidad para la variable aleatoria \\(T\\), tiempo de supervivencia, y \\(F(t)\\) la correspondiente función de distribución, entonces se define la función de supervivencia \\(S(t)\\) como la probabilidad de sobrevivir al menos hasta el instante \\(t\\), esto es,\\[S(t) = P(T > t) = 1 - F(t)\\]Se define el riesgo instantáneo de morir o función hazard, \\(h(t)\\), como el cociente entre la función de densidad y la función de supervivencia, es decir:\\[h(t) = \\frac{f(t)}{S(t)}\\]De hecho, \\(h(t)dt\\) o incremento de la función hazard representa la intensidad del proceso de ocurrencia del evento, o lo que es lo mismo, la probabilidad de que ocurra el evento en un intervalo pequeño de tiempo \\(dt\\) dado que el individuo ha registrado el evento de interés hasta el instante \\(t\\).Una distribución para los tiempos de supervivencia ha de tener una función hazard con buenas propiedades; por ejemplo, es de esperar que la función hazard decrezca con \\(t\\), esto es, más tiempo transcurrido, mayor riesgo de de que ocurra el evento de interés. Teniendo esto en cuenta, se define la función hazard acumulada, \\(H(t)\\) como:\\[H(t) = - log S(t)\\]Una característica importante de la función de supervivencia es la denominada mediana de supervivencia que es el valor de \\(t_{0.5}\\) de forma que:\n\\[S(t_{0.5}) = 0.5\\]. Asociado con este valor se puede obtener un intervalo de confianza para la mediana de supervivencia.Para la modelización de este tipo de datos necesitamos instalar las librerías survival y survminer.","code":""},{"path":"glmsuperv.html","id":"supervivencia-completa","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.1.1 Supervivencia completa","text":"En este caso estamos interesados en estimar la curva de supervivencia completa para todo el conjunto de sujetos. La estimación de dichas funciones de supervivencia se realizan mediante las funciones:donde time identifica la variable con los tiempo de supervivencia, status es la variable que identifica si el dato está censurado o , y \\(~ 1\\) indica que obtenemos la curva de supervivencia completa. Habitualmente se usa la codificación 0-1 donde el cero indica que el tiempo está censurado y 1 cuando ha ocurrido el evento de interés. El resultado de esta función se puede representar para obtener una primera aproximación de la función de supervivencia o conocido como estimador de Kaplan-Meier.","code":"\n# Tiempo de supervivencia y estado de la censura\nSurv(time, status)\n# Estimación de la supervivencia sin variables predictoras\nsurvfit(Surv(time, status) ~ 1, data = dataset)"},{"path":"glmsuperv.html","id":"supervivencia-por-un-factor","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.1.2 Supervivencia por un factor","text":"La primera aproximación para la comparación de curvas de supervivencia es utilizar un factor que separa la población de sujeto en grupos. En esta situación se puede estimar la curva de supervivencia para cada uno de los grupos. Este proceso se realiza mediante la función:donde factor identifica la variable de clasificación.","code":"\n# Estimación de la supervivencia sin variables predictoras\nsurvfit(Surv(time,status) ~ factor, data = dataset)"},{"path":"glmsuperv.html","id":"test-de-comparación","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.1.3 Test de comparación","text":"Dadas dos o más curvas de supervivencia estas se pueden comparar través de las funciones hazard asociadas cada una de ellas. Si tenemos \\(k\\) curvas de supervivencia y sus funciones hazard asociadas (\\(h_1(t), h_2(t),...,h_k(t)\\)) el test de comparación viene dado por:\\[\\begin{array}{ll}\nH_0:& h_1(t) = h_2(t) = ... = h_k(t)\\\\\nH_1:& \\text{existen al menos dos } h_i(t) \\text{ y } h_j(t) \\text{ distintas}\\\\\n\\end{array}\\]La resolución de este contraste se basa en un test ji-cuadrado. Si rechazamos \\(H_0\\) concluiremos que existen al menos dos curvas de supervivencia distintas. Si tenemos solamente dos curvas el contraste permite establecer si las consideramos iguales o distintas.","code":""},{"path":"glmsuperv.html","id":"análisis-preliminar-1","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.2 Análisis preliminar","text":"En este análisis preliminar nos preocupamos por el posible efecto que pueden tener las diferentes variables predictoras en la supervivencia, sino más bien en un estudio descriptivo de la supervivencia global o por un factor de clasificación. Mostramos los resultados para diferentes bancos de datos de los presentados al inicio de este tema. Todos los bancos de datos son accesibles directamente ya que se encuentran alojados dentro de la librería survival.","code":""},{"path":"glmsuperv.html","id":"leucemia","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.2.1 Leucemia","text":"Para este conjunto de datos obtenemos y representamos la estimación de la curva de supervivencia global así como la mediana de supervivencia.La mediana de supervivencia se sitúa en 27 semanas con un intervalo de confianza del 95% entre 18 y 45 semanas. En el resumen podemos ver como evoluciona la supervivencia con el tiempo. partir del gráfico podemos ver que la probabilidad de sobrevivir más de 40 semanas se sitúa por debajo de 0.25.Realizamos ahora el análisis diferenciando por la variable ‘x’ que indica si el sujeto ha mantenido o el ciclo de quimioterapia. Representamos las curvas de supervivencia, la mediana de supervivencia, y el pvalor del contraste de comparación de funciones hazard. Añadimos además el intervalo de confianza para la curva de supervivencia de cada grupo.Se observa como los sujetos que han mantenido el ciclo de quimioterapia muestran una mayor probabilidad de supervivencia que los que lo han mantenido. Sin embargo, las medianas de supervivencia entre ambos grupos se diferencia únicamente en 8 meses, y el p-valor resulta significativo. Tenemos evidencias para concluir que hay evidencias estadísticas para concluir que las curvas de supervivencia son distintas para cada uno de los grupos considerados.","code":"\n# Estimación\nfit <- survfit(Surv(time,status) ~ 1, data = aml)\nfit## Call: survfit(formula = Surv(time, status) ~ 1, data = aml)\n## \n##       n events median 0.95LCL 0.95UCL\n## [1,] 23     18     27      18      45\nsummary(fit)## Call: survfit(formula = Surv(time, status) ~ 1, data = aml)\n## \n##  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n##     5     23       2   0.9130  0.0588       0.8049        1.000\n##     8     21       2   0.8261  0.0790       0.6848        0.996\n##     9     19       1   0.7826  0.0860       0.6310        0.971\n##    12     18       1   0.7391  0.0916       0.5798        0.942\n##    13     17       1   0.6957  0.0959       0.5309        0.912\n##    18     14       1   0.6460  0.1011       0.4753        0.878\n##    23     13       2   0.5466  0.1073       0.3721        0.803\n##    27     11       1   0.4969  0.1084       0.3240        0.762\n##    30      9       1   0.4417  0.1095       0.2717        0.718\n##    31      8       1   0.3865  0.1089       0.2225        0.671\n##    33      7       1   0.3313  0.1064       0.1765        0.622\n##    34      6       1   0.2761  0.1020       0.1338        0.569\n##    43      5       1   0.2208  0.0954       0.0947        0.515\n##    45      4       1   0.1656  0.0860       0.0598        0.458\n##    48      2       1   0.0828  0.0727       0.0148        0.462\n# Gráfica de la función de supervivencia y mediana de supervivencia\n# conf.int: permite obtner el intervalo de confinaza para la curva de supervivencia\n# surv.median.line: permite represenatr la mediana de supervivencia \nggsurvplot(fit, data = aml, \n           palette = \"blue\", \n           conf.int = FALSE, \n           surv.median.line = \"hv\")\n# Estimación\nfit <- survfit(Surv(time,status) ~ x, data = aml)\nfit## Call: survfit(formula = Surv(time, status) ~ x, data = aml)\n## \n##                  n events median 0.95LCL 0.95UCL\n## x=Maintained    11      7     31      18      NA\n## x=Nonmaintained 12     11     23       8      NA\n# Gráfica de la función de supervivencia y mediana de supervivencia\n# conf.int: permite obtner el intervalo de confinaza para la curva de supervivencia\n# surv.median.line: permite represenatr la mediana de supervivencia \nggsurvplot(fit, data = aml, \n           conf.int = TRUE, \n           conf.int.style = \"step\", \n           surv.median.line = \"hv\", \n           pval = TRUE)"},{"path":"glmsuperv.html","id":"veteranos","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.2.2 Veteranos","text":"Para este conjunto de datos obtenemos el resumen de la mediana de supervivencia para todo el conjunto de datos, y representamos la función de supervivencia y la mediana de supervivencia.La mediana de supervivencia se sitúa en 80 semanas con un intervalo de confianza del 95% entre 52 y 105 semanas Realizamos ahora el análisis considerando la variable ‘trt’ que indica el tratamiento del sujeto. Representamos las curvas de supervivencia, la mediana de supervivencia, y el pvalor del contraste de comparación de funciones hazard.Podemos ver que existen diferencias entre las curvas de supervivencia para los dos grupos considerados (p-valor superior 0.05). Probamos ahora con la variable celltype. Como esta variable tiene cuatro grupos representaremos los intervalos de confianza.En este caso podemos ver que si existen diferencias entre las curvas de supervivencia (p-valor inferior 0.05). AL menos existen dos funciones de supervivencia que pueden ser consideradas distintas. El problema es que el test nos permite conocer que grupos son los que son diferentes.","code":"\n# Estimación\nfit <- survfit(Surv(time,status) ~ 1, data = veteran)\nfit## Call: survfit(formula = Surv(time, status) ~ 1, data = veteran)\n## \n##        n events median 0.95LCL 0.95UCL\n## [1,] 137    128     80      52     105\n# Gráfica de la función de supervivencia e IC\nggsurvplot(fit, data = veteran, \n           palette = \"blue\", \n           conf.int = FALSE, \n           surv.median.line = \"hv\")\n# Estimación\nfit <- survfit(Surv(time, status) ~ trt, data = veteran)\nfit## Call: survfit(formula = Surv(time, status) ~ trt, data = veteran)\n## \n##        n events median 0.95LCL 0.95UCL\n## trt=1 69     64  103.0      59     132\n## trt=2 68     64   52.5      44      95\n# Gráfica de la función de supervivencia y mediana de supervivencia\nggsurvplot(fit, data = veteran, \n           conf.int = TRUE, \n           conf.int.style = \"step\", \n           surv.median.line = \"hv\", \n           pval = TRUE)\n# Estimación\nfit <- survfit(Surv(time,status) ~ celltype, data = veteran)\nfit## Call: survfit(formula = Surv(time, status) ~ celltype, data = veteran)\n## \n##                     n events median 0.95LCL 0.95UCL\n## celltype=squamous  35     31    118      82     314\n## celltype=smallcell 48     45     51      25      63\n## celltype=adeno     27     26     51      35      92\n## celltype=large     27     26    156     105     231\n# Gráfica de la función de supervivencia y mediana de supervivencia\nggsurvplot(fit, data = veteran, \n           surv.median.line = \"hv\", \n           pval = TRUE)"},{"path":"glmsuperv.html","id":"cáncer-de-pulmón","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.2.3 Cáncer de pulmón","text":"Para este conjunto de datos obtenemos el resumen de la mediana de supervivencia para todo el conjunto de datos, y representamos la función de supervivencia y la mediana de supervivencia. En este caso la variable status está codificado con 0-1 y debemos construir una nueva variable. Además añadimos las etiquetas de los factores presentes en el banco de datos.Estimamos ahora la función de supervivencia¿Cómo interpretamos esta función de supervivencia?Utilizamos ahora la variable sex¿Cómo interpretamos estos resultados? ¿Podemos considerar las curvas de supervivencia iguales o distintas?","code":"\n# Creamos la nueva varaible\nlung <- lung %>% \n  mutate(censurado = status - 1,\n         sex = fct_recode(as.factor(sex), \"Male\" = \"1\", \"Female\" = \"2\"))\n# Estimación\nfit <- survfit(Surv(time, censurado) ~ 1, data = lung)\nfit## Call: survfit(formula = Surv(time, censurado) ~ 1, data = lung)\n## \n##        n events median 0.95LCL 0.95UCL\n## [1,] 228    165    310     285     363\n# Gráfica de la función de supervivencia e IC\nggsurvplot(fit, data = lung, \n           palette = \"blue\", \n           conf.int = FALSE, \n           surv.median.line = \"hv\")\n# Estimación\nfit <- survfit(Surv(time, censurado) ~ sex, data = lung)\nfit## Call: survfit(formula = Surv(time, censurado) ~ sex, data = lung)\n## \n##              n events median 0.95LCL 0.95UCL\n## sex=Male   138    112    270     212     310\n## sex=Female  90     53    426     348     550\n# Gráfica de la función de supervivencia e IC\nggsurvplot(fit, data = lung, \n           conf.int = TRUE, \n           conf.int.style = \"step\", \n           surv.median.line = \"hv\", \n           pval = TRUE)"},{"path":"glmsuperv.html","id":"modelos-de-supervivencia","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3 Modelos de supervivencia","text":"En las situaciones experimentales en las que deseamos estudiar la supervivencia de un conjunto de sujetos en función de un conjunto \\(X = (X_1,...,X_p)\\) de variables predictoras, es decir, variables que pueden afectar o caracterizar su supervivencia, es necesario establecer modelos estadísticos capaces de analizar dichas relaciones. La construcción de este tipo de modelos que depende del tiempo y de las predictoras se hace través el análisis de las función hazard asociada \\(h(t;X)\\). El modelo más habitual en esta situación es el modelo hazard proporcional que separa en dos componentes la función hazard, una correspondiente al tiempo de supervivencia y otra las variables predictoras, de la forma siguiente:\n\\[h(t;X) = h(t)exp(X\\beta)\\]donde \\(h(t)\\) es la función hazard base y describe el riesgo para individuos cuando las predictoras toman valor 0, es decir \\(x = O\\); \\(exp(X\\beta)\\) es el riesgo relativo, y representa la reducción o incremento del riesgo asociada con el conjunto de características \\(X\\), que es similar para todas las duraciones \\(t\\). La exponencial está justificada por la necesidad de positivista y la hipótesis de un efecto multiplicativo de las covariables en los hazards. Este modelo implica que el cociente de hazards (\\(hr\\)) de dos individuos depende de la diferencia entre sus predictores lineales en cualquier instante t, es decir, si \\(x\\) y \\(x^*\\) denotan los valores de las predictoras para ambos sujetos tendríamos que:\n\\[hr(x,x^*) = \\frac{h(t)exp(x\\beta)}{h(t)exp(x^*\\beta)} = exp((x-x^*)\\beta)\\]\nSi solo tenemos una variable predictora de tipo factor con posibles valores 0 o 1, tendríamos la expresión para la función hazard asociada cada respuesta:\n\\[h(t;X = 0) = h(t); \\text{ y } h(t;X = 1) = h(t)exp(\\beta)\\]\nde forma que el el hazard rate viene dado por \\(hr(x = 1,x = 0) = exp(\\beta)\\). Esto implica que dicho coeficiente representa el incremento en el logaritmo del hazard rate, esto es:\\[log(hr(x = 1, x = 0)) = \\beta\\]\nEn este caso cuando \\(exp(\\beta)>0\\) tenemos que \\(hr > 1\\) indicando que hay un incremento el riesgo, y por tanto una reducción de la supervivencia. Cuando \\(exp(\\beta) < 0\\) tenemos que \\(hr < 1\\) indicando que hay un descenso en el riesgo, y por tanto un aumento de la supervivencia.Si la variable predictora es de tipo continuo de forma que \\(\\delta = x -x^*\\) es la diferencia entre los valores para dos sujetos, el logaritmo del hazard rate nos da:\\[log(hr(x,x^*)) = \\delta\\beta\\]\nSi el incremento entre los dos valores es de un unidad (\\(\\delta = 1\\)), \\(\\beta\\) representa el incremento en el hazard rate por cada unidad que aumentamos el valor de la variable predictora. La interpretación del \\(hr\\) es igual que el caso de un factor. Si \\(hr > 1\\) aumenta el riesgo y si es menor que uno disminuye el riesgoPara facilitar la interpretación de los coeficientes se plantea la modelización del logaritmo del hazard mediante la expresión:\\[log(h(t;X)) = log(h(t)) + X\\beta\\]\ndonde queda claro que el efecto de las predictoras afecta de forma lineal la función hazard. Esta formulación es muy similar la de los modelos lineales generalizados con función de enlace \\(log()\\) y con un offset dado por \\(log(h(t))\\). EL problema es que en muchas ocasiones desconocemos la forma de la función \\(h\\), y por tanto, podemos determinar de forma exacta el offset asociado. Los modelos de regresión de Cox o de supervivencia paramétrica asumen una forma específica para \\(h\\) al estilo de los modelos de suavizado.La estimación de este tipo de modelos se hace mediante la función:Para el resumen del modelo utilizaremos las funciones summary(modelo) que nos proporciona el proceso de estimación del modelo, y ggforest(modelo) que nos proporciona un resumen gráfico del modelo ajustado (estimaciones del \\(hr\\), intervalos de confianza y p-valores asociados para cada variable predictora presente en el modelo).Para representar le modelo ajustado utilizaremos la función ggcoxadjustedcurves(modelo, dataset, variable) que permite representar la función de supervivencia asociada con la variable de interés o con una factor creado partir de una de las variables presente en el modelo ajustado.","code":"\nmodelo <- coxph(Surv(time,status) ~ predictoras, data = dataset)"},{"path":"glmsuperv.html","id":"hipótesis-del-modelo","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.1 Hipótesis del modelo","text":"Las hipótesis de este modelo son:Proporcionalidad en la función hazardEl efecto de las predictoras es lineal sobre el logaritmo de la función hazard (asumiendo que hay interacciones entre las predictoras)Estas hipótesis deben verificarse una vez estimado el modelo de supervivencia través de la obtención de los residuos del modelo. En este caso trabajamos con los residuos de Schoenfeld. El incumplimiento de alguna de estas hipótesis implicará el planteamiento de un nuevo modelo.Para la verificación de las hipótesis utilizamos las funciones:cox.zph(modelo) que permite verificar la hipótesis de hazards proporcionales mediante los residuos de Schoenfeld. La verificación gráfica se puede realizar mediante la función ggcoxzph(modelo) que representa para cada predictora el gráfico de residuos vs tiempo.ggcoxdiagnostics(modelo) que permite representar los residuos en función del tiempo, en función del predictor lineal o del tiempo para verificar la hipótesis de linealidad.","code":""},{"path":"glmsuperv.html","id":"análisis-cáncer-de-pulmón","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.2 Análisis Cáncer de pulmón","text":"Procedemos en este punto mostrar el análisis completo de los datos correspondientes al ejemplo de cáncer de pulmón. Realizaremos el análisis completo mostrando el uso de todas las funciones tanto para la estimación y diagnóstico del modelo de regresión de Cox aplicado en este caso.En primer lugar recodificamos la variable de status para que coincida con una variable 0-1.","code":"\n# Creamos la nueva varaible\nlung <- lung %>% \n  mutate(censurado = status - 1,\n         sex = fct_recode(as.factor(sex),\"Male\" = \"1\", \"Female\" = \"2\"))"},{"path":"glmsuperv.html","id":"estimación-del-modelo","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.2.1 Estimación del modelo","text":"Por el momento plateamos un modelo en el que solo tenemos una variable predictora. Consideremos la variable sex dado que ya pudimos ver en el análisis preliminar que se apreciaban diferencias en la supervivencia por este factos.El hazard rate asociado con le factor (columna exp(coef)) es menor que 1 lo que indica que las mujeres tienen más supervivencia que los hombres. Además el test de bondad de ajuste del modelo (“likelihood ratio test”) resulta significativo indicando que el sexo contribuye claramente explicar la supervivencia de los sujetos del estudio. continuación se muestran gráficamente los intervalos de confianza y pvalores asociados con le modelo estimado.Como podemos ver se usa la categoría “Male” como referencia para la estimación de efectos del modelo. El hazard ratio asociado con las mujeres se sitúa en 0.58, lo que indica una fuerte relación entre el sexo de los pacientes y la disminución del riesgo de muerte.Por tanto, ser mujer reduce el riesgo de muerte por un factor de 0,58 o equivalentemente un 42% (100*(1-0.58)). Concluimos que ser mujer se asocia con buen pronóstico. El intervalo de confianza para el hazard ratio en las mujeres se sitúa entre 0.42 y 0.82 indicando una reducción del riesgo de entre el 18% y el 58%.partir de la solución gráfica es posible obtener los datos de supervivencia para ambos grupos, así como la diferencia de supervivencia. El código necesario esSe observa que entre los 375 y 500 días la probabilidad de supervivencia es aproximadamente 0.2 superior en las mujeres que en los hombres.","code":"\n# Ajuste del modelo\nfit <- coxph(Surv(time, status) ~ sex, data = lung)\n# Resumen del modelo ajustado\nfit## Call:\n## coxph(formula = Surv(time, status) ~ sex, data = lung)\n## \n##              coef exp(coef) se(coef)      z       p\n## sexFemale -0.5310    0.5880   0.1672 -3.176 0.00149\n## \n## Likelihood ratio test=10.63  on 1 df, p=0.001111\n## n= 228, number of events= 165\n# Gráfico de la relevancia de los coeficientes\nggforest(fit)\n# Gráfico de supervivencia\nggadjustedcurves(fit, data =lung, \n                 variable = \"sex\", \n                 palette = \"lancet\")\n# Gráfico con intervalos de confianza\nsex.df <- with(lung, data.frame(sex = c(\"Male\", \"Female\")))\nggsurvplot(survfit(fit,newdata = sex.df), data = lung,\n           palette = \"lancet\",\n           conf.int = TRUE, \n           conf.int.style = \"step\",\n           legend.labs=c(\"Male\", \"Female\"))\n# Guardamos la información del gráfico\ndatosgraf <- ggadjustedcurves(fit, data=lung, variable = \"sex\", palette = \"lancet\")$dat\ndatossurv <- spread(datosgraf, key = variable, value =surv)\ndatossurv <- datossurv %>% mutate(dif = Female - Male)\n# Gráfico\nggplot(datossurv,aes(x = time, y = dif)) + geom_smooth() + theme_minimal()"},{"path":"glmsuperv.html","id":"diagnóstico-del-modelo-1","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.2.2 Diagnóstico del modelo","text":"El modelo de Cox hace varias suposiciones. Por lo tanto, es importante evaluar si un modelo de regresión de Cox ajustado describe adecuadamente los datos. En concreto debemos comprobar:Suposición de riesgos proporcionales.Existen observaciones influyentes (o valores atípicos).Detectar la linealidad de los efectos de las variables predictoras en la función hazard.Para verificar estas suposiciones del modelo, se utilizan diferentes tipos de residuos. Los residuos considerar son:Residuos de Schoenfeld vs time para verificar la suposición de riesgos proporcionalesResiduos de Schoenfeld vs time para cada predictora para evaluar la linealidadDesviación residual (transformación simétrica de los residuos de Martingale) para examinar observaciones influyentes","code":""},{"path":"glmsuperv.html","id":"riesgos-porporcionales","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.2.2.1 Riesgos porporcionales","text":"Utilizamos la función cox.zph() para evaluar mediante tests estadísticos la hipótesis de riesgos proporcionales, y la función ggcoxzph() para el análisis gráfico.Dado que el pvalor asociado del test resulta significativo podemos descartar la hipótesis de riesgos proporcionales asociados con la variable sexo.se observa ningún tipo de tendencia en la curva ajustada (línea continua) entre tiempo y residuos, lo que da indicios de la verificación de la hipótesis de riesgos proporcionales.","code":"\n# Tests de riesgos proporcionales\nftest <- cox.zph(fit)\nftest##        chisq df     p\n## sex     2.86  1 0.091\n## GLOBAL  2.86  1 0.091\n# Tests de riesgos proporcionales\nggcoxzph(ftest)"},{"path":"glmsuperv.html","id":"observaciones-influyentes","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.2.2.2 Observaciones influyentes","text":"La detección de observaciones influyentes se realiza mediante métodos gráficosEl patrón de residuos es bastante simétrico respecto de cero. Además hay residuos con valores claramente fuera del rango (-3, 3), indicando que hay ninguna observación que pueda identificarse como anómala. En caso de existir alguna deberíamos eliminarla y ajustar el nuevo modelo.","code":"\n# Observaciones influyentes\nggcoxdiagnostics(fit, type = \"deviance\",\n                 linear.predictions = FALSE)"},{"path":"glmsuperv.html","id":"no-linealidad","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.2.2.3 No linealidad","text":"El test de linealidad sólo se puede interpretar para variables de tipo numérico donde se pueden plantear diferentes tendencia en al respuesta en función de diferentes transformaciones numéricas de la predictora. En este caso realizamos el gráfico de análisis pero lo interpretamos.","code":"\n# Observaciones influyentes\nggcoxdiagnostics(fit, type = \"schoenfeld\",\n                 ox.scale = \"time\")"},{"path":"glmsuperv.html","id":"múltiples-predictoras","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.3 Múltiples predictoras","text":"Pasamos ahora analizar el modelo con múltiples predictoras. En principio en este tipo de modelos se consideran interacciones entre las predictoras para determinar el efecto individual que cada una de ellas puede tener en la supervivencia de los sujetos considerados. En nuestro caso consideramos las variables: sex (factor), age (numérica), y wt.loss (numérica). El modelo ajustado viene dado por:la hora de interpretar los resultados de este modelo hay que tener en cuenta que las estimaciones proporcionadas corresponden al efecto de cada variable cuando el resto se mantienen constantes. Eso quiere decir que el riesgo de 0.59 para la variable sexo se obtiene fijando el resto de variables en su valor medio. En este caso tendríamos que la edad se fijaría en 62.47 y wt.loss en 9.8. Por tanto, el riesgo de muerte se reduce en las mujeres en un 43% (0.57) para una edad de 62 años y una pérdida de peso de 9.8 kg. En cuanto los pvalores podemos ver que tanto la variable sex como age contribuyen significativamente la supervivencia de los sujetos, pero así la variable wt.loss Por cada aumento de una unidad de edad el riesgo de morir aumenta en un 2% (1.02). El test global indica que el modelo construido es útil para explicar la supervivencia de los sujetos. Veamos el análisis gráfico de los coeficientes del modelo:Representamos ahora las curvas de supervivencia ajustadas. Recordemos que sólo se representan la curvas de supervivencia comparando por variables de tipo factor. En este caso tenemos dos variables de tipo numérico, de forma que si queremos representar la supervivencia asociada con cada una de ellas las deberemos convertir en factores de una forma adecuada. Esta forma de proceder es muy habitual den los modelos de supervivencia donde siempre son preferibles variables de tipo factor las numéricas. En este caso los gráficos son condicionales los valores medios de las variables numéricas, tal y como hemos explicado en la interpretación de los parámetros del modelo.Para representar las curvas de supervivencia para la variable numérica procedemos construyendo una nueva variable categórica artificial partir de los cuantiles de la variable numérica, y sin necesitad de ajustar el modelo.Podemos ver que los sujetos de mayor edad tienen una supervivencia menor, así como los que han perdido más peso en los último seis meses. Sin embargo, esta forma de proceder es óptima ya que hay muchas curvas de supervivencia muy similares. Para evitar esto podemos buscar el punto de la variable numérica que separa los sujeto en dos grupos que podemos considerar que tienen supervivencias distintas. Para realizar esto utilizamos la función surv_cutpoint.El valor para edad se sitúa en los 70 años mientras que para la pérdida de peso es de 9kg. Veamos los gráfico con las variables artificialesPara variables numéricas con pocos valores posibles resulta más útil introducir dicha variable en el modelo como un nivel de estratificación son necesidad de estimar el coeficiente asociado. En este caso las variables ph.ecog y ph.karno se pueden tomar como estratos, ya que se tratan de valoraciones sobre los sujetos que tienen porque tener relación directa con la supervivencia. En este caso el modelo ajustar viene dado por:¿Cómo interpretaríamos este modelo?También podríamos ajustar el modelo de supervivencia con los nuevos factores creados:Se puede ver como el efecto de la edad es mucho más claro en este modelo. ¿Cuáles serían los riesgos asociados con cada una de las variables?Procedemos con el gráfico de curvas de supervivencia estimadas:Con este tipo de gráficos resulta posible estudiar como se modifica la supervivencia cuando tenemos variables que están en el modelo. Partimos del modelo más simple donde sólo consideramos la variable sexo, y estudiamos las funciones de supervivencia si incluimos las variables edad y pérdida de peso en forma de factores:Este gráfico nos permite comparar el efecto de las combinaciones de los nuevos factores teniendo en cuenta el modelo por sexo. Es casi como un estudio de la interacción o de la inclusión de un factor en el modelo.","code":"\n# Ajuste del modelo\nfitcom <- coxph(Surv(time, status) ~ sex + age + wt.loss, data = lung)\n# Resumen del modelo ajustado\nfitcom## Call:\n## coxph(formula = Surv(time, status) ~ sex + age + wt.loss, data = lung)\n## \n##                 coef  exp(coef)   se(coef)      z      p\n## sexFemale -0.5210319  0.5939074  0.1743541 -2.988 0.0028\n## age        0.0200882  1.0202913  0.0096644  2.079 0.0377\n## wt.loss    0.0007596  1.0007599  0.0061934  0.123 0.9024\n## \n## Likelihood ratio test=14.67  on 3 df, p=0.002122\n## n= 214, number of events= 152 \n##    (14 observations deleted due to missingness)\n# Gráfico de la relevancia de los coeficientes\nggforest(fitcom)\n# Gráfico de supervivencia para sex\nggadjustedcurves(fitcom, data = lung, \n                 variable = \"sex\", \n                 palette = \"lancet\")\n# Gráfico con intervalos de confianza tomando los valores medios de las numéricas\nsex.df <- with(lung, data.frame(sex = c(\"Male\", \"Female\"),\n                                age = rep(62.5,2),\n                                wt.loss = rep(9.5,2)))\nggsurvplot(survfit(fit, newdata = sex.df), data = lung,\n           palette = \"lancet\",\n           conf.int = TRUE, \n           conf.int.style = \"step\",\n           legend.labs=c(\"Male\", \"Female\"))\n# Edad\n# Variable  artificial\nlung$agenew <- cut(lung$age, quantile(lung$age, na.rm = TRUE))\nlung$wtnew <- cut(lung$wt.loss, quantile(lung$wt.loss, na.rm = TRUE))\n# Ajuste del modelo\nfitcom <- coxph(Surv(time, status) ~ sex + agenew + wtnew, data = lung)\n# Gráfico de supervivencia para edad\nggadjustedcurves(fitcom, data = lung, \n                 variable = \"agenew\", \n                 palette = \"lancet\")\n# Pérdida de peso\n# Gráfico de supervivencia para sex\nggadjustedcurves(fitcom, data = lung, \n                 variable = \"wtnew\", \n                 palette = \"lancet\")\n# Puntos de corte\ncorte <- surv_cutpoint(lung, time = \"time\", \n              event = \"status\", \n              variables = c(\"age\", \"wt.loss\"))\ncorte##         cutpoint statistic\n## age           70  2.013619\n## wt.loss        9  1.756750\n# Variable  artificial edad\nlung$agenew <- ifelse(lung$age >= corte$age$estimate,\"+70\",\"-70\")\nlung$wtnew <- ifelse(lung$wt.loss >= corte$wt.loss$estimate,\"+9\",\"-9\")\n# Ajuste del modelo\nfitcom <- coxph(Surv(time, status) ~ sex + agenew + wtnew, data = lung)\n# Gráfico de supervivencia para edad\nggadjustedcurves(fitcom, data = lung, \n                 variable = \"agenew\", \n                 palette = \"lancet\")\n# Gráfico de supervivencia para pérdida de peso\nggadjustedcurves(fitcom, data = lung, \n                 variable = \"wtnew\", \n                 palette = \"lancet\")\n# Ajuste del modelo\ncoxph(Surv(time, status) ~ sex + age + wt.loss + strata(ph.ecog) + strata(ph.karno), data = lung)## Call:\n## coxph(formula = Surv(time, status) ~ sex + age + wt.loss + strata(ph.ecog) + \n##     strata(ph.karno), data = lung)\n## \n##                coef exp(coef)  se(coef)      z        p\n## sexFemale -0.632793  0.531106  0.188563 -3.356 0.000791\n## age        0.020913  1.021133  0.010562  1.980 0.047702\n## wt.loss   -0.009286  0.990757  0.007059 -1.315 0.188359\n## \n## Likelihood ratio test=17.16  on 3 df, p=0.0006537\n## n= 213, number of events= 151 \n##    (15 observations deleted due to missingness)\n# Ajuste del modelo\nfitcomp2 <- coxph(Surv(time, status) ~ sex + agenew + wtnew, data = lung)\nfitcomp2## Call:\n## coxph(formula = Surv(time, status) ~ sex + agenew + wtnew, data = lung)\n## \n##              coef exp(coef) se(coef)      z       p\n## sexFemale -0.5160    0.5969   0.1744 -2.960 0.00308\n## agenew+70  0.3754    1.4555   0.1800  2.086 0.03702\n## wtnew+9    0.2370    1.2675   0.1628  1.456 0.14549\n## \n## Likelihood ratio test=16.34  on 3 df, p=0.0009675\n## n= 214, number of events= 152 \n##    (14 observations deleted due to missingness)\n# Gráfico conjunto de todos los fatores\nggsurvplot_facet(fitcomp2, lung, facet.by = \"wtnew\",\n                palette = \"lancet\", pval = TRUE)\n# Eliminamos valores pérdidos\nlung <- na.omit(lung)\n# Gráfico conjunto de todos los fatores\nggsurvplot_facet(fit, lung, facet.by = c(\"agenew\",\"wtnew\"),\n                palette = \"lancet\", pval = TRUE)"},{"path":"glmsuperv.html","id":"diagnóstico-del-modelo-2","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.4 Diagnóstico del modelo","text":"Comenzamos con el diagnóstico del modelo. En principio trabajamos con le modelo tomando las variables edad y peso perdido como numéricas.","code":"\n# Ajuste del modelo\nfitcomp <- coxph(Surv(time, status) ~ sex + age + wt.loss, data = lung)"},{"path":"glmsuperv.html","id":"riesgos-porporcionales-1","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.4.1 Riesgos porporcionales","text":"Todos los pvalores (tanto los individuales de cada variable como el global para el modelo) son superiores 0.05 indicando que se verifica la hipótesis de riesgos proporcionales. Veamos el análisis gráfico:","code":"\n# Tests de riesgos proporcionales\nftest <- cox.zph(fitcomp)\nftest##         chisq df    p\n## sex     1.003  1 0.32\n## age     0.668  1 0.41\n## wt.loss 0.351  1 0.55\n## GLOBAL  1.868  3 0.60\n# Gráfico de riesgos proporcionales\nggcoxzph(ftest)"},{"path":"glmsuperv.html","id":"observaciones-influyentes-1","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.3.4.2 Observaciones influyentes","text":"se aprecian observaciones influyentes.Podemos ver que se cumplen todas la hipótesis del modelo, pero ¿podemos considerar que los modelos ajustados son iguales en su capacidad explicativa? Para poder verificar esto realizamos el test de comparación de modelos basado en el test Ji-cuadrado. ¿Podríamos prescindir de alguna variable? Comparamos los siguientes modelos para obtener el modelo con menor número de predictoras pero con la misma capacidad explicativa. Construimos modelos anidados para favorecer la comparación:Puesto que hay ningún pvalor significativo podríamos concluir que el mejor modelo es aquel que sólo contiene la variable sexo, pero podríamos llegar modelos erróneos. En el caso de tener factores que parecen aportar capacidad explicativa pero que si tienen sentido desde el punto de vista experimental, es necesario introducirlos en el modelo de alguna forma. En este caso utilizamos la denominada estratificación. Ajustamos los nuevos modelos considerando la edad como variable estratificada. Al considerarla de esta forma podremos evaluar su riesgo de forma directa en la supervivencia, pero si en la supervivencia global.En este caso podemos ver que el modelo 2 es claramente distinto del modelo 1, pero es distinto del modelo 3. Por tanto, nuestro modelo para estos datos sería el 2.Veamos el diagnósticoSe cumplen todas las hipótesis. Representamos las funciones de supervivencia para este modeloSe puede ver como si se aprecian diferencias por sexo para los menores de 70 años, pero ocurre lo mismo para los mayores de 70 años (el intervalo de confianza de los hombres contiene la estimación de supervivencia para las mujeres).Existen más modificaciones de este tipo de modelos que por límite de tiempo consideramos aquí como son: la posible inclusión de interacciones, modelos doble censurados, incorporación de efectos de grupos o sujetos (camadas, hospitales,…). parte de estas situaciones aparecen en los ejemplos planteados al inicio de este tema y se trataran con lago de detalle en las sesiones prácticas.","code":"\n# Observaciones influyentes\nggcoxdiagnostics(fitcomp, type = \"deviance\",\n                 linear.predictions = FALSE)\n# Ajuste del modelos\nfit1 <- coxph(Surv(time, status) ~ sex, data = lung)\nfit2 <- coxph(Surv(time, status) ~ sex + agenew, data = lung)\nfit3 <- coxph(Surv(time, status) ~ sex + agenew + wt.loss, data = lung)\n# Comparación\nsummary(fit)\nsummary(fit2)\nsummary(fit3)\n# Ajuste del modelo\nfitcomp <- coxph(Surv(time, status) ~ sex + strata(agenew), data = lung)\nfitcomp## Call:\n## coxph(formula = Surv(time, status) ~ sex + strata(agenew), data = lung)\n## \n##              coef exp(coef) se(coef)      z      p\n## sexFemale -0.4645    0.6285   0.1981 -2.344 0.0191\n## \n## Likelihood ratio test=5.75  on 1 df, p=0.01645\n## n= 167, number of events= 120\n# Tests de riesgos proporcionales\nftest <- cox.zph(fitcomp)\nftest##        chisq df    p\n## sex    0.944  1 0.33\n## GLOBAL 0.944  1 0.33\n# Observaciones influyentes\nggcoxdiagnostics(fitcomp, type = \"deviance\",\n                 linear.predictions = FALSE)\n# No linealidad\nggcoxdiagnostics(fitcomp, type = \"schoenfeld\",\n                 ox.scale = \"time\")\n# Supervivencia para menores de 70 años\nsex.df <- with(lung, data.frame(sex = c(\"Male\", \"Female\"),\n                                agenew = c(\"-70\",\"-70\")))\nggsurvplot(survfit(fitcomp,newdata = sex.df), data = lung,\n           palette = \"lancet\",\n           conf.int = TRUE, \n           conf.int.style = \"step\",\n           legend.labs=c(\"Male\", \"Female\"))\n# Supervivencia para mayores de 70 años\nsex.df <- with(lung, data.frame(sex = c(\"Male\", \"Female\"),\n                                agenew = c(\"+70\",\"+70\")))\nggsurvplot(survfit(fitcomp,newdata = sex.df), data = lung,\n           palette = \"lancet\",\n           conf.int = TRUE, \n           conf.int.style = \"step\",\n           legend.labs=c(\"Male\", \"Female\"))"},{"path":"glmsuperv.html","id":"ejercicios-6","chapter":"Unidad 16 GLM para datos de supervivencia","heading":"16.4 Ejercicios","text":"Colección de ejercicios sobre modelos de supervivencia.olvides cargar las librerías para realizar los ejercicios.Ejercicio 1. Los datos siguientes corresponden un pequeño experimento en el que 7 de un total de 16 ratones fueron aleatoriamente seleccionados para recibir un nuevo tratamiento médico. Los 9 ratones restantes fueron asignados un grupo control en el que se administró ningún tipo de tratamiento. El objetivo del tratamiento era prolongar el tiempo de supervivencia después de una operación quinírgica. El banco de datos contiene los tiempos de supervivencia (en días) tras la operación para los 16 ratones. El objetivo principal es saber si el tratamiento prolongaba la vida de los ratones tras la operaciónEjercicio 2. Los datos de la Tabla siguiente son: ‘tiemsup’, que es el tiempo hasta la muerte (en semanas) desde la diagnosis de leucemia, y ‘logcel’, igual al log 10 del número inicial de células blancas en la sangre. Se dispone de datos de diecisiete pacientes que sufrían de leucemia. ¿Cuáles son tus conclusiones sobre la utilidad de saber el número de células blancas en la sangre para predecir el tiempo de supervivencia? ¿Cuál es la probabilidad de sobrevivir de un paciente con 5000 células blancas en la sangre?Ejercicio 3. Un total de 90 pacientes que sufrían ele cancer gástrico fueron asignados aleatoriamente dos grupos. Un gupo fue tratado con quimioterapia y radiación, mientras que el otro sólo recibió quimioterapia. Se recoge además el tiempo de superviviencia si dicho datos está censurado (0) o (1). ¿Existe evidencia para concluir que la radiación prolonga el tiempo de supervivencia?Ejercicio 4. Los datos siguientes contienen los tiempos de remisión de pacientes con leucemia para dos grupos de pacientes ¿Existen diferencias en los tiempos de supervivencia para ambos grupos? Realiza un estudio completo del modelo de superviviencia asocaido con estos datos.Ejercicio 5. El conjunto de datos siguiente contiene los datos de supervivencia (en meses) de pacientes hepatitis crónica activa y que son dividios en dos grupos. Al grupo tratado se le suministra Prednisona mientras que el otro grupo actúa como control. ¿Existen diferencias en los tiempos de supervivencia para ambos grupos? Realiza un estudio completo del modelo de superviviencia asocaido con estos datos.Ejercicio 6. Los siguientes datos representan la supervivencia en días desde el ingreso al ensayo de pacientes con linfoma histiocítico difuso. Se comparan dos grupos diferentes de pacientes, aquellos con estadio III y aquellos con enfermedad en estadio IV.","code":"\ngrupo <- c(rep(\"Tratado\", 7), rep(\"Control\", 9))\ntiempo <- c(94, 38, 23, 19, 99, 16,141, 52, 104, 146, 10, 51, 30, 40, 27, 46)\ntiemsup <- c(65, 156, 100, 134, 16, 108, 121, 5, 65, 4, 39, 143, 56, 26, 22, 1, 1)\nlogcel <- c(3.36, 2.88, 3.63, 3.41, 3.78, 4.02, 4.00, 4.72, 5.00, 4.23, 3.73, 3.85, 3.97, 4.51, 4.54, 5.00, 5.00)\ngroup <- c(rep(\"Q+R\", 45),rep(\"Q\", 45))\ncensor <- c(rep(1, 36),rep(0, 9),rep(1, 36),rep(0, 9)) \ntime <- c(17, 42, 44, 48, 60, 72, 74, 95, 103, 108, 122, 144, 167, 170, 183, 185, 193, 195, 197, 208, 234, 235, 254, 307, 315, 401, 445, 464, 484, 528, 542, 567, 577, 580, 795, 855, 1174, 1214, 1232, 1366, 1455, 1585, 1622, 1626, 1736, 1, 63, 105, 125, 182, 216, 250, 262, 301, 301, 342, 354, 356, 358, 380, 383, 383, 388, 394, 408, 460, 489, 499, 523, 524, 535, 562, 569, 675, 676, 748, 778, 786, 797, 955, 968, 977, 1245, 1271, 1420, 1460, 1516, 1551, 1690, 1694) \nglm_surv_01=read_csv(\"https://goo.gl/d4YI9g\", col_types = \"cid\")\nstr(glm_surv_01)## spec_tbl_df [42 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ group : chr [1:42] \"control\" \"control\" \"control\" \"control\" ...\n##  $ censor: int [1:42] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ time  : num [1:42] 1 1 2 2 3 4 4 5 5 8 ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   group = col_character(),\n##   ..   censor = col_integer(),\n##   ..   time = col_double()\n##   .. )\n##  - attr(*, \"problems\")=<externalptr>\nglm_surv_02=read_csv(\"https://goo.gl/Rzxo3o\", col_types = \"dcc\")\nstr(glm_surv_02)## spec_tbl_df [44 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ stime : num [1:44] 2 6 12 54 56 68 89 96 96 125 ...\n##  $ censor: chr [1:44] \"died\" \"died\" \"died\" \"died\" ...\n##  $ group : chr [1:44] \"prednisolone\" \"prednisolone\" \"prednisolone\" \"prednisolone\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   stime = col_double(),\n##   ..   censor = col_character(),\n##   ..   group = col_character()\n##   .. )\n##  - attr(*, \"problems\")=<externalptr>\ngroup <- c(rep(\"SIII\", 19),rep(\"SIV\", 61))\ncensor <- c(1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) \ntime <- c(6, 19, 32, 42, 42, 43, 94, 126, 169, 207, 211, 227, 253, 255, 270, 310, 316, 335, 346, 4, 6, 10, 11, 11, 11, 13, 17, 20, 20, 21, 22, 24, 24, 29, 30, 30, 31, 33, 34, 35, 39, 40, 41, 43, 45, 46, 50, 56, 61, 61, 63, 68, 82, 85, 88, 89, 90, 93, 104, 110, 134, 137, 160, 169, 171, 173, 175, 184, 201, 222, 235, 247, 260, 284, 290, 291, 302, 304, 341, 345) "},{"path":"referencias.html","id":"referencias","chapter":"Referencias","heading":"Referencias","text":"","code":""}]
